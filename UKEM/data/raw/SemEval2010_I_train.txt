Exchanging Reputation Values among Heterogeneous
Agent Reputation Models: An Experience on ART Testbed
Anarosa A. F. Brandão1
, Laurent Vercouter2
, Sara Casare1
and Jaime Sichman1
1
Laboratório de Técnicas Inteligentes - EP/USP
Av. Prof. Luciano Gualberto, 158, trav. 3, 05508-970, São Paulo - Brazil
+55 11 3091 5397
anarosabrandao@gmail.com, {sara.casare,jaime.sichman}@poli.usp.br
2
Ecole Nationale Supérieure des Mines de Saint-Etienne
158, cours Fauriel, 42023 Saint-Etienne Cedex 2, France
Laurent.Vercouter@emse.fr
ABSTRACT
In open MAS it is often a problem to achieve agents'
interoperability. The heterogeneity of its components turns the
establishment of interaction or cooperation among them into a
non trivial task, since agents may use different internal models
and the decision about trust other agents is a crucial condition to
the formation of agents' cooperation. In this paper we propose the
use of an ontology to deal with this issue. We experiment this idea
by enhancing the ART reputation model with semantic data
obtained from this ontology. This data is used during interaction
among heterogeneous agents when exchanging reputation values
and may be used for agents that use different reputation models.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Multiagent systems
General Terms
Design, Experimentation, Standardization.
1. INTRODUCTION
Open multiagent systems (MAS) are composed of autonomous
distributed agents that may enter and leave the agent society at
their will because open systems have no centralized control over
the development of its parts [1]. Since agents are considered as
autonomous entities, we cannot assume that there is a way to
control their internal behavior. These features are interesting to
obtain flexible and adaptive systems but they also create new risks
about the reliability and the robustness of the system. Solutions to
this problem have been proposed by the way of trust models
where agents are endowed with a model of other agents that
allows them to decide if they can or cannot trust another agent.
Such trust decision is very important because it is an essential
condition to the formation of agents' cooperation. The trust
decision processes use the concept of reputation as the basis of a
decision. Reputation is a subject that has been studied in several
works [4][5][8][9] with different approaches, but also with
different semantics attached to the reputation concept. Casare and
Sichman [2][3] proposed a Functional Ontology of Reputation
(FORe) and some directions about how it could be used to allow
the interoperability among different agent reputation models. This
paper describes how the FORe can be applied to allow
interoperability among agents that have different reputation
models. An outline of this approach is sketched in the context of a
testbed for the experimentation and comparison of trust models,
the ART testbed [6].
2. THE FUNCTIONAL ONTOLOGY OF
REPUTATION (FORe)
In the last years several computational models of reputation have
been proposed [7][10][13][14]. As an example of research
produced in the MAS field we refer to three of them: a cognitive
reputation model [5], a typology of reputation [7] and the
reputation model used in the ReGret system [9][10]. Each model
includes its own specific concepts that may not exist in other
models, or exist with a different name. For instance, Image and
Reputation are two central concepts in the cognitive reputation
model. These concepts do not exist in the typology of reputation
or in the ReGret model. In the typology of reputation, we can find
some similar concepts such as direct reputation and indirect
reputation but there are some slight semantic differences. In the
same way, the ReGret model includes four kinds of reputation
(direct, witness, neighborhood and system) that overlap with the
concepts of other models but that are not exactly the same.
The Functional Ontology of Reputation (FORe) was defined as a
common semantic basis that subsumes the concepts of the main
reputation models. The FORe includes, as its kernel, the following
concepts: reputation nature, roles involved in reputation formation
and propagation, information sources for reputation, evaluation of
reputation, and reputation maintenance. The ontology concept
ReputationNature is composed of concepts such as
IndividualReputation, GroupReputation and ProductReputation.
Reputation formation and propagation involves several roles,
played by the entities or agents that participate in those processes.
The ontology defines the concepts ReputationProcess and
ReputationRole. Moreover, reputation can be classified according
to the origin of beliefs and opinions that can derive from several
sources. The ontology defines the concept ReputationType which
can be PrimaryReputation or SecondaryReputation.
PrimaryReputation is composed of concepts ObservedReputation
and DirectReputation and the concept SecondaryReputation is
composed of concepts such as PropagatedReputation and
CollectiveReputation. More details about the FORe can be found
on [2][3].
3. MAPPING THE AGENT REPUTATION
MODELS TO THE FORe
Visser et al [12] suggest three different ways to support semantic
integration of different sources of information: a centralized
approach, where each source of information is related to one
common domain ontology; a decentralized approach, where every
source of information is related to its own ontology; and a hybrid
approach, where every source of information has its own ontology
and the vocabulary of these ontologies are related to a common
ontology. This latter organizes the common global vocabulary in
order to support the source ontologies comparison. Casare and
Sichman [3] used the hybrid approach to show that the FORe
serves as a common ontology for several reputation models.
Therefore, considering the ontologies which describe the agent
reputation models we can define a mapping between these
ontologies and the FORe whenever the ontologies use a common
vocabulary. Also, the information concerning the mappings
between the agent reputation models and the FORe can be directly
inferred by simply classifying the resulting ontology from the
integration of a given reputation model ontology and the FORe in
an ontology tool with reasoning engine.
For instance, a mapping between the Cognitive Reputation Model
ontology and the FORe relates the concepts Image and Reputation
to PrimaryReputation and SecondaryReputation from FORe,
respectively. Also, a mapping between the Typology of
Reputation and the FORe relates the concepts Direct Reputation
and Indirect Reputation to PrimaryReputation and
SecondaryReputation from FORe, respectively. Nevertheless, the
concepts Direct Trust and Witness Reputation from the Regret
System Reputation Model are mapped to PrimaryReputation and
PropagatedReputation from FORe. Since PropagatedReputation is
a sub-concept of SecondaryReputation, it can be inferred that
Witness Reputation is also mapped to SecondaryReputation.
4. EXPERIMENTAL SCENARIOS USING
THE ART TESTBED
To exemplify the use of mappings from last section, we define a
scenario where several agents are implemented using different
agent reputation models. This scenario includes the agents"
interaction during the simulation of the game defined by ART [6]
in order to describe the ways interoperability is possible between
different trust models using the FORe.
4.1 The ART testbed
The ART testbed provides a simulation engine on which several
agents, using different trust models, may run. The simulation
consists in a game where the agents have to decide to trust or not
other agents. The game"s domain is art appraisal, in which agents
are required to evaluate the value of paintings based on
information exchanged among other agents during agents"
interaction. The information can be an opinion transaction, when
an agent asks other agents to help it in its evaluation of a painting;
or a reputation transaction, when the information required is
about the reputation of another agent (a target) for a given era.
More details about the ART testbed can be found in [6].
The ART common reputation model was enhanced with semantic
data obtained from FORe. A general agent architecture for
interoperability was defined [11] to allow agents to reason about
the information received from reputation interactions. This
architecture contains two main modules: the Reputation Mapping
Module (RMM) which is responsible for mapping concepts
between an agent reputation model and FORe; and the Reputation
Reasoning Module (RRM) which is responsible for deal with
information about reputation according to the agent reputation
model.
4.2 Reputation transaction scenarios
While including the FORe to the ART common reputation model,
we have incremented it to allow richer interactions that involve
reputation transaction. In this section we describe scenarios
concerning reputation transactions in the context of ART testbed,
but the first is valid for any kind of reputation transaction and the
second is specific for the ART domain.
4.2.1 General scenario
Suppose that agents A, B and C are implemented according to the
aforementioned general agent architecture with the enhanced ART
common reputation model, using different reputation models.
Agent A uses the Typology of Reputation model, agent B uses the
Cognitive Reputation Model and agent C uses the ReGret System
model. Consider the interaction about reputation where agents A
and B receive from agent C information about the reputation of
agent Y. A big picture of this interaction is showed in Figure 2.
ReGret
Ontology
(Y, value=0.8,
witnessreputation)
C
Typol.
Ontology
(Y, value=0.8,
propagatedreputation)
A
CogMod.
Ontology
(Y, value=0.8,
reputation)
B
(Y, value=0.8,
PropagatedReputation)
(Y, value=0.8,
PropagatedReputation)
ReGret
Ontology
(Y, value=0.8,
witnessreputation)
C
ReGret
Ontology
(Y, value=0.8,
witnessreputation)
ReGret
Ontology
(Y, value=0.8,
witnessreputation)
(Y, value=0.8,
witnessreputation)
C
Typol.
Ontology
(Y, value=0.8,
propagatedreputation)
A
Typol.
Ontology
(Y, value=0.8,
propagatedreputation)
Typol.
Ontology
(Y, value=0.8,
propagatedreputation)
(Y, value=0.8,
propagatedreputation)
A
CogMod.
Ontology
(Y, value=0.8,
reputation)
B
CogMod.
Ontology
(Y, value=0.8,
reputation)
CogMod.
Ontology
(Y, value=0.8,
reputation)
(Y, value=0.8,
reputation)
B
(Y, value=0.8,
PropagatedReputation)
(Y, value=0.8,
PropagatedReputation)
(Y, value=0.8,
PropagatedReputation)
(Y, value=0.8,
PropagatedReputation)
Figure 1. Interaction about reputation
The information witness reputation from agent C is treated by its
RMM and is sent as PropagatedReputation to both agents. The
corresponding information in agent A reputation model is
propagated reputation and in agent B reputation model is
reputation. The way agents A and B make use of the information
depends on their internal reputation model and their RRM
implementation.
1048 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
4.2.2 ART scenario
Considering the same agents A and B and the art appraisal domain
of ART, another interesting scenario describes the following
situation: agent A asks to agent B information about agents it
knows that have skill on some specific painting era. In this case
agent A wants information concerning the direct reputation agent
B has about agents that have skill on an specific era, such as
cubism. Following the same steps of the previous scenario, agent
A message is prepared in its RRM using information from its
internal model. A big picture of this interaction is in Figure 2.
Typol.
Ontology
(agent = ?, value = ?,
skill = cubism,
reputation = directreputation)
A
(agent = ?, value = ?, skill = cubism,
reputation = PrimaryReputation)
CogMod.
Ontology
(agent = ?, value = ?,
skill = cubism,
reputation = image)
B
Typol.
Ontology
(agent = ?, value = ?,
skill = cubism,
reputation = directreputation)
A
(agent = ?, value = ?, skill = cubism,
reputation = PrimaryReputation)
CogMod.
Ontology
(agent = ?, value = ?,
skill = cubism,
reputation = image)
B
Figure 2. Interaction about specific types of reputation values
Agent B response to agent A is processed in its RRM and it is
composed of tuples (agent, value, cubism, image) , where
the pair (agent, value) is composed of all agents and associated
reputation values whose agent B knows their expertise about
cubism by its own opinion. This response is forwarded to the
RMM in order to be translated to the enriched common model and
to be sent to agent A. After receiving the information sent by
agent B, agent A processes it in its RMM and translates it to its
own reputation model to be analyzed by its RRM.
5. CONCLUSION
In this paper we present a proposal for reducing the
incompatibility between reputation models by using a general
agent architecture for reputation interaction which relies on a
functional ontology of reputation (FORe), used as a globally
shared reputation model. A reputation mapping module allows
agents to translate information from their internal reputation
model into the shared model and vice versa. The ART testbed has
been enriched to use the ontology during agent transactions. Some
scenarios were described to illustrate our proposal and they seem
to be a promising way to improve the process of building
reputation just using existing technologies.
6. ACKNOWLEDGMENTS
Anarosa A. F. Brandão is supported by CNPq/Brazil grant
310087/2006-6 and Jaime Sichman is partially supported by
CNPq/Brazil grants 304605/2004-2, 482019/2004-2 and
506881/2004-1. Laurent Vercouter was partially supported by
FAPESP grant 2005/02902-5.
7. REFERENCES
[1] Agha, G. A. Abstracting Interaction Patterns: A
Programming Paradigm for Open Distributed Systems, In
(Eds) E. Najm and J.-B. Stefani, Formal Methods for Open
Object-based Distributed Systems IFIP Transactions, 1997,
Chapman Hall.
[2] Casare,S. and Sichman, J.S. Towards a Functional Ontology
of Reputation, In Proc of the 4th
Intl Joint Conference on
Autonomous Agents and Multi Agent Systems (AAMAS"05),
Utrecht, The Netherlands, 2005, v.2, pp. 505-511.
[3] Casare, S. and Sichman, J.S. Using a Functional Ontology of
Reputation to Interoperate Different Agent Reputation
Models, Journal of the Brazilian Computer Society, (2005),
11(2), pp. 79-94.
[4] Castelfranchi, C. and Falcone, R. Principles of trust in MAS:
cognitive anatomy, social importance and quantification. In
Proceedings of ICMAS"98, Paris, 1998, pp. 72-79.
[5] Conte, R. and Paolucci, M. Reputation in Artificial Societies:
Social Beliefs for Social Order, Kluwer Publ., 2002.
[6] Fullam, K.; Klos, T.; Muller, G.; Sabater, J.; Topol, Z.;
Barber, S.;Rosenchein, J.; Vercouter, L. and Voss, M. A
specification of the agent reputation and trust (art) testbed:
experimentation and competition for trust in agent societies.
In Proc. of the 4th
Intl. Joint Conf on Autonomous Agents
and Multiagent Systems (AAMAS"05), ACM, 2005, 512-158.
[7] Mui, L.; Halberstadt, A.; Mohtashemi, M. Notions of
Reputation in Multi-Agents Systems: A Review. In: Proc of
1st Intl. Joint Conf. on Autonomous Agents and Multi-agent
Systems (AAMAS 2002), Bologna, Italy, 2002, 1, 280-287.
[8] Muller, G. and Vercouter, L. Decentralized monitoring of
agent communication with a reputation model. In Trusting
Agents for Trusting Electronic Societies, LNCS 3577, 2005,
pp. 144-161.
[9] Sabater, J. and Sierra, C. ReGret: Reputation in gregarious
societies. In Müller, J. et al (Eds) Proc. of the 5th
Intl. Conf.
on Autonomous Agents, Canada, 2001, ACM, 194-195.
[10] Sabater, J. and Sierra, C. Review on Computational Trust
and Reputation Models. In: Artificial Intelligence Review,
Kluwer Acad. Publ., (2005), v. 24, n. 1, pp. 33 - 60.
[11] Vercouter,L, Casare, S., Sichman, J. and Brandão, A. An
experience on reputation models interoperability based on a
functional ontology In Proc. of the 20th
IJCAI, Hyderabad,
India, 2007, pp.617-622.
[12] Visser, U.; Stuckenschmidt, H.; Wache, H. and Vogele, T.
Enabling technologies for inter-operability. In: In U. Visser
and H. Pundt, Eds, Workshop on the 14th Intl Symp. of
Computer Science for Environmental Protection, Bonn,
Germany, 2000, pp. 35-46.
[13] Yu, B. and Singh, M.P. An Evidential Model of Distributed
Reputation Management. In: Proc. of the 1st Intl Joint Conf.
on Autonomous Agents and Multi-agent Systems (AAMAS
2002), Bologna, Italy, 2002, part 1, pp. 294 - 301.
[14] Zacharia, G. and Maes, P. Trust Management Through
Reputation Mechanisms. In: Applied Artificial Intelligence,
14(9), 2000, pp. 881-907.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1049
A Unified and General Framework for
Argumentation-based Negotiation
Leila Amgoud
IRIT - CNRS
118, route de Narbonne
31062, Toulouse, France
amgoud@irit.fr
Yannis Dimopoulos
University of Cyprus
75 Kallipoleos Str.
PO Box 20537, Cyprus
yannis@cs.ucy.ac.cy
Pavlos Moraitis
Paris-Descartes University
45 rue des Saints-Pères
75270 Paris Cedex 06, France

pavlos@math-info.univparis5.fr
ABSTRACT
This paper proposes a unified and general framework for
argumentation-based negotiation, in which the role of 
argumentation is formally analyzed. The framework makes it
possible to study the outcomes of an argumentation-based
negotiation. It shows what an agreement is, how it is related
to the theories of the agents, when it is possible, and how
this can be attained by the negotiating agents in this case.
It defines also the notion of concession, and shows in which
situation an agent will make one, as well as how it influences
the evolution of the dialogue.
Categories and Subject Descriptors
I.2.3 [Deduction and Theorem Proving]: 
Nonmonotonic reasoning and belief revision
; I.2.11 [Distributed Artificial Intelligence]: Intelligent
agents
General Terms
Human Factors, Theory
1. INTRODUCTION
Roughly speaking, negotiation is a process aiming at 
finding some compromise or consensus between two or several
agents about some matters of collective agreement, such
as pricing products, allocating resources, or choosing 
candidates. Negotiation models have been proposed for the
design of systems able to bargain in an optimal way with
other agents for example, buying or selling products in 
ecommerce.
Different approaches to automated negotiation have been
investigated, including game-theoretic approaches (which 
usually assume complete information and unlimited 
computation capabilities) [11], heuristic-based approaches which try
to cope with these limitations [6], and argumentation-based
approaches [2, 3, 7, 8, 9, 12, 13] which emphasize the 
importance of exchanging information and explanations between
negotiating agents in order to mutually influence their 
behaviors (e.g. an agent may concede a goal having a small
priority), and consequently the outcome of the dialogue. 
Indeed, the two first types of settings do not allow for the 
addition of information or for exchanging opinions about offers.
Integrating argumentation theory in negotiation provides a
good means for supplying additional information and also
helps agents to convince each other by adequate arguments
during a negotiation dialogue. Indeed, an offer supported
by a good argument has a better chance to be accepted by
an agent, and can also make him reveal his goals or give
up some of them. The basic idea behind an 
argumentationbased approach is that by exchanging arguments, the 
theories of the agents (i.e. their mental states) may evolve, and
consequently, the status of offers may change. For instance,
an agent may reject an offer because it is not acceptable for
it. However, the agent may change its mind if it receives a
strong argument in favor of this offer.
Several proposals have been made in the literature for
modeling such an approach. However, the work is still 
preliminary. Some researchers have mainly focused on relating
argumentation with protocols. They have shown how and
when arguments in favor of offers can be computed and 
exchanged. Others have emphasized on the decision making
problem. In [3, 7], the authors argued that selecting an offer
to propose at a given step of the dialogue is a decision 
making problem. They have thus proposed an 
argumentationbased decision model, and have shown how such a model
can be related to the dialogue protocol.
In most existing works, there is no deep formal analysis
of the role of argumentation in negotiation dialogues. It is
not clear how argumentation can influence the outcome of
the dialogue. Moreover, basic concepts in negotiation such
as agreement (i.e. optimal solutions, or compromise) and
concession are neither defined nor studied.
This paper aims to propose a unified and general framework
for argumentation-based negotiation, in which the role of
argumentation is formally analyzed, and where the existing
systems can be restated. In this framework, a negotiation
dialogue takes place between two agents on a set O of offers,
whose structure is not known. The goal of a negotiation is to
find among elements of O, an offer that satisfies more or less
967
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
the preferences of both agents. Each agent is supposed to
have a theory represented in an abstract way. A theory 
consists of a set A of arguments whose structure and origin are
not known, a function specifying for each possible offer in O,
the arguments of A that support it, a non specified conflict
relation among the arguments, and finally a preference 
relation between the arguments. The status of each argument is
defined using Dung"s acceptability semantics. Consequently,
the set of offers is partitioned into four subsets: acceptable,
rejected, negotiable and non-supported offers. We show how
an agent"s theory may evolve during a negotiation dialogue.
We define formally the notions of concession, compromise,
and optimal solution. Then, we propose a protocol that 
allows agents i) to exchange offers and arguments, and ii) to
make concessions when necessary. We show that dialogues
generated under such a protocol terminate, and even reach
optimal solutions when they exist.
This paper is organized as follows: Section 2 introduces the
logical language that is used in the rest of the paper. 
Section 3 defines the agents as well as their theories. In section
4, we study the properties of these agents" theories. 
Section 5 defines formally an argumentation-based negotiation,
shows how the theories of agents may evolve during a 
dialogue, and how this evolution may influence the outcome of
the dialogue. Two kinds of outcomes: optimal solution and
compromise are defined, and we show when such outcomes
are reached. Section 6 illustrates our general framework
through some examples. Section 7 compares our formalism
with existing ones. Section 8 concludes and presents some
perspectives. Due to lack of space, the proofs are not 
included. These last are in a technical report that we will
make available online at some later time.
2. THE LOGICAL LANGUAGE
In what follows, L will denote a logical language, and ≡
is an equivalence relation associated with it.
From L, a set O = {o1, . . . , on} of n offers is identified, such
that oi, oj ∈ O such that oi ≡ oj. This means that the
offers are different. Offers correspond to the different 
alternatives that can be exchanged during a negotiation dialogue.
For instance, if the agents try to decide the place of their
next meeting, then the set O will contain different towns.
Different arguments can be built from L. The set Args(L)
will contain all those arguments. By argument, we mean a
reason in believing or of doing something. In [3], it has been
argued that the selection of the best offer to propose at a
given step of the dialogue is a decision problem. In [4], it has
been shown that in an argumentation-based approach for
decision making, two kinds of arguments are distinguished:
arguments supporting choices (or decisions), and arguments
supporting beliefs. Moreover, it has been acknowledged that
the two categories of arguments are formally defined in 
different ways, and they play different roles. Indeed, an 
argument in favor of a decision, built both on an agent"s 
beliefs and goals, tries to justify the choice; whereas an 
argument in favor of a belief, built only from beliefs, tries
to destroy the decision arguments, in particular the beliefs
part of those decision arguments. Consequently, in a 
negotiation dialogue, those two kinds of arguments are generally
exchanged between agents. In what follows, the set Args(L)
is then divided into two subsets: a subset Argso(L) of 
arguments supporting offers, and a subset Argsb(L) of arguments
supporting beliefs. Thus, Args(L) = Argso(L) ∪ Argsb(L).
As in [5], in what follows, we consider that the structure of
the arguments is not known.
Since the knowledge bases from which arguments are built
may be inconsistent, the arguments may be conflicting too.
In what follows, those conflicts will be captured by the 
relation RL, thus RL ⊆ Args(L) × Args(L). Three assumptions
are made on this relation: First the arguments supporting
different offers are conflicting. The idea behind this 
assumption is that since offers are exclusive, an agent has to choose
only one at a given step of the dialogue. Note that, the
relation RL is not necessarily symmetric between the 
arguments of Argsb(L). The second hypothesis says that 
arguments supporting the same offer are also conflicting. The
idea here is to return the strongest argument among these
arguments. The third condition does not allow an argument
in favor of an offer to attack an argument supporting a 
belief. This avoids wishful thinking. Formally:
Definition 1. RL ⊆ Args(L) × Args(L) is a conflict
relation among arguments such that:
• ∀a, a ∈ Argso(L), s.t. a = a , a RL a
• a ∈ Argso(L) and a ∈ Argsb(L) such that a RL a
Note that the relation RL is not symmetric. This is due
to the fact that arguments of Argsb(L) may be conflicting
but not necessarily in a symmetric way. In what follows, we
assume that the set Args(L) of arguments is finite, and each
argument is attacked by a finite number of arguments.
3. NEGOTIATING AGENTS THEORIES AND
REASONING MODELS
In this section we define formally the negotiating agents,
i.e. their theories, as well as the reasoning model used by
those agents in a negotiation dialogue.
3.1 Negotiating agents theories
Agents involved in a negotiation dialogue, called 
negotiating agents, are supposed to have theories. In this paper, the
theory of an agent will not refer, as usual, to its mental states
(i.e. its beliefs, desires and intentions). However, it will be
encoded in a more abstract way in terms of the arguments
owned by the agent, a conflict relation among those 
arguments, a preference relation between the arguments, and a
function that specifies which arguments support offers of the
set O. We assume that an agent is aware of all the 
arguments of the set Args(L). The agent is even able to express
a preference between any pair of arguments. This does not
mean that the agent will use all the arguments of Args(L),
but it encodes the fact that when an agent receives an 
argument from another agent, it can interpret it correctly, and it
can also compare it with its own arguments. Similarly, each
agent is supposed to be aware of the conflicts between 
arguments. This also allows us to encode the fact that an agent
can recognize whether the received argument is in conflict
or not with its arguments. However, in its theory, only the
conflicts between its own arguments are considered.
Definition 2 (Negotiating agent theory). Let O
be a set of n offers. A negotiating agent theory is a tuple
A, F, , R, Def such that:
• A ⊆ Args(L).
968 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
• F: O → 2A
s.t ∀i, j with i = j, F(oi) ∩ F(oj) = ∅.
Let AO = ∪F(oi) with i = 1, . . . , n.
• ⊆ Args(L) × Args(L) is a partial preorder denoting
a preference relation between arguments.
• R ⊆ RL such that R ⊆ A × A
• Def ⊆ A × A such that ∀ a, b ∈ A, a defeats b, denoted
a Def b iff:
- a R b, and
- not (b a)
The function F returns the arguments supporting offers in
O. In [4], it has been argued that any decision may have
arguments supporting it, called arguments PRO, and 
arguments against it, called arguments CONS. Moreover, these
two types of arguments are not necessarily conflicting. For
simplicity reasons, in this paper we consider only arguments
PRO. Moreover, we assume that an argument cannot 
support two distinct offers. However, it may be the case that an
offer is not supported at all by arguments, thus F(oi) may
be empty.
Example 1. Let O = {o1, o2, o3} be a set of offers. The
following theory is the theory of agent i:
• A = {a1, a2, a3, a4}
• F(o1) = {a1}, F(o2) = {a2}, F(o3) = ∅. Thus, Ao =
{a1, a2}
• = {(a1, a2), (a2, a1), (a3, a2), (a4, a3)}
• R = {a1, a2), (a2, a1), (a3, a2), (a4, a3)}
• Def = {(a4, a3), (a3, a2)}
From the above definition of agent theory, the following hold:
Property 1.
• Def ⊆ R
• ∀a, a ∈ F(oi), a R a
3.2 The reasoning model
From the theory of an agent, one can define the 
argumentation system used by that agent for reasoning about the
offers and the arguments, i.e. for computing the status of
the different offers and arguments.
Definition 3 (Argumentation system). Let A, F,
, R, Def be the theory of an agent. The argumentation
system of that agent is the pair A, Def .
In [5], different acceptability semantics have been introduced
for computing the status of arguments. These are based
on two basic concepts, defence and conflict-free, defined as
follows:
Definition 4 (Defence/conflict-free). Let S ⊆ A.
• S defends an argument a iff each argument that defeats
a is defeated by some argument in S.
• S is conflict-free iff there exist no a, a in S such that
a Def a .
Definition 5 (Acceptability semantics). Let S be
a conflict-free set of arguments, and let T : 2A
→ 2A
be a
function such that T (S) = {a | a is defended by S}.
• S is a complete extension iff S = T (S).
• S is a preferred extension iff S is a maximal (w.r.t set
⊆) complete extension.
• S is a grounded extension iff it is the smallest (w.r.t
set ⊆) complete extension.
Let E1, . . . , Ex denote the different extensions under a given
semantics.
Note that there is only one grounded extension. It 
contains all the arguments that are not defeated, and those
arguments that are defended directly or indirectly by 
nondefeated arguments.
Theorem 1. Let A, Def the argumentation system 
defined as shown above.
1. It may have x ≥ 1 preferred extensions.
2. The grounded extensions is S = i≥1
T (∅).
Note that when the grounded extension (or the preferred
extension) is empty, this means that there is no acceptable
offer for the negotiating agent.
Example 2. In example 1, there is one preferred 
extension, E = {a1, a2, a4}.
Now that the acceptability semantics is defined, we are ready
to define the status of any argument.
Definition 6 (Argument status). Let A, Def be an
argumentation system, and E1, . . . , Ex its extensions under a
given semantics. Let a ∈ A.
1. a is accepted iff a ∈ Ei, ∀Ei with i = 1, . . . , x.
2. a is rejected iff Ei such that a ∈ Ei.
3. a is undecided iff a is neither accepted nor rejected.
This means that a is in some extensions and not in
others.
Note that A = {a|a is accepted} ∪ {a|a is rejected} ∪ {a|a
is undecided}.
Example 3. In example 1, the arguments a1, a2 and a4
are accepted, whereas the argument a3 is rejected.
As said before, agents use argumentation systems for 
reasoning about offers. In a negotiation dialogue, agents propose
and accept offers that are acceptable for them, and reject
bad ones. In what follows, we will define the status of an
offer. According to the status of arguments, one can define
four statuses of the offers as follows:
Definition 7 (Offers status). Let o ∈ O.
• The offer o is acceptable for the negotiating agent iff
∃ a ∈ F(o) such that a is accepted. Oa = {oi ∈ O,
such that oi is acceptable}.
• The offer o is rejected for the negotiating agent iff ∀
a ∈ F(o), a is rejected. Or = {oi ∈ O, such that oi is
rejected}.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 969
• The offer o is negotiable iff ∀ a ∈ F(o), a is undecided.
On = {oi ∈ O, such that oi is negotiable}.
• The offer o is non-supported iff it is neither 
acceptable, nor rejected or negotiable. Ons = {oi ∈ O, such
that oi is non-supported offers}.
Example 4. In example 1, the two offers o1 and o2 are
acceptable since they are supported by accepted arguments,
whereas the offer o3 is non-supported since it has no 
argument in its favor.
From the above definitions, the following results hold:
Property 2. Let o ∈ O.
• O = Oa ∪ Or ∪ On ∪ Ons.
• The set Oa may contain more than one offer.
From the above partition of the set O of offers, a preference
relation between offers is defined. Let Ox and Oy be two
subsets of O. Ox Oy means that any offer in Ox is 
preferred to any offer in the set Oy. We can write also for two
offers oi, oj, oi oj iff oi ∈ Ox, oj ∈ Oy and Ox Oy.
Definition 8 (Preference between offers). Let O
be a set of offers, and Oa, Or, On, Ons its partition. Oa
On Ons Or.
Example 5. In example 1, we have o1 o3, and o2 o3.
However, o1 and o2 are indifferent.
4. THE STRUCTURE OF NEGOTIATION 
THEORIES
In this section, we study the properties of the system 
developed above. We first show that in the particular case
where A = AO (ie. all of the agent"s arguments refer to
offers), the corresponding argumentation system will return
at least one non-empty preferred extension.
Theorem 2. Let A, Def an argumentation system such
that A = AO. Then the system returns at least one 
extension E, such that |E| ≥ 1.
We now present some results that demonstrate the 
importance of indifference in negotiating agents, and more 
specifically its relation to acceptable outcomes. We first show that
the set Oa may contain several offers when their 
corresponding accepted arguments are indifferent w.r.t the preference
relation .
Theorem 3. Let o1, o2 ∈ O. o1, o2 ∈ Oa iff ∃ a1 ∈
F(o1), ∃ a2 ∈ F(o2), such that a1 and a2 are accepted and
are indifferent w.r.t (i.e. a b and b a).
We now study acyclic preference relations that are defined
formally as follows.
Definition 9 (Acyclic relation). A relation R on
a set A is acyclic if there is no sequence a1, a2, . . . , an ∈ A,
with n > 1, such that (ai, ai+1) ∈ R and (an, a1) ∈ R, with
1 ≤ i < n.
Note that acyclicity prohibits pairs of arguments a, b such
that a b and b a, ie., an acyclic preference relation
disallows indifference.
Theorem 4. Let A be a set of arguments, R the 
attacking relation of A defined as R ⊆ A × A, and an acyclic
relation on A. Then for any pair of arguments a, b ∈ A,
such that (a, b) ∈ R, either (a, b) ∈ Def or (b, a) ∈ Def (or
both).
The previous result is used in the proof of the following
theorem that states that acyclic preference relations 
sanction extensions that support exactly one offer.
Theorem 5. Let A be a set of arguments, and an
acyclic relation on A. If E is an extension of <A, Def>,
then |E ∩ AO| = 1.
An immediate consequence of the above is the following.
Property 3. Let A be a set of arguments such that A =
AO. If the relation on A is acyclic, then each extension
Ei of <A, Def>, |Ei| = 1.
Another direct consequence of the above theorem is that
in acyclic preference relations, arguments that support offers
can participate in only one preferred extension.
Theorem 6. Let A be a set of arguments, and an
acyclic relation on A. Then the preferred extensions of
A, Def are pairwise disjoint w.r.t arguments of AO.
Using the above results we can prove the main theorem of
this section that states that negotiating agents with acyclic
preference relations do not have acceptable offers.
Theorem 7. Let A, F, R, , Def be a negotiating
agent such that A = AO and is an acyclic relation. Then
the set of accepted arguments w.r.t A, Def is emtpy. 
Consequently, the set of acceptable offers, Oa is empty as well.
5. ARGUMENTATION-BASED NEGOTIATION
In this section, we define formally a protocol that 
generates argumentation-based negotiation dialogues between
two negotiating agents P and C. The two agents 
negotiate about an object whose possible values belong to a set
O. This set O is supposed to be known and the same for
both agents. For simplicity reasons, we assume that this
set does not change during the dialogue. The agents are
equipped with theories denoted respectively AP
, FP
, P
,
RP
, DefP
, and AC
, FC
, C
, RC
, DefC
. Note that the
two theories may be different in the sense that the agents
may have different sets of arguments, and different 
preference relations. Worst yet, they may have different 
arguments in favor of the same offers. Moreover, these theories
may evolve during the dialogue.
5.1 Evolution of the theories
Before defining formally the evolution of an agent"s theory,
let us first introduce the notion of dialogue moves, or moves
for short.
Definition 10 (Move). A move is a tuple mi = pi,
ai, oi, ti such that:
• pi ∈ {P, C}
• ai ∈ Args(L) ∪ θ1
1
In what follows θ denotes the fact that no argument, or no
offer is given
970 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
• oi ∈ O ∪ θ
• ti ∈ N∗
is the target of the move, such that ti < i
The function Player (resp. Argument, Offer, Target) 
returns the player of the move (i.e. pi) (resp. the argument
of a move, i.e ai, the offer oi, and the target of the move,
ti). Let M denote the set of all the moves that can be built
from {P, C}, Arg(L), O .
Note that the set M is finite since Arg(L) and O are 
assumed to be finite. Let us now see how an agent"s theory
evolves and why. The idea is that if an agent receives an
argument from another agent, it will add the new argument
to its theory. Moreover, since an argument may bring new
information for the agent, thus new arguments can emerge.
Let us take the following example:
Example 6. Suppose that an agent P has the following
propositional knowledge base: ΣP = {x, y → z}. From this
base one cannot deduce z. Let"s assume that this agent 
receives the following argument {a, a → y} that justifies y.
It is clear that now P can build an argument, say {a, a →
y, y → z} in favor of z.
In a similar way, if a received argument is in conflict with the
arguments of the agent i, then those conflicts are also added
to its relation Ri
. Note that new conflicts may arise between
the original arguments of the agent and the ones that emerge
after adding the received arguments to its theory. Those new
conflicts should also be considered. As a direct consequence
of the evolution of the sets Ai
and Ri
, the defeat relation
Defi
is also updated.
The initial theory of an agent i, (i.e. its theory before the
dialogue starts), is denoted by Ai
0, Fi
0, i
0, Ri
0, Defi
0 , with
i ∈ {P, C}. Besides, in this paper, we suppose that the
preference relation i
of an agent does not change during
the dialogue.
Definition 11 (Theory evolution). Let m1, . . ., mt,
. . ., mj be a sequence of moves. The theory of an agent i at
a step t > 0 is: Ai
t, Fi
t , i
t, Ri
t, Defi
t such that:
• Ai
t = Ai
0 ∪ {ai, i = 1, . . . , t, ai = Argument(mi)} ∪
A with A ⊆ Args(L)
• Fi
t = O → 2Ai
t
• i
t = i
0
• Ri
t = Ri
0 ∪ {(ai, aj) | ai = Argument(mi),
aj = Argument(mj), i, j ≤ t, and ai RL aj} ∪ R with
R ⊆ RL
• Defi
t ⊆ Ai
t × Ai
t
The above definition captures the monotonic aspect of an
argument. Indeed, an argument cannot be removed. 
However, its status may change. An argument that is accepted
at step t of the dialogue by an agent may become rejected
at step t + i. Consequently, the status of offers also change.
Thus, the sets Oa, Or, On, and Ons may change from one
step of the dialogue to another. That means for example
that some offers could move from the set Oa to the set Or
and vice-versa. Note that in the definition of Rt, the 
relation RL is used to denote a conflict between exchanged
arguments. The reason is that, such a conflict may not be
in the set Ri
of the agent i. Thus, in order to recognize
such conflicts, we have supposed that the set RL is known
to the agents. This allows us to capture the situation where
an agent is able to prove an argument that it was unable
to prove before, by incorporating in its beliefs some 
information conveyed through the exchange of arguments with
another agent. This, unknown at the beginning of the 
dialogue argument, could give to this agent the possibility to
defeat an argument that it could not by using its initial 
arguments. This could even lead to a change of the status of
these initial arguments and this change would lead to the
one of the associated offers" status.
In what follows, Oi
t,x denotes the set of offers of type x,
where x ∈ {a, n, r, ns}, of the agent i at step t of the 
dialogue. In some places, we can use for short the notation Oi
t
to denote the partition of the set O at step t for agent i.
Note that we have: not(Oi
t,x ⊆ Oi
t+1,x).
5.2 The notion of agreement
As said in the introduction, negotiation is a process aiming
at finding an agreement about some matters. By agreement,
one means a solution that satisfies to the largest possible 
extent the preferences of both agents. In case there is no such
solution, we say that the negotiation fails. In what follows,
we will discuss the different kinds of solutions that may be
reached in a negotiation. The first one is the optimal 
solution. An optimal solution is the best offer for both agents.
Formally:
Definition 12 (Optimal solution). Let O be a set
of offers, and o ∈ O. The offer o is an optimal solution at
a step t ≥ 0 iff o ∈ OP
t,a ∩ OC
t,a
Such a solution does not always exist since agents may have
conflicting preferences. Thus, agents make concessions by
proposing/accepting less preferred offers.
Definition 13 (Concession). Let o ∈ O be an offer.
The offer o is a concession for an agent i iff o ∈ Oi
x such
that ∃Oi
y = ∅, and Oi
y Oi
x.
During a negotiation dialogue, agents exchange first their
most preferred offers, and if these last are rejected, they
make concessions. In this case, we say that their best offers
are no longer defendable. In an argumentation setting, this
means that the agent has already presented all its arguments
supporting its best offers, and it has no counter argument
against the ones presented by the other agent. Formally:
Definition 14 (Defendable offer). Let Ai
t, Fi
t , i
t,
Ri
t, Defi
t be the theory of agent i at a step t > 0 of the 
dialogue. Let o ∈ O such that ∃j ≤ t with Player(mj) = i and
offer(mj) = o. The offer o is defendable by the agent i iff:
• ∃a ∈ Fi
t (o), and k ≤ t s.t. Argument(mk) = a, or
• ∃a ∈ At
\Fi
t (o) s.t. a Defi
t b with
- Argument(mk) = b, k ≤ t, and Player(mk) = i
- l ≤ t, Argument(ml) = a
The offer o is said non-defendable otherwise and NDi
t is the
set of non-defendable offers of agent i at a step t.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 971
5.3 Negotiation dialogue
Now that we have shown how the theories of the agents
evolve during a dialogue, we are ready to define formally
an argumentation-based negotiation dialogue. For that 
purpose, we need to define first the notion of a legal 
continuation.
Definition 15 (Legal move). A move m is a legal
continuation of a sequence of moves m1, . . . , ml iff j, k < l,
such that:
• Offer(mj) = Offer(mk), and
• Player(mj) = Player(mk)
The idea here is that if the two agents present the same
offer, then the dialogue should terminate, and there is no
longer possible continuation of the dialogue.
Definition 16 (Argumentation-based negotiation).
An argumentation-based negotiation dialogue d between two
agents P and C is a non-empty sequence of moves m1, . . . , ml
such that:
• pi = P iff i is even, and pi = C iff i is odd
• Player(m1) = P, Argument(m1) = θ, Offer(m1) = θ,
and Target(m1) = 02
• ∀ mi, if Offer(mi) = θ, then Offer(mi) oj, ∀ oj ∈
O\(O
Player(mi)
i,r ∪ ND
Player(mi)
i )
• ∀i = 1, . . . , l, mi is a legal continuation of m1, . . . , mi−1
• Target(mi) = mj such that j < i and Player(mi) =
Player(mj)
• If Argument(mi) = θ, then:
- if Offer(mi) = θ then Argument(mi) ∈ F(Offer(mi))
- if Offer(mi) = θ then Argument(mi) Def
Player(mi)
i
Argument(Target(mi))
• i, j ≤ l such that mi = mj
• m ∈ M such that m is a legal continuation of m1, . . . , ml
Let D be the set of all possible dialogues.
The first condition says that the two agents take turn. The
second condition says that agent P starts the negotiation
dialogue by presenting an offer. Note that, in the first turn,
we suppose that the agent does not present an argument.
This assumption is made for strategical purposes. Indeed,
arguments are exchanged as soon as a conflict appears. The
third condition ensures that agents exchange their best 
offers, but never the rejected ones. This condition takes also
into account the concessions that an agent will have to make
if it was established that a concession is the only option for
it at the current state of the dialogue. Of course, as we
have shown in a previous section, an agent may have several
good or acceptable offers. In this case, the agent chooses
one of them randomly. The fourth condition ensures that
the moves are legal. This condition allows to terminate the
dialogue as soon as an offer is presented by both agents.
The fifth condition allows agents to backtrack. The sixth
2
The first move has no target.
condition says that an agent may send arguments in favor
of offers, and in this case the offer should be stated in the
same move. An agent can also send arguments in order to
defeat arguments of the other agent. The next condition
prevents repeating the same move. This is useful for 
avoiding loops. The last condition ensures that all the possible
legal moves have been presented.
The outcome of a negotiation dialogue is computed as
follows:
Definition 17 (Dialogue outcome). Let d = m1, . . .,
ml be a argumentation-based negotiation dialogue. The 
outcome of this dialogue, denoted Outcome, is Outcome(d) =
Offer(ml) iff ∃j < l s.t. Offer(ml) = Offer(mj), and
Player(ml) = Player(mj). Otherwise, Outcome(d) = θ.
Note that when Outcome(d) = θ, the negotiation fails, and
no agreement is reached by the two agents. However, if
Outcome(d) = θ, the negotiation succeeds, and a solution
that is either optimal or a compromise is found.
Theorem 8. ∀di ∈ D, the argumentation-based 
negotiation di terminates.
The above result is of great importance, since it shows that
the proposed protocol avoids loops, and dialogues terminate.
Another important result shows that the proposed protocol
ensures to reach an optimal solution if it exists. Formally:
Theorem 9 (Completeness). Let d = m1, . . . , ml be
a argumentation-based negotiation dialogue. If ∃t ≤ l such
that OP
t,a ∩ OC
t,a = ∅, then Outcome(d) ∈ OP
t,a ∩ OC
t,a.
We show also that the proposed dialogue protocol is sound
in the sense that, if a dialogue returns a solution, then that
solution is for sure a compromise. In other words, that 
solution is a common agreement at a given step of the 
dialogue. We show also that if the negotiation fails, then there
is no possible solution.
Theorem 10 (Soundness). Let d = m1, . . . , ml be a
argumentation-based negotiation dialogue.
1. If Outcome(d) = o, (o = θ), then ∃t ≤ l such that o ∈
OP
t,x ∩ OC
t,y, with x, y ∈ {a, n, ns}.
2. If Outcome(d) = θ, then ∀t ≤ l, OP
t,x ∩ OC
t,y = ∅, ∀
x, y ∈ {a, n, ns}.
A direct consequence of the above theorem is the following:
Property 4. Let d = m1, . . . , ml be a 
argumentationbased negotiation dialogue. If Outcome(d) = θ, then ∀t ≤ l,
• OP
t,r = OC
t,a ∪ OC
t,n ∪ OC
t,ns, and
• OC
t,r = OP
t,a ∪ OP
t,n ∪ OP
t,ns.
6. ILLUSTRATIVE EXAMPLES
In this section we will present some examples in order to
illustrate our general framework.
Example 7 (No argumentation). Let O = {o1, o2}
be the set of all possible offers. Let P and C be two agents,
equipped with the same theory: A, F, , R, Def such that
A = ∅, F(o1) = F(o2) = ∅, = ∅, R = ∅, Def = ∅. In
this case, it is clear that the two offers o1 and o2 are 
nonsupported. The proposed protocol (see Definition 16) will
generate one of the following dialogues:
972 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
P: m1 = P, θ, o1, 0
C: m2 = C, θ, o1, 1
This dialogue ends with o1 as a compromise. Note that this
solution is not considered as optimal since it is not an 
acceptable offer for the agents.
P: m1 = P, θ, o1, 0
C: m2 = C, θ, o2, 1
P: m3 = P, θ, o2, 2
This dialogue ends with o2 as a compromise.
P: m1 = P, θ, o2, 0
C: m2 = C, θ, o2, 1
This dialogue also ends with o2 as a compromise. The last
possible dialgue is the following that ends with o1 as a 
compromise.
P: m1 = P, θ, o2, 0
C: m2 = C, θ, o1, 1
P: m3 = P, θ, o1, 2
Note that in the above example, since there is no exchange
of arguments, the theories of both agents do not change. Let
us now consider the following example.
Example 8 (Static theories). Let O = {o1, o2} be
the set of all possible offers. The theory of agent P is AP
,
FP
, P
, RP
, DefP
such that: AP
= {a1, a2}, FP
(o1) =
{a1}, FP
(o2) = {a2}, P
= {(a1, a2)}, RP
= {(a1, a2), (a2, a1)},
DefP
= {a1, a2}. The argumentation system AP
, DefP
of
this agent will return a1 as an accepted argument, and a2 as
a rejected one. Consequently, the offer o1 is acceptable and
o2 is rejected.
The theory of agent C is AC
, FC
, C
, RC
, DefC
such
that: AC
= {a1, a2}, FC
(o1) = {a1}, FC
(o2) = {a2}, C
=
{(a2, a1)}, RC
= {(a1, a2), (a2, a1)}, DefC
= {a2, a1}. The
argumentation system AC
, DefC
of this agent will return
a2 as an accepted argument, and a1 as a rejected one. 
Consequently, the offer o2 is acceptable and o1 is rejected.
The only possible dialogues that may take place between
the two agents are the following:
P: m1 = P, θ, o1, 0
C: m2 = C, θ, o2, 1
P: m3 = P, a1, o1, 2
C: m4 = C, a2, o2, 3
The second possible dialogue is the following:
P: m1 = P, θ, o1, 0
C: m2 = C, a2, o2, 1
P: m3 = P, a1, o1, 2
C: m4 = C, θ, o2, 3
Both dialogues end with failure. Note that in both dialogues,
the theories of both agents do not change. The reason is that
the exchanged arguments are already known to both agents.
The negotiation fails because the agents have conflicting 
preferences.
Let us now consider an example in which argumentation will
allow agents to reach an agreement.
Example 9 (Dynamic theories). Let O = {o1, o2} be
the set of all possible offers. The theory of agent P is AP
,
FP
, P
, RP
, DefP
such that: AP
= {a1, a2}, FP
(o1)
= {a1}, FP
(o2) = {a2}, P
= {(a1, a2), (a3, a1)}, RP
=
{(a1, a2), (a2, a1)}, DefP
= {(a1, a2)}. The argumentation
system AP
, DefP
of this agent will return a1 as an accepted
argument, and a2 as a rejected one. Consequently, the offer
o1 is acceptable and o2 is rejected.
The theory of agent C is AC
, FC
, C
, RC
, DefC
such
that: AC
= {a1, a2, a3}, FC
(o1) = {a1}, FC
(o2) = {a2},
C
= {(a1, a2), (a3, a1)}, RC
= {(a1, a2), (a2, a1), (a3, a1)},
DefC
= {(a1, a2), (a3, a1)}. The argumentation system
AC
, DefC
of this agent will return a3 and a2 as accepted
arguments, and a1 as a rejected one. Consequently, the offer
o2 is acceptable and o1 is rejected.
The following dialogue may take place between the two
agents:
P: m1 = P, θ, o1, 0
C: m2 = C, θ, o2, 1
P: m3 = P, a1, o1, 2
C: m4 = C, a3, θ, 3
C: m5 = P, θ, o2, 4
At step 4 of the dialogue, the agent P receives the 
argument a3 from P. Thus, its theory evolves as follows: AP
= {a1, a2, a3}, RP
= {(a1, a2), (a2, a1), (a3, a1)}, DefP
=
{(a1, a2), (a3, a1)}. At this step, the argument a1 which was
accepted will become rejected, and the argument a2 which
was at the beginning of the dialogue rejected will become 
accepted. Thus, the offer o2 will be acceptable for the agent,
whereas o1 will become rejected. At this step 4, the offer o2
is acceptable for both agents, thus it is an optimal solution.
The dialogue ends by returning this offer as an outcome.
7. RELATED WORK
Argumentation has been integrated in negotiation 
dialogues at the early nineties by Sycara [12]. In that work, the
author has emphasized the advantages of using 
argumentation in negotiation dialogues, and a specific framework has
been introduced. In [8], the different types of arguments
that are used in a negotiation dialogue, such as threats and
rewards, have been discussed. Moreover, a particular 
framework for negotiation have been proposed. In [9, 13], 
different other frameworks have been proposed. Even if all these
frameworks are based on different logics, and use different
definitions of arguments, they all have at their heart an 
exchange of offers and arguments. However, none of those
proposals explain when arguments can be used within a 
negotiation, and how they should be dealt with by the agent
that receives them. Thus the protocol for handling 
arguments was missing. Another limitation of the above 
frameworks is the fact that the argumentation frameworks they
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 973
use are quite poor, since they use a very simple 
acceptability semantics. In [2] a negotiation framework that fills the
gap has been suggested. A protocol that handles the 
arguments was proposed. However, the notion of concession
is not modeled in that framework, and it is not clear what
is the status of the outcome of the dialogue. Moreover, it
is not clear how an agent chooses the offer to propose at a
given step of the dialogue. In [1, 7], the authors have 
focused mainly on this decision problem. They have proposed
an argumentation-based decision framework that is used by
agents in order to choose the offer to propose or to accept
during the dialogue. In that work, agents are supposed to
have a beliefs base and a goals base.
Our framework is more general since it does not impose
any specific structure for the arguments, the offers, or the 
beliefs. The negotiation protocol is general as well. Thus this
framework can be instantiated in different ways by creating,
in such manner, different specific argumentation-based 
negotiation frameworks, all of them respecting the same 
properties. Our framework is also a unified one because frameworks
like the ones presented above can be represented within this
framework. For example the decision making mechanism
proposed in [7] for the evaluation of arguments and 
therefore of offers, which is based on a priority relation between
mutually attacked arguments, can be captured by the 
relation defeat proposed in our framework. This relation takes
simultaneously into account the attacking and preference
relations that may exist between two arguments.
8. CONCLUSIONS AND FUTURE WORK
In this paper we have presented a unified and general
framework for argumentation-based negotiation. Like any
other argumentation-based negotiation framework, as it is
evoked in (e.g. [10]), our framework has all the advantages
that argumentation-based negotiation approaches present
when related to the negotiation approaches based either on
game theoretic models (see e.g. [11]) or heuristics ([6]). This
work is a first attempt to formally define the role of 
argumentation in the negotiation process. More precisely, for the
first time, it formally establishes the link that exists between
the status of the arguments and the offers they support, it
defines the notion of concession and shows how it influences
the evolution of the negotiation, it determines how the 
theories of agents evolve during the dialogue and performs an
analysis of the negotiation outcomes. It is also the first time
where a study of the formal properties of the negotiation
theories of the agents as well as of an argumentative 
negotiation dialogue is presented.
Our future work concerns several points. A first point is
to relax the assumption that the set of possible offers is the
same to both agents. Indeed, it is more natural to assume
that agents may have different sets of offers. During a 
negotiation dialogue, these sets will evolve. Arguments in favor
of the new offers may be built from the agent theory. Thus,
the set of offers will be part of the agent theory. Another
possible extension of this work would be to allow agents
to handle both arguments PRO and CONS offers. This is
more akin to the way human take decisions. Considering
both types of arguments will refine the evaluation of the
offers status. In the proposed model, a preference relation
between offers is defined on the basis of the partition of the
set of offers. This preference relation can be refined. For
instance, among the acceptable offers, one may prefer the
offer that is supported by the strongest argument. In [4], 
different criteria have been proposed for comparing decisions.
Our framework can thus be extended by integrating those
criteria. Another interesting point to investigate is that of
considering negotiation dialogues between two agents with
different profiles. By profile, we mean the criterion used by
an agent to compare its offers.
9. REFERENCES
[1] L. Amgoud, S. Belabbes, and H. Prade. Towards a
formal framework for the search of a consensus
between autonomous agents. In Proceedings of the 4th
International Joint Conference on Autonomous Agents
and Multi-Agents systems, pages 537-543, 2005.
[2] L. Amgoud, S. Parsons, and N. Maudet. Arguments,
dialogue, and negotiation. In Proceedings of the 14th
European Conference on Artificial Intelligence, 2000.
[3] L. Amgoud and H. Prade. Reaching agreement
through argumentation: A possibilistic approach. In 9
th International Conference on the Principles of
Knowledge Representation and Reasoning, KR"2004,
2004.
[4] L. Amgoud and H. Prade. Explaining qualitative
decision under uncertainty by argumentation. In 21st
National Conference on Artificial Intelligence,
AAAI"06, pages 16 - 20, 2006.
[5] P. M. Dung. On the acceptability of arguments and its
fundamental role in nonmonotonic reasoning, logic
programming and n-person games. Artificial
Intelligence, 77:321-357, 1995.
[6] N. R. Jennings, P. Faratin, A. R. Lumuscio,
S. Parsons, and C. Sierra. Automated negotiation:
Prospects, methods and challenges. International
Journal of Group Decision and Negotiation, 2001.
[7] A. Kakas and P. Moraitis. Adaptive agent negotiation
via argumentation. In Proceedings of the 5th
International Joint Conference on Autonomous Agents
and Multi-Agents systems, pages 384-391, 2006.
[8] S. Kraus, K. Sycara, and A. Evenchik. Reaching
agreements through argumentation: a logical model
and implementation. Artificial Intelligence, 104:1-69,
1998.
[9] S. Parsons and N. R. Jennings. Negotiation through
argumentation-a preliminary report. In Proceedings
of the 2nd International Conference on Multi Agent
Systems, pages 267-274, 1996.
[10] I. Rahwan, S. D. Ramchurn, N. R. Jennings,
P. McBurney, S. Parsons, and E. Sonenberg.
Argumentation-based negotiation. Knowledge
Engineering Review, 18 (4):343-375, 2003.
[11] J. Rosenschein and G. Zlotkin. Rules of Encounter:
Designing Conventions for Automated Negotiation
Among Computers,. MIT Press, Cambridge,
Massachusetts, 1994., 1994.
[12] K. Sycara. Persuasive argumentation in negotiation.
Theory and Decision, 28:203-242, 1990.
[13] F. Tohm´e. Negotiation and defeasible reasons for
choice. In Proceedings of the Stanford Spring
Symposium on Qualitative Preferences in Deliberation
and Practical Reasoning, pages 95-102, 1997.
974 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Modular Interpreted Systems
Wojciech Jamroga
Department of Informatics
Clausthal University of Technology, Germany
wjamroga@in.tu-clausthal.de
Thomas Ågotnes
Department of Computer Engineering
Bergen University College, Norway
tag@hib.no
ABSTRACT
We propose a new class of representations that can be used for 
modeling (and model checking) temporal, strategic and epistemic 
properties of agents and their teams. Our representations borrow the
main ideas from interpreted systems of Halpern, Fagin et al.; 
however, they are also modular and compact in the way concurrent 
programs are. We also mention preliminary results on model checking
alternating-time temporal logic for this natural class of models.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
IntelligenceMultiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge 
Representation Formalisms and Methods-Modal logic
General Terms
Theory
1. INTRODUCTION
The logical foundations of multi-agent systems have received
much attention in recent years. Logic has been used to represent
and reason about, e.g., knowledge [7], time [6], cooperation and
strategic ability [3]. Lately, an increasing amount of research has
focused on higher level representation languages for models of such
logics, motivated mainly by the need for compact representations,
and for representations that correspond more closely to the actual
systems which are modeled. Multi-agent systems are open systems,
in the sense that agents interact with an environment only partially
known in advance. Thus, we need representations of models of
multi-agent systems which are modular, in the sense that a 
component, such as an agent, can be replaced, removed, or added, without
major changes to the representation of the whole model. However,
as we argue in this paper, few existing representation languages are
both modular, compact and computationally grounded on the one
hand, and allow for representing properties of both knowledge and
strategic ability, on the other.
In this paper we present a new class of representations for 
models of open multi-agent systems, which are modular, compact and
come with an implicit methodology for modeling and designing
actual systems.
The structure of the paper is as follows. First, in Section 2, we
present the background of our work - that is, logics that combine
time, knowledge, and strategies. More precisely: modal logics that
combine branching time, knowledge, and strategies under 
incomplete information. We start with computation tree logic CTL, then
we add knowledge (CTLK), and then we discuss two variants of
alternating-time temporal logic (ATL): one for the perfect, and one
for the imperfect information case. The semantics of logics like the
ones presented in Section 2 are usually defined over explicit models
(Kripke structures) that enumerate all possible (global) states of the
system. However, enumerating these states is one of the things one
mostly wants to avoid, because there are too many of them even
for simple systems. Thus, we usually need representations that are
more compact. Another reason for using a more specialized class of
models is that general Kripke structures do not always give enough
help in terms of methodology, both at the stage of design, nor at 
implementation. This calls for a semantics which is more grounded, in
the sense that the correspondence between elements of the model,
and the entities that are modeled, is more immediate. In Section 3,
we present an overview of representations that have been used for
modeling and model checking systems in which time, action (and
possibly knowledge) are important; we mention especially 
representations used for theoretical analysis. We point out that the 
compact and/or grounded representations of temporal models do not
play their role in a satisfactory way when agents" strategies are
considered. Finally, in Section 4, we present our framework of
modular interpreted systems (MIS), and show where it fits in the
picture. We conclude with a somewhat surprising hypothesis, that
model checking ability under imperfect information for MIS can be
computationally cheaper than model checking perfect information.
Until now, almost all complexity results were distinctly in favor of
perfect information strategies (and the others were indifferent).
2. LOGICS OF TIME, KNOWLEDGE, AND
STRATEGIC ABILITY
First, we present the logics CTL, CTLK, ATL and ATLir that are
the starting point of our study.
2.1 Branching Time: CTL
Computation tree logic CTL [6] includes operators for temporal
properties of systems: i.e., path quantifier E (there is a path), 
together with temporal operators: f(in the next state), 2 (always
from now on) and U (until).1
Every occurrence of a temporal
operator is immediately preceded by exactly one path quantifier
(this variant of the language is sometimes called vanilla CTL).
Let Π be a set of atomic propositions with a typical element p.
CTL formulae ϕ are defined as follows:
ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | E fϕ | E2ϕ | Eϕ U ϕ.
The semantics of CTL is based on Kripke models M = St, R, π ,
which include a nonempty set of states St, a state transition relation
R ⊆ St × St, and a valuation of propositions π : Π → P(St).
A path λ in M refers to a possible behavior (or computation) of
system M, and can be represented as an infinite sequence of states
q0q1q2... such that qiRqi+1 for every i = 0, 1, 2, .... We denote
the ith state in λ by λ[i]. A q-path is a path that starts in q. 
Interpretation of a formula in a state q in model M is defined as follows:
M, q |= p iff q ∈ π(p);
M, q |= ¬ϕ iff M, q |= ϕ;
M, q |= ϕ ∧ ψ iff M, q |= ϕ and M, q |= ψ;
M, q |= E fϕ iff there is a q-path λ such that M, λ[1] |= ϕ;
M, q |= E2ϕ iff there is a q-path λ such that M, λ[i] |= ϕ for
every i ≥ 0;
M, q |= Eϕ U ψ iff there is a q-path λ and i ≥ 0 such that
M, λ[i] |= ψ and M, λ[j] |= ϕ for every 0 ≤ j < i.
2.2 Adding Knowledge: CTLK
CTLK [19] is a straightforward combination of CTL and standard
epistemic logic [10, 7]. Let Agt = {1, ..., k} be a set of agents with
a typical element a. Epistemic logic uses operators for representing
agents" knowledge: Kaϕ is read as agent a knows that ϕ. Models
of CTLK extend models of CTL with epistemic indistinguishability
relations ∼a⊆ St × St (one per agent). We assume that all ∼a are
equivalences. The semantics of epistemic operators is defined as
follows:
M, q |= Kaϕ iff M, q |= ϕ for every q such that q ∼a q .
Note that, when talking about agents" knowledge, we 
implicitly assume that agents may have imperfect information about the
actual current state of the world (otherwise the notion of 
knowledge would be trivial). This does not have influence on the way
we model evolution of a system as a single unit, but it will become
important when particular agents and their strategies come to the
fore.
2.3 Agents and Their Strategies: ATL
Alternating-time temporal logic ATL [3] is a logic for 
reasoning about temporal and strategic properties of open computational
systems (multi-agent systems in particular). The language of ATL
consists of the following formulae:
ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | A fϕ | A 2ϕ | A ϕ U ϕ.
where A ⊆ Agt. Informally, A ϕ says that agents A have a 
collective strategy to enforce ϕ. It should be noted that the CTL path
quantifiers A, E can be expressed with ∅ , Agt respectively.
The semantics of ATL is defined in so called concurrent game
structures (CGSs). A CGS is a tuple
M = Agt, St, Act, d, o, Π, π ,
1
Additional operators A (for every path) and 3 (sometime in
the future) are defined in the usual way.
consisting of: a set Agt = {1, . . . , k} of agents; set St of states;
valuation of propositions π : Π → P(St); set Act of atomic 
actions. Function d : Agt × St → P(Act) indicates the actions
available to agent a ∈ Agt in state q ∈ St. Finally, o is a 
deterministic transition function which maps a state q ∈ St and an
action profile α1, . . . , αk ∈ Actk
, αi ∈ d(i, q), to another state
q = o(q, α1, . . . , αk).
DEFINITION 1. A (memoryless) strategy of agent a is a function
sa : St → Act such that sa(q) ∈ d(a, q).2
A collective strategy SA
for a team A ⊆ Agt specifies an individual strategy for each agent
a ∈ A. Finally, the outcome of strategy SA in state q is defined as
the set of all computations that may result from executing SA from
q on:
out(q, SA) = {λ = q0q1q2... | q0 = q and for every i = 1, 2, ...
there exists αi−1
1 , ..., αi−1
k such that αi−1
a = SA(a)(qi−1)
for each a ∈ A, αi−1
a ∈ d(a, qi−1) for each a /∈ A, and
o(qi−1, αi−1
1 , ..., αi−1
k ) = qi}.
The semantics of cooperation modalities is as follows:
M, q |= A fϕ iff there is a collective strategy SA such that,
for every λ ∈ out(q, SA), we have M, λ[1] |= ϕ;
M, q |= A 2ϕ iff there exists SA such that, for every λ ∈
out(q, SA), we have M, λ[i] for every i ≥ 0;
M, q |= A ϕ U ψ iff there exists SA such that for every λ ∈
out(q, SA) there is a i ≥ 0, for which M, λ[i] |= ψ, and
M, λ[j] |= ϕ for every 0 ≤ j < i.
2.4 Agents with Imperfect Information: ATLir
As ATL does not include incomplete information in its scope, it
can be seen as a logic for reasoning about agents who always have
complete knowledge about the current state of the whole system.
ATLir [21] includes the same formulae as ATL, except that the 
cooperation modalities are presented with a subscript: A ir indicates
that they address agents with imperfect information and imperfect
recall. Formally, the recursive definition of ATLir formulae is:
ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | A ir
fϕ | A ir2ϕ | A irϕ U ϕ
Models of ATLir, concurrent epistemic game structures (CEGS),
can be defined as tuples M = Agt, St, Act, d, o, ∼1, ..., ∼k, Π, π ,
where Agt, St, Act, d, o, Π, π is a CGS, and ∼1, ..., ∼k are 
epistemic (equivalence) relations. It is required that agents have the
same choices in indistinguishable states: q ∼a q implies d(a, q) =
d(a, q ). ATLir restricts the strategies that can be used by agents
to uniform strategies, i.e. functions sa : St → Act, such that: (1)
sa(q) ∈ d(a, q), and (2) if q ∼a q then sa(q) = sa(q ). A collective
strategy is uniform if it contains only uniform individual strategies.
Again, the function out(q, SA) returns the set of all paths that may
result from agents A executing collective strategy SA from state q.
The semantics of ATLir formulae can be defined as follows:
M, q |= A ir
fϕ iff there is a uniform collective strategy SA
such that, for every a ∈ A, q such that q ∼a q , and λ ∈
out(SA, q ), we have M, λ[1] |= ϕ;
2
This is a deviation from the original semantics of ATL [3], where
strategies assign agents" choices to sequences of states, which 
suggests that agents can by definition recall the whole history of each
game. While the choice of one or another notion of strategy affects
the semantics of the full ATL
∗
, and most ATL extensions (e.g. for
games with imperfect information), it should be pointed out that
both types of strategies yield equivalent semantics for pure ATL
(cf. [21]).
898 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
M, q |= A ir2ϕ iff there exists SA such that, for every a ∈ A,
q such that q ∼a q , and λ ∈ out(SA, q ), we have M, λ[i]
for every i ≥ 0;
M, q |= A irϕ U ψ iff there exist SA such that, for every a ∈ A,
q such that q ∼a q , and λ ∈ out(SA, q ), there is i ≥ 0 for
which M, λ[i] |= ψ, and M, λ[j] |= ϕ for every 0 ≤ j < i.
That is, A irϕ holds iff A have a uniform collective strategy, such
that for every path that can possibly result from execution of the
strategy according to at least one agent from A, ϕ is the case.
3. MODELS AND MODEL CHECKING
In this section, we present and discuss various (existing) 
representations of systems that can be used for modeling and model
checking. We believe that the two most important points of 
reference are in this case: (1) the modeling formalism (i.e., the logic
and the semantics we use), and (2) the phenomenon, or more 
generally, the domain we are going to model (to which we will often
refer as the real world). Our aim is a representation which is 
reasonably close to the real world (i.e., it is sufficiently compact and
grounded), and still not too far away from the formalism (so that
it e.g. easily allows for theoretical analysis of computational 
problems). We begin with discussing the merits of explicit 
modelsin our case, these are transition systems, concurrent game structures
and CEGSs, presented in the previous section.
3.1 Explicit Models
Obviously, an advantage of explicit models is that they are very
close to the semantics of our logics (simply because they are the
semantics). On the other hand, they are in many ways difficult to
use to describe an actual system:
• Exponential size: temporal models usually have an 
exponential number of states with respect to any higher-level 
description (e.g. Boolean variables, n-ary attributes etc.). Also, their
size is exponential in the number of processes (or agents)
if the evolution of a system results from joint (synchronous
or asynchronous) actions of several active entities [15]. For
CGSs the situation is even worse: here, also the number of
transitions is exponential, even if we fix the number of states.3
In practice, this means that such representations are very 
seldom scalable.
• Explicit models include no modularity. States in a model
refer to global states of the system; transitions in the model
correspond to global transitions as well, i.e., they represent
(in an atomic way) everything that may happen in one single
step, regardless of who has done it, to whom, and in what
way.
• Logics like ATL are often advertised as frameworks for 
modeling and reasoning about open computational systems. 
Ideally, one would like the elements of such a system to have
as little interdependencies as possible, so that they can be
plugged in and out without much hassle, for instance when
we want to test various designs or implementations of the
active component. In the case of a multi-agent system the
3
Another class of ATL models, alternating transition systems [2]
represent transitions in a more succinct way. While we still have
exponentially many states in an ATS, the number of transitions is
simply quadratic wrt. to states (like for CTL models). 
Unfortunately, ATS are even less modular and harder to design than 
concurrent game structures, and they cannot be easily extended to handle
incomplete information (cf. [9]).
need is perhaps even more obvious. We do not only need
to re-plug various designs of a single agent in the overall
architecture; we usually also need to change (e.g., increase)
the number of agents acting in a given environment without
necessarily changing the design of the whole system. 
Unfortunately, ATL models are anything but open in this sense.
Theoretical complexity results for explicit models are as follows.
Model checking CTL and CTLK is P-complete, and can be done in
time O(ml), where m is the number of transitions in the model,
and l is the length of the formula [5]. Alternatively, it can be done
in time O(n2
l), where n is the number of states. Model checking
ATL is P-complete wrt. m, l and ΔP
3 -complete wrt. n, k, l (k being
the number of agents) [3, 12, 16]. Model checking ATLir is ΔP

2complete wrt. m, l and ΔP
3 -complete wrt. n, k, l [21, 13].
3.2 Compressed Representations
Explicit representation of all states and transitions is inefficient
in many ways. An alternative is to represent the state/transition
space in a symbolic way [17, 18].
Such models offer some hope for feasible model checking 
properties of open/multi-agent systems, although it is well known that
they are compact only in a fraction of all cases.4
For us, however,
they are insufficient for another reason: they are merely optimized
representations of explicit models. Thus, they are neither more
open nor better grounded: they were meant to optimize 
implementation rather than facilitate design or modeling methodology.
3.3 Interpreted Systems
Interpreted systems [11, 7] are held by many as a prime example
of computationally grounded models of distributed systems. An 
interpreted system can be defined as a tuple IS = St1, ..., Stk, Stenv, R, π .
St1, ..., Stk are local state spaces of agents 1, ..., k, and Stenv is the
set of states of the environment. The set of global states is defined
as St = St1 × ... × Stk × Stenv; R ⊆ St × St is a transition relation,
and π : Π → P(St). While the transition relation encapsulates
the (possible) evolution of the system over time, the epistemic 
dimension is defined by the local components of each global state:
q1, ..., qk, qenv ∼i q1, ..., qk, qenv iff qi = qi .
It is easy to see that such a representation is modular and 
compact as far as we are concerned with states. Moreover, it gives a
natural (grounded) approach to knowledge, and suggests an 
intuitive methodology for modeling epistemic states. Unfortunately,
the way transitions are represented in interpreted systems is neither
compact, nor modular, nor grounded: the temporal aspect of the
system is given by a joint transition function, exactly like in 
explicit models. This is not without a reason: if we separate activities
of the agents too much, we cannot model interaction in the 
framework any more, and interaction is the most interesting thing here.
But the bottom line is that the temporal dimension of an interpreted
system has exponential representation. And it is almost as difficult
to plug components in and out of an interpreted system, as for an
ordinary CTL or ATL model, since the local activity of an agent is
completely merged with his interaction with the rest of the system.
3.4 Concurrent Programs
The idea of concurrent programs has been long known in the
literature on distributed systems. Here, we use the formulation
from [15]. A concurrent program P is composed of k 
concurrent processes, each described by a labeled transition system Pi =
Sti, Acti, Ri, Πi, πi , where Sti is the set of local states of process
4
Representation R of an explicit model M is compact if the size of
R is logarithmic with respect to the size of M.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 899
i, Acti is the set of local actions, Ri ⊆ Sti ×Acti ×Sti is a transition
relation, and Πi, πi are the set of local propositions and their 
valuation. The behavior of program P is given by the product 
automaton of P1, ..., Pk under the assumption that processes work 
asynchronously, actions are interleaved, and synchronization is obtained
through common action names.
Concurrent programs have several advantages. First of all, they
are modular and compact. They allow for local modeling of 
components - much more so than interpreted systems (not only states,
but also actions are local here). Moreover, they allow for 
representing explicit interaction between local transitions of reactive 
processes, like willful communication, and synchronization. On the
other hand, they do not allow for representing implicit, 
incidental, or not entirely benevolent interaction between processes. For
example, if we want to represent the act of pushing somebody, the
pushed object must explicitly execute an action of being pushed,
which seems somewhat ridiculous. Side effects of actions are also
not easy to model. Still, this is a minor complaint in the context
of CTL, because for temporal logics we are only interested in the
flow of transitions, and not in the underlying actions. For temporal
reasoning about k asynchronous processes with no implicit 
interaction, concurrent programs seem just about perfect.
The situation is different when we talk about autonomous, 
proactive components (like agents), acting together (cooperatively or
adversely) in a common environment - and we want to address
their strategies and abilities. Now, particular actions are no less 
important than the resulting transitions. Actions may influence other
agents" local states without their consent, they may have side 
effects on other agents" states etc. Passing messages and/or calling
procedures is by no means the only way of interaction between
agents. Moreover, the availability of actions (to an agent) should
not depend on the actions that will be executed by other agents at
the same time - these are the outcome states that may depend on
these actions! Finally, we would often like to assume that agents act
synchronously. In particular, all agents play simultaneously in 
concurrent game structures. But, assuming synchrony and autonomy
of actions, synchronization can no longer be a means of 
coordination.
To sum up, we need a representation which is very much like
concurrent programs, but allows for modeling agents that play 
synchronously, and which enables modeling more sophisticated 
interaction between agents" actions. The first postulate is easy to satisfy,
as we show in the following section. The second will be addressed
in Section 4.
We note that model checking CTL against concurrent programs
is PSPACE-complete in the number of local states and the length
of the formula [15].
3.5 Synchronous CP and Simple Reactive 
Modules
The semantics of ATL is based on synchronous models where
availability of actions does not depend on the actions currently 
executed by the other players. A slightly different variant of 
concurrent programs can be defined via synchronous product of programs,
so that all agents play simultaneously.5
Unfortunately, under such
interpretation, no direct interaction between agents" actions can be
modeled at all.
DEFINITION 2. A synchronous concurrent program consists of
k concurrent processes Pi = Sti, Acti, Ri, Πi, πi with the 
follow5
The concept is not new, of course, and has already existed in folk
knowledge, although we failed to find an explicit definition in the
literature.
ing unfolding to a CGS: Agt = {1, ..., k}, St =
Qk
i=1 Sti, Act =
Sk
i=1 Acti, d(i, q1, ..., qk ) = {αi | qi, αi, qi ∈ Ri for some qi ∈
Sti}, o( q1, ..., qk , α1, ..., αk) = q1, ..., qk such that qi, αi, qi ∈
Ri for every i; Π =
Sk
i=1 Πi, and π(p) = πi(p) for p ∈ Πi.
We note that the simple reactive modules (SRML) from [22] can
be seen as a particular implementation of synchronous concurrent
programs.
DEFINITION 3. A SRML system is a tuple Σ, Π, m1, . . . , mk ,
where Σ = {1, . . . , k} is a set of modules (or agents), Π is a
set of Boolean variables, and, for each i ∈ Σ, we have mi =
ctri, initi, updatei , where ctri ⊆ Π. Sets initi and updatei consist
of guarded commands of the form φ ; v1 := ψ1; . . . ; vk := ψk,
where every vj ∈ ctri, and φ, ψ1, . . . , ψk are propositional 
formulae over Π. It is required that ctr1, . . . ctrk partitions Π.
The idea is that agent i controls the variables ctri. The init guarded
commands are used to initialize the controlled variables, while the
update guarded commands can change their values in each round.
A guarded command is enabled if the guard φ is true in the current
state of the system. In each round an enabled update guarded 
command is executed: each ψj is evaluated against the current state of
the system, and its logical value is assigned to vj. Several guarded
commands being enabled at the same time model non-deterministic
choice. Model checking ATL for SRML has been proved 
EXPTIMEcomplete in the size of the model and the length of the formula [22].
3.6 Concurrent Epistemic Programs
Concurrent programs (both asynchronous and synchronous) can
be used to encode epistemic relations too - exactly in the same
way as interpreted systems do [20]. That is, when unfolding a
concurrent program to a model of CTLK or ATLir, we define that
q1, ..., qk ∼i q1, ..., qk iff qi = qi . Model checking CTLK
against concurrent epistemic programs is PSPACE-complete [20].
SRML can be also interpreted in the same way; then, we would
assume that every agent can see only the variables he controls.
Concurrent epistemic programs are modular and have a grounded
semantics. They are usually compact (albeit not always: for 
example, an agent with perfect information will always blow up the size
of such a program). Still, they inherit all the problems of 
concurrent programs with perfect information, discussed in Section 3.4:
limited interaction between components, availability of local 
actions depending on the actual transition etc. The problems were
already important for agents with perfect information, but they 
become even more crucial when agents have only limited knowledge
of the current situation. One of the most important applications of
logics that combine strategic and epistemic properties is 
verification of communication protocols (e.g., in the context of security).
Now, we may want to, e.g., check agents" ability to pass an 
information between them, without letting anybody else intercept the
message. The point is that the action of intercepting is by definition
enabled; we just look for a protocol in which the transition of 
successful interception is never carried out. So, availability of actions
must be independent of the actions chosen by the other agents under
incomplete information. On the other hand, interaction is arguably
the most interesting feature of multi-agent systems, and it is really
hard to imagine models of strategic-epistemic logics, in which it is
not possible to represent communication.
3.7 Reactive Modules
Reactive modules [1] can be seen as a refinement of 
concurrent epistemic programs (primarily used by the MOCHA model
checker [4]), but they are much more powerful, expressive and
900 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
grounded. We have already mentioned a very limited variant of
RML (i.e., SRML). The vocabulary of RML is very close to 
implementations (in terms of general computational systems): the 
modules are essentially collections of variables, states are just 
valuations of variables; events/actions are variable updates. However,
the sets of variables controlled by different agents can overlap,
they can change over time etc. Moreover, reactive modules 
support incomplete information (through observability of variables),
although it is not the main focus of RML. Again, the relationship
between sets of observable variables (and to sets of controlled 
variables) is mostly left up to the designer of a system. Agents can act
synchronously as well as asynchronously.
To sum up, RML define a powerful framework for modeling 
distributed systems with various kinds of synchrony and asynchrony.
However, we believe that there is still a need for a simpler and
slightly more abstract class of representations. First, the 
framework of RML is technically complicated, involving a number 
auxiliary concepts and their definitions. Second, it is not always 
convenient to represent all that is going on in a multi-agent system
as reading and/or writing from/to program variables. This view
of a multi-agent system is arguably close to its computer 
implementation, but usually rather distant from the real world 
domainhence the need for a more abstract, and more conceptually flexible
framework. Third, the separation of the local complexity, and the
complexity of interaction is not straightforward. Our new proposal,
more in the spirit of interpreted systems, takes these observations
as the starting point. The proposed framework is presented in 
Section 4.
4. MODULAR INTERPRETED SYSTEMS
The idea behind distributed systems (multi-agent systems even
more so) is that we deal with several loosely coupled components,
where most of the processing goes on inside components (i.e., 
locally), and only a small fraction of the processing occurs between
the components. Interaction is crucial (which makes concurrent
programs an insufficient modeling tool), but it usually consumes
much less of the agent"s resources than local computations (which
makes the explicit transition tables of CGS, CEGS, and interpreted
systems an overkill). Modular interpreted systems, proposed here,
extrapolate the modeling idea behind interpreted systems in a way
that allows for a tight control of the interaction complexity.
DEFINITION 4. A modular interpreted system (MIS) is defined
as a tuple
S = Agt, env, Act, In ,
where Agt = {a1, ..., ak} is a set of agents, env is the environment,
Act is a set of actions, and In is a set of symbols called interaction
alphabet. Each agent has the following internal structure:
ai = Sti, di, outi, ini, oi, Πi, πi , where:
• Sti is a set of local states,
• di : Sti → P(Act) defines local availability of actions; for
convenience of the notation, we additionally define the set of
situated actions as Di = { qi, α | qi ∈ Sti, α ∈ di(qi)},
• outi, ini are interaction functions; outi : Di → In refers to
the influence that a given situated action (of agent ai) may
possibly have on the external world, and ini : Sti ×Ink
→ In
translates external manifestations of the other agents (and
the environment) into the impression that they make on
ai"s transition function depending on the local state of ai,
• oi : Di × In → Sti is a (deterministic) local transition 
function,
• Πi is a set of local propositions of agent ai where we require
that Πi and Πj are disjunct when i = j, and
• πi : Πi → P(Sti) is a valuation of these propositions.
The environment env = Stenv, outenv, inenv, oenv, Πenv, πenv has the
same structure as an agent except that it does not perform actions,
and that thus outenv : Stenv → In and oenv : Stenv × In → Stenv.
Within our framework, we assume that every action is executed
by an actor, that is, an agent. As a consequence, every actor is
explicitly represented in a MIS as an agent, just like in the case of
CGS and CEGS. The environment, on the other hand, represents the
(passive) context of agents" actions. In practice, it serves to capture
the aspects of the global state that are not observable by any of the
agents.
The input functions ini seem to be the fragile spots here: when
given explicitly as tables, they have size exponential wrt. the 
number of agents (and linear wrt. the size of In). However, we can
use, e.g., a construction similar to the one from [16] to represent
interaction functions more compactly.
DEFINITION 5. Implicit input function for state q ∈ Sti is given
by a sequence ϕ1, η1 , ..., ϕn, ηn , where each ηj ∈ In is an 
interaction symbol, and each ϕj is a boolean combination of 
propositions ˆηi
, with η ∈ In; ˆηi
stands for η is the symbol currently
generated by agent i. The input function is now defined as 
follows: ini(q, 1, ..., k, env) = ηj iff j is the lowest index such that
{ˆ1
1, ..., ˆk
k, ˆenv
env} |= ϕj. It is required that ϕn ≡ , so that the 
mapping is effective.
REMARK 1. Every ini can be encoded as an implicit input 
function, with each ϕj being of polynomial size with respect to the 
number of interaction symbols (cf. [16]).
Note that, for some domains, the MIS representation of a system
requires exponentially many symbols in the interaction alphabet In.
In such a case, the problem is inherent to the domain, and ini will
have size exponential wrt the number of agents.
4.1 Representing Agent Systems with MIS
Let Stg = (
Qk
i=1 Sti)×Stenv be the set of all possible global states
generated by a modular interpreted system S.
DEFINITION 6. The unfolding of a MIS S for initial states Q ⊆
Stg to a CEGS cegs(S, Q) = Agt , St , Π , π , Act , d , o , ∼1, ..., ∼k
is defined as follows:
• Agt = {1, ..., k} and Act = Act,
• St is the set of global states from Stg which are reachable
from some state in Q via the transition relation defined by o
(below),
• Π =
Sk
i=1 Πi ∪ Πenv,
• For each q = q1, . . . , qk, qenv ∈ St and i = 1, ..., k, env,
we define q ∈ π (p) iff p ∈ Πi and qi ∈ πi(p),
• d (i, q) = di(qi) for global state q = q1, ..., qk, qenv ,
• The transition function is constructed as follows. Let q =
q1, ..., qk, qenv ∈ St , and α = α1, ..., αk be an action
profile s.t. αi ∈ d (i, q). We define inputi(q, α) =
ini
`
qi, out1(q1, α1), . . . , outi−1(qi−1, αi−1), outi+1(qi+1, αi+1),
. . . , outk(qk, αk), outenv(qenv)
´
for each agent i = 1, . . . , k,
and inputenv(q, α) = inenv
`
qenv, out1(q1, α1), . . . , outk(qk, αk)
´
.
Then, o (q, α) = o1( q1, α1 , input1(q, α)), . . . ,
ok( qk, αk , inputk(q, α)), oenv(qenv, inputenv(q, α)) ;
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 901
• For each i = 1, ..., k: q1, ..., qk, qenv ∼i q1, ..., qk, qenv iff
qi = qi .6
REMARK 2. Note that MISs can be used as representations of
CGSs too. In that case, epistemic relations ∼i are simply omitted in
the unfolding. We denote the unfolding of a MIS S for initial states
Q into a CGS by cgs(S, Q).
Propositions 3 and 5 state that modular interpreted systems can
be used as representations for explicit models of multi-agent 
systems. On the other hand, these representations are not always 
compact, as demonstrated by Propositions 7 and 8.
PROPOSITION 3. For every CEGS M, there is a MIS SM
and a
set of global states Q of SM
such that cegs(SM
, Q) is isomorphic to
M.7
PROOF. Let M = {1, . . . , k}, St, Act, d, o, Π, π, ∼1, . . . , ∼k
be a CEGS. We construct a MIS SM
= {a1, . . . , ak}, env, Act, In
with agents ai = Sti, di, outi, ini, oi, Πi, πi and environment env =
Stenv, outenv, inenv, oenv, Πenv, πenv , plus a set Q ⊆ Stg of global
states, as follows.
• In = Act ∪ St ∪ (Actk−1
× St),
• Sti = {[q]∼i | q ∈ St} for 1 ≤ i ≤ k (i.e., Sti is the set of i"s
indistinguishability classes in M),
• Stenv = St,
• di([q]∼i ) = d(i, q) for 1 ≤ i ≤ k (this is well-defined since
d(i, q) = d(i, q ) whenever q ∼i q ),
• outi([q]∼i , αi) = αi for 1 ≤ i ≤ k; outenv(q) = q,
• ini([q]∼i , α1, . . . , αi−1, αi+1, . . . , αk, qenv) =
α1, . . . , αi−1, αi+1, . . . , αk, qenv for i ∈ {1, . . . , k};
inenv(q, α1 . . . , αk) = α1, . . . , αk ;
ini(x) and inenv(x) are arbitrary for other arguments x,
• oi( [q]∼i , αi , α1, . . . , αi−1, αi+1, . . . , αk, qenv ) =
[o(qenv, α1, . . . , αk)]∼i for 1 ≤ i ≤ k and αi ∈ di([q]∼i );
oenv(q, α1, . . . , αk ) = o(q, α1, . . . , αk);
oi and oenv are arbitrary for other arguments,
• Πi = ∅ for 1 ≤ i ≤ k, and Πenv = Π,
• πenv(p) = π(p)
• Q = { [q]∼1 , . . . , [q]∼k , q : q ∈ St}
Let M = cegs(SM
, Q) = Agt , St , Act , d , o , Π , π , ∼1, . . . , ∼k .
We argue that M and M are isomorphic by establishing a 
oneto-one correspondence between the respective sets of states, and
showing that the other parts of the structures agree on 
corresponding states.
First we show that, for any ˆq = [q ]∼1 , . . . , [q ]∼k , q ∈ Q
and any α = α1, . . . , αk such that αi ∈ d (i, ˆq ), we have
o (ˆq , α) = [q]∼1 , . . . , [q]∼k , q where q = o(q , α) (1)
Let ˆq = o (ˆq , α). Now, for any i: inputi(ˆq , α) = ini([q ]∼i ,
out1([q ]∼1 , α1), ..., outi−1([q ]∼i−1 , αi−1), outi+1([q ]∼i+1 , αi+1),
. . . , outk([q ]∼k , αk), outenv(q )) = ini([q ]∼i , α1, . . . , αi−1, αi+1,
6
This shows another difference between the environment and the
agents: the environment does not possess knowledge.
7
We say that two CEGS are isomorphic if they only differ in the
names of states and/or actions.
. . . , αk, q ) = α1, . . . , αi−1, αi+1, . . . , αk, q . Similarly, we get
that inputenv(ˆq , α) = α1, . . . , αk . Thus we get that o (ˆq , α) =
o1( [q ]∼1 , α1 , input1(ˆq , α)), . . . , ok( [q ]∼k , αk , inputk(ˆq , α)),
oenv(q , inputenv(ˆq , α)) = [o(q , α1, . . . , αk)]∼1 , . . . ,
[o(q , α1, . . . , αk)]∼k , o(q , α1, . . . , αk) . Thus, ˆq =
[q]∼1 , . . . , [q]∼k , q for q = o(q , α1, . . . , αk), which completes
the proof of (1).
We now argue that St = Q. Clearly, Q ⊆ St . Let ˆq ∈ St ;
we must show that ˆq ∈ Q. The argument is on induction on the
length of the least o path from Q to ˆq. The base case, ˆq ∈ Q, is
immediate. For the inductive step, ˆq = o (ˆq , α) for some ˆq ∈ Q,
and then we have that ˆq ∈ Q by (1). Thus, St = Q.
Now we have a one-to-one correspondence between St and St :
r ∈ St corresponds to [r]∼1 , . . . , [r]∼k , r ∈ St . It remains to
be shown that the other parts of the structures M and M agree on
corresponding states:
• Agt = Agt,
• Act = Act,
• Π =
Sk
i=1 Πi ∪ Πenv = Π,
• For p ∈ Π = Π: [q ]∼1 , . . . , [q ]∼k , q ∈ π (p) iff q ∈
πenv(p) iff q ∈ π(p) (same valuations at corresponding states),
• d (i, [q ]∼1 , . . . , [q ]∼k , q ) = di([q ]∼i ) = d(i, q),
• It follows immediately from (1), and the fact that Q = St ,
that o ( [q ]∼1 , . . . , [q ]∼k , q , α) = [r ]∼1 , . . . , [r ]∼k , r
iff o(q , α) = r (transitions on the same joint action in 
corresponding states lead to corresponding states),
• [q ]∼1 , . . . , [q ]∼k , q ∼i [r ]∼1 , . . . , [r ]∼k , r iff [q ]∼i =
[r ]∼i iff q ∼i r (the accessibility relations relate 
corresponding states), which completes the proof.
COROLLARY 4. For every CEGS M, there is an ATLir-equivalent
MIS S with initial states Q, that is, for every state q in M there is
a state q in cegs(S, Q) satisfying exactly the same ATLir formulae,
and vice versa.
PROPOSITION 5. For every CGS M, there is a MIS SM
and a set
of global states Q of SM
such that cgs(SM
, Q) is isomorphic to M.
PROOF. Let M = Agt, St, Act, d, o, Π, π be given. Now, let
ˆM = Agt, St, Act, d, o, Π, π, ∼1, . . . , ∼k for some arbitrary 
accessibility relations ∼i over St. By Proposition 3, there exists a MIS
S
ˆM
with global states Q such that ˆM = cegs(S
ˆM
, Q) is isomorphic
to ˆM. Let M be the CGS obtained by removing the accessibility
relations from ˆM . Clearly, M is isomorphic to M.
COROLLARY 6. For every CGS M, there is an ATL-equivalent
MIS S with initial states Q. That is, for every state q in M there is a
state q in cgs(S, Q) satisfying exactly the same ATL formulae, and
vice versa.
PROPOSITION 7. The local state spaces in a MIS are not 
always compact with respect to the underlying concurrent epistemic
game structure.
PROOF. Take a CEGS M in which agent i has always perfect 
information about the current global state of the system. When 
constructing a modular interpreted system S such that M = cegs(S, Q),
we have that Sti must be isomorphic with St.
902 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
The above property is a part of the interpreted systems heritage.
The next proposition stems from the fact that explicit models (and
interpreted systems) allow for intensive interaction between agents.
PROPOSITION 8. The size of In in S is, in general, exponential
with respect to the number of local states and local actions. This is
the case even when epistemic relations are not relevant (i.e., when
S is taken as a representation of an ordinary CGS).
PROOF. Consider a CGS M with agents Agt = {1, ..., k}, global
states St =
Qk
i=1{qi
0, ..., qi
i}, and actions Act = {0, 1}, all enabled
everywhere. The transition function is defined as
o( q1
j1
, ..., qk
jk
, α1, ..., αk) = q1
l1
, ..., qk
lk
, where li = (ji + α1 +
... + αk) mod i. Note that M can be represented as a modular 
interpreted system with succinct local state spaces Sti = {qi
0, ..., qi
i}.
Still, the current actions of all agents are relevant to determine the
resulting local transition of agent i.
We will call items In, outi, ini the interaction layer of a 
modular interpreted system S; the other elements of S constitute the local
layer of the MIS. In this paper we are ultimately interested in model
checking complexity with respect to the size of the local layer. To
this end, we will assume that the size of interaction layer is 
polynomial in the number of local states and actions. Note that, by
Propositions 7 and 8, not every explicit model submits to compact
representation with a MIS. Still, as we declared at the beginning of
Section 4, we are mainly interested in a modeling framework for
systems of loosely coupled components, where interaction is 
essential, but most processing is done locally anyway. More 
importantly, the framework of MIS allows for separating the interaction
of agents from their local structure to a larger extent. Moreover, we
can control and measure the complexity of each layer in a finer way
than before. First, we can try to abstract from the complexity of a
layer (e.g. like in this paper, by assuming that the other layer is kept
within certain complexity bounds). Second, we can also measure
separately the interaction complexity of different agents.
4.2 Modular Interpreted Systems vs. Simple
Reactive Modules
In this section we show that simple reactive modules are (as we
already suggested) a specific (and somewhat limited) 
implementation of modular interpreted systems. First, we define our (quite
strong) notion of equivalence of representations.
DEFINITION 7. Two representations are equivalent if they 
unfold to isomorphic concurrent epistemic game structures. They are
CGS-equivalent if they unfold to the same CGS.
PROPOSITION 9. For any SRML there is a CGS-equivalent MIS.
PROOF. Consider an SRML R with k modules and n variables.
We construct S = Agt, Act, In with Agt = {a1, ..., ak}, Act =
{ 1, ..., n, ⊥1, ..., ⊥n}, and In =
Sk
i=1 Sti × Sti (the local state
spaces Sti will be defined in a moment). Let us assume without loss
of generality that ctri = {x1, ..., xr}. Also, we consider all guarded
commands of i to be of type γi,ψ : ψ ; xi := , or γ⊥
i,ψ : ψ ;
xi := ⊥. Now, agent ai in S has the following components: Sti =
P(ctri) (i.e., local states of ai are valuations of variables controlled
by i); di(qi) = { 1, ..., r, ⊥1, ..., ⊥r}; outi(qi, α) = qi, qi ;
ini(qi, q1, q1 , ..., qi−1, qi−1 , qi+1, qi+1 , qk, qk ) =
{xi ∈ ctri | q1, ..., qk |=
W
γi,ψ
ψ}, {xi ∈ ctri | q1, ..., qk |=
W
γ⊥
i,ψ
ψ} . To define local transitions, we consider three cases. If
t = f = ∅ (no update is enabled), then oi(qi, α, t, f ) = qi for
every action α. If t = ∅, we take any arbitrary ˆx ∈ t, and 
define oi(qi, j, t, f ) = qi ∪ {xj} if xj ∈ t, and qi ∪ {ˆx} otherwise;
oi(qi, ⊥j, t, f ) = qi \ {xj} if xj ∈ f, and qi ∪ {ˆx} otherwise.
Moreover, if t = ∅ = f, we take any arbitrary ˆx ∈ f, and 
define oi(qi, j, t, f ) = qi ∪ {xj} if xj ∈ t, and qi \ {ˆx} otherwise;
oi(qi, ⊥j, t, f ) = qi \{xj} if xj ∈ f, and qi \{ˆx} otherwise. Finally,
Πi = ctri, and qi ∈ πi(xj) iff xj ∈ qi.
The above construction shows that SRML have more compact
representation of states than MIS: ri local variables of agent i give
rise to 2ri
local states. In a way, reactive modules (both simple
and full) are two-level representations: first, the system is 
represented as a product of modules; next, each module can be seen
as a product of its variables (together with their update operations).
Note, however, that specification of updates with respect to a single
variable in an SRML may require guarded commands of total length
O(2
Pk
i=1 ri
). Thus, the representation of transitions in SRML is (in
the worst case) no more compact than in MIS, despite the two-level
structure of SRML. We observe finally that MIS are more general,
because in SRML the current actions of other agents have no 
influence on the outcome of agent i"s current action (although the
outcome can be influenced by other agents" current local states).
4.3 Model Checking Modular Interpreted 
Systems
One of our main aims was to study the complexity of symbolic
model checking ATLir in a meaningful way. Following the 
reviewers" remarks, we state our complexity results only as conjectures.
Preliminary proofs can be found in [14].
CONJECTURE 10. Model checking ATL for modular interpreted
systems is EXPTIME-complete.
CONJECTURE 11. Model checking ATLir for the class of 
modular interpreted systems is PSPACE-complete.
A summary of complexity results for model checking 
temporal and strategic logics (with and without epistemic component)
is given in the table below. The table presents completeness 
results for various models and settings of input parameters. Symbols
n, k, m stand for the number of states, agents and transitions in an
explicit model; l is the length of the formula, and nlocal is the 
number of local states in a concurrent program or modular interpreted
system. The new results, conjectured in this paper, are printed in
italics. Note that the result for model checking ATL against modular
interpreted systems is an extension of the result from [22].
m, l n, k, l nlocal, k, l
CTL P [5] P [5] PSPACE [15]
CTLK P [5, 8] P [5, 8] PSPACE [20]
ATL P [3] ΔP
3 [12, 16] EXPTIME
ATLir ΔP
2 [21, 13] ΔP
3 [13] PSPACE
If we are right, then the results for ATL and ATLir form an 
intriguing pattern. When we compare model checking agents with
perfect vs. imperfect information, the first problem appears to be
much easier against explicit models measured with the number of
transitions; next, we get the same complexity class against explicit
models measured with the number of states and agents; finally,
model checking imperfect information turns out to be easier than
model checking perfect information for modular interpreted 
systems. Why can it be so?
First, a MIS unfolds into CEGS and CGS in a different way. In
the first case, the MIS is assumed to encode the epistemic relations
explicitly (which makes it explode when we model agents with 
perfect, or almost perfect information). In the latter case, the epistemic
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 903
aspect is ignored, which gives some extra room for encoding the
transition relation more efficiently. Another crucial factor is the
number of available strategies (relative to the size of input 
parameters). The number of all strategies is exponential in the number
of global states; for uniform strategies, there are usually much less
of them but still exponentially many in general. Thus, the fact that
perfect information strategies can be synthesized incrementally has
a substantial impact on the complexity of the problem. However,
measured in terms of local states and agents, the number of all
strategies is doubly exponential, while there are only 
exponentially many uniform strategies - which settles the results in favor of
imperfect information.
5. CONCLUSIONS
We have presented a new class of representations for open 
multiagent systems. Our representations, called modular interpreted 
systems, are: modular, in the sense that components can be changed,
replaced, removed or added, with as little changes to the whole 
representation as possible; more compact than traditional explicit 
representations; and grounded, in the sense that the correspondences
between the primitives of the model and the entities being 
modeled are more immediate, giving a methodology for designing and
implementing systems. We also conjecture that the complexity of
model checking strategic ability for our representations is higher if
we assume perfect information than if we assume imperfect 
information.
The solutions, proposed in this paper, are not necessarily 
perfect (for example, the impression functions ini seem to be the
main source of non-modularity in MIS, and can be perhaps 
improved), but we believe them to be a step in the right direction.
We also do not mean to claim that our representations should 
replace more elaborate modeling languages like Promela or reactive
modules. We only suggest that there is a need for compact, modular
and reasonably grounded models that are more expressive than 
concurrent (epistemic) programs, and still allow for easier theoretical
analysis than reactive modules. We also suggest that MIS might be
better suited for modeling simple multi-agent domains, especially
for human-oriented (as opposed to computer-oriented) design.
6. ACKNOWLEDGMENTS
We thank the anonymous reviewers and Andrzej Tarlecki for
their helpful remarks. Thomas Ågotnes" work on this paper was
supported by grants 166525/V30 and 176853/S10 from the 
Research Council of Norway.
7. REFERENCES
[1] R. Alur and T. A. Henzinger. Reactive modules. Formal
Methods in System Design, 15(1):7-48, 1999.
[2] R. Alur, T. A. Henzinger, and O. Kupferman.
Alternating-time Temporal Logic. Lecture Notes in
Computer Science, 1536:23-60, 1998.
[3] R. Alur, T. A. Henzinger, and O. Kupferman.
Alternating-time Temporal Logic. Journal of the ACM,
49:672-713, 2002.
[4] R. Alur, T.A. Henzinger, F.Y.C. Mang, S. Qadeer, S.K.
Rajamani, and S. Tasiran. MOCHA user manual. In
Proceedings of CAV"98, volume 1427 of Lecture Notes in
Computer Science, pages 521-525, 1998.
[5] E.M. Clarke, E.A. Emerson, and A.P. Sistla. Automatic
verification of finite-state concurrent systems using temporal
logic specifications. ACM Transactions on Programming
Languages and Systems, 8(2):244-263, 1986.
[6] E.A. Emerson and J.Y. Halpern. "sometimes" and "not never"
revisited: On branching versus linear time temporal logic. In
Proceedings of the Annual ACM Symposium on Principles of
Programming Languages, pages 151-178, 1982.
[7] R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi.
Reasoning about Knowledge. MIT Press: Cambridge, MA,
1995.
[8] M. Franceschet, A. Montanari, and M. de Rijke. Model
checking for combined logics. In Proceedings of the 3rd
International Conference on Temporal Logic (ICTL), 2000.
[9] V. Goranko and W. Jamroga. Comparing semantics of logics
for multi-agent systems. Synthese, 139(2):241-280, 2004.
[10] J. Y. Halpern. Reasoning about knowledge: a survey. In
D. M. Gabbay, C. J. Hogger, and J. A. Robinson, editors, The
Handbook of Logic in Artificial Intelligence and Logic
Programming, Volume IV, pages 1-34. Oxford University
Press, 1995.
[11] J.Y. Halpern and R. Fagin. Modelling knowledge and action
in distributed systems. Distributed Computing,
3(4):159-177, 1989.
[12] W. Jamroga and J. Dix. Do agents make model checking
explode (computationally)? In M. P˘echou˘cek, P. Petta, and
L.Z. Varga, editors, Proceedings of CEEMAS 2005, volume
3690 of Lecture Notes in Computer Science, pages 398-407.
Springer Verlag, 2005.
[13] W. Jamroga and J. Dix. Model checking abilities of agents:
A closer look. Submitted, 2006.
[14] W. Jamroga and T. Ågotnes. Modular interpreted systems: A
preliminary report. Technical Report IfI-06-15, Clausthal
University of Technology, 2006.
[15] O. Kupferman, M.Y. Vardi, and P. Wolper. An
automata-theoretic approach to branching-time model
checking. Journal of the ACM, 47(2):312-360, 2000.
[16] F. Laroussinie, N. Markey, and G. Oreiby. Expressiveness
and complexity of ATL. Technical Report LSV-06-03, CNRS
& ENS Cachan, France, 2006.
[17] K.L. McMillan. Symbolic Model Checking: An Approach to
the State Explosion Problem. Kluwer Academic Publishers,
1993.
[18] K.L. McMillan. Applying SAT methods in unbounded
symbolic model checking. In Proceedings of CAV"02,
volume 2404 of Lecture Notes in Computer Science, pages
250-264, 2002.
[19] W. Penczek and A. Lomuscio. Verifying epistemic properties
of multi-agent systems via bounded model checking. In
Proceedings of AAMAS"03, pages 209-216, New York, NY,
USA, 2003. ACM Press.
[20] F. Raimondi and A. Lomuscio. The complexity of symbolic
model checking temporal-epistemic logics. In L. Czaja,
editor, Proceedings of CS&P"05, 2005.
[21] P. Y. Schobbens. Alternating-time logic with imperfect
recall. Electronic Notes in Theoretical Computer Science,
85(2), 2004.
[22] W. van der Hoek, A. Lomuscio, and M. Wooldridge. On the
complexity of practical ATL model checking. In P. Stone and
G. Weiss, editors, Proceedings of AAMAS"06, pages
201-208, 2006.
904 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
An Agent-Based Approach for Privacy-Preserving
Recommender Systems
Richard Cissée
DAI-Labor, TU Berlin
Franklinstrasse 28/29
10587 Berlin
richard.cissee@dai-labor.de
Sahin Albayrak
DAI-Labor, TU Berlin
Franklinstrasse 28/29
10587 Berlin
sahin.albayrak@dai-labor.de
ABSTRACT
Recommender Systems are used in various domains to 
generate personalized information based on personal user data.
The ability to preserve the privacy of all participants is an 
essential requirement of the underlying Information Filtering
architectures, because the deployed Recommender Systems
have to be accepted by privacy-aware users as well as 
information and service providers. Existing approaches neglect
to address privacy in this multilateral way.
We have developed an approach for privacy-preserving
Recommender Systems based on Multi-Agent System 
technology which enables applications to generate 
recommendations via various filtering techniques while preserving the
privacy of all participants. We describe the main modules of
our solution as well as an application we have implemented
based on this approach.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: 
Information Search and Retrieval-Information Filtering; I.2.11
[Artificial Intelligence]: Distributed Artificial 
Intelligence-Multiagent Systems
General Terms
Management, Security, Human Factors, Standardization
1. INTRODUCTION
Information Filtering (IF) systems aim at countering 
information overload by extracting information that is 
relevant for a given user out of a large body of information
available via an information provider. In contrast to 
Information Retrieval (IR) systems, where relevant information
is extracted based on search queries, IF architectures 
generate personalized information based on user profiles 
containing, for each given user, personal data, preferences, and
rated items. The provided body of information is usually
structured and collected in provider profiles. Filtering 
techniques operate on these profiles in order to generate 
recommendations of items that are probably relevant for a given
user, or in order to determine users with similar interests,
or both. Depending on the respective goal, the resulting
systems constitute Recommender Systems [5], Matchmaker
Systems [10], or a combination thereof.
The aspect of privacy is an essential issue in all IF systems:
Generating personalized information obviously requires the
use of personal data. According to surveys indicating major
privacy concerns of users in the context of Recommender
Systems and e-commerce in general [23], users can be 
expected to be less reluctant to provide personal information
if they trust the system to be privacy-preserving with regard
to personal data. Similar considerations also apply to the 
information provider, who may want to control the 
dissemination of the provided information, and to the provider of the
filtering techniques, who may not want the details of the 
utilized filtering algorithms to become common knowledge. A
privacy-preserving IF system should therefore balance these
requirements and protect the privacy of all parties involved
in a multilateral way, while addressing general requirements
regarding performance, security and quality of the 
recommendations as well. As described in the following section,
there are several approaches with similar goals, but none of
these provide a generic approach in which the privacy of all
parties is preserved.
We have developed an agent-based approach for 
privacypreserving IF which has been utilized for realizing a 
combined Recommender/Matchmaker System as part of an 
application supporting users in planning entertainment-related
activities. In this paper, we focus on the Recommender 
System functionality. Our approach is based on Multi-Agent
System (MAS) technology because fundamental features of
agents such as autonomy, adaptability and the ability to
communicate are essential requirements of our approach. In
other words, the realized approach does not merely 
constitute a solution for privacy-preserving IF within a MAS
context, but rather utilizes a MAS architecture in order to
realize a solution for privacy-preserving IF, which could not
be realized easily otherwise.
The paper is structured as follows: Section 2 describes
related work. Section 3 describes the general ideas of our
approach. In Section 4, we describe essential details of the
319
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
modules of our approach and their implementation. In 
Section 5, we evaluate the approach, mainly via the realized
application. Section 6 concludes the paper with an outlook
and outlines further work.
2. RELATED WORK
There is a large amount of work in related areas, such as
Private Information Retrieval [7], Privacy-Preserving Data
Mining [2], and other privacy-preserving protocols [4, 16],
most of which is based on Secure Multi-Party Computation
[27]. We have ruled out Secure Multi-Party Computation
approaches mainly because of their complexity, and because
the algorithm that is computed securely is not considered to
be private in these approaches.
Various enforcement mechanisms have been suggested
that are applicable in the context of privacy-preserving 
Information Filtering, such as enterprise privacy policies [17]
or hippocratic databases [1], both of which annotate user
data with additional meta-information specifying how the
data is to be handled on the provider side. These approaches
ultimately assume that the provider actually intends to 
protect the privacy of the user data, and offer support for this
task, but they are not intended to prevent the provider
from acting in a malicious manner. Trusted computing, as
specified by the Trusted Computing Group, aims at 
realizing trusted systems by increasing the security of open 
systems to a level comparable with the level of security that
is achievable in closed systems. It is based on a 
combination of tamper-proof hardware and various software 
components. Some example applications, including peer-to-peer
networks, distributed firewalls, and distributed computing
in general, are listed in [13].
There are some approaches for privacy-preserving 
Recommender Systems based on distributed collaborative filtering,
in which recommendations are generated via a public model
aggregating the distributed user profiles without containing
explicit information about user profiles themselves. This
is achieved via Secure Multi-Party Computation [6], or via
random perturbation of the user data [20]. In [19], 
various approaches are integrated within a single architecture.
In [10], an agent-based approach is described in which user
agents representing similar users are discovered via a 
transitive traversal of user agents. Privacy is preserved through
pseudonymous interaction between the agents and through
adding obfuscating data to personal information. More 
recent related approaches are described in [18].
In [3], an agent-based architecture for privacy-preserving
demographic filtering is described which may be 
generalized in order to support other kinds of filtering techniques.
While in some aspects similar to our approach, this 
architecture addresses at least two aspects inadequately, namely
the protection of the filter against manipulation attempts,
and the prevention of collusions between the filter and the
provider.
3. PRIVACY-PRESERVING 
INFORMATION FILTERING
We identify three main abstract entities participating in
an information filtering process within a distributed system:
A user entity, a provider entity, and a filter entity. Whereas
in some applications the provider and filter entities 
explicitly trust each other, because they are deployed by the same
party, our solution is applicable more generically because it
does not require any trust between the main abstract 
entities. In this paper, we focus on aspects related to the
information filtering process itself, and omit all aspects 
related to information collection and processing, i.e. the stages
in which profiles are generated and maintained, mainly 
because these stages are less critical with regard to privacy, as
they involve fewer different entities.
3.1 Requirements
Our solution aims at meeting the following requirements
with regard to privacy:
• User Privacy: No linkable information about user 
profiles should be acquired permanently by any other 
entity or external party, including other user entities.
Single user profile items, however, may be acquired
permanently if they are unlinkable, i.e. if they 
cannot be attributed to a specific user or linked to other
user profile items. Temporary acquisition of private
information is permitted as well. Sets of 
recommendations may be acquired permanently by the provider,
but they should not be linkable to a specific user.
These concessions simplify the resulting protocol and
allow the provider to obtain recommendations and 
single unlinkable user profile items, and thus to determine
frequently requested information and optimize the 
offered information accordingly.
• Provider Privacy: No information about provider 
profiles, with the exception of the recommendations,
should be acquired permanently by other entities or
external parties. Again, temporary acquisition of 
private information is permitted. Additionally, the 
propagation of provider information is entirely under the
control of the provider. Thus, the provider is enabled
to prevent misuse such as the automatic large-scale
extraction of information.
• Filter Privacy: Details of the algorithms applied by
the filtering techniques should not be acquired 
permanently by any other entity or external party. General
information about the algorithm may be provided by
the filter entity in order to help other entities to reach
a decision on whether to apply the respective filtering
technique.
In addition, general requirements regarding the quality
of the recommendations as well as security aspects, 
performance and broadness of the resulting system have to be 
addressed as well. While minor trade-offs may be acceptable,
the resulting system should reach a level similar to regular
Recommender Systems with regard to these requirements.
3.2 Outline of the Solution
The basic idea for realizing a protocol fulfilling these
privacy-related requirements in Recommender Systems is
implied by allowing the temporary acquisition of private
information (see [8] for the original approach): User and
provider entity both propagate the respective profile data to
the filter entity. The filter entity provides the 
recommendations, and subsequently deletes all private information, thus
fulfilling the requirement regarding permanent acquisition
of private information.
320 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
The entities whose private information is propagated have
to be certain that the respective information is actually 
acquired temporarily only. Trust in this regard may be 
established in two main ways:
• Trusted Software: The respective entity itself is trusted
to remove the respective information as specified.
• Trusted Environment: The respective entity operates
in an environment that is trusted to control the 
communication and life cycle of the entity to an extent
that the removal of the respective information may
be achieved regardless of the attempted actions of the
entity itself. Additionally, the environment itself is
trusted not to act in a malicious manner (e.g. it is
trusted not to acquire and propagate the respective
information itself).
In both cases, trust may be established in various ways.
Reputation-based mechanisms, additional trusted third 
parties certifying entities or environments, or trusted 
computing mechanisms may be used. Our approach is based on a
trusted environment realized via trusted computing 
mechanisms, because we see this solution as the most generic
and realistic approach. This decision is discussed briefly in
Section 5.
We are now able to specify the abstract information 
filtering protocol as shown in Figure 1: The filter entity deploys a
Temporary Filter Entity (TFE) operating in a trusted 
environment. The user entity deploys an additional relay entity
operating in the same environment. Through mechanisms
provided by this environment, the relay entity is able to 
control the communication of the TFE, and the provider entity
is able to control the communication of both relay entity
and the TFE. Thus, it is possible to ensure that the 
controlled entities are only able to propagate recommendations,
but no other private information. In the first stage (steps
1.1 to 1.3 of Figure 1), the relay entity establishes control of
the TFE, and thus prevents it from propagating user profile
information. User profile data is propagated without 
participation of the provider entity from the user entity to the
TFE via the relay entity. In the second stage (steps 2.1 to
2.3 of Figure 1), the provider entity establishes control of
both relay and TFE, and thus prevents them from 
propagating provider profile information. Provider profile data is
propagated from the provider entity to the TFE via the 
relay entity. In the third stage (steps 3.1 to 3.5 of Figure 1),
the TFE returns the recommendations via the relay entity,
and the controlled entities are terminated. Taken together,
these steps ensure that all private information is acquired
temporarily only by the other main entities. The problems
of determining acceptable queries on the provider profile and
ensuring unlinkability of the recommendations are discussed
in the following section.
Our approach requires each entity in the distributed 
architecture to have the following five main abilities: The ability
to perform certain well-defined tasks (such as carrying out a
filtering process) with a high degree of autonomy, i.e. largely
independent of other entities (e.g. because the respective 
entity is not able to communicate in an unrestricted manner),
the ability to be deployable dynamically in a well-defined 
environment, the ability to communicate with other entities,
the ability to achieve protection against external 
manipulation attempts, and the ability to control and restrict the
communication of other entities.
Figure 1: The abstract privacy-preserving 
information filtering protocol. All communication across
the environments indicated by dashed lines is 
prevented with the exception of communication with
the controlling entity.
MAS architectures are an ideal solution for realizing a
distributed system characterized by these features, because
they provide agents constituting entities that are actually
characterized by autonomy, mobility and the ability to 
communicate [26], as well as agent platforms as environments
providing means to realize the security of agents. In this
context, the issue of malicious hosts, i.e. hosts attacking
agents, has to be addressed explicitly. Furthermore, existing
MAS architectures generally do not allow agents to control
the communication of other agents. It is possible, however,
to expand a MAS architecture and to provide designated
agents with this ability. For these reasons, our solution is
based on a FIPA[11]-compliant MAS architecture. The 
entities introduced above are mapped directly to agents, and
the trusted environment in which they exist is realized in
the form of agent platforms.
In addition to the MAS architecture itself, which is 
assumed as given, our solution consists of the following five
main modules:
• The Controller Module described in Section 4.1 
provides functionality for controlling the communication
capabilities of agents.
• The Transparent Persistence Module facilitates
the use of different data storage mechanisms, and 
provides a uniform interface for accessing persistent 
information, which may be utilized for monitoring critical
interactions involving potentially private information
e.g. as part of queries. Its description is outside the
scope of this paper.
• The Recommender Module, details of which are 
described in Section 4.2, provides Recommender System
functionality.
• The Matchmaker Module provides Matchmaker
System functionality. It additionally utilizes social 
aspects of MAS technology. Its description is outside the
scope of this paper.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 321
• Finally, a separate module described in Section 4.4
provides Exemplary Filtering Techniques in order
to show that various restrictions imposed on filtering
techniques by our approach may actually be fulfilled.
The trusted environment introduced above encompasses
the MAS architecture itself and the Controller Module,
which have to be trusted to act in a non-malicious manner
in order to rule out the possibility of malicious hosts.
4. MAIN MODULES AND 
IMPLEMENTATION
In this section, we describe the main modules of our 
approach, and outline the implementation. While we have 
chosen a specific architecture for the implementation, the 
specification of the module is applicable to any FIPA-compliant
MAS architecture. A module basically encompasses 
ontologies, functionality provided by agents via agent services, and
internal functionality. Throughout this paper, {m}KX 
denotes a message m encrypted via a non-specified symmetric
encryption scheme with a secret key KX used for 
encryption and decryption which is initially known only to 
participant X. A key KXY is a key shared by participants
X and Y . A cryptographic hash function is used at 
various points of the protocol, i.e. a function returning a hash
value h(x) for given data x that is both preimage-resistant
and collision-resistant1
. We denote a set of hash values for
a data set X = {x1, .., xn} as H(X) = {h(x1), .., h(xn)},
whereas h(X) denotes a single hash value of a data set.
4.1 Controller Module
As noted above, the ability to control the communication
of agents is generally not a feature of existing MAS 
architectures2
but at the same time a central requirement of our
approach for privacy-preserving Information Filtering. The
required functionality cannot be realized based on regular
agent services or components, because an agent on a 
platform is usually not allowed to interfere with the actions of
other agents in any way. Therefore, we add additional 
infrastructure providing the required functionality to the MAS
architecture itself, resulting in an agent environment with
extended functionality and responsibilities.
Controlling the communication capabilities of an agent is
realized by restricting via rules, in a manner similar to a
firewall, but with the consent of the respective agent, its
incoming and outgoing communication to specific platforms
or agents on external platforms as well as other possible
communication channels, such as the file system. Consent
is required because otherwise the overall security would be
compromised, as attackers could arbitrarily block various
communication channels. Our approach does not require
controlling the communication between agents on the same
platform, and therefore this aspect is not addressed. 
Consequently, all rules addressing communication capabilities
have to be enforced across entire platforms, because 
otherwise a controlled agent could just use a non-controlled agent
1
In the implementation, we have used the Advanced 
Encryption Standard (AES) as the symmetric encryption scheme
and SHA-1 as the cryptographic hash function.
2
A recent survey on agent environments [24] concludes that
aspects related to agent environments are often neglected,
and does not indicate any existing work in this particular
area.
on the same platform as a relay for communicating with
agents residing on external platforms. Various agent services
provide functionality for adding and revoking control of 
platforms, including functionality required in complex scenarios
where controlled agents in turn control further platforms.
The implementation of the actual control mechanism 
depends on the actual MAS architecture. In our 
implementation, we have utilized methods provided via the Java 
Security Manager as part of the Java security model. Thus,
the supervisor agent is enabled to define custom security
policies, thereby granting or denying other agents access to
resources required for communication with other agents as
well as communication in general, such as files or sockets for
TCP/IP-based communication.
4.2 Recommender Module
The Recommender Module is mainly responsible for 
carrying out information filtering processes, according to the
protocol described in Table 1. The participating entities are
realized as agents, and the interactions as agent services. We
assume that mechanisms for secure agent communication are
available within the respective MAS architecture. Two 
issues have to be addressed in this module: The relevant parts
of the provider profile have to be retrieved without 
compromising the user"s privacy, and the recommendations have to
be propagated in a privacy-preserving way.
Our solution is based on a threat model in which no main
abstract entity may safely assume any other abstract entity
to act in an honest manner: Each entity has to assume that
other entities may attempt to obtain private information,
either while following the specified protocol or even by 
deviating from the protocol. According to [15], we classify the
former case as honest-but-curious behavior (as an example,
the TFE may propagate recommendations as specified, but
may additionally attempt to propagate private information),
and the latter case as malicious behavior (as an example, the
filter may attempt to propagate private information instead
of the recommendations).
4.2.1 Retrieving the Provider Profile
As outlined above, the relay agent relays data between
the TFE agent and the provider agent. These agents are
not allowed to communicate directly, because the TFE agent
cannot be assumed to act in an honest manner. Unlike the
user profile, which is usually rather small, the provider 
profile is often too voluminous to be propagated as a whole
efficiently. A typical example is a user profile containing
ratings of about 100 movies, while the provider profile 
contains some 10,000 movies. Retrieving only the relevant part
of the provider profile, however, is problematic because it
has to be done without leaking sensitive information about
the user profile. Therefore, the relay agent has to analyze all
queries on the provider profile, and reject potentially critical
queries, such as queries containing a set of user profile items.
Because the propagation of single unlinkable user profile
items is assumed to be uncritical, we extend the 
information filtering protocol as follows: The relevant parts of the
provider profile are retrieved based on single anonymous 
interactions between the relay and the provider. If the MAS
architecture used for the implementation does not provide
an infrastructure for anonymous agent communication, this
feature has to be provided explicitly: The most 
straightforward way is to use additional relay agents deployed via
322 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Table 1: The basic information filtering protocol
with participants U = user agent, P = provider
agent, F = TFE agent, R = relay agent, based on the
abstract protocol shown in Figure 1. UP denotes the
user profile with UP = {up1, .., upn}, PP denotes the
provider profile, and REC denotes the set of 
recommendations with REC = {rec1, .., recm}.
Phase. Sender → Message or Action
Step Receiver
1.1 R → F establish control
1.2 U → R UP
1.3 R → F UP
2.1 P → R, F establish control
2.2 P → R PP
2.3 R → F PP
3.1 F → R REC
3.2 R → P REC
3.3 P → U REC
3.4 R → F terminate F
3.5 P → R terminate R
the main relay agent and used once for a single anonymous
interaction. Obviously, unlinkability is only achieved if 
multiple instances of the protocol are executed simultaneously
between the provider and different users. Because agents
on controlled platforms are unable to communicate 
anonymously with the respective controlling agent, control has to
be established after the anonymous interactions have been
completed. To prevent the uncontrolled relay agents from
propagating provider profile data, the respective data is 
encrypted and the key is provided only after control has been
established. Therefore, the second phase of the protocol 
described in Table 1 is replaced as described in Table 2.
Additionally, the relay agent may allow other interactions
as long as no user profile items are used within the queries.
In this case, the relay agent has to ensure that the provider
does not obtain any information exceeding the information
deducible via the recommendations themselves. The 
clusterbased filtering technique described in Section 4.3 is an 
example for a filtering technique operating in this manner.
4.2.2 Recommendation Propagation
The propagation of the recommendations is even more
problematic mainly because more participants are involved:
Recommendations have to be propagated from the TFE
agent via the relay and provider agent to the user agent. No
participant should be able to alter the recommendations or
use them for the propagation of private information. 
Therefore, every participant in this chain has to obtain and verify
the recommendations in unencrypted form prior to the next
agent in the chain, i.e. the relay agent has to verify the 
recommendations before the provider obtains them, and so on.
Therefore, the final phase of the protocol described in Table
1 is replaced as described in Table 3. It basically consists of
two parts (Step 3.1 to 3.4, and Step 3.5 to Step 3.8), each of
which provide a solution for a problem related to the 
prisoners" problem [22], in which two participants (the prisoners)
intend to exchange a message via a third, untrusted 
participant (the warden) who may read the message but must not
be able to alter it in an undetectable manner. There are 
various solutions for protocols addressing the prisoners" 
probTable 2: The updated second stage of the 
information filtering protocol with definitions as above. PPq
is the part of the provider profile PP returned as the
result of the query q.
Phase. Sender → Message or Action
Step Receiver
repeat 2.1 to 2.3 ∀ up ∈ UP:
2.1 F → R q(up) (a query based on up)
2.2 R anon
→ P q(up) (R remains anonymous)
2.3 P → R anon
{PPq(up)}KP
2.4 P → R, F establish control
2.5 P → R KP
2.6 R → F PPq(UP )
Table 3: The updated final stage of the information
filtering protocol with definitions as above.
Phase. Sender → Message or Action
Step Receiver
3.1 F → R REC, {H(REC)}KPF
3.2 R → P h(KR), {{H(REC)}KPF }KR
3.3 P → R KP F
3.4 R → P KR
repeat 3.5 ∀ rec ∈ REC:
3.5 R → P {rec}KURrec
repeat 3.6 ∀ rec ∈ REC:
3.6 P → U h(KPrec ), {{rec}KURrec
}KPrec
repeat 3.7 to 3.8 ∀ rec ∈ REC:
3.7 U → P KURrec
3.8 P → U KPrec
3.9 U → F terminate F
3.10 P → U terminate U
lem. The more obvious of these, however, such as protocols
based on the use of digital signatures, introduce additional
threats e.g. via the possibility of additional subliminal 
channels [22]. In order to minimize the risk of possible threats,
we have decided to use a protocol that only requires a 
symmetric encryption scheme.
The first part of the final phase is carried out as follows:
In order to prevent the relay from altering 
recommendations, they are propagated by the filter together with an
encrypted hash in Step 3.1. Thus, the relay is able to verify
the recommendations before they are propagated further.
The relay, however, may suspect the data propagated as
the encrypted hash to contain private information instead
of the actual hash value. Therefore, the encrypted hash is
encrypted again and propagated together with a hash on
the respective key in Step 3.2. In Step 3.3, the key KP F
is revealed to the relay, allowing the relay to validate the
encrypted hash. In Step 3.4, the key KR is revealed to the
provider, allowing the provider to decrypt the data received
in Step 3.2 and thus to obtain H(REC). Propagating the
hash of the key KR prevents the relay from altering the 
recommendations to REC after Step 3.3, which would be 
undetectable otherwise because the relay could choose a key
KR so that {{H(REC)}KPF }KR = {{H(REC )}KPF }KR
.
The encryption scheme used for encrypting the hash has to
be secure against known-plaintext attacks, because 
otherwise the relay may be able to obtain KP F after Step 3.1 and
subsequently alter the recommendations in an undetectable
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 323
way. Additionally, the encryption scheme must not be 
commutative for similar reasons.
The remaining protocol steps are interactions between 
relay, provider and user agent. The interactions of Step 3.5
to Step 3.8 ensure, via the same mechanisms used in Step
3.1 to 3.4, that the provider is able to analyze the 
recommendations before the user obtains them, but at the same
time prevent the provider from altering the 
recommendations. Additionally, the recommendations are not processed
at once, but rather one at a time, to prevent the provider
from withholding all recommendations.
Upon completion of the protocol, both user and provider
have obtained a set of recommendations. If the user wants
these recommendations to be unlinkable to himself, the user
agent has to carry out the entire protocol anonymously.
Again, the most straightforward way to achieve this is to use
additional relay agents deployed via the user agent which are
used once for a single information filtering process.
4.3 Exemplary Filtering Techniques
The filtering technique applied by the TFE agent cannot
be chosen freely: All collaboration-based approaches, such
as collaborative filtering techniques based on the profiles
of a set of users, are not applicable because the provider
profile does not contain user profile data (unless this data
has been collected externally). Instead, these approaches
are realized via the Matchmaker Module, which is outside
the scope of this paper. Learning-based approaches are not
applicable because the TFE agent cannot propagate any 
acquired data to the filter, which effectively means that the
filter is incapable of learning. Filtering techniques that are
actually applicable are feature-based approaches, such as
content-based filtering (in which profile items are compared
via their attributes) and knowledge-based filtering (in which
domain-specific knowledge is applied in order to match user
and provider profile items). An overview of different classes
and hybrid combinations of filtering techniques is given in
[5]. We have implemented two generic content-based 
filtering approaches that are applicable within our approach:
A direct content-based filtering technique based on the
class of item-based top-N recommendation algorithms [9] is
used in cases where the user profile contains items that are
also contained in the provider profile. In a preprocessing
stage, i.e. prior to the actual information filtering processes,
a model is generated containing the k most similar items for
each provider profile item. While computationally rather
complex, this approach is feasible because it has to be done
only once, and it is carried out in a privacy-preserving way
via interactions between the provider agent and a TFE
agent. The resulting model is stored by the provider agent
and can be seen as an additional part of the provider 
profile. In the actual information filtering process, the k most
similar items are retrieved for each single user profile item
via queries on the model (as described in Section 4.2.1, this
is possible in a privacy-preserving way via anonymous 
communication). Recommendations are generated by selecting
the n most frequent items from the result sets that are not
already contained within the user profile.
As an alternative approach applicable when the user
profile contains information in addition to provider profile
items, we provide a cluster-based approach in which provider
profile items are clustered in a preprocessing stage via an 
agglomerative hierarchical clustering approach. Each cluster is
represented by a centroid item, and the cluster elements are
either sub-clusters or, on the lowest level, the items 
themselves. In the information filtering stage, the relevant items
are retrieved by descending through the cluster hierarchy in
the following manner: The cluster items of the highest level
are retrieved independent of the user profile. By 
comparing these items with the user profile data, the most relevant
sub-clusters are determined and retrieved in a subsequent
iteration. This process is repeated until the lowest level
is reached, which contains the items themselves as 
recommendations. Throughout the process, user profile items are
never propagated to the provider as such. The 
information deducible about the user profile does not exceed the
information deducible via the recommendations themselves
(because essentially only a chain of cluster centroids leading
to the recommendations is retrieved), and therefore it is not
regarded as privacy-critical.
4.4 Implementation
We have implemented the approach for privacy-preserving
IF based on JIAC IV [12], a FIPA-compliant MAS 
architecture. JIAC IV integrates fundamental aspects of 
autonomous agents regarding pro-activeness, intelligence, 
communication capabilities and mobility by providing a scalable
component-based architecture. Additionally, JIAC IV offers
components realizing management and security 
functionality, and provides a methodology for Agent-Oriented 
Software Engineering. JIAC IV stands out among MAS 
architectures as the only security-certified architecture, since it
has been certified by the German Federal Office for 
Information Security according to the EAL3 of the Common Criteria
for Information Technology Security standard [14]. JIAC IV
offers several security features in the areas of access control
for agent services, secure communication between agents,
and low-level security based on Java security policies [21],
and thus provides all security-related functionality required
for our approach.
We have extended the JIAC IV architecture by adding the
mechanisms for communication control described in Section
4.1. Regarding the issue of malicious hosts, we currently
assume all providers of agent platforms to be trusted. We
are additionally developing a solution that is actually based
on a trusted computing infrastructure.
5. EVALUATION
For the evaluation of our approach, we have examined
whether and to which extent the requirements (mainly 
regarding privacy, performance, and quality) are actually met.
Privacy aspects are directly addressed by the modules and
protocols described above and therefore not evaluated 
further here. Performance is a critical issue, mainly because of
the overhead caused by creating additional agents and agent
platforms for controlling communication, and by the 
additional interactions within the Recommender Module. 
Overall, a single information filtering process takes about ten
times longer than a non-privacy-preserving information 
filtering process leading to the same results, which is a 
considerable overhead but still acceptable under certain conditions,
as described in the following section.
5.1 The Smart Event Assistant
As a proof of concept, and in order to evaluate 
performance and quality under real-life conditions, we have 
ap324 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 2: The Smart Event Assistant, a 
privacypreserving Recommender System supporting users
in planning entertainment-related activities.
plied our approach within the Smart Event Assistant, a
MAS-based Recommender System which integrates various
personalized services for entertainment planning in 
different German cities, such as a restaurant finder and a movie
finder [25]. Additional services, such as a calendar, a 
routing service and news services complement the information
services. An intelligent day planner integrates all 
functionality by providing personalized recommendations for the 
various information services, based on the user"s preferences
and taking into account the location of the user as well as
the potential venues. All services are accessible via mobile
devices as well3
. Figure 2 shows a screenshot of the 
intelligent day planner"s result dialog. The Smart Event 
Assistant is entirely realized as a MAS system providing, among
other functionality, various filter agents and different service
provider agents, which together with the user agents utilize
the functionality provided by our approach.
Recommendations are generated in two ways: A push 
service delivers new recommendations to the user in regular
intervals (e.g. once per day) via email or SMS. Because the
user is not online during these interactions, they are less
critical with regard to performance and the protracted 
duration of the information filtering process is acceptable in
this case. Recommendations generated for the intelligent
day planner, however, have to be delivered with very little
latency because the process is triggered by the user, who
expects to receive results promptly. In this scenario, the
overall performance is substantially improved by setting up
the relay agent and the TFE agent oﬄine, i.e. prior to the
user"s request, and by an efficient retrieval of the relevant
3
The Smart Event Assistant may be accessed online via
http://www.smarteventassistant.de.
Table 4: Complexity of typical privacy-preserving
(PP) vs. non-privacy-preserving (NPP) filtering
processes in the realized application. In the 
nonprivacy-preserving version, an agent retrieves the
profiles directly and propagates the result to a
provider agent.
scenario push day planning
version NPP PP NPP PP
profile size (retrieved/total amount of items)
user 25/25 25/25
provider 125/10,000 500/10,000
elapsed time in filtering process (in seconds)
setup n/a 2.2 n/a oﬄine
database access 0.2 0.5 0.4 0.4
profile propagation n/a 0.8 n/a 0.3
filtering algorithm 0.2 0.2 0.2 0.2
result propagation 0.1 1.1 0.1 1.1
complete time 0.5 4.8 0.7 2.0
part of the provider profile: Because the user is only 
interested in items, such as movies, available within a certain
time period and related to specific locations, such as 
screenings at cinemas in a specific city, the relevant part of the
provider profile is usually small enough to be propagated
entirely. Because these additional parameters are not seen
as privacy-critical (as they are not based on the user 
profile, but rather constitute a short-term information need),
the relevant part of the provider profile may be propagated
as a whole, with no need for complex interactions. Taken
together, these improvements result in a filtering process
that takes about three times as long as the respective 
nonprivacy-preserving filtering process, which we regard as an
acceptable trade-off for the increased level of privacy. Table
4 shows the results of the performance evaluation in more
detail. In these scenarios, a direct content-based filtering
technique similar to the one described in Section 4.3 is 
applied. Because equivalent filtering techniques have been 
applied successfully in regular Recommender Systems [9], there
are no negative consequences with regard to the quality of
the recommendations.
5.2 Alternative Approaches
As described in Section 3.2, our solution is based on
trusted computing. There are more straightforward ways
to realize privacy-preserving IF, e.g. by utilizing a 
centralized architecture in which the privacy-preserving 
providerside functionality is realized as trusted software based on
trusted computing. However, we consider these approaches
to be unsuitable because they are far less generic: Whenever
some part of the respective software is patched, upgraded or
replaced, the entire system has to be analyzed again in order
to determine its trustworthiness, a process that is 
problematic in itself due to its complexity. In our solution, only a
comparatively small part of the overall system is based on
trusted computing. Because agent platforms can be utilized
for a large variety of tasks, and because we see trusted 
computing as the most promising approach to realize secure and
trusted agent environments, it seems reasonable to assume
that these respective mechanisms will be generally available
in the future, independent of specific solutions such as the
one described here.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 325
6. CONCLUSION & FURTHER WORK
We have developed an agent-based approach for 
privacypreserving Recommender Systems. By utilizing 
fundamental features of agents such as autonomy, adaptability and
the ability to communicate, by extending the capabilities
of agent platform managers regarding control of agent 
communication, by providing a privacy-preserving protocol for
information filtering processes, and by utilizing suitable 
filtering techniques we have been able to realize an approach
which actually preserves privacy in Information Filtering 
architectures in a multilateral way. As a proof of concept, we
have used the approach within an application supporting
users in planning entertainment-related activities.
We envision various areas of future work: To achieve 
complete user privacy, the protocol should be extended in order
to keep the recommendations themselves private as well.
Generally, the feedback we have obtained from users of the
Smart Event Assistant indicates that most users are 
indifferent to privacy in the context of entertainment-related 
personal information. Therefore, we intend to utilize the 
approach to realize a Recommender System in a more 
privacysensitive domain, such as health or finance, which would
enable us to better evaluate user acceptance.
7. ACKNOWLEDGMENTS
We would like to thank our colleagues Andreas Rieger
and Nicolas Braun, who co-developed the Smart Event 
Assistant. The Smart Event Assistant is based on a project
funded by the German Federal Ministry of Education and
Research under Grant No. 01AK037, and a project funded
by the German Federal Ministry of Economics and Labour
under Grant No. 01MD506.
8. REFERENCES
[1] R. Agrawal, J. Kiernan, R. Srikant, and Y. Xu.
Hippocratic databases. In 28th Int"l Conf. on Very
Large Databases (VLDB), Hong Kong, 2002.
[2] R. Agrawal and R. Srikant. Privacy-preserving data
mining. In Proc. of the ACM SIGMOD Conference on
Management of Data, pages 439-450. ACM Press,
May 2000.
[3] E. A¨ımeur, G. Brassard, J. M. Fernandez, and F. S.
Mani Onana. Privacy-preserving demographic
filtering. In SAC "06: Proceedings of the 2006 ACM
symposium on Applied computing, pages 872-878, New
York, NY, USA, 2006. ACM Press.
[4] M. Bawa, R. Bayardo, Jr., and R. Agrawal.
Privacy-preserving indexing of documents on the
network. In Proc. of the 2003 VLDB, 2003.
[5] R. Burke. Hybrid recommender systems: Survey and
experiments. User Modeling and User-Adapted
Interaction, 12(4):331-370, 2002.
[6] J. Canny. Collaborative filtering with privacy. In
IEEE Symposium on Security and Privacy, pages
45-57, 2002.
[7] B. Chor, O. Goldreich, E. Kushilevitz, and M. Sudan.
Private information retrieval. In IEEE Symposium on
Foundations of Computer Science, pages 41-50, 1995.
[8] R. Ciss´ee. An architecture for agent-based
privacy-preserving information filtering. In Proceedings
of the 6th International Workshop on Trust, Privacy,
Deception and Fraud in Agent Systems, 2003.
[9] M. Deshpande and G. Karypis. Item-based top-N
recommendation algorithms. ACM Trans. Inf. Syst.,
22(1):143-177, 2004.
[10] L. Foner. Political artifacts and personal privacy: The
yenta multi-agent distributed matchmaking system.
PhD thesis, MIT, 1999.
[11] Foundation for Intelligent Physical Agents. FIPA
Abstract Architecture Specification, Version L, 2002.
[12] S. Fricke, K. Bsufka, J. Keiser, T. Schmidt,
R. Sesseler, and S. Albayrak. Agent-based telematic
services and telecom applications. Communications of
the ACM, 44(4), April 2001.
[13] T. Garfinkel, M. Rosenblum, and D. Boneh. Flexible
OS support and applications for trusted computing. In
Proceedings of HotOS-IX, May 2003.
[14] T. Geissler and O. Kroll-Peters. Applying security
standards to multi agent systems. In AAMAS 
Workshop: Safety & Security in Multiagent Systems, 2004.
[15] O. Goldreich, S. Micali, and A. Wigderson. How to
play any mental game. In Proc. of STOC "87, pages
218-229, New York, NY, USA, 1987. ACM Press.
[16] S. Jha, L. Kruger, and P. McDaniel. Privacy
preserving clustering. In ESORICS 2005, volume 3679
of LNCS. Springer, 2005.
[17] G. Karjoth, M. Schunter, and M. Waidner. The
platform for enterprise privacy practices:
Privacy-enabled management of customer data. In
PET 2002, volume 2482 of LNCS. Springer, 2003.
[18] H. Link, J. Saia, T. Lane, and R. A. LaViolette. The
impact of social networks on multi-agent recommender
systems. In Proc. of the Workshop on Cooperative
Multi-Agent Learning (ECML/PKDD "05), 2005.
[19] B. N. Miller, J. A. Konstan, and J. Riedl. PocketLens:
Toward a personal recommender system. ACM Trans.
Inf. Syst., 22(3):437-476, 2004.
[20] H. Polat and W. Du. SVD-based collaborative filtering
with privacy. In Proc. of SAC "05, pages 791-795,
New York, NY, USA, 2005. ACM Press.
[21] T. Schmidt. Advanced Security Infrastructure for
Multi-Agent-Applications in the Telematic Area. PhD
thesis, Technische Universit¨at Berlin, 2002.
[22] G. J. Simmons. The prisoners" problem and the
subliminal channel. In D. Chaum, editor, Proc. of
Crypto "83, pages 51-67. Plenum Press, 1984.
[23] M. Teltzrow and A. Kobsa. Impacts of user privacy
preferences on personalized systems: a comparative
study. In Designing personalized user experiences in
eCommerce, pages 315-332. 2004.
[24] D. Weyns, H. Parunak, F. Michel, T. Holvoet, and
J. Ferber. Environments for multiagent systems:
State-of-the-art and research challenges. In
Environments for Multiagent Systems, volume 3477 of
LNCS. Springer, 2005.
[25] J. Wohltorf, R. Ciss´ee, and A. Rieger. Berlintainment:
An agent-based context-aware entertainment planning
system. IEEE Communications Magazine, 43(6), 2005.
[26] M. Wooldridge and N. R. Jennings. Intelligent agents:
Theory and practice. Knowledge Engineering Review,
10(2):115-152, 1995.
[27] A. Yao. Protocols for secure computation. In Proc. of
IEEE FOGS "82, pages 160-164, 1982.
326 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
A Framework for Agent-Based Distributed Machine
Learning and Data Mining
Jan Tozicka
Gerstner Laboratory
Czech Technical University
Technick«a 2, Prague, 166 27
Czech Republic
tozicka@labe.felk.cvut.cz
Michael Rovatsos
School of Informatics
The University of Edinburgh
Edinburgh EH8 9LE
United Kingdom
mrovatso@inf.ed.ac.uk
Michal Pechoucek
Gerstner Laboratory
Czech Technical University
Technick«a 2, Prague, 166 27
Czech Republic
pechouc@labe.felk.cvut.cz
ABSTRACT
This paper proposes a framework for agent-based 
distributed machine learning and data mining based on (i)
the exchange of meta-level descriptions of individual 
learning processes among agents and (ii) online reasoning about
learning success and learning progress by learning agents.
We present an abstract architecture that enables agents to
exchange models of their local learning processes and 
introduces a number of different methods for integrating these
processes. This allows us to apply existing agent 
interaction mechanisms to distributed machine learning tasks,
thus leveraging the powerful coordination methods available
in agent-based computing, and enables agents to engage in
meta-reasoning about their own learning decisions. We 
apply this architecture to a real-world distributed clustering
application to illustrate how the conceptual framework can
be used in practical systems in which different learners may
be using different datasets, hypotheses and learning 
algorithms. We report on experimental results obtained using
this system, review related work on the subject, and discuss
potential future extensions to the framework.
General Terms
Theory
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
Intelligence-Multiagent Systems
1. INTRODUCTION
In the areas of machine learning and data mining (cf. [14,
17] for overviews), it has long been recognised that 
parallelisation and distribution can be used to improve learning
performance. Various techniques have been suggested in
this respect, ranging from the low-level integration of 
independently derived learning hypotheses (e.g. combining 
different classifiers to make optimal classification decisions [4,
7], model averaging of Bayesian classifiers [8], or 
consensusbased methods for integrating different clusterings [11]), to
the high-level combination of learning results obtained by
heterogeneous learning agents using meta-learning (e.g. [3,
10, 21]).
All of these approaches assume homogeneity of agent 
design (all agents apply the same learning algorithm) and/or
agent objectives (all agents are trying to cooperatively solve
a single, global learning problem). Therefore, the techniques
they suggest are not applicable in societies of autonomous
learners interacting in open systems. In such systems, 
learners (agents) may not be able to integrate their datasets or
learning results (because of different data formats and 
representations, learning algorithms, or legal restrictions that
prohibit such integration [11]) and cannot always be 
guaranteed to interact in a strictly cooperative fashion (discovered
knowledge and collected data might be economic assets that
should only be shared when this is deemed profitable; 
malicious agents might attempt to adversely influence others"
learning results, etc.).
Examples for applications of this kind abound. Many 
distributed learning domains involve the use of sensitive data
and prohibit the exchange of this data (e.g. exchange of 
patient data in distributed brain tumour diagnosis [2]) - 
however, they may permit the exchange of local learning 
hypotheses among different learners. In other areas, training
data might be commercially valuable, so that agents would
only make it available to others if those agents could 
provide something in return (e.g. in remote ship surveillance
and tracking, where the different agencies involved are 
commercial service providers [1]). Furthermore, agents might
have a vested interest in negatively affecting other agents"
learning performance. An example for this is that of 
fraudulent agents on eBay which may try to prevent 
reputationlearning agents from the construction of useful models for
detecting fraud.
Viewing learners as autonomous, self-directed agents is
the only appropriate view one can take in modelling these
distributed learning environments: the agent metaphor 
becomes a necessity as oppossed to preferences for scalability,
dynamic data selection, interactivity [13], which can also
be achieved through (non-agent) distribution and 
parallelisation in principle.
Despite the autonomy and self-directedness of learning
agents, many of these systems exhibit a sufficient overlap
in terms of individual learning goals so that beneficial 
cooperation might be possible if a model for flexible 
interaction between autonomous learners was available that allowed
agents to
1. exchange information about different aspects of their
own learning mechanism at different levels of detail
without being forced to reveal private information that
should not be disclosed,
2. decide to what extent they want to share information
about their own learning processes and utilise 
information provided by other learners, and
3. reason about how this information can best be used to
improve their own learning performance.
Our model is based on the simple idea that autonomous
learners should maintain meta-descriptions of their own
learning processes (see also [3]) in order to be able to 
exchange information and reason about them in a rational way
(i.e. with the overall objective of improving their own 
learning results). Our hypothesis is a very simple one:
If we can devise a sufficiently general, abstract
view of describing learning processes, we will be
able to utilise the whole range of methods for (i)
rational reasoning and (ii) communication and
coordination offered by agent technology so as to
build effective autonomous learning agents.
To test this hypothesis, we introduce such an abstract 
architecture (section 2) and implement a simple, concrete 
instance of it in a real-world domain (section 3). We report
on empirical results obtained with this implemented system
that demonstrate the viability of our approach (section 4).
Finally, we review related work (section 5) and conclude
with a summary, discussion of our approach and outlook to
future work on the subject (section 6).
2. ABSTRACT ARCHITECTURE
Our framework is based on providing formal (meta-level)
descriptions of learning processes, i.e. representations of all
relevant components of the learning machinery used by a
learning agent, together with information about the state of
the learning process.
To ensure that this framework is sufficiently general, we
consider the following general description of a learning 
problem:
Given data D ⊆ D taken from an instance space
D, a hypothesis space H and an (unknown) 
target function c ∈ H1
, derive a function h ∈ H that
approximates c as well as possible according to
some performance measure g : H → Q where Q
is a set of possible levels of learning performance.
1
By requiring this we are ensuring that the learning problem
can be solved in principle using the given hypothesis space.
This very broad definition includes a number of components
of a learning problem for which more concrete specifications
can be provided if we want to be more precise. For the cases
of classification and clustering, for example, we can further
specify the above as follows: Learning data can be described
in both cases as D = ×n
i=1[Ai] where [Ai] is the domain of
the ith attribute and the set of attributes is A = {1, . . . , n}.
For the hypothesis space we obtain
H ⊆ {h|h : D → {0, 1}}
in the case of classification (i.e. a subset of the set of all
possible classifiers, the nature of which depends on the 
expressivity of the learning algorithm used) and
H ⊆ {h|h : D → N, h is total with range {1, . . . , k}}
in the case of clustering (i.e. a subset of all sets of possible
cluster assignments that map data points to a finite number
of clusters numbered 1 to k). For classification, g might be
defined in terms of the numbers of false negatives and false
positives with respect to some validation set V ⊆ D, and
clustering might use various measures of cluster validity to
evaluate the quality of a current hypothesis, so that Q = R
in both cases (but other sets of learning quality levels can
be imagined).
Next, we introduce a notion of learning step, which 
imposes a uniform basic structure on all learning processes that
are supposed to exchange information using our framework.
For this, we assume that each learner is presented with a
finite set of data D = d1, . . . dk in each step (this is an 
ordered set to express that the order in which the samples are
used for training matters) and employs a training/update
function f : H × D∗
→ H which updates h given a series of
samples d1, . . . , dk. In other words, one learning step always
consists of applying the update function to all samples in D
exactly once. We define a learning step as a tuple
l = D, H, f, g, h
where we require that H ⊆ H and h ∈ H.
The intuition behind this definition is that each learning
step completely describes one learning iteration as shown
in Figure 1: in step t, the learner updates the current 
hypothesis ht−1 with data Dt, and evaluates the resulting new
hypothesis ht according to the current performance measure
gt. Such a learning step is equivalent to the following steps
of computation:
1. train the algorithm on all samples in D (once), i.e. 
calculate ft(ht−1, Dt) = ht,
2. calculate the quality gt of the resulting hypothesis
gt(ht).
We denote the set of all possible learning steps by L. For
ease of notation, we denote the components of any l ∈ L by
D(l), H(l), f(l) and g(l), respectively. The reason why such
learning step specifications use a subset H of H instead of
H itself is that learners often have explicit knowledge about
which hypotheses are effectively ruled out by f given h in
the future (if this is not the case, we can still set H = H).
A learning process is a finite, non-empty sequence
l = l1 → l2 → . . . → ln
of learning steps such that
∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li))
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 679
training function
ht
performance measure solution quality
qtgtft
training set
Dt
hypothesis
hypothesis
ht−1
Figure 1: A generic model of a learning step
i.e. the only requirement the transition relation →⊆ L × L
makes is that the new hypothesis is the result of training the
old hypothesis on all available sample data that belongs to
the current step. We denote the set of all possible learning
processes by L (ignoring, for ease of notation, the fact that
this set depends on H, D and the spaces of possible 
training and evaluation functions f and g). The performance
trace associated with a learning process l is the sequence
q1, . . . , qn ∈ Qn
where qi = g(li)(h(li)), i.e. the sequence
of quality values calculated by the performance measures of
the individual learning steps on the respective hypotheses.
Such specifications allow agents to provide a 
selfdescription of their learning process. However, in 
communication among learning agents, it is often useful to 
provide only partial information about one"s internal learning
process rather than its full details, e.g. when advertising
this information in order to enter information exchange 
negotiations with others. For this purpose, we will assume
that learners describe their internal state in terms of sets of
learning processes (in the sense of disjunctive choice) which
we call learning process descriptions (LPDs) rather than by
giving precise descriptions about a single, concrete learning
process.
This allows us to describe properties of a learning 
process without specifying its details exhaustively. As an 
example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all
processes that have a training set of at most 100 
samples (where all the other elements are arbitrary). Likewise,
{l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing
information about a single sample {d} and no other details
about the process (this can be useful to model, for 
example, data received from the environment). Therefore, we use
℘(L), that is the set of all LPDs, as the basis for 
designing content languages for communication in the protocols
we specify below.
In practice, the actual content language chosen will of
course be more restricted and allow only for a special type
of subsets of L to be specified in a compact way, and its
choice will be crucial for the interactions that can occur
between learning agents. For our examples below, we simply
assume explicit enumeration of all possible elements of the
respective sets and function spaces (D, H, etc.) extended by
the use of wildcard symbols ∗ (so that our second example
above would become ({d}, ∗, ∗, ∗, ∗)).
2.1 Learning agents
In our framework, a learning agent is essentially a 
metareasoning function that operates on information about 
learning processes and is situated in an environment co-inhabited
by other learning agents. This means that it is not only 
capable of meta-level control on how to learn, but in doing
so it can take information into account that is provided by
other agents or the environment. Although purely 
cooperative or hybrid cases are possible, for the purposes of this
paper we will assume that agents are purely self-interested,
and that while there may be a potential for cooperation
considering how agents can mutually improve each others"
learning performance, there is no global mechanism that can
enforce such cooperative behaviour.2
Formally speaking, an agent"s learning function is a 
function which, given a set of histories of previous learning 
processes (of oneself and potentially of learning processes about
which other agents have provided information) and outputs
a learning step which is its next learning action. In the
most general sense, our learning agent"s internal learning
process update can hence be viewed as a function
λ : ℘(L) → L × ℘(L)
which takes a set of learning histories of oneself and others
as inputs and computes a new learning step to be executed
while updating the set of known learning process histories
(e.g. by appending the new learning action to one"s own
learning process and leaving all information about others"
learning processes untouched). Note that in λ({l1, . . . ln}) =
(l, {l1, . . . ln }) some elements li of the input learning process
set may be descriptions of new learning data received from
the environment.
The λ-function can essentially be freely chosen by the
agent as long as one requirement is met, namely that the
learning data that is being used always stems from what
has been previously observed. More formally,
∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln })
⇒
„
D(l) ∪
 [
l =li[j]
D(l )
«
⊆
[
l =li[j]
D(l )
i.e. whatever λ outputs as a new learning step and updated
set of learning histories, it cannot invent new data; it has
to work with the samples that have been made available
to it earlier in the process through the environment or from
other agents (and it can of course re-train on previously used
data).
The goal of the agent is to output an optimal learning
step in each iteration given the information that it has. One
possibility of specifying this is to require that
∀{l1, . . . ln} ∈ ℘(L).λ({l1, . . . ln}) = (l, {l1, . . . ln })
⇒ l = arg max
l ∈L
g(l )(h(l ))
but since it will usually be unrealistic to compute the 
optimal next learning step in every situation, it is more useful
2
Note that our outlook is not only different from common,
cooperative models of distributed machine learning and data
mining, but also delineates our approach from multiagent
learning systems in which agents learn about other agents
[25], i.e. the learning goal itself is not affected by agents"
behaviour in the environment.
680 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
i j Dj Hj fj gj hj
Di
pD→D
1 (Di, Dj)
.
..
pD→D
kD→D
(Di, Dj)
. . . . . . n/a . . .
Hi
.
..
... n/a
fi
.
..
... n/a
gi
.
.. n/a
pg→h
1 (gi, hj)
.
..
pg→h
kg→h
(gi, hj)
hi
.
.. n/a
...
Table 1: Matrix of integration functions for 
messages sent from learner i to j
to simply use g(l )(h(l )) as a running performance measure
to evaluate how well the agent is performing.
This is too abstract and unspecific for our purposes: While
it describes what agents should do (transform the settings
for the next learning step in an optimal way), it doesn"t
specify how this can be achieved in practice.
2.2 Integrating learning process information
To specify how an agent"s learning process can be affected
by integrating information received from others, we need to
flesh out the details of how the learning steps it will perform
can be modified using incoming information about learning
processes described by other agents (this includes the 
acquisition of new learning data from the environment as a
special case). In the most general case, we can specify this
in terms of the potential modifications to the existing 
information about learning histories that can be performed using
new information. For ease of presentation, we will assume
that agents are stationary learning processes that can only
record the previously executed learning step and only 
exchange information about this one individual learning step
(our model can be easily extended to cater for more complex
settings).
Let lj = Dj, Hj, fj, gj, hj be the current state of
agent j when receiving a learning process description li =
Di, Hi, fi, gi, hi from agent i (for the time being, we 
assume that this is a specific learning step and not a more
vague, disjunctive description of properties of the 
learning step of i). Considering all possible interactions at an
abstract level, we basically obtain a matrix of 
possibilities for modifications of j"s learning step specification as
shown in Table 1. In this matrix, each entry specifies
a family of integration functions pc→c
1 , . . . , pc→c
kc→c
where
c, c ∈ {D, H, f, g, h} and which define how agent j"s 
component cj will be modified using the information ci provided
about (the same or a different component of) i"s learning
step by applying pc→c
r (ci, cj) for some r ∈ {1, . . . , kc→c }.
To put it more simply, the collections of p-functions an agent
j uses specifies how it will modify its own learning behaviour
using information obtained from i.
For the diagonal of this matrix, which contains the most
common ways of integrating new information in one"s own
learning model, obvious ways of modifying one"s own 
learning process include replacing cj by ci or ignoring ci 
altogether. More complex/subtle forms of learning process 
integration include:
• Modification of Dj: append Di to Dj; filter out all
elements from Dj which also appear in Di; append
Di to Dj discarding all elements with attributes 
outside ranges which affect gj, or those elements already
correctly classified by hj;
• Modification of Hi: use the union/intersection of Hi
and Hj; alternatively, discard elements of Hj that are
inconsistent with Dj in the process of intersection or
union, or filter out elements that cannot be obtained
using fj (unless fj is modified at the same time)
• Modification of fj: modify parameters or background
knowledge of fj using information about fi; assess
their relevance by simulating previous learning steps
on Dj using gj and discard those that do not help
improve own performance
• Modification of hj: combine hj with hi using (say) 
logical or mathematical operators; make the use of hi 
contingent on a pre-integration assessment of its quality
using own data Dj and gj
While this list does not include fully fledged, concrete 
integration operations for learning processes, it is indicative of
the broad range of interactions between individual agents"
learning processes that our framework enables.
Note that the list does not include any modifications to
gj. This is because we do not allow modifications to the
agent"s own quality measure as this would render the model
of rational (learning) action useless (if the quality measure
is relative and volatile, we cannot objectively judge learning
performance). Also note that some of the above examples
require consulting other elements of lj than those appearing
as arguments of the p-operations; we omit these for ease
of notation, but emphasise that information-rich operations
will involve consulting many different aspects of lj.
Apart from operations along the diagonal of the matrix,
more exotic integration operations are conceivable that
combine information about different components. In theory
we could fill most of the matrix with entries for them, but
for lack of space we list only a few examples:
• Modification of Dj using fi: pre-process samples in
fi, e.g. to achieve intermediate representations that fj
can be applied to
• Modification of Dj using hi: filter out samples from
Dj that are covered by hi and build hj using fj only
on remaining samples
• Modification of Hj using fi: filter out hypotheses from
Hj that are not realisable using fi
• Modification of hj using gi: if hj is composed of several
sub-components, filter out those sub-components that
do not perform well according to gi
• . . .
Finally, many messages received from others describing
properties of their learning processes will contain 
information about several elements of a learning step, giving rise to
yet more complex operations that depend on which kinds of
information are available.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 681
Figure 2: Screenshot of our simulation system, 
displaying online vessel tracking data for the North Sea
region
3. APPLICATION EXAMPLE
3.1 Domain description
As an illustration of our framework, we present an 
agentbased data mining system for clustering-based surveillance
using AIS (Automatic Identification System [1]) data. In
our application domain, different commercial and 
governmental agencies track the journeys of ships over time 
using AIS data which contains structured information 
automatically provided by ships equipped with shipborne 
mobile AIS stations to shore stations, other ships and aircrafts.
This data contains the ship"s identity, type, position, course,
speed, navigational status and other safety-related 
information. Figure 2 shows a screenshot of our simulation system.
It is the task of AIS agencies to detect anomalous 
behaviour so as to alarm police/coastguard units to further
investigate unusual, potentially suspicious behaviour. Such
behaviour might include things such as deviation from the
standard routes between the declared origin and destination
of the journey, unexpected close encounters between 
different vessels on sea, or unusual patterns in the choice of
destination over multiple journeys, taking the type of 
vessel and reported freight into account. While the reasons for
such unusual behaviour may range from pure coincidence or
technical problems to criminal activity (such as smuggling,
piracy, terrorist/military attacks) it is obviously useful to
pre-process the huge amount of vessel (tracking) data that
is available before engaging in further analysis by human
experts.
To support this automated pre-processing task, software
used by these agencies applies clustering methods in order
to identify outliers and flag those as potentially suspicious
entities to the human user. However, many agencies active
in this domain are competing enterprises and use their 
(partially overlapping, but distinct) datasets and learning 
hypotheses (models) as assets and hence cannot be expected
to collaborate in a fully cooperative way to improve 
overall learning results. Considering that this is the reality of
the domain in the real world, it is easy to see that a 
framework like the one we have suggested above might be useful
to exploit the cooperation potential that is not exploited by
current systems.
3.2 Agent-based distributed learning system
design
To describe a concrete design for the AIS domain, we need
to specify the following elements of the overall system:
1. The datasets and clustering algorithms available to 
individual agents,
2. the interaction mechanism used for exchanging 
descriptions of learning processes, and
3. the decision mechanism agents apply to make learning
decisions.
Regarding 1., our agents are equipped with their own private
datasets in the form of vessel descriptions. Learning samples
are represented by tuples containing data about individual
vessels in terms of attributes A = {1, . . . , n} including things
such as width, length, etc. with real-valued domains ([Ai] =
R for all i).
In terms of learning algorithm, we consider clustering
with a fixed number of k clusters using the k-means and
k-medoids clustering algorithms [5] (fixed meaning that
the learning algorithm will always output k clusters; 
however, we allow agents to change the value of k over different
learning cycles). This means that the hypothesis space can
be defined as H = { c1, . . . , ck |ci ∈ R|A|
} i.e. the set of all
possible sets of k cluster centroids in |A|-dimensional 
Euclidean space. For each hypothesis h = c1, . . . , ck and any
data point d ∈ ×n
i=1[Ai] given domain [Ai] for the ith 
attribute of each sample, the assignment to clusters is given
by
C( c1, . . . , ck , d) = arg min
1≤j≤k
|d − cj|
i.e. d is assigned to that cluster whose centroid is closest to
the data point in terms of Euclidean distance.
For evaluation purposes, each dataset pertaining to a 
particular agent i is initially split into a training set Di and a
validation Vi. Then, we generate a set of fake vessels Fi
such that |Fi| = |Vi|. These two sets assess the agent"s
ability to detect suspicious vessels. For this, we assign a
confidence value r(h, d) to every ship d:
r(h, d) =
1
|d − cC(h,d)|
where C(h, d) is the index of the nearest centroid. Based
on this measure, we classify any vessel in Fi ∪ Vi as fake if
its r-value is below the median of all the confidences r(h, d)
for d ∈ Fi ∪ Vi. With this, we can compute the quality
gi(h) ∈ R as the ratio between all correctly classified vessels
and all vessels in Fi ∪ Vi.
As concerns 2., we use a simple Contract-Net Protocol
(CNP) [20] based hypothesis trading mechanism: Before
each learning iteration, agents issue (publicly broadcasted)
Calls-For-Proposals (CfPs), advertising their own numerical
model quality. In other words, the initiator of a CNP
describes its own current learning state as (∗, ∗, ∗, gi(h), ∗)
where h is their current hypothesis/model. We assume that
agents are sincere when advertising their model quality, but
note that this quality might be of limited relevance to other
agents as they may specialise on specific regions of the data
space not related to the test set of the sender of the CfP.
Subsequently, (some) agents may issue bids in which they
advertise, in turn, the quality of their own model. If the
682 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
bids (if any) are accepted by the initiator of the protocol
who issued the CfP, the agents exchange their hypotheses
and the next learning iteration ensues.
To describe what is necessary for 3., we have to specify
(i) under which conditions agents submit bids in response
to a CfP, (ii) when they accept bids in the CNP negotiation
process, and (iii) how they integrate the received 
information in their own learning process. Concerning (i) and (ii),
we employ a very simple rule that is identical in both cases:
let g be one"s own model quality and g that advertised by
the CfP (or highest bid, respectively). If g > g we respond
to the CfP (accept the bid), else respond to the CfP 
(accept the bid) with probability P(g /g) and ignore (reject) it
else. If two agents make a deal, they exchange their 
learning hypotheses (models). In our experiments, g and g are
calculated by an additional agent that acts as a global 
validation mechanism for all agents (in a more realistic setting a
comparison mechanism for different g functions would have
to be provided).
As for (iii), each agent uses a single model merging 
operator taken from the following two classes of operators (hj is
the receiver"s own model and hi is the provider"s model):
• ph→h
(hi, hj) :
- m-join: The m best clusters (in terms of coverage
of Dj) from hypothesis hi are appended to hj.
- m-select: The set of the m best clusters (in terms
of coverage of Dj) from the union hi ∪hj is chosen
as a new model. (Unlike m-join this method does
not prefer own clusters over others".)
• ph→D
(hi, Dj) :
- m-filter: The m best clusters (as above) from
hi are identified and appended to a new model
formed by using those samples not covered by
these clusters applying the own learning 
algorithm fj.
Whenever m is large enough to encompass all clusters, we
simply write join or filter for them. In section 4 we analyse
the performance of each of these two classes for different
choices of m.
It is noteworthy that this agent-based distributed data
mining system is one of the simplest conceivable instances
of our abstract architecture. While we have previously 
applied it also to a more complex market-based architecture
using Inductive Logic Programming learners in a transport
logistics domain [22], we believe that the system described
here is complex enough to illustrate the key design decisions
involved in using our framework and provides simple 
example solutions for these design issues.
4. EXPERIMENTAL RESULTS
Figure 3 shows results obtained from simulations with
three learning agents in the above system using the k-means
and k-medoids clustering methods respectively. We 
partition the total dataset of 300 ships into three disjoint sets of
100 samples each and assign each of these to one learning
agent. The Single Agent is learning from the whole dataset.
The parameter k is set to 10 as this is the optimal value for
the total dataset according to the Davies-Bouldin index [9].
For m-select we assume m = k which achieves a constant
Figure 3: Performance results obtained for different
integration operations in homogeneous learner 
societies using the k-means (top) and k-medoids 
(bottom) methods
model size. For m-join and m-filter we assume m = 3 to
limit the extent to which models increase over time.
During each experiment the learning agents receive ship
descriptions in batches of 10 samples. Between these
batches, there is enough time to exchange the models among
the agents and recompute the models if necessary. Each
ship is described using width, length, draught and speed
attributes with the goal of learning to detect which vessels
have provided fake descriptions of their own properties. The
validation set contains 100 real and 100 randomly generated
fake ships. To generate sufficiently realistic properties for
fake ships, their individual attribute values are taken from
randomly selected ships in the validation set (so that each
fake sample is a combination of attribute values of several
existing ships).
In these experiments, we are mainly interested in 
investigating whether a simple form of knowledge sharing between
self-interested learning agents could improve agent 
performance compared to a setting of isolated learners. Thereby,
we distinguish between homogeneous learner societies where
all agents use the same clustering algorithm and 
heterogeneous ones where different agents use different algorithms.
As can be seen from the performance plots in Figure 3
(homogeneous case) and 4 (heterogeneous case, two agents
use the same method and one agent uses the other) this is
clearly the case for the (unrestricted) join and filter 
integration operations (m = k) in both cases. This is quite natural,
as these operations amount to sharing all available model
knowledge among agents (under appropriate constraints 
depending on how beneficial the exchange seems to the agents).
We can see that the quality of these operations is very close
to the Single Agent that has access to all training data.
For the restricted (m < k) m-join, m-filter and m-select
methods we can also observe an interesting distinction,
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 683
Figure 4: Performance results obtained for 
different integration operations in heterogeneous societies
with the majority of learners using the k-means
(top) and k-medoids (bottom) methods
namely that these perform similarly to the isolated learner
case in homogeneous agent groups but better than isolated
learners in more heterogeneous societies. This suggests that
heterogeneous learners are able to benefit even from rather
limited knowledge sharing (and this is what using a rather
small m = 3 amounts to given that k = 10) while this is
not always true for homogeneous agents. This nicely 
illustrates how different learning or data mining algorithms can
specialise on different parts of the problem space and then
integrate their local results to achieve better individual 
performance.
Apart from these obvious performance benefits, 
integrating partial learning results can also have other advantages:
The m-filter operation, for example, decreases the number
of learning samples and thus can speed up the learning 
process. The relative number of filtered examples measured in
our experiments is shown in the following table.
k-means k-medoids
filtering 30-40 % 10-20 %
m-filtering 20-30 % 5-15 %
The overall conclusion we can draw from these initial 
experiments with our architecture is that since a very 
simplistic application of its principles has proven capable of 
improving the performance of individual learning agents, it is
worthwhile investigating more complex forms of 
information exchange about learning processes among autonomous
learners.
5. RELATED WORK
We have already mentioned work on distributed 
(nonagent) machine learning and data mining in the 
introductory chapter, so in this section we shall restrict ourselves to
approaches that are more closely related to our outlook on
distributed learning systems.
Very often, approaches that are allegedly agent-based
completely disregard agent autonomy and prescribe local
decision-making procedures a priori. A typical example for
this type of system is the one suggested by Caragea et al. [6]
which is based on a distributed support-vector machine 
approach where agents incrementally join their datasets 
together according to a fixed distributed algorithm. A similar
example is the work of Weiss [24], where groups of 
classifier agents learn to organise their activity so as to optimise
global system behaviour.
The difference between this kind of collaborative 
agentbased learning systems [16] and our own framework is that
these approaches assume a joint learning goal that is pursued
collaboratively by all agents.
Many approaches rely heavily on a homogeneity 
assumption: Plaza and Ontanon [15] suggest methods for 
agentbased intelligent reuse of cases in case-based reasoning but
is only applicable to societies of homogeneous learners (and
coined towards a specific learning method). An 
agentbased method for integrating distributed cluster analysis
processes using density estimation is presented by Klusch
et al. [13] which is also specifically designed for a 
particular learning algorithm. The same is true of [22, 23] which
both present market-based mechanisms for aggregating the
output of multiple learning agents, even though these 
approaches consider more interesting interaction mechanisms
among learners.
A number of approaches for sharing learning data [18]
have also been proposed: Grecu and Becker [12] suggest an
exchange of learning samples among agents, and Ghosh et
al. [11] is a step in the right direction in terms of revealing
only partial information about one"s learning process as it
deals with limited information sharing in distributed 
clustering.
Papyrus [3] is a system that provides a markup language
for meta-description of data, hypotheses and intermediate
results and allows for an exchange of all this information
among different nodes, however with a strictly cooperative
goal of distributing the load for massively distributed data
mining tasks.
The MALE system [19] was a very early multiagent 
learning system in which agents used a blackboard approach to
communicate their hypotheses. Agents were able to critique
each others" hypotheses until agreement was reached. 
However, all agents in this system were identical and the system
was strictly cooperative.
The ANIMALS system [10] was used to simulate 
multistrategy learning by combining two or more learning 
techniques (represented by heterogeneous agents) in order to
overcome weaknesses in the individual algorithms, yet it was
also a strictly cooperative system.
As these examples show and to the best of our knowledge,
there have been no previous attempts to provide a 
framework that can accommodate both independent and 
heterogeneous learning agents and this can be regarded as the main
contribution of our work.
6. CONCLUSION
In this paper, we outlined a generic, abstract framework
for distributed machine learning and data mining. This
framework constitutes, to our knowledge, the first attempt
684 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
to capture complex forms of interaction between 
heterogeneous and/or self-interested learners in an architecture that
can be used as the foundation for implementing systems that
use complex interaction and reasoning mechanisms to enable
agents to inform and improve their learning abilities with
information provided by other learners in the system, 
provided that all agents engage in a sufficiently similar learning
activity.
To illustrate that the abstract principles of our 
architecture can be turned into concrete, computational systems,
we described a market-based distributed clustering system
which was evaluated in the domain of vessel tracking for
purposes of identifying deviant or suspicious behaviour. 
Although our experimental results only hint at the potential
of using our architecture, they underline that what we are
proposing is feasible in principle and can have beneficial 
effects even in its most simple instantiation.
Yet there is a number of issues that we have not addressed
in the presentation of the architecture and its empirical 
evaluation: Firstly, we have not considered the cost of 
communication and made the implicit assumption that the required
communication comes for free. This is of course 
inadequate if we want to evaluate our method in terms of the
total effort required for producing a certain quality of 
learning results. Secondly, we have not experimented with agents
using completely different learning algorithms (e.g. symbolic
and numerical). In systems composed of completely different
agents the circumstances under which successful information
exchange can be achieved might be very different from those
described here, and much more complex communication and
reasoning methods may be necessary to achieve a useful 
integration of different agents" learning processes. Finally, more
sophisticated evaluation criteria for such distributed 
learning architectures have to be developed to shed some light
on what the right measures of optimality for autonomously
reasoning and communicating agents should be.
These issues, together with a more systematic and 
thorough investigation of advanced interaction and 
communication mechanisms for distributed, collaborating and 
competing agents will be the subject of our future work on the
subject.
Acknowledgement: We gratefully acknowledge the 
support of the presented research by Army Research 
Laboratory project N62558-03-0819 and Office for Naval Research
project N00014-06-1-0232.
7. REFERENCES
[1] http://www.aislive.com.
[2] http://www.healthagents.com.
[3] S. Bailey, R. Grossman, H. Sivakumar, and
A. Turinsky. Papyrus: A System for Data Mining over
Local and Wide Area Clusters and Super-Clusters. In
Proc. of the Conference on Supercomputing. 1999.
[4] E. Bauer and R. Kohavi. An Empirical Comparison of
Voting Classification Algorithms: Bagging, Boosting,
and Variants. Machine Learning, 36, 1999.
[5] P. Berkhin. Survey of Clustering Data Mining
Techniques, Technical Report, Accrue Software, 2002.
[6] D. Caragea, A. Silvescu, and V. Honavar. Agents that
Learn from Distributed Dynamic Data sources. In
Proc. of the Workshop on Learning Agents, 2000.
[7] N. Chawla and S. E. abd L. O. Hall. Creating
ensembles of classifiers. In Proceedings of ICDM 2001,
pages 580-581, San Jose, CA, USA, 2001.
[8] D. Dash and G. F. Cooper. Model Averaging for
Prediction with Discrete Bayesian Networks. Journal
of Machine Learning Research, 5:1177-1203, 2004.
[9] D. L. Davies and D. W. Bouldin. A Cluster
Separation Measure. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 4:224-227, 1979.
[10] P. Edwards and W. Davies. A Heterogeneous
Multi-Agent Learning System. In Proceedings of the
Special Interest Group on Cooperating Knowledge
Based Systems, pages 163-184, 1993.
[11] J. Ghosh, A. Strehl, and S. Merugu. A Consensus
Framework for Integrating Distributed Clusterings
Under Limited Knowledge Sharing. In NSF Workshop
on Next Generation Data Mining, 99-108, 2002.
[12] D. L. Grecu and L. A. Becker. Coactive Learning for
Distributed Data Mining. In Proceedings of KDD-98,
pages 209-213, New York, NY, August 1998.
[13] M. Klusch, S. Lodi, and G. Moro. Agent-based
distributed data mining: The KDEC scheme. In
AgentLink, number 2586 in LNCS. Springer, 2003.
[14] T. M. Mitchell. Machine Learning, pages 29-36.
McGraw-Hill, New York, 1997.
[15] S. Ontanon and E. Plaza. Recycling Data for
Multi-Agent Learning. In Proc. of ICML-05, 2005.
[16] L. Panait and S. Luke. Cooperative multi-agent
learning: The state of the art. Autonomous Agents
and Multi-Agent Systems, 11(3):387-434, 2005.
[17] B. Park and H. Kargupta. Distributed Data Mining:
Algorithms, Systems, and Applications. In N. Ye,
editor, Data Mining Handbook, pages 341-358, 2002.
[18] F. J. Provost and D. N. Hennessy. Scaling up:
Distributed machine learning with cooperation. In
Proc. of AAAI-96, pages 74-79. AAAI Press, 1996.
[19] S. Sian. Extending learning to multiple agents: Issues
and a model for multi-agent machine learning
(ma-ml). In Y. Kodratoff, editor, Machine 
LearningEWSL-91, pages 440-456. Springer-Verlag, 1991.
[20] R. Smith. The contract-net protocol: High-level
communication and control in a distributed problem
solver. IEEE Transactions on Computers,
C-29(12):1104-1113, 1980.
[21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee,
D. W. Fan, and P. K. Chan. Jam: Java Agents for
Meta-Learning over Distributed Databases. In Proc. of
the KDD-97, pages 74-81, USA, 1997.
[22] J. Toˇziˇcka, M. Jakob, and M. Pˇechouˇcek.
Market-Inspired Approach to Collaborative Learning.
In Cooperative Information Agents X (CIA 2006),
volume 4149 of LNCS, pages 213-227. Springer, 2006.
[23] Y. Z. Wei, L. Moreau, and N. R. Jennings.
Recommender systems: a market-based design. In
Proceedings of AAMAS-03), pages 600-607, 2003.
[24] G. Weiß. A Multiagent Perspective of Parallel and
Distributed Machine Learning. In Proceedings of
Agents"98, pages 226-230, 1998.
[25] G. Weiss and P. Dillenbourg. What is "multi" in
multi-agent learning? Collaborative-learning:
Cognitive and Computational Approaches, 64-80, 1999.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 685
Approximate and Online Multi-Issue Negotiation
Shaheen S. Fatima
Department of
Computer Science
University of Liverpool
Liverpool L69 3BX, UK.
shaheen@csc.liv.ac.uk
Michael Wooldridge
Department of
Computer Science
University of Liverpool
Liverpool L69 3BX, UK.
mjw@csc.liv.ac.uk
Nicholas R. Jennings
School of Electronics and
Computer Science
University of Southampton
Southampton SO17 1BJ, UK.
nrj@ecs.soton.ac.uk
ABSTRACT
This paper analyzes bilateral multi-issue negotiation between 
selfinterested autonomous agents. The agents have time constraints in
the form of both deadlines and discount factors. There are m > 1
issues for negotiation where each issue is viewed as a pie of size
one. The issues are indivisible (i.e., individual issues cannot be
split between the parties; each issue must be allocated in its 
entirety to either agent). Here different agents value different issues
differently. Thus, the problem is for the agents to decide how to
allocate the issues between themselves so as to maximize their 
individual utilities. For such negotiations, we first obtain the 
equilibrium strategies for the case where the issues for negotiation are
known a priori to the parties. Then, we analyse their time 
complexity and show that finding the equilibrium offers is an NP-hard
problem, even in a complete information setting. In order to 
overcome this computational complexity, we then present negotiation
strategies that are approximately optimal but computationally 
efficient, and show that they form an equilibrium. We also analyze the
relative error (i.e., the difference between the true optimum and the
approximate). The time complexity of the approximate equilibrium
strategies is O(nm/ 2
) where n is the negotiation deadline and
the relative error. Finally, we extend the analysis to online 
negotiation where different issues become available at different time points
and the agents are uncertain about their valuations for these issues.
Specifically, we show that an approximate equilibrium exists for
online negotiation and show that the expected difference between
the optimum and the approximate is O(
√
m) . These approximate
strategies also have polynomial time complexity.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems
General Terms
Algorithms, Design, Theory
1. INTRODUCTION
Negotiation is a key form of interaction in multiagent systems. It
is a process in which disputing agents decide how to divide the
gains from cooperation. Since this decision is made jointly by the
agents themselves [20, 19, 13, 15], each party can only obtain what
the other is prepared to allow them. Now, the simplest form of
negotiation involves two agents and a single issue. For example,
consider a scenario in which a buyer and a seller negotiate on the
price of a good. To begin, the two agents are likely to differ on the
price at which they believe the trade should take place, but through
a process of joint decision-making they either arrive at a price that
is mutually acceptable or they fail to reach an agreement. Since
agents are likely to begin with different prices, one or both of them
must move toward the other, through a series of offers and counter
offers, in order to obtain a mutually acceptable outcome. However,
before the agents can actually perform such negotiations, they must
decide the rules for making offers and counter offers. That is, they
must set the negotiation protocol [20]. On the basis of this protocol,
each agent chooses its strategy (i.e., what offers it should make
during the course of negotiation). Given this context, this work
focuses on competitive scenarios with self-interested agents. For
such cases, each participant defines its strategy so as to maximise
its individual utility.
However, in most bilateral negotiations, the parties involved need
to settle more than one issue. For this case, the issues may be 
divisible or indivisible [4]. For the former, the problem for the agents
is to decide how to split each issue between themselves [21]. For
the latter, the individual issues cannot be divided. An issue, in its
entirety, must be allocated to either of the two agents. Since the
agents value different issues differently, they must come to terms
about who will take which issue. To date, most of the existing
work on multi-issue negotiation has focussed on the former case
[7, 2, 5, 23, 11, 6]. However, in many real-world settings, the 
issues are indivisible. Hence, our focus here is on negotiation for
indivisible issues. Such negotiations are very common in 
multiagent systems. For example, consider the case of task allocation
between two agents. There is a set of tasks to be carried out and
different agents have different preferences for the tasks. The tasks
cannot be partitioned; a task must be carried out by one agent. The
problem then is for the agents to negotiate about who will carry out
which task.
A key problem in the study of multi-issue negotiation is to 
determine the equilibrium strategies. An equally important problem,
especially in the context of software agents, is to find the time 
complexity of computing the equilibrium offers. However, such 
computational issues have so far received little attention. As we will
show, this is mainly due to the fact that existing work (describe in
Section 5) has mostly focused on negotiation for divisible issues
951
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
and finding the equilibrium for this case is computationally easier
than that for the case of indivisible issues. Our primary objective is,
therefore, to answer the computational questions for the latter case
for the types of situations that are commonly faced by agents in
real-world contexts. Thus, we consider negotiations in which there
is incomplete information and time constraints. Incompleteness of
information on the part of negotiators is a common feature of most
practical negotiations. Also, agents typically have time constraints
in the form of both deadlines and discount factors. Deadlines are an
essential element since negotiation cannot go on indefinitely, rather
it must end within a reasonable time limit. Likewise, discount 
factors are essential since the goods may be perishable or their value
may decline due to inflation. Moreover, the strategic behaviour of
agents with deadlines and discount factors differs from those 
without (see [21] for single issue bargaining without deadlines and [23,
13] for bargaining with deadlines and discount factors in the 
context of divisible issues).
Given this, we consider indivisible issues and first analyze the
strategic behaviour of agents to obtain the equilibrium strategies
for the case where all the issues for negotiation are known a priori
to both agents. For this case, we show that the problem of finding
the equilibrium offers is NP-hard, even in a complete information
setting. Then, in order to overcome the problem of time 
complexity, we present strategies that are approximately optimal but 
computationally efficient, and show that they form an equilibrium. We
also analyze the relative error (i.e., the difference between the true
optimum and the approximate). The time complexity of the 
approximate equilibrium strategies is O(nm/ 2
) where n is the 
negotiation deadline and the relative error. Finally, we extend the
analysis to online negotiation where different issues become 
available at different time points and the agents are uncertain about their
valuations for these issues. Specifically, we show that an 
approximate equilibrium exists for online negotiation and show that the
expected difference between the optimum and the approximate is
O(
√
m) . These approximate strategies also have polynomial time
complexity.
In so doing, our contribution lies in analyzing the computational
complexity of the above multi-issue negotiation problem, and 
finding the approximate and online equilibria. No previous work has
determined these equilibria. Since software agents have limited
computational resources, our results are especially relevant to such
resource bounded agents.
The remainder of the paper is organised as follows. We begin by
giving a brief overview of single-issue negotiation in Section 2. In
Section 3, we obtain the equilibrium for multi-issue negotiation and
show that finding equilibrium offers is an NP-hard problem. We
then present an approximate equilibrium and evaluate its 
approximation error. Section 4 analyzes online multi-issue negotiation.
Section 5 discusses the related literature and Section 6 concludes.
2. SINGLE-ISSUE NEGOTIATION
We adopt the single issue model of [27] because this is a model
where, during negotiation, the parties are allowed to make offers
from a set of discrete offers. Since our focus is on indivisible issues
(i.e., parties are allowed to make one of two possible offers: zero
or one), our scenario fits in well with [27]. Hence we use this basic
single issue model and extend it to multiple issues. Before doing
so, we give an overview of this model and its equilibrium strategies.
There are two strategic agents: a and b. Each agent has time
constraints in the form of deadlines and discount factors. The two
agents negotiate over a single indivisible issue (i). This issue is a
‘pie" of size 1 and the agents want to determine who gets the pie.
There is a deadline (i.e., a number of rounds by which negotiation
must end). Let n ∈ N+
denote this deadline. The agents use an
alternating offers protocol (as the one of Rubinstein [18]), which
proceeds through a series of time periods. One of the agents, say
a, starts negotiation in the first time period (i.e., t = 1) by making
an offer (xi = 0 or 1) to b. Agent b can either accept or reject the
offer. If it accepts, negotiation ends in an agreement with a getting
xi and b getting yi = 1 − xi. Otherwise, negotiation proceeds to
the next time period, in which agent b makes a counter-offer. This
process of making offers continues until one of the agents either
accepts an offer or quits negotiation (resulting in a conflict). Thus,
there are three possible actions an agent can take during any time
period: accept the last offer, make a new counter-offer, or quit the
negotiation.
An essential feature of negotiations involving alternating offers
is that the agents" utilities decrease with time [21]. Specifically,
the decrease occurs at each step of offer and counteroffer. This
decrease is represented with a discount factor denoted 0 < δi ≤ 1
for both1
agents.
Let [xt
i, yt
i ] denote the offer made at time period t where xt
i and
yt
i denote the share for agent a and b respectively. Then, for a given
pie, the set of possible offers is:
{[xt
i, yt
i ] : xt
i = 0 or 1, yt
i = 0 or 1, and xt
i + yt
i = 1}
At time t, if a and b receive a share of xt
i and yt
i respectively, then
their utilities are:
ua
i (xt
i, t) =
j
xt
i × δt−1
if t ≤ n
0 otherwise
ub
i (yt
i , t) =
j
yt
i × δt−1
if t ≤ n
0 otherwise
The conflict utility (i.e., the utility received in the event that no deal
is struck) is zero for both agents.
For the above setting, the agents reason as follows in order to
determine what to offer at t = 1. We let A(1) (B(1)) denote a"s
(b"s) equilibrium offer for the first time period. Let agent a denote
the first mover (i.e., at t = 1, a proposes to b who should get the
pie). To begin, consider the case where the deadline for both agents
is n = 1. If b accepts, the division occurs as agreed; if not, neither
agent gets anything (since n = 1 is the deadline). Here, a is in a
powerful position and is able to propose to keep 100 percent of the
pie and give nothing to b 2
. Since the deadline is n = 1, b accepts
this offer and agreement takes place in the first time period.
Now, consider the case where the deadline is n = 2. In order to
decide what to offer in the first round, a looks ahead to t = 2 and
reasons backwards. Agent a reasons that if negotiation proceeds
to the second round, b will take 100 percent of the pie by offering
[0, 1] and leave nothing for a. Thus, in the first time period, if a
offers b anything less than the whole pie, b will reject the offer.
Hence, during the first time period, agent a offers [0, 1]. Agent b
accepts this and an agreement occurs in the first time period.
In general, if the deadline is n, negotiation proceeds as follows.
As before, agent a decides what to offer in the first round by 
looking ahead as far as t = n and then reasoning backwards. Agent a"s
1
Having a different discount factor for different agents only makes
the presentation more involved without leading to any changes in
the analysis of the strategic behaviour of the agents or the time 
complexity of finding the equilibrium offers. Hence we have a single
discount factor for both agents.
2
It is possible that b may reject such a proposal. However, 
irrespective of whether b accepts or rejects the proposal, it gets zero utility
(because the deadline is n = 1). Thus, we assume that b accepts
a"s offer.
952 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
offer for t = 1 depends on who the offering agent is for the last
time period. This, in turn, depends on whether n is odd or even.
Since a makes an offer at t = 1 and the agents use the alternating
offers protocol, the offering agent for the last time period is b if n
is even and it is a if n is odd. Thus, depending on whether n is odd
or even, a makes the following offer at t = 1:
A(1) =
j
OFFER [1, 0] IF ODD n
ACCEPT IF b"s TURN
B(1) =
j
OFFER [0, 1] IF EVEN n
ACCEPT IF a"s TURN
Agent b accepts this offer and negotiation ends in the first time
period. Note that the equilibrium outcome depends on who makes
the first move. Since we have two agents and either of them could
move first, we get two possible equilibrium outcomes.
On the basis of the above equilibrium for single-issue 
negotiation with complete information, we first obtain the equilibrium for
multiple issues and then show that computing these offers is a hard
problem. We then present a time efficient approximate equilibrium.
3. MULTI-ISSUE NEGOTIATION
We first analyse the complete information setting. This section
forms the base which we extend to the case of information 
uncertainty in Section 4.
Here a and b negotiate over m > 1 indivisible issues. These
issues are m distinct pies and the agents want to determine how
to distribute the pies between themselves. Let S = {1, 2, . . . , m}
denote the set of m pies. As before, each pie is of size 1. Let the
discount factor for issue c, where 1 ≤ c ≤ m, be 0 < δc ≤ 1.
For each issue, let n denote each agent"s deadline. In the offer for
time period t (where 1 ≤ t ≤ n), agent a"s (b"s) share for each of
the m issues is now represented as an m element vector xt
∈ Bm
(yt
∈ Bm
) where B denotes the set {0, 1}. Thus, if agent a"s share
for issue c at time t is xt
c, then agent b"s share is yt
c = (1−xt
c). The
shares for a and b are together represented as the package [xt
, yt
].
As is traditional in multi-issue utility theory, we define an agent"s
cumulative utility using the standard additive form [12]. The 
functions Ua
: Bm
× Bm
× N+
→ R and Ub
: Bm
× Bm
× N+
→ R
give the cumulative utilities for a and b respectively at time t. These
are defined as follows:
Ua
([xt
, yt
], t) =
(
Σm
c=1ka
c ua
c (xt
c, t) if t ≤ n
0 otherwise
(1)
Ub
([xt
, yt
], t) =
(
Σm
c=1kb
cub
c(yt
c, t) if t ≤ n
0 otherwise
(2)
where ka
∈ Nm
+ denotes an m element vector of constants for
agent a and kb
∈ Nm
+ that for b. Here N+ denotes the set of 
positive integers. These vectors indicate how the agents value different
issues. For example, if ka
c > ka
c+1, then agent a values issue c
more than issue c + 1. Likewise for agent b. In other words, the m
issues are perfect substitutes (i.e., all that matters to an agent is its
total utility for all the m issues and not that for any subset of them).
In all the settings we study, the issues will be perfect substitutes.
To begin each agent has complete information about all negotiation
parameters (i.e., n, m, ka
c , kb
c, and δc for 1 ≤ c ≤ m).
Now, multi-issue negotiation can be done using different 
procedures. Broadly speaking, there are three key procedures for 
negotiating multiple issues [19]:
1. the package deal procedure where all the issues are settled
together as a bundle,
2. the sequential procedure where the issues are discussed one
after another, and
3. the simultaneous procedure where the issues are discussed in
parallel.
Between these three procedures, the package deal is known to 
generate Pareto optimal outcomes [19, 6]. Hence we adopt it here. We
first give a brief description of the procedure and then determine
the equilibrium strategies for it.
3.1 The package deal procedure
In this procedure, the agents use the same protocol as for 
singleissue negotiation (described in Section 2). However, an offer for the
package deal includes a proposal for each issue under negotiation.
Thus, for m issues, an offer includes m divisions, one for each
issue. Agents are allowed to either accept a complete offer (i.e., all
m issues) or reject a complete offer. An agreement can therefore
take place either on all m issues or on none of them.
As per the single-issue negotiation, an agent decides what to 
offer by looking ahead and reasoning backwards. However, since an
offer for the package deal includes a share for all the m issues, the
agents can now make tradeoffs across the issues in order to 
maximise their cumulative utilities.
For 1 ≤ c ≤ m, the equilibrium offer for issue c at time t is
denoted as [at
c, bt
c] where at
c and bt
c denote the shares for agent a
and b respectively. We denote the equilibrium package at time t
as [at
, bt
] where at
∈ Bm
(bt
∈ Bm
) is an m element vector
that denotes a"s (b"s) share for each of the m issues. Also, for
1 ≤ c ≤ m, δc is the discount factor for issue c. The symbols 0
and 1 denote m element vectors of zeroes and ones respectively.
Note that for 1 ≤ t ≤ n, at
c + bt
c = 1 (i.e., the sum of the agents"
shares (at time t) for each pie is one). Finally, for time period t (for
1 ≤ t ≤ n) we let A(t) (respectively B(t)) denote the equilibrium
strategy for agent a (respectively b).
3.2 Equilibrium strategies
As mentioned in Section 1, the package deal allows agents to make
tradeoffs. We let TRADEOFFA (TRADEOFFB) denote agent a"s (b"s)
function for making tradeoffs. We let P denote a set of parameters
to the procedure TRADEOFFA (TRADEOFFB) where P = {ka
, kb
, δ, m}.
Given this, the following theorem characterises the equilibrium for
the package deal procedure.
THEOREM 1. For the package deal procedure, the following
strategies form a Nash equilibrium. The equilibrium strategy for
t = n is:
A(n) =
j
OFFER [1, 0] IF a"s TURN
ACCEPT IF b"s TURN
B(n) =
j
OFFER [0, 1] IF b"s TURN
ACCEPT IF a"s TURN
For all preceding time periods t < n, if [xt
, yt
] denotes the 
offer made at time t, then the equilibrium strategies are defined as
follows:
A(t) =
8
<
:
OFFER TRADEOFFA(P, UB(t), t) IF a"s TURN
If (Ua
([xt
, yt
], t) ≥ UA(t)) ACCEPT
else REJECT IF b"s TURN
B(t) =
8
<
:
OFFER TRADEOFFB(P, UA(t), t) IF b"s TURN
If (Ub
([xt
, yt
], t) ≥ UB(t)) ACCEPT
else REJECT IF a"s TURN
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 953
where UA(t) = Ua
([at+1
, bt+1
], t + 1) and UB(t) = Ub
([at+1
,
bt+1
], t + 1).
PROOF. We look ahead to the last time period (i.e., t = n) and
then reason backwards. To begin, if negotiation reaches the 
deadline (n), then the agent whose turn it is takes everything and leaves
nothing for its opponent. Hence, we get the strategies A(n) and
B(n) as given in the statement of the theorem.
In all the preceding time periods (t < n), the offering agent 
proposes a package that gives its opponent a cumulative utility equal
to what the opponent would get from its own equilibrium offer for
the next time period. During time period t, either a or b could
be the offering agent. Consider the case where a makes an offer
at t. The package that a offers at t gives b a cumulative utility
of Ub
([at+1
, bt+1
], t + 1). However, since there is more than one
issue, there is more than one package that gives b this cumulative
utility. From among these packages, a offers the one that maximises
its own cumulative utility (because it is a utility maximiser). Thus,
the problem for a is to find the package [at
, bt
] so as to:
maximize
mX
c=1
ka
c (1 − bt
c)δt−1
c (3)
such that
mX
c=1
bt
ckb
c ≥ UB(t)
bt
c = 0 or 1 for 1 ≤ c ≤ m
where UB(t), δt−1
c , ka
c , and kb
c are constants and bt
c (1 ≤ c ≤ m)
is a variable.
Assume that the function TRADEOFFA takes parameters P, UB(t),
and t, to solve the maximisation problem given in Equation 3 and
returns the corresponding package. If there is more than one 
package that solves Equation 3, then TRADEOFFA returns any one of
them (because agent a gets equal utility from all such packages
and so does agent b). The function TRADEOFFB for agent b is 
analogous to that for a.
On the other hand, the equilibrium strategy for the agent that
receives an offer is as follows. For time period t, let b denote the
receiving agent. Then, b accepts [xt
, yt
] if UB(t) ≤ Ub
([xt
, yt
], t),
otherwise it rejects the offer because it can get a higher utility in
the next time period. The equilibrium strategy for a as receiving
agent is defined analogously.
In this way, we reason backwards and obtain the offers for the
first time period. Thus, we get the equilibrium strategies (A(t) and
B(t)) given in the statement of the theorem.
The following example illustrates how the agents make tradeoffs
using the above equilibrium strategies.
EXAMPLE 1. Assume there are m = 2 issues for negotiation,
the deadline for both issues is n = 2, and the discount factor for
both issues for both agents is δ = 1/2. Let ka
1 = 3, ka
2 = 1,
kb
1 = 1, and kb
2 = 5. Let agent a be the first mover. By 
using backward reasoning, a knows that if negotiation reaches the
second time period (which is the deadline), then b will get a 
hundred percent of both the issues. This gives b a cumulative utility of
UB(2) = 1/2 + 5/2 = 3. Thus, in the first time period, if b gets
anything less than a utility of 3, it will reject a"s offer. So, at t = 1,
a offers the package where it gets issue 1 and b gets issue 2. This
gives a cumulative utility of 3 to a and 5 to b. Agent b accepts the
package and an agreement takes place in the first time period.
The maximization problem in Equation 3 can be viewed as the 0-1
knapsack problem3
. In the 0-1 knapsack problem, we have a set
3
Note that for the case of divisible issues this is the fractional 
knapof m items where each item has a profit and a weight. There is a
knapsack with a given capacity. The objective is to fill the knapsack
with items so as to maximize the cumulative profit of the items in
the knapsack. This problem is analogous to the negotiation problem
we want to solve (i.e., the maximization problem of Equation 3).
Since ka
c and δt−1
c are constants, maximizing
Pm
c=1 ka
c (1−bt
c)δt−1
c
is the same as minimizing
Pm
c=1 ka
c bt
c. Hence Equation 3 can be
written as:
minimize
mX
c=1
ka
c bt
c (4)
such that
mX
c=1
bt
ckb
c ≥ UB(t)
bt
c = 0 or 1 for 1 ≤ c ≤ m
Equation 4 is a minimization version of the standard 0-1 knapsack
problem4
with m items where ka
c represents the profit for item c,
kb
c the weight for item c, and UB(t) the knapsack capacity.
Example 1 was for two issues and so it was easy to find the 
equilibrium offers. But, in general, it is not computationally easy to
find the equilibrium offers of Theorem 1. The following theorem
proves this.
THEOREM 2. For the package deal procedure, the problem of
finding the equilibrium offers given in Theorem 1 is NP-hard.
PROOF. Finding the equilibrium offers given in Theorem 1 
requires solving the 0-1 knapsack problem given in Equation 4. Since
the 0-1 knapsack problem is NP-hard [17], the problem of finding
equilibrium for the package deal is also NP-hard.
3.3 Approximate equilibrium
Researchers in the area of algorithms have found time efficient
methods for computing approximate solutions to 0-1 knapsack 
problems [10]. Hence we use these methods to find a solution to our 
negotiation problem. At this stage, we would like to point out the
main difference between solving the 0-1 knapsack problem and
solving our negotiation problem. The 0-1 knapsack problem 
involves decision making by a single agent regarding which items to
place in the knapsack. On the other hand, our negotiation problem
involves two players and they are both strategic. Hence, in our case,
it is not enough to just find an approximate solution to the knapsack
problem, we must also show that such an approximation forms an
equilibrium.
The traditional approach for overcoming the computational 
complexity in finding an equilibrium has been to use an approximate
equilibrium (see [14, 26] for example). In this approach, a strategy
profile is said to form an approximate Nash equilibrium if neither
agent can gain more than the constant by deviating. Hence, our
aim is to use the solution to the 0-1 knapsack problem proposed
in [10] and show that it forms an approximate equilibrium to our
negotiation problem. Before doing so, we give a brief overview of
the key ideas that underlie approximation algorithms.
There are two key issues in the design of approximate algorithms
[1]:
sack problem. The factional knapsack problem is computationally
easy; it can be solved in time polynomial in the number of items in
the knapsack problem [17]. In contrast, the 0-1 knapsack problem
is computationally hard.
4
Note that for the standard 0-1 knapsack problem the weights, 
profits and the capacity are positive integers. However a 0-1 knapsack
problem with fractions and non positive values can easily be 
transformed to one with positive integers in time linear in m using the
methods given in [8, 17].
954 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
1. the quality of their solution, and
2. the time taken to compute the approximation.
The quality of an approximate algorithm is determined by 
comparing its performance to that of the optimal algorithm and measuring
the relative error [3, 1]. The relative error is defined as (z−z∗
)/z∗
where z is the approximate solution and z∗
the optimal one. In
general, we are interested in finding approximate algorithms whose
relative error is bounded from above by a certain constant , i.e.,
(z − z∗
)/z∗
≤ (5)
Regarding the second issue of time complexity, we are interested in
finding fully polynomial approximation algorithms. An 
approximation algorithm is said to be fully polynomial if for any > 0 it finds
a solution satisfying Equation 5 in time polynomially bounded by
size of the problem (for the 0-1 knapsack problem, the problem size
is equal to the number of items) and by 1/ [1].
For the 0-1 knapsack problem, Ibarra and Kim [10] presented a
fully polynomial approximation method. This method is based on
dynamic programming. It is a parametric method that takes as a
parameter and for any > 0, finds a heuristic solution z with 
relative error at most , such that the time and space complexity grow
polynomially with the number of items m and 1/ . More 
specifically, the space and time complexity are both O(m/ 2
) and hence
polynomial in m and 1/ (see [10] for the detailed approximation
algorithm and proof of time and space complexity).
Since the Ibarra and Kim method is fully polynomial, we use it to
solve our negotiation problem. This is done as follows. For agent
a, let APRX-TRADEOFFA(P, UB(t), t, ) denote a procedure that
returns an approximate solution to Equation 4 using the Ibarra and
Kim method. The procedure APRX-TRADEOFFB(P, UA(t), t, ) for
agent b is analogous.
For 1 ≤ c ≤ m, the approximate equilibrium offer for issue c
at time t is denoted as [¯at
c,¯bt
c] where ¯at
c and ¯bt
c denote the shares
for agent a and b respectively. We denote the equilibrium package
at time t as [¯at
,¯bt
] where ¯at
∈ Bm
(¯bt
∈ Bm
) is an m element
vector that denotes a"s (b"s) share for each of the m issues. Also,
as before, for 1 ≤ c ≤ m, δc is the discount factor for issue c.
Note that for 1 ≤ t ≤ n, ¯at
c + ¯bt
c = 1 (i.e., the sum of the agents"
shares (at time t) for each pie is one). Finally, for time period t (for
1 ≤ t ≤ n) we let ¯A(t) (respectively ¯B(t)) denote the approximate
equilibrium strategy for agent a (respectively b).The following 
theorem uses this notation and characterizes an approximate 
equilibrium for multi-issue negotiation.
THEOREM 3. For the package deal procedure, the following
strategies form an approximate Nash equilibrium. The 
equilibrium strategy for t = n is:
¯A(n) =
j
OFFER [1, 0] IF a"s TURN
ACCEPT IF b"s TURN
¯B(n) =
j
OFFER [0, 1] IF b"s TURN
ACCEPT IF a"s TURN
For all preceding time periods t < n, if [xt
, yt
] denotes the 
offer made at time t, then the equilibrium strategies are defined as
follows:
¯A(t) =
8
<
:
OFFER APRX-TRADEOFFA(P, UB(t), t, ) IF a"s TURN
If (Ua
([xt
, yt
], t) ≥ UA(t)) ACCEPT
else REJECT IF b"s TURN
¯B(t) =
8
<
:
OFFER APRX-TRADEOFFB(P, UA(t), t, ) IF b"s TURN
If (Ub
([xt
, yt
], t) ≥ UB(t)) ACCEPT
else REJECT IF a"s TURN
where UA(t) = Ua
([¯at+1
,¯bt+1
], t + 1) and UB(t) = Ub
([¯at+1
,
¯bt+1
], t + 1). An agreement takes place at t = 1.
PROOF. As in the proof for Theorem 1, we use backward 
reasoning. We first obtain the strategies for the last time period t = n.
It is straightforward to get these strategies; the offering agent gets
a hundred percent of all the issues.
Then for t = n − 1, the offering agent must solve the 
maximization problem of Equation 4 by substituting t = n−1 in it. For agent
a (b), this is done by APPROX-TRADEOFFA (APPROX-TRADEOFFB).
These two functions are nothing but the Ibarra and Kim"s 
approximation method for solving the 0-1 knapsack problem. These two
functions take as a parameter and use the Ibarra and Kim"s 
approximation method to return a package that approximately 
maximizes Equation 4. Thus, the relative error for these two functions
is the same as that for Ibarra and Kim"s method (i.e., it is at most
where is given in Equation 5).
Assume that a is the offering agent for t = n − 1. Agent a must
offer a package that gives b a cumulative utility equal to what it
would get from its own approximate equilibrium offer for the next
time period (i.e., Ub
([¯at+1
,¯bt+1
], t + 1) where [¯at+1
,¯bt+1
] is the
approximate equilibrium package for the next time period). Recall
that for the last time period, the offering agent gets a hundred 
percent of all the issues. Since a is the offering agent for t = n − 1
and the agents use the alternating offers protocol, it is b"s turn at
t = n. Thus Ub
([¯at+1
,¯bt+1
], t + 1) is equal to b"s cumulative
utility from receiving a hundred percent of all the issues. Using this
utility as the capacity of the knapsack, a uses APPROX-TRADEOFFA
and obtains the approximate equilibrium package for t = n − 1.
On the other hand, if b is the offering agent at t = n − 1, it uses
APPROX-TRADEOFFB to obtain the approximate equilibrium 
package.
In the same way for t < n − 1, the offering agent (say a) uses
APPROX-TRADEOFFA to find an approximate equilibrium package
that gives b a utility of Ub
([¯at+1
,¯bt+1
], t + 1). By reasoning 
backwards, we obtain the offer for time period t = 1. If a (b) is the 
offering agent, it proposes the offer APPROX-TRADEOFFA(P, UB(1), 1, )
(APPROX-TRADEOFFB(P, UA(1), 1, )). The receiving agent 
accepts the offer. This is because the relative error in its cumulative
utility from the offer is at most . An agreement therefore takes
place in the first time period.
THEOREM 4. The time complexity of finding the approximate
equilibrium offer for the first time period is O(nm/ 2
).
PROOF. The time complexity of APPROX-TRADEOFFA and 
APPROXTRADEOFFB is the same as the time complexity of the Ibarra and
Kim method [10] i.e., O(m/ 2
)). In order to find the equilibrium
offer for the first time period using backward reasoning, 
APPROXTRADEOFFA (or APPROX- TRADEOFFB) is invoked n times. Hence
the time complexity of finding the approximate equilibrium offer
for the first time period is O(nm/ 2
).
This analysis was done in a complete information setting. 
However an extension of this analysis to an incomplete information 
setting where the agents have probability distributions over some 
uncertain parameter is straightforward, as long as the negotiation is
done offline; i.e., the agents know their preference for each 
individual issue before negotiation begins. For instance, consider the case
where different agents have different discount factors, and each
agent is uncertain about its opponent"s discount factor although it
knows its own. This uncertainty is modelled with a probability 
distribution over the possible values for the opponent"s discount factor
and having this distribution as common knowledge to the agents.
All our analysis for the complete information setting still holds for
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 955
this incomplete information setting, except for the fact that an agent
must now use the given probability distribution and find its 
opponent"s expected utility instead of its actual utility. Hence, instead of
analyzing an incomplete information setting for offline negotiation,
we focus on online multi-issue negotiation.
4. ONLINE MULTI-ISSUE NEGOTIATION
We now consider a more general and, arguably more realistic, 
version of multi-issue negotiation, where the agents are uncertain about
the issues they will have to negotiate about in future. In this setting,
when negotiating an issue, the agents know that they will negotiate
more issues in the future, but they are uncertain about the details of
those issues. As before, let m be the total number of issues that are
up for negotiation. The agents have a probability distribution over
the possible values of ka
c and kb
c. For 1 ≤ c ≤ m let ka
c and kb
c be
uniformly distributed over [0,1]. This probability distribution, n,
and m are common knowledge to the agents. However, the agents
come to know ka
c and kb
c only just before negotiation for issue c
begins. Once the agents reach an agreement on issue c, it cannot be
re-negotiated.
This scenario requires online negotiation since the agents must
make decisions about an issue prior to having the information about
the future issues [3]. We first give a brief introduction to online
problems and then draw an analogy between the online knapsack
problem and the negotiation problem we want to solve.
In an online problem, data is given to the algorithm 
incrementally, one unit at a time [3]. The online algorithm must also 
produce the output incrementally: after seeing i units of input it must
output the ith unit of output. Since decisions about the output are
made with incomplete knowledge about the entire input, an 
online algorithm often cannot produce an optimal solution. Such an
algorithm can only approximate the performance of the optimal 
algorithm that sees all the inputs in advance. In the design of online
algorithms, the main aim is to achieve a performance that is close
to that of the optimal offline algorithm on each input. An online 
algorithm is said to be stochastic if it makes decisions on the basis of
the probability distributions for the future inputs. The performance
of stochastic online algorithms is assessed in terms of the expected
difference between the optimum and the approximate solution 
(denoted E[z∗
m −zm] where z∗
m is the optimal and zm the approximate
solution). Note that the subscript m is used to indicate the fact that
this difference depends on m.
We now describe the protocol for online negotiation and then
obtain an approximate equilibrium. The protocol is defined as 
follows. Let agent a denote the first mover (since we focus on the
package deal procedure, the first mover is the same for all the m
issues).
Step 1. For c = 1, the agents are given the values of ka
c and kb
c.
These two values are now common5
knowledge.
Step 2. The agents settle issue c using the alternating offers 
protocol described in Section 2. Negotiation for issue c must end
within n time periods from the start of negotiation on the 
issue. If an agreement is not reached within this time, then
negotiation fails on this and on all remaining issues.
Step 3. The above steps are repeated for issues c = 2, 3, . . . , m.
Negotiation for issue c (2 ≤ c ≤ m) begins in the time
period following an agreement on issue c − 1.
5
We assume common knowledge because it simplifies exposition.
However, if ka
c (kb
c) is a"s (b"s) private knowledge, then our analysis
will still hold but now an agent must find its opponent"s expected
utility on the basis of the p.d.fs for ka
c and kb
c.
Thus, during time period t, the problem for the offering agent (say
a) is to find the optimal offer for issue c on the basis of ka
c and
kb
c and the probability distribution for ka
i and kb
i (c < i ≤ m).
In order to solve this online negotiation problem we draw analogy
with the online knapsack problem. Before doing so, however, we
give a brief overview of the online knapsack problem.
In the online knapsack problem, there are m items. The agent
must examine the m items one at a time according to the order they
are input (i.e., as their profit and size coefficients become known).
Hence, the algorithm is required to decide whether or not to 
include each item in the knapsack as soon as its weight and profit
become known, without knowledge concerning the items still to be
seen, except for their total number. Note that since the agents have
a probability distribution over the weights and profits of the future
items, this is a case of stochastic online knapsack problem. Our 
online negotiation problem is analogous to the online knapsack 
problem. This analogy is described in detail in the proof for Theorem 5.
Again, researchers in algorithms have developed time efficient 
approximate solutions to the online knapsack problem [16]. Hence
we use this solution and show that it forms an equilibrium.
The following theorem characterizes an approximate equilibrium
for online negotiation. Here the agents have to choose a 
strategy without knowing the features of the future issues. Because of
this information incompleteness, the relevant equilibrium solution
is that of a Bayes" Nash Equilibrium (BNE) in which each agent
plays the best response to the other agents with respect to their 
expected utilities [18]. However, finding an agent"s BNE strategy is
analogous to solving the online 0-1 knapsack problem. Also, the
online knapsack can only be solved approximately [16]. Hence
the relevant equilibrium solution concept is approximate BNE (see
[26] for example). The following theorem finds this equilibrium
using procedures ONLINE- TRADEOFFA and ONLINE-TRADEOFFB
which are defined in the proof of the theorem. For a given time
period, we let zm denote the approximately optimal solution 
generated by ONLINE-TRADEOFFA (or ONLINE-TRADEOFFB) and z∗
m
the actual optimum.
THEOREM 5. For the package deal procedure, the following
strategies form an approximate Bayes" Nash equilibrium. The 
equilibrium strategy for t = n is:
A(n) =
j
OFFER [1, 0] IF a"s TURN
ACCEPT IF b"s TURN
B(n) =
j
OFFER [0, 1] IF b"s TURN
ACCEPT IF a"s TURN
For all preceding time periods t < n, if [xt
, yt
] denotes the 
offer made at time t, then the equilibrium strategies are defined as
follows:
A(t) =
8
<
:
OFFER ONLINE-TRADEOFFA(P, UB(t), t) IF a"s TURN
If (Ua
([xt
, yt
], t) ≥ UA(t)) ACCEPT
else REJECT IF b"s TURN
B(t) =
8
<
:
OFFER ONLINE-TRADEOFFB(P, UA(t), t) IF b"s TURN
If (Ub
([xt
, yt
], t) ≥ UB(t)) ACCEPT
else REJECT IF a"s TURN
where UA(t) = Ua
([¯at+1
,¯bt+1
], t + 1) and UB(t) = Ub
([¯at+1
,
¯bt+1
], t + 1). An agreement on issue c takes place at t = c. For a
given time period, the expected difference between the solution 
generated by the optimal strategy and that by the approximate strategy
is E[z∗
m − zm] = O(
√
m).
956 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
PROOF. As in Theorem 1 we find the equilibrium offer for time
period t = 1 using backward induction. Let a be the offering agent
for t = 1 for all the m issues. Consider the last time period t = n
(recall from Step 2 of the online protocol that n is the deadline for
completing negotiation on the first issue). Since the first mover is
the same for all the issues, and the agents make offers alternately,
the offering agent for t = n is also the same for all the m issues.
Assume that b is the offering agent for t = n. As in Section 3,
the offering agent for t = n gets a hundred percent of all the m
issues. Since b is the offering agent for t = n, his utility for this
time period is:
UB(n) = kb
1δn−1
1 + 1/2
mX
i=2
δ
i(n−1)
i (6)
Recall that ka
i and kb
i (for c < i ≤ m) are not known to the
agents. Hence, the agents can only find their expected utilities from
the future issues on the basis of the probability distribution 
functions for ka
i and kb
i . However, during the negotiation for issue c
the agents know ka
c but not kb
c (see Step 1 of the online protocol).
Hence, a computes UB(n) as follows. Agent b"s utility from issue
c = 1 is kb
1δn−1
1 (which is the first term of Equation 6). Then,
on the basis of the probability distribution functions for ka
i and
kb
i , agent a computes b"s expected utility from each future issue i
as δ
i(n−1)
i /2 (since ka
i and kb
i are uniformly distributed on [0, 1]).
Thus, b"s expected cumulative utility from these m − c issues is
1/2
Pm
i=2 δ
i(n−1)
i (which is the second term of Equation 6).
Now, in order to decide what to offer for issue c = 1, the offering
agent for t = n − 1 (i.e., agent a) must solve the following online
knapsack problem:
maximize Σm
i=1ka
i (1 − ¯bt
i)δn−1
i (7)
such that Σm
i=1kb
i
¯bt
i ≥ UB(n)
¯bt
i = 0 or 1 for 1 ≤ i ≤ m
The only variables in the above maximization problem are ¯bt
i. Now,
maximizing Σm
i=1ka
i (1−¯bt
i)δn−1
i is the same as minimizing Σm
i=1ka
i
¯bt
i
since δn−1
i and ka
i are constants. Thus, we write Equation 7 as:
minimize Σm
i=1ka
i
¯bt
i (8)
such that Σm
i=1kb
i
¯bt
i ≥ UB(n)
¯bt
i = 0 or 1 for 1 ≤ i ≤ m
The above optimization problem is analogous to the online 0-1
knapsack problem. An algorithm to solve the online knapsack 
problem has already proposed in [16]. This algorithm is called the
fixed-choice online algorithm. It has time complexity linear in the
number of items (m) in the knapsack problem. We use this to solve
our online negotiation problem. Thus, our ONLINE-TRADEOFFA
algorithm is nothing but the fixed-choice online algorithm and 
therefore has the same time complexity as the latter. This algorithm takes
the values of ka
i and kb
i one at a time and generates an approximate
solution to the above knapsack problem. The expected difference
between the optimum and approximate solution is E[z∗
m − zm] =
O(
√
m) [16] (see [16] for the detailed fixed-choice online 
algorithm and a proof for E[z∗
m − zm] = O(
√
m)).
The fixed-choice online algorithm of [16] is a generalization of
the basic greedy algorithm for the offline knapsack problem; the
idea behind it is as follows. A threshold value is determined on the
basis of the information regarding weights and profits for the 0-1
knapsack problem. The method then includes into the knapsack all
items whose profit density (profit density of an item is its profit per
unit weight) exceeds the threshold until either the knapsack is filled
or all the m items have been considered.
In more detail, the algorithm ONLINE-TRADEOFFA works as 
follows. It first gets the values of ka
1 and kb
1 and finds ¯bt
c. Since we
have a 0-1 knapsack problem, ¯bt
c can be either zero or one. Now, if
¯bt
c = 1 for t = n, then ¯bt
c must be one for 1 ≤ t < n (i.e., a must
offer ¯bt
c = 1 at t = 1). If ¯bt
c = 1 for t = n, but a offers ¯bt
c = 0
at t = 1, then agent b gets less utility than what it expects from a"s
offer and rejects the proposal. Thus, if ¯bt
c = 1 for t = n, then the
optimal strategy for a is to offer ¯bt
c = 1 at t = 1. Agent b accepts
the offer. Thus, negotiation on the first issue starts at t = 1 and an
agreement on it is also reached at t = 1.
In the next time period (i.e., t = 2), negotiation proceeds to the
next issue. The deadline for the second issue is n time periods from
the start of negotiation on the issue. For c = 2, the algorithm
ONLINE-TRADEOFFA is given the values of ka
2 and kb
2 and finds ¯bt
c
as described above. Agent offers bc at t = 2 and b accepts. Thus,
negotiation on the second issue starts at t = 2 and an agreement
on it is also reached at t = 2.
This process repeats for the remaining issues c = 3, . . . , m.
Thus, each issue is agreed upon in the same time period in which
it starts. As negotiation for the next issue starts in the following
time period (see step 3 of the online protocol), agreement on issue
i occurs at time t = i.
On the other hand, if b is the offering agent at t = 1, he uses
the algorithm ONLINE-TRADEOFFB which is defined analogously.
Thus, irrespective of who makes the first move, all the m issues are
settled at time t = m.
THEOREM 6. The time complexity of finding the approximate
equilibrium offers of Theorem 5 is linear in m.
PROOF. The time complexity of ONLINE-TRADEOFFA and 
ONLINETRADEOFFB is the same as the time complexity of the fixed-choice
online algorithm of [16]. Since the latter has time complexity linear
in m, the time complexity of ONLINE-TRADEOFFA and 
ONLINETRADEOFFB is also linear in m.
It is worth noting that, for the 0-1 knapsack problem, the lower
bound on the expected difference between the optimum and the 
solution found by any online algorithm is Ω(1) [16]. Thus, it follows
that this lower bound also holds for our negotiation problem.
5. RELATED WORK
Work on multi-issue negotiation can be divided into two main types:
that for indivisible issues and that for divisible issues. We first
describe the existing work for the case of divisible issues. Since
Schelling [24] first noted that the outcome of negotiation depends
on the choice of negotiation procedure, much research effort has
been devoted to the study of different procedures for negotiating
multiple issues. However, most of this work has focussed on the 
sequential procedure [7, 2]. For this procedure, a key issue is the 
negotiation agenda. Here the term agenda refers to the order in which
the issues are negotiated. The agenda is important because each
agent"s cumulative utility depends on the agenda; if we change the
agenda then these utilities change. Hence, the agents must decide
what agenda they will use. Now, the agenda can be decided before
negotiating the issues (such an agenda is called exogenous) or it
may be decided during the process of negotiation (such an agenda
is called endogenous). For instance, Fershtman [7] analyze 
sequential negotiation with exogenous agenda. A number of researchers
have also studied negotiations with an endogenous agenda [2].
In contrast to the above work that mainly deals with sequential
negotiation, [6] studies the equilibrium for the package deal 
procedure. However, all the above mentioned work differs from ours in
that we focus on indivisible issues while others focus on the case
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 957
where each issue is divisible. Specifically, no previous work has
determined an approximate equilibrium for multi-issue negotiation
or for online negotiation.
Existing work for the case of indivisible issues has mostly dealt
with task allocation problems (for tasks that cannot be partioned)
to a group of agents. The problem of task allocation has been 
previously studied in the context of coalitions involving more than
two agents. For example [25] analyze the problem for the case
where the agents act so as to maximize the benefit of the system
as a whole. In contrast, our focus is on two agents where both of
them are self-interested and want to maximize their individual 
utilities. On the other hand [22] focus on the use of contracts for task
allocation to multiple self interested agents but this work concerns
finding ways of decommitting contracts (after the initial allocation
has been done) so as to improve an agent"s utility. In contrast, our
focuses on negotiation regarding who will carry out which task.
Finally, online and approximate mechanisms have been studied
in the context of auctions [14, 9] but not for bilateral negotiations
(which is the focus of our work).
6. CONCLUSIONS
This paper has studied bilateral multi-issue negotiation between
self-interested autonomous agents with time constraints. The issues
are indivisible and different agents value different issues 
differently. Thus, the problem is for the agents to decide how to allocate
the issues between themselves so as to maximize their individual
utilities. Specifically, we first showed that finding the equilibrium
offers is an NP-hard problem even in a complete information 
setting. We then presented approximately optimal negotiation 
strategies and showed that they form an equilibrium. These strategies
have polynomial time complexity. We also analysed the difference
between the true optimum and the approximate optimum. Finally,
we extended the analysis to online negotiation where the issues 
become available at different time points and the agents are uncertain
about the features of these issues. Specifically, we showed that an
approximate equilibrium exists for online negotiation and analysed
the approximation error. These approximate strategies also have
polynomial time complexity.
There are several interesting directions for future work. First,
for online negotiation, we assumed that the constants ka
c and kb
c are
both uniformly distributed. It will be interesting to analyze the case
where ka
c and kb
c have other, possibly different, probability 
distributions. Apart from this, we treated the number of issues as being
common knowledge to the agents. In future, it will be interesting
to treat the number of issues as uncertain.
7. REFERENCES
[1] G. Ausiello, P. Crescenzi, G. Gambosi, V. Kann,
A. Marchetti-Spaccamela, and M. Protasi. Complexity and
approximation: Combinatorial optimization problems and
their approximability properties. Springer, 2003.
[2] M. Bac and H. Raff. Issue-by-issue negotiations: the role of
information and time preference. Games and Economic
Behavior, 13:125-134, 1996.
[3] A. Borodin and R. El-Yaniv. Online Computation and
Competitive Analysis. Cambridge University Press, 1998.
[4] S. J. Brams. Fair division: from cake cutting to dispute
resolution. Cambridge University Press, 1996.
[5] L. A. Busch and I. J. Horstman. Bargaining frictions,
bargaining procedures and implied costs in multiple-issue
bargaining. Economica, 64:669-680, 1997.
[6] S. S. Fatima, M. Wooldridge, and N. R. Jennings.
Multi-issue negotiation with deadlines. Journal of Artificial
Intelligence Research, 27:381-417, 2006.
[7] C. Fershtman. The importance of the agenda in bargaining.
Games and Economic Behavior, 2:224-238, 1990.
[8] F. Glover. A multiphase dual algorithm for the zero-one
integer programming problem. Operations Research,
13:879-919, 1965.
[9] M. T. Hajiaghayi, R. Kleinberg, and D. C. Parkes. Adaptive
limited-supply online auctions. In ACM Conference on
Electronic Commerce (ACMEC-04), pages 71-80, New
York, 2004.
[10] O. H. Ibarra and C. E. Kim. Fast approximation algorithms
for the knapsack and sum of subset problems. Journal of
ACM, 22:463-468, 1975.
[11] R. Inderst. Multi-issue bargaining with endogenous agenda.
Games and Economic Behavior, 30:64-82, 2000.
[12] R. Keeney and H. Raiffa. Decisions with Multiple
Objectives: Preferences and Value Trade-offs. New York:
John Wiley, 1976.
[13] S. Kraus. Strategic negotiation in multi-agent environments.
The MIT Press, Cambridge, Massachusetts, 2001.
[14] D. Lehman, L. I. O"Callaghan, and Y. Shoham. Truth
revelation in approximately efficient combinatorial auctions.
Journal of the ACM, 49(5):577-602, 2002.
[15] A. Lomuscio, M. Wooldridge, and N. R. Jennings. A
classification scheme for negotiation in electronic commerce.
International Journal of Group Decision and Negotiation,
12(1):31-56, 2003.
[16] A. Marchetti-Spaccamela and C. Vercellis. Stochastic online
knapsack problems. Mathematical Programming,
68:73-104, 1995.
[17] S. Martello and P. Toth. Knapsack problems: Algorithms and
computer implementations. John Wiley and Sons, 1990.
[18] M. J. Osborne and A. Rubinstein. A Course in Game Theory.
The MIT Press, 1994.
[19] H. Raiffa. The Art and Science of Negotiation. Harvard
University Press, Cambridge, USA, 1982.
[20] J. S. Rosenschein and G. Zlotkin. Rules of Encounter. MIT
Press, 1994.
[21] A. Rubinstein. Perfect equilibrium in a bargaining model.
Econometrica, 50(1):97-109, January 1982.
[22] T. Sandholm and V. Lesser. Levelled commitment contracts
and strategic breach. Games and Economic Behavior:
Special Issue on AI and Economics, 35:212-270, 2001.
[23] T. Sandholm and N. Vulkan. Bargaining with deadlines. In
AAAI-99, pages 44-51, Orlando, FL, 1999.
[24] T. C. Schelling. An essay on bargaining. American Economic
Review, 46:281-306, 1956.
[25] O. Shehory and S. Kraus. Methods for task allocation via
agent coalition formation. Artificial Intelligence Journal,
101(1-2):165-200, 1998.
[26] S. Singh, V. Soni, and M. Wellman. Computing approximate
Bayes Nash equilibria in tree games of incomplete
information. In Proceedings of the ACM Conference on
Electronic Commerce ACM-EC, pages 81-90, New York,
May 2004.
[27] I. Stahl. Bargaining Theory. Economics Research Institute,
Stockholm School of Economics, Stockholm, 1972.
958 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Distributed Agent-Based Air Traffic Flow Management
Kagan Tumer
Oregon State University
204 Rogers Hall
Corvallis, OR 97331, USA
kagan.tumer@oregonstate.edu
Adrian Agogino
UCSC, NASA Ames Research Center
Mailstop 269-3
Moffett Field, CA 94035, USA
adrian@email.arc.nasa.gov
ABSTRACT
Air traffic flow management is one of the fundamental 
challenges facing the Federal Aviation Administration (FAA)
today. The FAA estimates that in 2005 alone, there were
over 322,000 hours of delays at a cost to the industry in 
excess of three billion dollars. Finding reliable and adaptive
solutions to the flow management problem is of paramount
importance if the Next Generation Air Transportation 
Systems are to achieve the stated goal of accommodating three
times the current traffic volume. This problem is 
particularly complex as it requires the integration and/or 
coordination of many factors including: new data (e.g., changing
weather info), potentially conflicting priorities (e.g., 
different airlines), limited resources (e.g., air traffic controllers)
and very heavy traffic volume (e.g., over 40,000 flights over
the US airspace).
In this paper we use FACET - an air traffic flow simulator
developed at NASA and used extensively by the FAA and
industry - to test a multi-agent algorithm for traffic flow
management. An agent is associated with a fix (a specific
location in 2D space) and its action consists of setting the
separation required among the airplanes going though that
fix. Agents use reinforcement learning to set this separation
and their actions speed up or slow down traffic to manage
congestion. Our FACET based results show that agents 
receiving personalized rewards reduce congestion by up to 45%
over agents receiving a global reward and by up to 67% over
a current industry approach (Monte Carlo estimation).
Categories and Subject Descriptors
I.2.11 [Computing Methodologies]: Artificial 
IntelligenceMultiagent systems
General Terms
Algorithms, Performance
1. INTRODUCTION
The efficient, safe and reliable management of our ever
increasing air traffic is one of the fundamental challenges
facing the aerospace industry today. On a typical day, more
than 40,000 commercial flights operate within the US airspace
[14]. In order to efficiently and safely route this air traffic,
current traffic flow control relies on a centralized, 
hierarchical routing strategy that performs flow projections ranging
from one to six hours. As a consequence, the system is
slow to respond to developing weather or airport conditions
leading potentially minor local delays to cascade into large
regional congestions. In 2005, weather, routing decisions
and airport conditions caused 437,667 delays, accounting for
322,272 hours of delays. The total cost of these delays was
estimated to exceed three billion dollars by industry [7].
Furthermore, as the traffic flow increases, the current 
procedures increase the load on the system, the airports, and
the air traffic controllers (more aircraft per region) 
without providing any of them with means to shape the traffic
patterns beyond minor reroutes. The Next Generation Air
Transportation Systems (NGATS) initiative aims to address
this issues and, not only account for a threefold increase in
traffic, but also for the increasing heterogeneity of aircraft
and decreasing restrictions on flight paths. Unlike many
other flow problems where the increasing traffic is to some
extent absorbed by improved hardware (e.g., more servers
with larger memories and faster CPUs for internet routing)
the air traffic domain needs to find mainly algorithmic 
solutions, as the infrastructure (e.g., number of the airports) will
not change significantly to impact the flow problem. There
is therefore a strong need to explore new, distributed and
adaptive solutions to the air flow control problem.
An adaptive, multi-agent approach is an ideal fit to this
naturally distributed problem where the complex interaction
among the aircraft, airports and traffic controllers renders a
pre-determined centralized solution severely suboptimal at
the first deviation from the expected plan. Though a truly
distributed and adaptive solution (e.g., free flight where 
aircraft can choose almost any path) offers the most potential
in terms of optimizing flow, it also provides the most 
radical departure from the current system. As a consequence, a
shift to such a system presents tremendous difficulties both
in terms of implementation (e.g., scheduling and airport 
capacity) and political fallout (e.g., impact on air traffic 
controllers). In this paper, we focus on agent based system that
can be implemented readily. In this approach, we assign an
342
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
agent to a fix, a specific location in 2D. Because aircraft
flight plans consist of a sequence of fixes, this 
representation allows localized fixes (or agents) to have direct impact
on the flow of air traffic1
. In this approach, the agents"
actions are to set the separation that approaching aircraft
are required to keep. This simple agent-action pair allows
the agents to slow down or speed up local traffic and allows
agents to a have significant impact on the overall air traffic
flow. Agents learn the most appropriate separation for their
location using a reinforcement learning (RL) algorithm [15].
In a reinforcement learning approach, the selection of the
agent reward has a large impact on the performance of the
system. In this work, we explore four different agent reward
functions, and compare them to simulating various changes
to the system and selecting the best solution (e.g, 
equivalent to a Monte-Carlo search). The first explored reward
consisted of the system reward. The second reward was a
personalized agent reward based on collectives [3, 17, 18].
The last two rewards were personalized rewards based on
estimations to lower the computational burden of the 
reward computation. All three personalized rewards aim to
align agent rewards with the system reward and ensure that
the rewards remain sensitive to the agents" actions.
Previous work in this domain fell into one of two distinct
categories: The first principles based modeling approaches
used by domain experts [5, 8, 10, 13] and the algorithmic
approaches explored by the learning and/or agents 
community [6, 9, 12]. Though our approach comes from the second
category, we aim to bridge the gap by using FACET to test
our algorithms, a simulator introduced and widely used (i.e.,
over 40 organizations and 5000 users) by work in the first
category [4, 11].
The main contribution of this paper is to present a 
distributed adaptive air traffic flow management algorithm that
can be readily implemented and test that algorithm using
FACET. In Section 2, we describe the air traffic flow problem
and the simulation tool, FACET. In Section 3, we present
the agent-based approach, focusing on the selection of the
agents and their action space along with the agents" learning
algorithms and reward structures. In Section 4 we present
results in domains with one and two congestions, explore
different trade-offs of the system objective function, discuss
the scaling properties of the different agent rewards and 
discuss the computational cost of achieving certain levels of
performance. Finally, in Section 5, we discuss the 
implications of these results and provide and map the required work
to enable the FAA to reach its stated goal of increasing the
traffic volume by threefold.
2. AIR TRAFFIC FLOW MANAGEMENT
With over 40,000 flights operating within the United States
airspace on an average day, the management of traffic flow
is a complex and demanding problem. Not only are there
concerns for the efficiency of the system, but also for 
fairness (e.g., different airlines), adaptability (e.g., developing
weather patterns), reliability and safety (e.g., airport 
management). In order to address such issues, the management
of this traffic flow occurs over four hierarchical levels:
1. Separation assurance (2-30 minute decisions);
1
We discuss how flight plans with few fixes can be handled
in more detail in Section 2.
2. Regional flow (20 minutes to 2 hours);
3. National flow (1-8 hours); and
4. Dynamic airspace configuration (6 hours to 1 year).
Because of the strict guidelines and safety concerns 
surrounding aircraft separation, we will not address that control
level in this paper. Similarly, because of the business and
political impact of dynamic airspace configuration, we will
not address the outermost flow control level either. Instead,
we will focus on the regional and national flow management
problems, restricting our impact to decisions with time 
horizons between twenty minutes and eight hours. The proposed
algorithm will fit between long term planning by the FAA
and the very short term decisions by air traffic controllers.
The continental US airspace consists of 20 regional centers
(handling 200-300 flights on a given day) and 830 sectors
(handling 10-40 flights). The flow control problem has to
address the integration of policies across these sectors and
centers, account for the complexity of the system (e.g., over
5200 public use airports and 16,000 air traffic controllers)
and handle changes to the policies caused by weather 
patterns. Two of the fundamental problems in addressing the
flow problem are: (i) modeling and simulating such a large
complex system as the fidelity required to provide reliable 
results is difficult to achieve; and (ii) establishing the method
by which the flow management is evaluated, as directly 
minimizing the total delay may lead to inequities towards 
particular regions or commercial entities. Below, we discuss
how we addressed both issues, namely, we present FACET
a widely used simulation tool and discuss our system 
evaluation function.
Figure 1: FACET screenshot displaying traffic
routes and air flow statistics.
2.1 FACET
FACET (Future ATM Concepts Evaluation Tool), a physics
based model of the US airspace was developed to accurately
model the complex air traffic flow problem [4]. It is based on
propagating the trajectories of proposed flights forward in
time. FACET can be used to either simulate and display air
traffic (a 24 hour slice with 60,000 flights takes 15 minutes to
simulate on a 3 GHz, 1 GB RAM computer) or provide rapid
statistics on recorded data (4D trajectories for 10,000 flights
including sectors, airports, and fix statistics in 10 seconds
on the same computer) [11]. FACET is extensively used by
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343
the FAA, NASA and industry (over 40 organizations and
5000 users) [11].
FACET simulates air traffic based on flight plans and
through a graphical user interface allows the user to analyze
congestion patterns of different sectors and centers (Figure
1). FACET also allows the user to change the flow patterns
of the aircraft through a number of mechanisms, including
metering aircraft through fixes. The user can then observe
the effects of these changes to congestion. In this paper,
agents use FACET directly through batch mode, where
agents send scripts to FACET asking it to simulate air 
traffic based on metering orders imposed by the agents. The
agents then produce their rewards based on receive feedback
from FACET about the impact of these meterings.
2.2 System Evaluation
The system performance evaluation function we select 
focuses on delay and congestion but does not account for 
fairness impact on different commercial entities. Instead it 
focuses on the amount of congestion in a particular sector and
on the amount of measured air traffic delay. The linear 
combination of these two terms gives the full system evaluation
function, G(z) as a function of the full system state z. More
precisely, we have:
G(z) = −((1 − α)B(z) + αC(z)) , (1)
where B(z) is the total delay penalty for all aircraft in the
system, and C(z) is the total congestion penalty. The 
relative importance of these two penalties is determined by the
value of α, and we explore various trade-offs based on α in
Section 4.
The total delay, B, is a sum of delays over a set of sectors
S and is given by:
B(z) =
X
s∈S
Bs(z) (2)
where
Bs(z) =
X
t
Θ(t − τs)kt,s(t − τs) , (3)
where ks,t is the number of aircraft in sector s at a 
particular time, τs is a predetermined time, and Θ(·) is the
step function that equals 1 when its argument is greater or
equal to zero, and has a value of zero otherwise. Intuitively,
Bs(z) provides the total number of aircraft that remain in
a sector s past a predetermined time τs, and scales their
contribution to count by the amount by which they are late.
In this manner Bs(z) provides a delay factor that not only
accounts for all aircraft that are late, but also provides a
scale to measure their lateness. This definition is based
on the assumption that most aircraft should have reached
the sector by time τs and that aircraft arriving after this
time are late. In this paper the value of τs is determined by
assessing aircraft counts in the sector in the absence of any
intervention or any deviation from predicted paths.
Similarly, the total congestion penalty is a sum over the
congestion penalties over the sectors of observation, S:
C(z) =
X
s∈S
Cs(z) (4)
where
Cs(z) = a
X
t
Θ(ks,t − cs)eb(ks,t−cs)
, (5)
where a and b are normalizing constants, and cs is the 
capacity of sector s as defined by the FAA. Intuitively, Cs(z)
penalizes a system state where the number of aircraft in a
sector exceeds the FAAs official sector capacity. Each sector
capacity is computed using various metrics which include the
number of air traffic controllers available. The exponential
penalty is intended to provide strong feedback to return the
number of aircraft in a sector to below the FAA mandated
capacities.
3. AGENT BASED AIR TRAFFIC FLOW
The multi agent approach to air traffic flow management
we present is predicated on adaptive agents taking 
independent actions that maximize the system evaluation function
discussed above. To that end, there are four critical 
decisions that need to be made: agent selection, agent action
set selection, agent learning algorithm selection and agent
reward structure selection.
3.1 Agent Selection
Selecting the aircraft as agents is perhaps the most 
obvious choice for defining an agent. That selection has the
advantage that agent actions can be intuitive (e.g., change
of flight plan, increase or decrease speed and altitude) and
offer a high level of granularity, in that each agent can have
its own policy. However, there are several problems with
that approach. First, there are in excess of 40,000 aircraft
in a given day, leading to a massively large multi-agent 
system. Second, as the agents would not be able to sample their
state space sufficiently, learning would be prohibitively slow.
As an alternative, we assign agents to individual ground 
locations throughout the airspace called fixes. Each agent is
then responsible for any aircraft going through its fix. Fixes
offer many advantages as agents:
1. Their number can vary depending on need. The 
system can have as many agents as required for a given
situation (e.g., agents coming live around an area
with developing weather conditions).
2. Because fixes are stationary, collecting data and 
matching behavior to reward is easier.
3. Because aircraft flight plans consist of fixes, agent will
have the ability to affect traffic flow patterns.
4. They can be deployed within the current air traffic
routing procedures, and can be used as tools to help air
traffic controllers rather than compete with or replace
them.
Figure 2 shows a schematic of this agent based system.
Agents surrounding a congestion or weather condition affect
the flow of traffic to reduce the burden on particular regions.
3.2 Agent Actions
The second issue that needs to be addressed, is 
determining the action set of the agents. Again, an obvious choice
may be for fixes to bid on aircraft, affecting their flight
plans. Though appealing from a free flight perspective, that
approach makes the flight plans too unreliable and 
significantly complicates the scheduling problem (e.g., arrival at
airports and the subsequent gate assignment process).
Instead, we set the actions of an agent to determining
the separation (distance between aircraft) that aircraft have
344 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
to maintain, when going through the agent"s fix. This is
known as setting the Miles in Trail or MIT. When an agent
sets the MIT value to d, aircraft going towards its fix are
instructed to line up and keep d miles of separation (though
aircraft will always keep a safe distance from each other
regardless of the value of d). When there are many aircraft
going through a fix, the effect of issuing higher MIT values
is to slow down the rate of aircraft that go through the fix.
By increasing the value of d, an agent can limit the amount
of air traffic downstream of its fix, reducing congestion at
the expense of increasing the delays upstream.
Figure 2: Schematic of agent architecture. The
agents corresponding to fixes surrounding a 
possible congestion become live and start setting new
separation times.
3.3 Agent Learning
The objective of each agent is to learn the best values of
d that will lead to the best system performance, G. In this
paper we assume that each agent will have a reward 
function and will aim to maximize its reward using its own 
reinforcement learner [15] (though alternatives such as 
evolving neuro-controllers are also effective [1]). For complex
delayed-reward problems, relatively sophisticated 
reinforcement learning systems such as temporal difference may have
to be used. However, due to our agent selection and agent
action set, the air traffic congestion domain modeled in this
paper only needs to utilize immediate rewards. As a 
consequence, simple table-based immediate reward reinforcement
learning is used. Our reinforcement learner is equivalent to
an -greedy Q-learner with a discount rate of 0 [15]. At every
episode an agent takes an action and then receives a reward
evaluating that action. After taking action a and receiving
reward R an agent updates its Q table (which contains its
estimate of the value for taking that action [15]) as follows:
Q (a) = (1 − l)Q(a) + l(R), (6)
where l is the learning rate. At every time step the agent
chooses the action with the highest table value with 
probability 1 − and chooses a random action with probability
. In the experiments described in this paper, α is equal
to 0.5 and is equal to 0.25. The parameters were chosen
experimentally, though system performance was not overly
sensitive to these parameters.
3.4 Agent Reward Structure
The final issue that needs to be addressed is selecting the
reward structure for the learning agents. The first and most
direct approach is to let each agent receive the system 
performance as its reward. However, in many domains such
a reward structure leads to slow learning. We will 
therefore also set up a second set of reward structures based on
agent-specific rewards. Given that agents aim to maximize
their own rewards, a critical task is to create good agent
rewards, or rewards that when pursued by the agents lead
to good overall system performance. In this work we focus
on difference rewards which aim to provide a reward that is
both sensitive to that agent"s actions and aligned with the
overall system reward [2, 17, 18].
3.4.1 Difference Rewards
Consider difference rewards of the form [2, 17, 18]:
Di ≡ G(z) − G(z − zi + ci) , (7)
where zi is the action of agent i. All the components of
z that are affected by agent i are replaced with the fixed
constant ci
2
.
In many situations it is possible to use a ci that is 
equivalent to taking agent i out of the system. Intuitively this
causes the second term of the difference reward to 
evaluate the performance of the system without i and therefore
D evaluates the agent"s contribution to the system 
performance. There are two advantages to using D: First, because
the second term removes a significant portion of the impact
of other agents in the system, it provides an agent with
a cleaner signal than G. This benefit has been dubbed
learnability (agents have an easier time learning) in 
previous work [2, 17]. Second, because the second term does not
depend on the actions of agent i, any action by agent i that
improves D, also improves G. This term which measures the
amount of alignment between two rewards has been dubbed
factoredness in previous work [2, 17].
3.4.2 Estimates of Difference Rewards
Though providing a good compromise between aiming
for system performance and removing the impact of other
agents from an agent"s reward, one issue that may plague D
is computational cost. Because it relies on the computation
of the counterfactual term G(z − zi + ci) (i.e., the system
performance without agent i) it may be difficult or 
impossible to compute, particularly when the exact mathematical
form of G is not known. Let us focus on G functions in the
following form:
G(z) = Gf (f(z)), (8)
where Gf () is non-linear with a known functional form and,
f(z) =
X
i
fi(zi) , (9)
where each fi is an unknown non-linear function. We 
assume that we can sample values from f(z), enabling us to
compute G, but that we cannot sample from each fi(zi).
2
This notation uses zero padding and vector addition rather
than concatenation to form full state vectors from partial
state vectors. The vector zi in our notation would be ziei
in standard vector notation, where ei is a vector with a value
of 1 in the ith component and is zero everywhere else.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345
In addition, we assume that Gf is much easier to compute
than f(z), or that we may not be able to even compute
f(z) directly and must sample it from a black box 
computation. This form of G matches our system evaluation
in the air traffic domain. When we arrange agents so that
each aircraft is typically only affected by a single agent, each
agent"s impact of the counts of the number of aircraft in a
sector, kt,s, will be mostly independent of the other agents.
These values of kt,s are the f(z)s in our formulation and
the penalty functions form Gf . Note that given aircraft
counts, the penalty functions (Gf ) can be easily computed
in microseconds, while aircraft counts (f) can only be 
computed by running FACET taking on the order of seconds.
To compute our counterfactual G(z − zi + ci) we need to
compute:
Gf (f(z − zi + ci)) = Gf
0
@
X
j=i
fj(zj) + fi(ci)
1
A (10)
= Gf (f(z) − fi(zi) + fi(ci)) .(11)
Unfortunately, we cannot compute this directly as the values
of fi(zi) are unknown. However, if agents take actions 
independently (it does not observe how other agents act before
taking its own action) we can take advantage of the linear
form of f(z) in the fis with the following equality:
E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12)
where E(f−i(z−i)|zi) is the expected value of all of the fs
other than fi given the value of zi and E(f−i(z−i)|ci) is the
expected value of all of the fs other than fi given the value
of zi is changed to ci. We can then estimate f(z − zi + ci):
f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci)
+ E(f−i(z−i)|ci) − E(f−i(z−i)|zi)
= f(z) − E(fi(zi)|zi) + E(fi(ci)|ci)
+ E(f−i(z−i)|ci) − E(f−i(z−i)|zi)
= f(z) − E(f(z)|zi) + E(f(z)|ci) .
Therefore we can evaluate Di = G(z) − G(z − zi + ci) as:
Dest1
i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13)
leaving us with the task of estimating the values of E(f(z)|zi)
and E(f(z)|ci)). These estimates can be computed by 
keeping a table of averages where we average the values of the 
observed f(z) for each value of zi that we have seen. This 
estimate should improve as the number of samples increases. To
improve our estimates, we can set ci = E(z) and if we make
the mean squared approximation of f(E(z)) ≈ E(f(z)) then
we can estimate G(z) − G(z − zi + ci) as:
Dest2
i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14)
This formulation has the advantage in that we have more
samples at our disposal to estimate E(f(z)) than we do to
estimate E(f(z)|ci)).
4. SIMULATION RESULTS
In this paper we test the performance of our agent based
air traffic optimization method on a series of simulations
using the FACET air traffic simulator. In all experiments
we test the performance of five different methods. The first
method is Monte Carlo estimation, where random policies
are created, with the best policy being chosen. The other
four methods are agent based methods where the agents are
maximizing one of the following rewards:
1. The system reward, G(z), as define in Equation 1.
2. The difference reward, Di(z), assuming that agents
can calculate counterfactuals.
3. Estimation to the difference reward, Dest1
i (z), where
agents estimate the counterfactual using E(f(z)|zi)
and E(f(z)|ci).
4. Estimation to the difference reward, Dest2
i (z), where
agents estimate the counterfactual using E(f(z)|zi)
and E(f(z)).
These methods are first tested on an air traffic domain with
300 aircraft, where 200 of the aircraft are going through
a single point of congestion over a four hour simulation.
Agents are responsible for reducing congestion at this single
point, while trying to minimize delay. The methods are then
tested on a more difficult problem, where a second point of
congestion is added with the 100 remaining aircraft going
through this second point of congestion.
In all experiments the goal of the system is to maximize
the system performance given by G(z) with the parameters,
a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to
175 minutes. These values of τ are obtained by examining
the time at which most of the aircraft leave the sectors, when
no congestion control is being performed. Except where
noted, the trade-off between congestion and lateness, α is
set to 0.5. In all experiments to make the agent results
comparable to the Monte Carlo estimation, the best policies
chosen by the agents are used in the results. All results are
an average of thirty independent trials with the differences
in the mean (σ/
√
n) shown as error bars, though in most
cases the error bars are too small to see.
Figure 3: Performance on single congestion 
problem, with 300 Aircraft , 20 Agents and α = .5.
4.1 Single Congestion
In the first experiment we test the performance of the five
methods when there is a single point of congestion, with
twenty agents. This point of congestion is created by setting
up a series of flight plans that cause the number of aircraft in
346 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
the sector of interest to be significantly more than the 
number allowed by the FAA. The results displayed in Figures
3 and 4 show the performance of all five algorithms on two
different system evaluations. In both cases, the agent based
methods significantly outperform the Monte Carlo method.
This result is not surprising since the agent based methods
intelligently explore their space, where as the Monte Carlo
method explores the space randomly.
Figure 4: Performance on single congestion 
problem, with 300 Aircraft , 20 Agents and α = .75.
Among the agent based methods, agents using difference
rewards perform better than agents using the system 
reward. Again this is not surprising, since with twenty agents,
an agent directly trying to maximize the system reward has
difficulty determining the effect of its actions on its own
reward. Even if an agent takes an action that reduces 
congestion and lateness, other agents at the same time may
take actions that increase congestion and lateness, causing
the agent to wrongly believe that its action was poor. In
contrast agents using the difference reward have more 
influence over the value of their own reward, therefore when an
agent takes a good action, the value of this action is more
likely to be reflected in its reward.
This experiment also shows that estimating the difference
reward is not only possible, but also quite effective, when
the true value of the difference reward cannot be computed.
While agents using the estimates do not achieve as high of
results as agents using the true difference reward, they still
perform significantly better than agents using the system
reward. Note, however, that the benefit of the estimated
difference rewards are only present later in learning. Earlier
in learning, the estimates are poor, and agents using the
estimated difference rewards perform no better then agents
using the system reward.
4.2 Two Congestions
In the second experiment we test the performance of the
five methods on a more difficult problem with two points of
congestion. On this problem the first region of congestion is
the same as in the previous problem, and the second region
of congestion is added in a different part of the country.
The second congestion is less severe than the first one, so
agents have to form different policies depending which point
of congestion they are influencing.
Figure 5: Performance on two congestion problem,
with 300 Aircraft, 20 Agents and α = .5.
Figure 6: Performance on two congestion problem,
with 300 Aircraft, 50 Agents and α = .5.
The results displayed in Figure 5 show that the relative
performance of the five methods is similar to the single 
congestion case. Again agent based methods perform better
than the Monte Carlo method and the agents using 
difference rewards perform better than agents using the system
reward. To verify that the performance improvement of our
methods is maintained when there are a different number of
agents, we perform additional experiments with 50 agents.
The results displayed in Figure 6 show that indeed the 
relative performances of the methods are comparable when the
number of agents is increased to 50. Figure 7 shows scaling
results and demonstrates that the conclusions hold over a
wide range of number of agents. Agents using Dest2

perform slightly better than agents using Dest1
in all cases but
for 50 agents. This slight advantage stems from Dest2

providing the agents with a cleaner signal, since its estimate
uses more data points.
4.3 Penalty Tradeoffs
The system evaluation function used in the experiments is
G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties
for both congestion and lateness. This evaluation function
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347
Figure 7: Impact of number of agents on system 
performance. Two congestion problem, with 300 
Aircraft and α = .5.
forces the agents to tradeoff these relative penalties 
depending on the value of α. With high α the optimization focuses
on reducing congestion, while with low α the system focuses
on reducing lateness. To verify that the results obtained
above are not specific to a particular value of α, we repeat
the experiment with 20 agents for α = .75. Figure 8 shows
that qualitatively the relative performance of the algorithms
remain the same.
Next, we perform a series of experiments where α ranges
from 0.0 to 1.0 . Figure 9 shows the results which lead to
three interesting observations:
• First, there is a zero congestion penalty solution. This
solution has agents enforce large MIT values to block
all air traffic, which appears viable when the system
evaluation does not account for delays. All algorithms
find this solution, though it is of little interest in 
practice due to the large delays it would cause.
• Second, if the two penalties were independent, an 
optimal solution would be a line from the two end points.
Therefore, unless D is far from being optimal, the two
penalties are not independent. Note that for α = 0.5
the difference between D and this hypothetical line is
as large as it is anywhere else, making α = 0.5 a 
reasonable choice for testing the algorithms in a difficult
setting.
• Third, Monte Carlo and G are particularly poor at
handling multiple objectives. For both algorithms, the
performance degrades significantly for mid-ranges of α.
4.4 Computational Cost
The results in the previous section show the performance
of the different algorithms after a specific number of episodes.
Those results show that D is significantly superior to the
other algorithms. One question that arises, though, is what
computational overhead D puts on the system, and what
results would be obtained if the additional computational
expense of D is made available to the other algorithms.
The computation cost of the system evaluation, G 
(Equation 1) is almost entirely dependent on the computation of
Figure 8: Performance on two congestion problem,
with 300 Aircraft, 20 Agents and α = .75.
Figure 9: Tradeoff Between Objectives on two 
congestion problem, with 300 Aircraft and 20 Agents.
Note that Monte Carlo and G are particularly bad
at handling multiple objectives.
the airplane counts for the sectors kt,s, which need to be
computed using FACET. Except when D is used, the 
values of k are computed once per episode. However, to 
compute the counterfactual term in D, if FACET is treated as
a black box, each agent would have to compute their own
values of k for their counterfactual resulting in n + 1 
computations of k per episode. While it may be possible to
streamline the computation of D with some knowledge of
the internals of FACET, given the complexity of the FACET
simulation, it is not unreasonable in this case to treat it as
a black box.
Table 1 shows the performance of the algorithms after
2100 G computations for each of the algorithms for the 
simulations presented in Figure 5 where there were 20 agents,
2 congestions and α = .5. All the algorithms except the
fully computed D reach 2100 k computations at time step
2100. D however computes k once for the system, and then
once for each agent, leading to 21 computations per time
step. It therefore reaches 2100 computations at time step
100. We also show the results of the full D computation
at t=2100, which needs 44100 computations of k as D44K
.
348 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Table 1: System Performance for 20 Agents, 2 
congestions and α = .5, after 2100 G evaluations (except
for D44K
which has 44100 G evaluations at t=2100).
Reward G σ/
√
n time
Dest2
-232.5 7.55 2100
Dest1
-234.4 6.83 2100
D -277.0 7.8 100
D44K
-219.9 4.48 2100
G -412.6 13.6 2100
MC -639.0 16.4 2100
Although D44K
provides the best result by a slight margin,
it is achieved at a considerable computational cost. Indeed,
the performance of the two D estimates is remarkable in this
case as they were obtained with about twenty times fewer
computations of k. Furthermore, the two D estimates, 
significantly outperform the full D computation for a given
number of computations of k and validate the assumptions
made in Section 3.4.2. This shows that for this domain, in
practice it is more fruitful to perform more learning steps
and approximate D, than few learning steps with full D 
computation when we treat FACET as a black box.
5. DISCUSSION
The efficient, safe and reliable management of air traffic
flow is a complex problem, requiring solutions that integrate
control policies with time horizons ranging from minutes
up to a year. The main contribution of this paper is to
present a distributed adaptive air traffic flow management
algorithm that can be readily implemented and to test that
algorithm using FACET, a simulation tool widely used by
the FAA, NASA and the industry. Our method is based on
agents representing fixes and having each agent determine
the separation between aircraft approaching its fix. It offers
the significant benefit of not requiring radical changes to
the current air flow management structure and is therefore
readily deployable. The agents use reinforcement learning to
learn control policies and we explore different agent reward
functions and different ways of estimating those functions.
We are currently extending this work in three directions.
First, we are exploring new methods of estimating agent 
rewards, to further speed up the simulations. Second we are
investigating deployment strategies and looking for 
modifications that would have larger impact. One such 
modification is to extend the definition of agents from fixes to
sectors, giving agents more opportunity to control the 
traffic flow, and allow them to be more efficient in eliminating
congestion. Finally, in cooperation with domain experts,
we are investigating different system evaluation functions,
above and beyond the delay and congestion dependent G
presented in this paper.
Acknowledgments: The authors thank Banavar 
Sridhar for his invaluable help in describing both current air
traffic flow management and NGATS, and Shon Grabbe for
his detailed tutorials on FACET.
6. REFERENCES
[1] A. Agogino and K. Tumer. Efficient evaluation
functions for multi-rover systems. In The Genetic and
Evolutionary Computation Conference, pages 1-12,
Seatle, WA, June 2004.
[2] A. Agogino and K. Tumer. Multi agent reward
analysis for learning in noisy domains. In Proceedings
of the Fourth International Joint Conference on
Autonomous Agents and Multi-Agent Systems,
Utrecht, Netherlands, July 2005.
[3] A. K. Agogino and K. Tumer. Handling communiction
restrictions and team formation in congestion games.
Journal of Autonous Agents and Multi Agent Systems,
13(1):97-115, 2006.
[4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S.
Shethand, and S. R. Grabbe. FACET: Future ATM
concepts evaluation tool. Air Traffic Control
Quarterly, 9(1), 2001.
[5] Karl D. Bilimoria. A geometric optimization approach
to aircraft conflict resolution. In AIAA Guidance,
Navigation, and Control Conf, Denver, CO, 2000.
[6] Martin S. Eby and Wallace E. Kelly III. Free flight
separation assurance using distributed algorithms. In
Proc of Aerospace Conf, 1999, Aspen, CO, 1999.
[7] FAA OPSNET data Jan-Dec 2005. US Department of
Transportation website.
[8] S. Grabbe and B. Sridhar. Central east pacific flight
routing. In AIAA Guidance, Navigation, and Control
Conference and Exhibit, Keystone, CO, 2006.
[9] Jared C. Hill, F. Ryan Johnson, James K. Archibald,
Richard L. Frost, and Wynn C. Stirling. A cooperative
multi-agent approach to free flight. In AAMAS "05:
Proceedings of the fourth international joint conference
on Autonomous agents and multiagent systems, pages
1083-1090, New York, NY, USA, 2005. ACM Press.
[10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.
Optimal strategies for free flight air traffic conflict
resolution. Journal of Guidance, Control, and
Dynamics, 22(2):202-211, 1999.
[11] 2006 NASA Software of the Year Award Nomination.
FACET: Future ATM concepts evaluation tool. Case
no. ARC-14653-1, 2006.
[12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.
Autonomous agents for air-traffic deconfliction. In
Proc of the Fifth Int Jt Conf on Autonomous Agents
and Multi-Agent Systems, Hakodate, Japan, May 2006.
[13] B. Sridhar and S. Grabbe. Benefits of direct-to in
national airspace system. In AIAA Guidance,
Navigation, and Control Conf, Denver, CO, 2000.
[14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.
Aggregate flow model for air-traffic management.
Journal of Guidance, Control, and Dynamics,
29(4):992-997, 2006.
[15] R. S. Sutton and A. G. Barto. Reinforcement
Learning: An Introduction. MIT Press, Cambridge,
MA, 1998.
[16] C. Tomlin, G. Pappas, and S. Sastry. Conflict
resolution for air traffic management. IEEE Tran on
Automatic Control, 43(4):509-521, 1998.
[17] K. Tumer and D. Wolpert, editors. Collectives and the
Design of Complex Systems. Springer, New York,
2004.
[18] D. H. Wolpert and K. Tumer. Optimal payoff
functions for members of collectives. Advances in
Complex Systems, 4(2/3):265-279, 2001.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349
Hypotheses Refinement under Topological
Communication Constraints ∗
Gauvain Bourgne, Gael Hette, Nicolas Maudet, and Suzanne Pinson
LAMSADE, Univ. Paris-Dauphine, France
{bourgne,hette,maudet,pinson}@lamsade.dauphine.fr
ABSTRACT
We investigate the properties of a multiagent system where
each (distributed) agent locally perceives its environment.
Upon perception of an unexpected event, each agent locally
computes its favoured hypothesis and tries to propagate it
to other agents, by exchanging hypotheses and supporting
arguments (observations). However, we further assume that
communication opportunities are severely constrained and
change dynamically. In this paper, we mostly investigate
the convergence of such systems towards global consistency.
We first show that (for a wide class of protocols that we
shall define), the communication constraints induced by the
topology will not prevent the convergence of the system, at
the condition that the system dynamics guarantees that no
agent will ever be isolated forever, and that agents have 
unlimited time for computation and arguments exchange. As
this assumption cannot be made in most situations though,
we then set up an experimental framework aiming at 
comparing the relative efficiency and effectiveness of different
interaction protocols for hypotheses exchange. We study a
critical situation involving a number of agents aiming at 
escaping from a burning building. The results reported here
provide some insights regarding the design of optimal 
protocol for hypotheses refinement in this context.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
Intelligence-Multiagent systems
General Terms
Theory, Experimentation
1. INTRODUCTION
We consider a multiagent system where each (distributed)
agent locally perceives its environment, and we assume that
some unexpected event occurs in that system. If each agent
computes only locally its favoured hypothesis, it is only 
natural to assume that agents will seek to coordinate and 
refine their hypotheses by confronting their observations with
other agents. If, in addition, the communication 
opportunities are severely constrained (for instance, agents can
only communicate when they are close enough to some other
agent), and dynamically changing (for instance, agents may
change their locations), it becomes crucial to carefully 
design protocols that will allow agents to converge to some
desired state of global consistency. In this paper we 
exhibit some sufficient conditions on the system dynamics and
on the protocol/strategy structures that allow to guarantee
that property, and we experimentally study some contexts
where (some of) these assumptions are relaxed.
While problems of diagnosis are among the venerable 
classics in the AI tradition, their multiagent counterparts have
much more recently attracted some attention. Roos and 
colleagues [8, 9] in particular study a situation where a number
of distributed entities try to come up with a satisfying global
diagnosis of the whole system. They show in particular that
the number of messages required to establish this global 
diagnosis is bound to be prohibitive, unless the communication
is enhanced with some suitable protocol. However, they do
not put any restrictions on agents" communication options,
and do not assume either that the system is dynamic.
The benefits of enhancing communication with supporting
information to make convergence to a desired global state
of a system more efficient has often been put forward in the
literature. This is for instance one of the main idea 
underlying the argumentation-based negotiation approach [7], where
the desired state is a compromise between agents with 
conflicting preferences. Many of these works however make the
assumption that this approach is beneficial to start with,
and study the technical facets of the problem (or instead
emphasize other advantages of using argumentation). 
Notable exceptions are the works of [3, 4, 2, 5], which studied in
contexts different from ours the efficiency of argumentation.
The rest of the paper is as follows. Section 2 specifies
the basic elements of our model, and Section 3 goes on to
presenting the different protocols and strategies used by the
agents to exchange hypotheses and observations. We put
special attention at clearly emphasizing the conditions on
the system dynamics and protocols/strategies that will be
exploited in the rest of the paper. Section 4 details one of
998
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
the main results of the paper, namely the fact that under the
aforementioned conditions, the constraints that we put on
the topology will not prevent the convergence of the system
towards global consistency, at the condition that no agent
ever gets completely lost forever in the system, and that
unlimited time is allowed for computation and argument 
exchange. While the conditions on protocols and strategies are
fairly mild, it is also clear that these system requirements
look much more problematic, even frankly unrealistic in 
critical situations where distributed approaches are precisely
advocated. To get a clearer picture of the situation induced
when time is a critical factor, we have set up an 
experimental framework that we introduce and discuss in Section 5.
The critical situation involves a number of agents aiming
at escaping from a burning building. The results reported
here show that the effectiveness of argument exchange 
crucially depends upon the nature of the building, and provide
some insights regarding the design of optimal protocol for
hypotheses refinement in this context.
2. BASIC NOTIONS
We start by defining the basic elements of our system.
Environment
Let O be the (potentially infinite) set of possible 
observations. We assume the sensors of our agents to be perfect,
hence the observations to be certain. Let H be the set of
hypotheses, uncertain and revisable. Let Cons(h, O) be the
consistency relation, a binary relation between a hypothesis
h ∈ H and a set of observations O ⊆ O. In most cases, Cons
will refer to classical consistency relation, however, we may
overload its meaning and add some additional properties to
that relation (in which case we will mention it).
The environment may include some dynamics, and change
over the course of time. We define below sequences of time
points to deal with it:
Definition 1 (Sequence of time points). A 
sequence of time points t1, t2, . . . , tn from t is an ordered
set of time points t1, t2, . . . , tn such that t1 ≥ t and
∀i ∈ [1, n − 1], ti+1 ≥ ti.
Agent
We take a system populated by n agents a1, . . . , an. Each
agent is defined as a tuple F, Oi, hi , where:
• F, the set of facts, common knowledge to all agents.
• Oi ∈ 2O
, the set of observations made by the agent
so far. We assume a perfect memory, hence this set
grows monotonically.
• hi ∈ H, the favourite hypothesis of the agent.
A key notion governing the formation of hypotheses is that
of consistency, defined below:
Definition 2 (Consistency). We say that:
• An agent is consistent (Cons(ai)) iff Cons(hi, Oi)
(that is, its hypothesis is consistent with its 
observation set).
• An agent ai consistent with a partner agent aj iff
Cons(ai) and Cons(hi, Oj) (that is, this agent is 
consistent and its hypothesis can explain the observation
set of the other agent).
• Two agents ai and aj are mutually consistent
(MCons(ai, aj)) iff Cons(ai, aj) and Cons(aj, ai).
• A system is consistent iff ∀(i, j)∈[1, n]2
it is the case
that MCons(ai, aj).
To ensure its consistency, each agent is equipped with an
abstract reasoning machinery that we shall call the 
explanation function Eh. This (deterministic) function takes a
set of observation and returns a single prefered hypothesis
(2O
→ H). We assume h = Eh(O) to be consistent with
O by definition of Eh, so using this function on its 
observation set to determine its favourite hypothesis is a sure way
for the agent to achieve consistency. Note however that an
hypothesis does not need to be generated by Eh to be 
consistent with an observation set. As a concrete example of such
a function, and one of the main inspiration of this work,
one can cite the Theorist reasoning system [6] -as long as
it is coupled with a filter selecting a single prefered theory
among the ones initially selected by Theorist.
Note also that hi may only be modified as a consequence
of the application Eh. We refer to this as the autonomy of the
agent: no other agent can directly impose a given 
hypothesis to an agent. As a consequence, only a new observation
(being it a new perception, or an observation communicated
by a fellow agent) can result in a modification of its prefered
hypothesis hi (but not necessarily of course).
We finally define a property of the system that we shall
use in the rest of the paper:
Definition 3 (Bounded Perceptions). A system
involves a bounded perception for agents iff ∃n0 s.t.
∀t| ∪N
i=1 Oi| ≤ n0. (That is, the number of observations to
be made by the agents in the system is not infinite.)
Agent Cycle
Now we need to see how these agents will evolve and interact
in their environment. In our context, agents evolve in a 
dynamic environment, and we classicaly assume the following
system cycle:
1. Environment dynamics: the environment evolves 
according to the defined rules of the system dynamics.
2. Perception step : agents get perceptions from the 
environment. These perceptions are typically partial (e.g.
the agent can only see a portion of a map).
3. Reasoning step: agents compare perception with
predictions, seek explanations for (potential) 
difference(s), refine their hypothesis, draw new conclusions.
4. Communication step: agents can communicate 
hypotheses and observations with other agents through
a defined protocol. Any agent can only be involved in
one communication with another agent by step.
5. Action step: agents do some practical reasoning using
the models obtained from the previous steps and select
an action. They can then modify the environment by
executing it.
The communication of the agents will be further 
constrained by topological consideration. At a given time, an
agent will only be able to communicate with a number of
neighbours. Its connexions with these others agents may
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 999
evolve with its situation in the environment. Typically, an
agent can only communicate with agents that it can sense,
but one could imagine evolving topological constraints on
communication based on a network of communications 
between agents where the links are not always active.
Communication
In our system, agents will be able to communicate with each
other. However, due to the aforementionned topological 
constraints, they will not be able to communicate with any
agents at anytime. Who an agent can communicate with
will be defined dynamically (for instance, this can be a 
consequence of the agents being close enough to get in touch).
We will abstractly denote by C(ai, aj, t) the communication
property, in other words, the fact that agents ai and aj can
communicate at time t (note that this relation is assumed
to be symetric, but of course not transitive). We are now in
a position to define two essential properties of our system.
Definition 4 (Temporal Path). There exists a 
temporal communication path at horizon tf (noted Ltf (aI , aJ ))
between ai and aj iff there exists a sequence of time points
t1, t2, . . . , tn from tf and a sequence of agents k1, k2, . . . , kn
s.t. (i) C(aI , ak1 , t1), (ii) C(akn , aJ , tn+1), (iii) ∀i ∈ [1, n],
C(aki , aki+1 , ti)
Intuitively, what this property says is that it is possible to
find a temporal path in the future that would allow to link
agent ai and aj via a sequence of intermediary agents. Note
that the time points are not necessarily successive, and that
the sequence of agents may involve the same agents several
times.
Definition 5 (Temporal Connexity). A system is
temporaly connex iff ∀t ∀(i, j)∈[1, n]2
Lt(ai, aj)
In short, a temporaly connex system guarantees that any
agent will be able to communicate with any other agents,
no matter how long it might take to do so, at any time. To
put it another way, it is never the case that an agent will be
isolated for ever from another agent of the system.
We will next discuss the detail of how communication 
concretely takes place in our system. Remember that in this
paper, we only consider the case of bilateral exchanges (an
agent can only speak to a single other agent), and that we
also assume that any agent can only engage in a single 
exchange in a given round.
3. PROTOCOLS AND STRATEGIES
In this section, we discuss the requirements of the 
interaction protocols that govern the exchange of messages between
agents, and provide some example instantiation of such 
protocols. To clarify the presentation, we distinguish two 
levels: the local level, which is concerned with the regulation
of bilateral exchanges; and the global level,which essentially
regulates the way agents can actually engage into a 
conversation. At each level, we separate what is specified by the
protocol, and what is left to agents" strategies.
Local Protocol and Strategies
We start by inspecting local protocols and strategies that
will regulate the communication between the agents of the
system. As we limit ourselves to bilateral communication,
these protocols will simply involve two agents. Such protocol
will have to meet one basic requirement to be satisfying.
• consistency (CONS)- a local protocol has to 
guarantee the mutual consistency of agents upon termination
(which implies termination of course).
Figure 1: A Hypotheses Exchange Protocol [1]
One example such protocol is the protocol described in [1]
that is pictured in Fig. 1. To further illustrate how such 
protocol can be used by agents, we give some details on a 
possible strategy: upon receiving a hypothesis h1 (propose(h1) or
counterpropose(h1)) from a1, agent a2 is in state 2 and has
the following possible replies: counterexample (if the agent
knows an example contradicting the hypothesis, or not 
explained by this hypothesis), challenge (if the agents lacks
evidence to accept this hypothesis), counterpropose (if the
agent agrees with the hypothesis but prefers another one),
or accept (if it is indeed as good as its favourite 
hypothesis). This strategy guarantees, among other properties, the
eventual mutual logical consistency of the involved agents
[1].
Global Protocol
The global protocol regulates the way bilateral exchanges
will be initiated between agents. At each turn, agents will
concurrently send one weighted request to communicate to
other agents. This weight is a value measuring the agent"s
willingness to converse with the targeted agent (in practice,
this can be based on different heuristics, but we shall make
some assumptions on agents" strategies, see below). Sending
such a request is a kind of conditional commitment for the
agent. An agent sending a weighted request commits to
engage in conversation with the target if he does not receive
and accept himself another request. Once all request have
been received, each agent replies with either an acccept or
a reject. By answering with an accept, an agent makes a
full commitment to engage in conversation with the sender.
Therefore, it can only send one accept in a given round, as an
agent can only participate in one conversation per time step.
When all response have been received, each agent receiving
an accept can either initiate a conversation using the local
protocol or send a cancel if it has accepted another request.
At the end of all the bilateral exchanges, the agents 
engaged in conversation are discarded from the protocol. Then
each of the remaining agents resends a request and the 
process iterates until no more requests are sent.
Global Strategy
We now define four requirements for the strategies used by
agents, depending on their role in the protocol: two are
concerned with the requestee role (how to decide who the
1000 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
agent wishes to communicate with?), the other two with the
responder role (how to decide which communication request
to accept or not?).
• Willingness to solve inconsistancies (SOLVE)-agents
want to communicate with any other agents unless
they know they are mutually consistent.
• Focus on solving inconsistencies (FOCUS)-agents do
not request communication with an agent with whom
they know they are mutually consistent.
• Willingness to communicate (COMM)-agents cannot
refuse a weighted communication request, unless they
have just received or send a request with a greater
weight.
• Commitment to communication request 
(REQU)agents cannot accept a weighted communication 
request if they have themselves sent a communication
request with a greater weight. Therefore, they will
not cancel their request unless they have received a
communicational request with greater weight.
Now the protocol structure, together with the properties
COMM+REQU, ensure that a request can only be rejected
if its target agent engages in communication with another
agent. Suppose indeed that agent ai wants to communicate
with aj by sending a request with weight w. COMM 
guarantees that an agent receiving a weighted request will either
accept this communication, accept a communication with a
greater weight or wait for the answer to a request with a
greater weight. This ensures that the request with 
maximal weight will be accepted and not cancelled (as REQU
ensures that an agent sending a request can only cancel it if
he accepts another request with greater weight). Therefore
at least two agents will engage in conversation per round
of the global protocol. As the protocol ensures that ai can
resend its request while aj is not engaged in a conversation,
there will be a turn in which aj must engage in a 
conversation, either with ai or another agent.
These requirements concern request sending and 
acceptation, but agents also need some strategy of weight 
attribution. We describe below an altruist strategy, used in our
experiments. Being cooperative, an agent may want to know
more of the communication wishes of other agents in order to
improve the overall allocation of exchanges to agents. A 
context request step is then added to the global protocol. Before
sending their chosen weighted request, agents attribute a
weight to all agents they are prepared to communicate with,
according to some internal factors. In the simplest case, this
weight will be 1 for all agent with whom the agent is not
sure of being mutually consistent (ensuring SOLVE), other
agent being not considered for communication (ensuring 
FOCUS). The agent then sends a context request to all agents
with whom communication is considered. This request also
provides information about the sender (list of considered
communications along with their weight). After reception
of all the context requests, agents will either reply with a
deny, iff they are already engaged in a conversation (in which
case, the requesting agent will not consider communication
with them anymore in this turn), or an inform giving the
requester information about the requests it has sent and 
received. When all replies have been received, each agent can
calculate the weight of all requests concerning it. It does so
by substracting from the weight of its request the weight of
all requests concerning either it or its target (that is, the 
final weight of the request from ai to aj is Wi,j = wi,j +wj,i −
(
P
k∈R(i)−{j} wi,k +
P
k∈S(i)−{j} wk,i +
P
k∈R(j)−{i} wj,k +
P
k∈S(j)−{i} wk,j) where wi,j is the weight of the request of
ai to aj, R(i) is the set of indice of agents having received a
request from ai and S(i) is the set of indice of agents 
having send a request to ai). It then finally sends a weighted
request to the agents who maximise this weight (or wait for
a request) as described in the global protocol.
4. (CONDITIONAL) CONVERGENCE TO
GLOBAL CONSISTENCY
In this section we will show that the requirements 
regarding protocols and strategies just discussed will be sufficient
to ensure that the system will eventually converge towards
global consistency, under some conditions. We first show
that, if two agents are not mutually consistent at some time,
then there will be necessarily a time in the future such that
an agent will learn a new observation, being it because it is
new for the system, or by learning it from another agent.
Lemma 1. Let S be a system populated by n agents
a1, a2, ..., an, temporaly connex, and involving bounded 
perceptions for these agents. Let n1 be the sum of 
cardinalities of the intersection of pairwise observation sets.
(n1 =
P
(i,j)∈[1,n]2 |Oi ∩ Oj|) Let n2 be the cardinality of
the union of all agents" observations sets. (n2 = | ∪N
i=1 Oi|).
If ¬MCons(ai, aj) at time t0, there is necessarily a time
t > t0 s.t. either n1 or n2 will increase.
Proof. Suppose that there exist a time t0
and indices (i, j) s.t. ¬MCons(ai, aj). We will
use mt0 =
P
(k,l)∈[1,n]2 εComm(ak, al, t0) where
εComm(ak, al, t0) = 1 if ak and al have 
communicated at least once since t0, and 0 otherwise. 
Temporal connexity guarantees that there exist t1, ..., tm+1
and k1, ..., km s.t. C(ai, ak1 , t1), C(akm , aj, tm+1), and
∀p ∈ [1, m], C(akp , akp+1 , tp). Clearly, if MCons(ai, ak1 ),
MCons(akm , aj) and ∀p, MCons(akp , akp+1 ), we have
MCons(ai, aj) which contradicts our hypothesis (MCons
being transitive, MCons(ai, ak1 )∧MCons(ak1 , ak2 ) implies
that MCons(ai, ak2 ) and so on till MCons(ai, akm )∧
MCons(akm , aj) which implies MCons(ai, aj) ).
At least two agents are then necessarily 
inconsistent (¬MCons(ai, ak1 ), or ¬MCons(akm , aj), or ∃p0 t.q.
¬MCons(akp0
, akp0+1 )). Let ak and al be these two 
neighbours at a time t > t0
1
. The SOLVE property ensures
that either ak or al will send a communication request to
the other agent at time t . As shown before, this in turn
ensures that at least one of these agents will be involved in
a communication. Then there are two possibilities:
(case i) ak and al communicate at time t . In this case,
we know that ¬MCons(ak, al). This and the CONS 
property ensures that at least one of the agents must change its
1
Strictly speaking, the transitivity of MCons only ensure
that ak and al are inconsistent at a time t ≥ t0 that can
be different from the time t at which they can 
communicate. But if they become consistent between t and t (or
inconsistent between t and t ), it means that at least one
of them have changed its hypothesis between t and t , that
is, after t0. We can then apply the reasoning of case iib.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1001
hypothesis, which in turn, since agents are autonomous, 
implies at least one exchange of observation. But then |Ok ∩Ol|
is bound to increase: n1(t ) > n1(t0).
(case ii) ak communicates with ap at time t . We then have
again two possibilities:
(case iia) ak and ap did not communicate since t0. But then
εComm(ak, ap, t0) had value 0 and takes value 1. Hence mt0
increases.
(case iib) ak and ap did communicate at some time t0 >
t0. The CONS property of the protocol ensures that
MCons(ak, ap) at that time. Now the fact that they 
communicate and FOCUS implies that at least one of them did
change its hypothesis in the meantime. The fact that agents
are autonomous implies in turn that a new observation 
(perceived or received from another agent) necessarily provoked
this change. The latter case would ensure the existence of a
time t > t0 and an agent aq s.t. either |Op ∩Oq| or |Ok ∩Oq|
increases of 1 at that time (implying n1(t ) > n1(t0)). The
former case means that the agent gets a new perception o
at time t . If that observation was unknown in the system
before, then n2(t ) > n2(t0). If some agent aq already knew
this observation before, then either Op ∩ Oq or Ok ∩ Oq 
increases of 1 at time t (which implies that n1(t ) > n1(t0)).
Hence, ¬MCons(ai, aj) at time t0 guarantees that, either:
−∃t > t0 t.q. n1(t ) > n1(t0); or
−∃t > t0 t.q. n2(t ) > n2(t0); or
−∃t > t0 t.q. mt0 increases of 1 at time t .
By iterating the reasoning with t (but keeping t0 as the
time reference for mt0 ), we can eliminate the third case
(mt0 is integer and bounded by n2
, which means that 
after a maximum of n2
iterations, we necessarily will be in
one of two other cases.) As a result, we have proven that if
¬MCons(ai, aj) at time t0, there is necessarily a time t s.t.
either n1 or n2 will increase.
Theorem 1 (Global consistency). Let S be a 
system populated by n agents a1, a2, ..., an, temporaly connex,
and involving bounded perceptions for these agents. Let
Cons(ai, aj) be a transitive consistency property. Then any
protocol and strategies satisfying properties CONS, SOLVE,
FOCUS, COMM and REQU guarantees that the system will
converge towards global consistency.
Proof. For the sake of contradiction, let us assume
∃I, J ∈ [1, N] s.t. ∀t, ∃t0 > t, t.q. ¬Cons(aI , aJ , t0).
Using the lemma, this implies that ∃t > t0 s.t. either
n1(t ) > n1(t0) or n2(t ) > n2(t0). But we can apply
the same reasoning taking t = t , which would give us
t1 > t > t0 s.t. ¬Cons(aI , aJ , t1), which gives us t > t1
s.t. either n1(t ) > n1(t1) or n2(t ) > n2(t1). By 
successive iterations we can then construct a sequence t0, t1, ..., tn,
which can be divided in two sub-sequences t0, t1, ...tn and
t0 , t1 , ..., tn s.t. n1(t0) < n1(t1) < ... < n1(tn) and
n2(t0 ) < n2(t1 ) < ... < n2(tn). One of these sub-sequences
has to be infinite. However, n1(ti) and n2(ti ) are strictly
growing, integer, and bounded, which implies that both are
finite. Contradiction.
What the previous result essentially shows is that, in a
system where no agent will be isolated from the rest of the
agents for ever, only very mild assumptions on the protocols
and strategies used by agents suffice to guarantee 
convergence towards system consistency in a finite amount of time
(although it might take very long). Unfortunately, in many
critical situations, it will not be possible to assume this
temporal connexity. As distributed approaches as the one
advocated in this paper are precisely often presented as a
good way to tackle problems of reliability or problems of 
dependence to a center that are of utmost importance in these
critical applications, it is certainly interesting to further 
explore how such a system would behave when we relax this
assumption.
5. EXPERIMENTAL STUDY
This experiment involves agents trying to escape from a
burning building. The environment is described as a spatial
grid with a set of walls and (thankfully) some exits. Time
and space are considered discrete. Time is divided in rounds.
Agents are localised by their position on the spatial grid.
These agents can move and communicate with other agents.
In a round, an agent can move of one cell in any of the four
cardinal directions, provided it is not blocked by a wall. In
this application, agents communicate with any other agent
(but, recall, a single one) given that this agent is in view,
and that they have not yet exchanged their current favoured
hypothesis. Suddenly, a fire erupts in these premises. From
this moment, the fire propagates. Each round, for each cases
where there is fire, the fire propagates in the four directions.
However, the fire cannot propagate through a wall. If the
fire propagates in a case where an agent is positioned, that
agent burns and is considered dead. It can of course no
longer move nor communicate. If an agent gets to an exit,
it is considered saved, and can no longer be burned. Agents
know the environment and the rules governing the 
dynamics of this environment, that is, they know the map as well
as the rules of fire propagation previously described. They
also locally perceive this environment, but cannot see 
further than 3 cases away, in any direction. Walls also block
the line of view, preventing agents from seeing behind them.
Within their sight, they can see other agents and whether
or not the cases they see are on fire. All these perceptions
are memorised.
We now show how this instantiates the abstract 
framework presented the paper.
• O = {Fire(x, y, t), NoFire(x, y, t), Agent(ai, x, y, t)}
Observations can then be positive (o ∈ P(O) iff ∃h ∈
H s.t. h |= o) or negative (o ∈ N(O) iff ∃h ∈ H s.t.
h |= ¬o).
• H={FireOrigin(x1, y1, t1)∧...∧FireOrigin(xl, yl, tl)}
Hypotheses are conjunctions of FireOrigins.
• Cons(h, O) consistency relation satisfies:
- coherence : ∀o ∈ N(O), h |= ¬o.
- completeness : ∀o ∈ P(O), h |= o.
- minimality : For all h ∈ H, if h is coherent and
complete for O, then h is prefered to h according
to the preference relation (h ≤p h ).2
2
Selects first the minimal number of origins, then the most
recent (least preemptive strategy [6]), then uses some 
arbitrary fixed ranking to discriminate ex-aequo. The resulting
relation is a total order, hence minimality implies that there
will be a single h s.t.Cons(O, h) for a given O. This in
turn means that MCons(ai, aj) iff Cons(ai), Cons(aj), and
hi = hj. This relation is then transitive and symmetric.
1002 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
• Eh takes O as argument and returns min≤p of the 
coherent and complete hypothesis for O
5.1 Experimental Evaluation
We will classically (see e.g. [3, 4]) assess the effectiveness
and efficiency of different interaction protocols.
Effectiveness of a protocol
The proportion of agents surviving the fire over the initial
number of agents involved in the experiment will determine
the effectiveness of a given protocol. If this value is high,
the protocol has been effective to propagate the information
and/or for the agents to refine their hypotheses and 
determine the best way to the exit.
Efficiency of a protocol
Typically, the use of supporting information will involve a
communication overhead. We will assume here that the 
efficiency of a given protocol is characterised by the data flow
induced by this protocol. In this paper we will only discuss
this aspect wrt. local protocols. The main measure that we
shall then use here is the mean total size of messages that
are exchanged by agents per exchange (hence taking into 
account both the number of messages and the actual size of the
messages, because it could be that messages happen to be
very big, containing e.g. a large number of observations,
which could counter-balance a low number of messages).
5.2 Experimental Settings
The chosen experimental settings are the following:
• Environmental topology- Performances of 
information propagation are highly constrained by the 
environment topology. The perception skills of the agents
depend on the openness of the environment. With
a large number of walls the perceptions of agents are
limited, and also the number of possible inter-agent
communications, whereas an open environment will
provide optimal possibilities of perception and 
information propagation. Thus, we propose a topological
index (see below) as a common basis to charaterize the
environments (maps) used during experimentations.
The topological index (TI) is the ratio of the number of
cells that can be perceived by agents summed up from
all possible positions, divided by the number of cells
that would be perceived from the same positions but
without any walls. (The closer to 1, the more open the
environment). We shall also use two additional, more
classical [10], measures: the characteristic path length3
(CPL) and the clustering coefficient4
(CC).
• Number of agents- The propagation of information
also depends on the initial number of agents involved
during an experimentation. For instance, the more
agents, the more potential communications there is.
This means that there will be more potential for 
propagation, but also that the bilateral exchange restriction
will be more crucial.
3
The CPL is the median of the means of the shortest path
lengths connecting each node to all other nodes.
4
characterising the isolation degree of a region of an 
environment in terms of acessibility (number of roads still usable
to reach this region).
Map T.I. (%) C.P.L. C.C.
69-1 69,23 4,5 0,69
69-2 68,88 4,38 0,65
69-3 69,80 4,25 0,67
53-1 53,19 5,6 0,59
53-2 53,53 6,38 0,54
53-3 53,92 6,08 0,61
38-1 38,56 8,19 0,50
38-2 38,56 7,3 0,50
38-3 38,23 8,13 0,50
Table 1: Topological Characteristics of the Maps
• Initial positions of the agents- Initial positions of the
agents have a significant influence on the overall 
behavior of an instance of our system: being close from
an exit will (in general) ease the escape.
5.3 Experimental environments
We choose to realize experiments on three very 
different topological indexes (69% for open environments, 53%
for mixed environments, and 38% for labyrinth-like 
environments).
Figure 2: Two maps (left: TI=69%, right TI=38%)
We designed three different maps for each index (Fig. 2
shows two of them), containing the same maximum number
of agents (36 agents max.) with a maximum density of one
agent per cell, the same number of exits and a similar fire
origin (e.g. starting time and position). The three differents
maps of a given index are designed as follows. The first map
is a model of an existing building floor. The second map has
the same enclosure, exits and fire origin as the first one,
but the number and location of walls are different (wall 
locations are designed by an heuristic which randomly creates
walls on the spatial grid such that no fully closed rooms are
created and that no exit is closed). The third map is 
characterised by geometrical enclosure in wich walls location
is also designed with the aforementioned heuristic. Table 1
summarizes the different topological measures 
characterizing these different maps. It is worth pointing out that the
values confirm the relevance of TI (maps with a high TI
have a low CPL and a high CC. However the CPL and CC
allows to further refine the difference between the maps, e.g.
between 53-1 and 53-2).
5.4 Experimental Results
For each triple of maps defined as above we conduct the
same experiments. In each experiment, the society differs in
terms of its initial proportion of involved agents, from 1%
to 100%. This initial proportion represents the percentage
of involved agents with regards to the possible maximum
number of agents. For each map and each initial proportion,
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1003
we select randomly 100 different initial agents" locations.
For each of those different locations we execute the system
one time for each different interaction protocol.
Effectiveness of Communication and Argumentation
The first experiment that we set up aims at testing how 
effective is hypotheses exchange (HE), and in particular how
the topological aspects will affect this effectiveness. In order
to do so, we have computed the ratio of improvement offered
by that protocol over a situation where agents could simply
not communicate (no comm). To get further insights as
to what extent the hypotheses exchange was really crucial,
we also tested a much less elaborated protocol consisting
of mere observation exchanges (OE). More precisely, this
protocol requires that each agent stores any unexpected
observation that it perceives, and agents simply exchange
their respective lists of observations when they discuss. In
this case, the local protocol is different (note in 
particular that it does not guarantee mutual consistency), but the
global protocol remains the same (at the only exception that
agents" motivation to communicate is to synchronise their
list of observations, not their hypothesis). If this protocol is
at best as effective as HE, it has the advantage of being more
efficient (this is obvious wrt the number of messages which
will be limited to 2, less straightforward as far as the size of
messages is concerned, but the rough observation that the
exchange of observations can be viewed as a flat version
of the challenge is helpful to see this). The results of these
experiments are reported in Fig. 3.
Figure 3: Comparative effectiveness ratio gain of
protocols when the proportion of agents augments
The first observation that needs to be made is that 
communication improves the effectiveness of the process, and
this ratio increases as the number of agents grows in the
system. The second lesson that we learn here is that 
closeness relatively makes communication more effective over non
communication. Maps exhibiting a T.I. of 38% are 
constantly above the two others, and 53% are still slightly but
significantly better than 69%. However, these curves also
suggest, perhaps surprisingly, that HE outperforms OE in
precisely those situations where the ratio gain is less 
important (the only noticeable difference occurs for rather open
maps where T.I. is 69%). This may be explained as follows:
when a map is open, agents have many potential explanation
candidates, and argumentation becomes useful to 
discriminate between those. When a map is labyrinth-like, there are
fewer possible explanations to an unexpected event.
Importance of the Global Protocol
The second set of experiments seeks to evaluate the 
importance of the design of the global protocol. We tested our
protocol against a local broadcast (LB) protocol. Local
broadcast means that all the neighbours agents perceived
by an agent will be involved in a communication with that
agent in a given round -we alleviate the constraint of a 
single communication by agent. This gives us a rough upper
bound upon the possible ratio gain in the system (for a given
local protocol). Again, we evaluated the ratio gain induced
by that LB over our classical HE, for the three different
classes of maps. The results are reported in Fig. 4.
Figure 4: Ratio gain of local broadcast over 
hypotheses exchange
Note to begin with that the ratio gain is 0 when the 
proportion of agents is 5%, which is easily explained by the fact
that it corresponds to situations involving only two agents.
We first observe that all classes of maps witness a ratio
gain increasing when the proportion of agents augments: the
gain reaches 10 to 20%, depending on the class of maps 
considered. If one compares this with the improvement reported
in the previous experiment, it appears to be of the same
magnitude. This illustrates that the design of the global
protocol cannot be ignored, especially when the proportion
of agents is high. However, we also note that the 
effectiveness ratio gain curves have very different shapes in both
cases: the gain induced by the accuracy of the local protocol
increases very quickly with the proportion of agents, while
the curve is really smooth for the global one.
Now let us observe more carefully the results reported
here: the curve corresponding to a TI of 53% is above that
corresponding to 38%. This is so because the more open
a map, the more opportunities to communicate with more
than one agent (and hence benefits from broadcast). 
However, we also observe that curve for 69% is below that for
53%. This is explained as follows: in the case of 69%, the
potential gain to be made in terms of surviving agents is
much lower, because our protocols already give rather 
efficient outcomes anyway (quickly reaching 90%, see Fig. 3).
A simple rule of thumb could be that when the number of
agents is small, special attention should be put on the local
1004 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
protocol, whereas when that number is large, one should
carefully design the global one (unless the map is so open
that the protocol is already almost optimally efficient).
Efficiency of the Protocols
The final experiment reported here is concerned with the
analysis of the efficiency of the protocols. We analysis here
the mean size of the totality of the messages that are 
exchanged by agents (mean size of exchanges, for short) using
the following protocols: HE, OE, and two variant 
protocols. The first one is an intermediary restricted hypotheses
exchange protocol (RHE). RHE is as follows: it does not 
involve any challenge nor counter-propose, which means that
agents cannot switch their role during the protocol (this 
differs from RE in that respect). In short, RHE allows an agent
to exhaust its partner"s criticism, and eventually this 
partner will come to adopt the agent"s hypothesis. Note that this
means that the autonomy of the agent is not preserved here
(as an agent will essentially accept any hypothesis it cannot
undermine), with the hope that the gain in efficiency will be
significant enough to compensate a loss in effectiveness. The
second variant protocol is a complete observation exchange
protocol (COE). COE uses the same principles as OE, but
includes in addition all critical negative examples (nofire) in
the exchange (thus giving all examples used as arguments
by the hypotheses exchanges protocol), hence improving 
effectiveness. Results for map 69-1 are shown on Fig. 5.
Figure 5: Mean size of exchanges
First we can observe the fact that the ordering of the 
protocols, from the least efficient to the most efficient, is COE,
HE, RHE and then OE. HE being more efficient than COE
proves that the argumentation process gains efficiency by 
selecting when it is needed to provide negative example, which
have less impact that positive ones in our specific testbed.
However, by communicating hypotheses before eventually
giving observation to support it (HE) instead of directly 
giving the most crucial observations (OE), the argumentation
process doubles the size of data exchanges. It is the cost for
ensuring consistency at the end of the exchange (a property
that OE does not support). Also significant is the fact the
the mean size of exchanges is slightly higher when the 
number of agents is small. This is explained by the fact that in
these cases only a very few agents have relevant informations
in their possession, and that they will need to communicate
a lot in order to come up with a common view of the 
situation. When the number of agents increases, this knowledge
is distributed over more agents which need shorter 
discussions to get to mutual consistency. As a consequence, the
relative gain in efficiency of using RHE appears to be better
when the number of agents is small: when it is high, they
will hardly argue anyway. Finally, it is worth noticing that
the standard deviation for these experiments is rather high,
which means that the conversation do not converge to any
stereotypic pattern.
6. CONCLUSION
This paper has investigated the properties of a 
multiagent system where each (distributed) agent locally perceives
its environment, and tries to reach consistency with other
agents despite severe communication restrictions. In 
particular we have exhibited conditions allowing convergence, and
experimentally investigated a typical situation where those
conditions cannot hold. There are many possible extensions
to this work, the first being to further investigate the 
properties of different global protocols belonging to the class we
identified, and their influence on the outcome. There are in
particular many heuristics, highly dependent on the context
of the study, that could intuitively yield interesting results
(in our study, selecting the recipient on the basis of what can
be inferred from his observed actions could be such a 
heuristic). One obvious candidate for longer term issues concern
the relaxation of the assumption of perfect sensing.
7. REFERENCES
[1] G. Bourgne, N. Maudet, and S. Pinson. When agents
communicate hypotheses in critical situations. In
Proceedings of DALT-2006, May 2006.
[2] P. Harvey, C. F. Chang, and A. Ghose. Support-based
distributed search: a new approach for multiagent
constraint processing. In Proceedings of AAMAS06,
2006.
[3] H. Jung and M. Tambe. Argumentation as distributed
constraint satisfaction: Applications and results. In
Proceedings of AGENTS01, 2001.
[4] N. C. Karunatillake and N. R. Jennings. Is it worth
arguing? In Proceedings of ArgMAS 2004, 2004.
[5] S. Onta˜n´on and E. Plaza. Arguments and
counterexamples in case-based joint deliberation. In
Proceedings of ArgMAS-2006, May 2006.
[6] D. Poole. Explanation and prediction: An architecture
for default and abductive reasoning. Computational
Intelligence, 5(2):97-110, 1989.
[7] I. Rahwan, S. D. Ramchurn, N. R. Jennings,
P. McBurney, S. Parsons, and L. Sonenberg.
Argumention-based negotiation. The Knowledge
Engineering Review, 4(18):345-375, 2003.
[8] N. Roos, A. ten Tije, and C. Witteveen. A protocol
for multi-agent diagnosis with spatially distributed
knowledge. In Proceedings of AAMAS03, 2003.
[9] N. Roos, A. ten Tije, and C. Witteveen. Reaching
diagnostic agreement in multiagent diagnosis. In
Proceedings of AAMAS04, 2004.
[10] T. Takahashi, Y. Kaneda, and N. Ito. Preliminary
study - using robocuprescue simulations for disasters
prevention. In Proceedings of SRMED2004, 2004.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1005
Learning Consumer Preferences Using Semantic
Similarity ∗
Reyhan Aydo˘gan
reyhan.aydogan@gmail.com
Pınar Yolum
pinar.yolum@boun.edu.tr
Department of Computer Engineering
Bo˘gaziçi University
Bebek, 34342, Istanbul,Turkey
ABSTRACT
In online, dynamic environments, the services requested by 
consumers may not be readily served by the providers. This requires
the service consumers and providers to negotiate their service needs
and offers. Multiagent negotiation approaches typically assume
that the parties agree on service content and focus on finding a
consensus on service price. In contrast, this work develops an 
approach through which the parties can negotiate the content of a 
service. This calls for a negotiation approach in which the parties
can understand the semantics of their requests and offers and learn
each other"s preferences incrementally over time. Accordingly, we
propose an architecture in which both consumers and producers
use a shared ontology to negotiate a service. Through repetitive
interactions, the provider learns consumers" needs accurately and
can make better targeted offers. To enable fast and accurate 
learning of preferences, we develop an extension to Version Space and
compare it with existing learning techniques. We further develop
a metric for measuring semantic similarity between services and
compare the performance of our approach using different 
similarity metrics.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems
General Terms
Algorithms, Experimentation
1. INTRODUCTION
Current approaches to e-commerce treat service price as the 
primary construct for negotiation by assuming that the service content
is fixed [9]. However, negotiation on price presupposes that other
properties of the service have already been agreed upon. 
Nevertheless, many times the service provider may not be offering the exact
requested service due to lack of resources, constraints in its 
business policy, and so on [3]. When this is the case, the producer and
the consumer need to negotiate the content of the requested service
[15].
However, most existing negotiation approaches assume that all
features of a service are equally important and concentrate on the
price [5, 2]. However, in reality not all features may be relevant and
the relevance of a feature may vary from consumer to consumer.
For instance, completion time of a service may be important for one
consumer whereas the quality of the service may be more important
for a second consumer. Without doubt, considering the preferences
of the consumer has a positive impact on the negotiation process.
For this purpose, evaluation of the service components with 
different weights can be useful. Some studies take these weights as a
priori and uses the fixed weights [4]. On the other hand, mostly
the producer does not know the consumer"s preferences before the
negotiation. Hence, it is more appropriate for the producer to learn
these preferences for each consumer.
Preference Learning: As an alternative, we propose an 
architecture in which the service providers learn the relevant features
of a service for a particular customer over time. We represent 
service requests as a vector of service features. We use an ontology
in order to capture the relations between services and to construct
the features for a given service. By using a common ontology, we
enable the consumers and producers to share a common 
vocabulary for negotiation. The particular service we have used is a wine
selling service. The wine seller learns the wine preferences of the
customer to sell better targeted wines. The producer models the
requests of the consumer and its counter offers to learn which 
features are more important for the consumer. Since no information is
present before the interactions start, the learning algorithm has to
be incremental so that it can be trained at run time and can revise
itself with each new interaction.
Service Generation: Even after the producer learns the important
features for a consumer, it needs a method to generate offers that
are the most relevant for the consumer among its set of possible
services. In other words, the question is how the producer uses the
information that was learned from the dialogues to make the best
offer to the consumer. For instance, assume that the producer has
learned that the consumer wants to buy a red wine but the producer
can only offer rose or white wine. What should the producer"s offer
1301
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
contain; white wine or rose wine? If the producer has some domain
knowledge about semantic similarity (e.g., knows that the red and
rose wines are taste-wise more similar than white wine), then it can
generate better offers. However, in addition to domain knowledge,
this derivation requires appropriate metrics to measure similarity
between available services and learned preferences.
The rest of this paper is organized as follows: Section 2 explains
our proposed architecture. Section 3 explains the learning 
algorithms that were studied to learn consumer preferences. Section 4
studies the different service offering mechanisms. Section 5 
contains the similarity metrics used in the experiments. The details of
the developed system is analyzed in Section 6. Section 7 provides
our experimental setup, test cases, and results. Finally, Section 8
discusses and compares our work with other related work.
2. ARCHITECTURE
Our main components are consumer and producer agents, which
communicate with each other to perform content-oriented 
negotiation. Figure 1 depicts our architecture. The consumer agent 
represents the customer and hence has access to the preferences of the
customer. The consumer agent generates requests in accordance
with these preferences and negotiates with the producer based on
these preferences. Similarly, the producer agent has access to the
producer"s inventory and knows which wines are available or not.
A shared ontology provides the necessary vocabulary and hence
enables a common language for agents. This ontology describes
the content of the service. Further, since an ontology can represent
concepts, their properties and their relationships semantically, the
agents can reason the details of the service that is being negotiated.
Since a service can be anything such as selling a car, reserving a
hotel room, and so on, the architecture is independent of the 
ontology used. However, to make our discussion concrete, we use the
well-known Wine ontology [19] with some modification to 
illustrate our ideas and to test our system. The wine ontology describes
different types of wine and includes features such as color, body,
winery of the wine and so on. With this ontology, the service that
is being negotiated between the consumer and the producer is that
of selling wine.
The data repository in Figure 1 is used solely by the producer
agent and holds the inventory information of the producer. The
data repository includes information on the products the producer
owns, the number of the products and ratings of those products.
Ratings indicate the popularity of the products among customers.
Those are used to decide which product will be offered when there
exists more than one product having same similarity to the request
of the consumer agent.
The negotiation takes place in a turn-taking fashion, where the
consumer agent starts the negotiation with a particular service 
request. The request is composed of significant features of the 
service. In the wine example, these features include color, winery and
so on. This is the particular wine that the customer is interested in
purchasing. If the producer has the requested wine in its inventory,
the producer offers the wine and the negotiation ends. Otherwise,
the producer offers an alternative wine from the inventory. When
the consumer receives a counter offer from the producer, it will
evaluate it. If it is acceptable, then the negotiation will end. 
Otherwise, the customer will generate a new request or stick to the
previous request. This process will continue until some service is
accepted by the consumer agent or all possible offers are put 
forward to the consumer by the producer.
One of the crucial challenges of the content-oriented negotiation
is the automatic generation of counter offers by the service 
producer. When the producer constructs its offer, it should consider
Figure 1: Proposed Negotiation Architecture
three important things: the current request, consumer preferences
and the producer"s available services. Both the consumer"s current
request and the producer"s own available services are accessible by
the producer. However, the consumer"s preferences in most cases
will not be available. Hence, the producer will have to understand
the needs of the consumer from their interactions and generate a
counter offer that is likely to be accepted by the consumer. This
challenge can be studied in three stages:
• Preference Learning: How can the producers learn about
each customer"s preferences based on requests and counter
offers? (Section 3)
• Service Offering: How can the producers revise their offers
based on the consumer"s preferences that they have learned
so far? (Section 4)
• Similarity Estimation: How can the producer agent estimate
similarity between the request and available services? 
(Section 5)
3. PREFERENCE LEARNING
The requests of the consumer and the counter offers of the 
producer are represented as vectors, where each element in the vector
corresponds to the value of a feature. The requests of the consumers
represent individual wine products whereas their preferences are
constraints over service features. For example, a consumer may
have preference for red wine. This means that the consumer is
willing to accept any wine offered by the producers as long as the
color is red. Accordingly, the consumer generates a request where
the color feature is set to red and other features are set to arbitrary
values, e.g. (Medium, Strong, Red).
At the beginning of negotiation, the producer agent does not
know the consumer"s preferences but will need to learn them 
using information obtained from the dialogues between the producer
and the consumer. The preferences denote the relative importance
of the features of the services demanded by the consumer agents.
For instance, the color of the wine may be important so the 
consumer insists on buying the wine whose color is red and rejects all
1302 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Table 1: How DCEA works
Type Sample The most The most
general set specific set
+ (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)}
{{(?-Full), ?, ? },
- (Full,Delicate,Rose) {?, (?-Delicate), ?}, {(Full,Strong,White)}
{?, ?, (?-Rose)}}
{{(?-Full), ?, ?}, {{(Full,Strong,White)},
+ (Medium,Moderate,Red) {?,(?-Delicate), ?}, {(Medium,Moderate,Red)}}
{?, ?, (?-Rose)}}
the offers involving the wine whose color is white or rose. On the
contrary, the winery may not be as important as the color for this
customer, so the consumer may have a tendency to accept wines
from any winery as long as the color is red.
To tackle this problem, we propose to use incremental learning
algorithms [6]. This is necessary since no training data is 
available before the interactions start. We particularly investigate two
approaches. The first one is inductive learning. This technique is
applied to learn the preferences as concepts. We elaborate on 
Candidate Elimination Algorithm (CEA) for Version Space [10]. CEA
is known to perform poorly if the information to be learned is 
disjunctive. Interestingly, most of the time consumer preferences are
disjunctive. Say, we are considering an agent that is buying wine.
The consumer may prefer red wine or rose wine but not white wine.
To use CEA with such preferences, a solid modification is 
necessary. The second approach is decision trees. Decision trees can
learn from examples easily and classify new instances as positive
or negative. A well-known incremental decision tree is ID5R [18].
However, ID5R is known to suffer from high computational 
complexity. For this reason, we instead use the ID3 algorithm [13] and
iteratively build decision trees to simulate incremental learning.
3.1 CEA
CEA [10] is one of the inductive learning algorithms that learns
concepts from observed examples. The algorithm maintains two
sets to model the concept to be learned. The first set is the most
general set G. G contains hypotheses about all the possible values
that the concept may obtain. As the name suggests, it is a 
generalization and contains all possible values unless the values have
been identified not to represent the concept. The second set is the
most specific set S. S contains only hypotheses that are known to
identify the concept that is being learned. At the beginning of the
algorithm, G is initialized to cover all possible concepts while S is
initialized to be empty.
During the interactions, each request of the consumer can be 
considered as a positive example and each counter offer generated by
the producer and rejected by the consumer agent can be thought of
as a negative example. At each interaction between the producer
and the consumer, both G and S are modified. The negative 
samples enforce the specialization of some hypotheses so that G does
not cover any hypothesis accepting the negative samples as 
positive. When a positive sample comes, the most specific set S should
be generalized in order to cover the new training instance. As a 
result, the most general hypotheses and the most special hypotheses
cover all positive training samples but do not cover any negative
ones. Incrementally, G specializes and S generalizes until G and
S are equal to each other. When these sets are equal, the algorithm
converges by means of reaching the target concept.
3.2 Disjunctive CEA
Unfortunately, CEA is primarily targeted for conjunctive 
concepts. On the other hand, we need to learn disjunctive concepts in
the negotiation of a service since consumer may have several 
alternative wishes. There are several studies on learning disjunctive
concepts via Version Space. Some of these approaches use multiple
version space. For instance, Hong et al. maintain several version
spaces by split and merge operation [7]. To be able to learn 
disjunctive concepts, they create new version spaces by examining the
consistency between G and S.
We deal with the problem of not supporting disjunctive concepts
of CEA by extending our hypothesis language to include 
disjunctive hypothesis in addition to the conjunctives and negation. Each
attribute of the hypothesis has two parts: inclusive list, which holds
the list of valid values for that attribute and exclusive list, which is
the list of values which cannot be taken for that feature.
EXAMPLE 1. Assume that the most specific set is {(Light, 
Delicate, Red)} and a positive example, (Light, Delicate, White) comes.
The original CEA will generalize this as (Light, Delicate, ?), 
meaning the color can take any value. However, in fact, we only know
that the color can be red or white. In the DCEA, we generalize it as
{(Light, Delicate, [White, Red] )}. Only when all the values exist
in the list, they will be replaced by ?. In other words, we let the
algorithm generalize more slowly than before.
We modify the CEA algorithm to deal with this change. The
modified algorithm, DCEA, is given as Algorithm 1. Note that
compared to the previous studies of disjunctive versions, our 
approach uses only a single version space rather than multiple version
space. The initialization phase is the same as the original algorithm
(lines 1, 2). If any positive sample comes, we add the sample to the
special set as before (line 4). However, we do not eliminate the 
hypotheses in G that do not cover this sample since G now contains a
disjunction of many hypotheses, some of which will be conflicting
with each other. Removing a specific hypothesis from G will result
in loss of information, since other hypotheses are not guaranteed
to cover it. After some time, some hypotheses in S can be merged
and can construct one hypothesis (lines 6, 7).
When a negative sample comes, we do not change S as before.
We only modify the most general hypotheses not to cover this 
negative sample (lines 11-15). Different from the original CEA, we
try to specialize the G minimally. The algorithm removes the 
hypothesis covering the negative sample (line 13). Then, we generate
new hypotheses as the number of all possible attributes by using
the removed hypothesis.
For each attribute in the negative sample, we add one of them
at each time to the exclusive list of the removed hypothesis. Thus,
all possible hypotheses that do not cover the negative sample are
generated (line 14). Note that, exclusive list contains the values that
the attribute cannot take. For example, consider the color attribute.
If a hypothesis includes red in its exclusive list and ? in its inclusive
list, this means that color may take any value except red.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303
Algorithm 1 Disjunctive Candidate Elimination Algorithm
1: G ←the set of maximally general hypotheses in H
2: S ←the set of maximally specific hypotheses in H
3: For each training example, d
4: if d is a positive example then
5: Add d to S
6: if s in S can be combined with d to make one element then
7: Combine s and d into sd {sd is the rule covers s and d}
8: end if
9: end if
10: if d is a negative example then
11: For each hypothesis g in G does cover d
12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn)
13: - Remove g from G
14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn),
g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn)
15: - Remove from G any hypothesis that is less general than
another hypothesis in G
16: end if
EXAMPLE 2. Table 1 illustrates the first three interactions and
the workings of DCEA. The most general set and the most specific
set show the contents of G and S after the sample comes in. After
the first positive sample, S is generalized to also cover the instance.
The second sample is negative. Thus, we replace (?, ?, ?) by three
disjunctive hypotheses; each hypothesis being minimally 
specialized. In this process, at each time one attribute value of negative
sample is applied to the hypothesis in the general set. The third
sample is positive and generalizes S even more.
Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from
the general set while having a positive sample such as (Full, Strong,
White). This stems from the possibility of using this rule in the 
generation of other hypotheses. For instance, if the example continues
with a negative sample (Full, Strong, Red), we can specialize the
previous rule such as {(?-Full), ?, (?-Red)}. By Algorithm 1, we
do not miss any information.
3.3 ID3
ID3 [13] is an algorithm that constructs decision trees in a 
topdown fashion from the observed examples represented in a vector
with attribute-value pairs. Applying this algorithm to our system
with the intention of learning the consumer"s preferences is 
appropriate since this algorithm also supports learning disjunctive 
concepts in addition to conjunctive concepts.
The ID3 algorithm is used in the learning process with the 
purpose of classification of offers. There are two classes: positive and
negative. Positive means that the service description will possibly
be accepted by the consumer agent whereas the negative implies
that it will potentially be rejected by the consumer. Consumer"s 
requests are considered as positive training examples and all rejected
counter-offers are thought as negative ones.
The decision tree has two types of nodes: leaf node in which the
class labels of the instances are held and non-leaf nodes in which
test attributes are held. The test attribute in a non-leaf node is one of
the attributes making up the service description. For instance, body,
flavor, color and so on are potential test attributes for wine service.
When we want to find whether the given service description is 
acceptable, we start searching from the root node by examining the
value of test attributes until reaching a leaf node.
The problem with this algorithm is that it is not an 
incremental algorithm, which means all the training examples should exist
before learning. To overcome this problem, the system keeps 
consumer"s requests throughout the negotiation interaction as positive
examples and all counter-offers rejected by the consumer as 
negative examples. After each coming request, the decision tree is
rebuilt. Without doubt, there is a drawback of reconstruction such
as additional process load. However, in practice we have evaluated
ID3 to be fast and the reconstruction cost to be negligible.
4. SERVICE OFFERING
After learning the consumer"s preferences, the producer needs to
make a counter offer that is compatible with the consumer"s 
preferences.
4.1 Service Offering via CEA and DCEA
To generate the best offer, the producer agent uses its service
ontology and the CEA algorithm. The service offering mechanism
is the same for both the original CEA and DCEA, but as explained
before their methods for updating G and S are different.
When producer receives a request from the consumer, the 
learning set of the producer is trained with this request as a positive
sample. The learning components, the most specific set S and the
most general set G are actively used in offering service. The most
general set, G is used by the producer in order to avoid offering the
services, which will be rejected by the consumer agent. In other
words, it filters the service set from the undesired services, since
G contains hypotheses that are consistent with the requests of the
consumer. The most specific set, S is used in order to find best 
offer, which is similar to the consumer"s preferences. Since the most
specific set S holds the previous requests and the current request,
estimating similarity between this set and every service in the 
service list is very convenient to find the best offer from the service
list.
When the consumer starts the interaction with the producer agent,
producer agent loads all related services to the service list object.
This list constitutes the provider"s inventory of services. Upon 
receiving a request, if the producer can offer an exactly matching
service, then it does so. For example, for a wine this corresponds
to selling a wine that matches the specified features of the 
consumer"s request identically. When the producer cannot offer the
service as requested, it tries to find the service that is most similar
to the services that have been requested by the consumer during the
negotiation. To do this, the producer has to compute the similarity
between the services it can offer and the services that have been
requested (in S).
We compute the similarities in various ways as will be explained
in Section 5. After the similarity of the available services with the
current S is calculated, there may be more than one service with
the maximum similarity. The producer agent can break the tie in a
number of ways. Here, we have associated a rating value with each
service and the producer prefers the higher rated service to others.
4.2 Service Offering via ID3
If the producer learns the consumer"s preferences with ID3, a
similar mechanism is applied with two differences. First, since ID3
does not maintain G, the list of unaccepted services that are 
classified as negative are removed from the service list. Second, the
similarities of possible services are not measured with respect to S,
but instead to all previously made requests.
4.3 Alternative Service Offering Mechanisms
In addition to these three service offering mechanisms (Service
Offering with CEA, Service Offering with DCEA, and Service 
Offering with ID3), we include two other mechanisms..
1304 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
• Random Service Offering (RO): The producer generates a
counter offer randomly from the available service list, 
without considering the consumer"s preferences.
• Service Offering considering only the current request (SCR):
The producer selects a counter offer according to the 
similarity of the consumer"s current request but does not consider
previous requests.
5. SIMILARITY ESTIMATION
Similarity can be estimated with a similarity metric that takes
two entries and returns how similar they are. There are several 
similarity metrics used in case based reasoning system such as weighted
sum of Euclidean distance, Hamming distance and so on [12].
The similarity metric affects the performance of the system while
deciding which service is the closest to the consumer"s request. We
first analyze some existing metrics and then propose a new 
semantic similarity metric named RP Similarity.
5.1 Tversky"s Similarity Metric
Tversky"s similarity metric compares two vectors in terms of
the number of exactly matching features [17]. In Equation (1),
common represents the number of matched attributes whereas
different represents the number of the different attributes. Our
current assumption is that α and β is equal to each other.
SMpq =
α(common)
α(common) + β(different)
(1)
Here, when two features are compared, we assign zero for 
dissimilarity and one for similarity by omitting the semantic closeness
among the feature values.
Tversky"s similarity metric is designed to compare two feature
vectors. In our system, whereas the list of services that can be 
offered by the producer are each a feature vector, the most specific
set S is not a feature vector. S consists of hypotheses of feature
vectors. Therefore, we estimate the similarity of each hypothesis
inside the most specific set S and then take the average of the 
similarities.
EXAMPLE 3. Assume that S contains the following two 
hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}}.
Take service s as (Light, Strong, Rose). Then the similarity of the
first one is equal to 1/3 and the second one is equal to 2/3 in 
accordance with Equation (1). Normally, we take the average of it
and obtain (1/3 + 2/3)/2, equally 1/2. However, the first 
hypothesis involves the effect of two requests and the second hypothesis
involves only one request. As a result, we expect the effect of the
first hypothesis to be greater than that of the second. Therefore,
we calculate the average similarity by considering the number of
samples that hypotheses cover.
Let ch denote the number of samples that hypothesis h covers
and (SM(h,service)) denote the similarity of hypothesis h with the
given service. We compute the similarity of each hypothesis with
the given service and weight them with the number of samples they
cover. We find the similarity by dividing the weighted sum of the
similarities of all hypotheses in S with the service by the number
of all samples that are covered in S.
AV G−SM(service,S) =
|S|
|h| (ch ∗ SM(h,service))
|S|
|h| ch
(2)
Figure 2: Sample taxonomy for similarity estimation
EXAMPLE 4. For the above example, the similarity of (Light,
Strong, Rose) with the specific set is (2 ∗ 1/3 + 2/3)/3, equally
4/9. The possible number of samples that a hypothesis covers can
be estimated with multiplying cardinalities of each attribute. For
example, the cardinality of the first attribute is two and the others
is equal to one for the given hypothesis such as {Light, Moderate,
(Red, White)}. When we multiply them, we obtain two (2 ∗ 1 ∗ 1 =
2).
5.2 Lin"s Similarity Metric
A taxonomy can be used while estimating semantic similarity 
between two concepts. Estimating semantic similarity in a Is-A 
taxonomy can be done by calculating the distance between the nodes
related to the compared concepts. The links among the nodes can
be considered as distances. Then, the length of the path between the
nodes indicates how closely similar the concepts are. An 
alternative estimation to use information content in estimation of semantic
similarity rather than edge counting method, was proposed by Lin
[8]. The equation (3) [8] shows Lin"s similarity where c1 and c2
are the compared concepts and c0 is the most specific concept that
subsumes both of them. Besides, P(C) represents the probability
of an arbitrary selected object belongs to concept C.
Similarity(c1, c2) =
2 × log P(c0)
log P(c1) + log P(c2)
(3)
5.3 Wu & Palmer"s Similarity Metric
Different from Lin, Wu and Palmer use the distance between the
nodes in IS-A taxonomy [20]. The semantic similarity is 
represented with Equation (4) [20]. Here, the similarity between c1 and
c2 is estimated and c0 is the most specific concept subsuming these
classes. N1 is the number of edges between c1 and c0. N2 is the
number of edges between c2 and c0. N0 is the number of IS-A
links of c0 from the root of the taxonomy.
SimW u&P almer(c1, c2) =
2 × N0
N1 + N2 + 2 × N0
(4)
5.4 RP Semantic Metric
We propose to estimate the relative distance in a taxonomy 
between two concepts using the following intuitions. We use Figure
2 to illustrate these intuitions.
• Parent versus grandparent: Parent of a node is more 
similar to the node than grandparents of that. Generalization of
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1305
a concept reasonably results in going further away that 
concept. The more general concepts are, the less similar they
are. For example, AnyWineColor is parent of ReddishColor
and ReddishColor is parent of Red. Then, we expect the 
similarity between ReddishColor and Red to be higher than that
of the similarity between AnyWineColor and Red.
• Parent versus sibling: A node would have higher similarity
to its parent than to its sibling. For instance, Red and Rose
are children of ReddishColor. In this case, we expect the
similarity between Red and ReddishColor to be higher than
that of Red and Rose.
• Sibling versus grandparent: A node is more similar to it"s
sibling then to its grandparent. To illustrate, AnyWineColor
is grandparent of Red, and Red and Rose are siblings. 
Therefore, we possibly anticipate that Red and Rose are more 
similar than AnyWineColor and Red.
As a taxonomy is represented in a tree, that tree can be traversed
from the first concept being compared through the second concept.
At starting node related to the first concept, the similarity value is
constant and equal to one. This value is diminished by a constant
at each node being visited over the path that will reach to the node
including the second concept. The shorter the path between the
concepts, the higher the similarity between nodes.
Algorithm 2 Estimate-RP-Similarity(c1,c2)
Require: The constants should be m > n > m2
where m, n ∈
R[0, 1]
1: Similarity ← 1
2: if c1 is equal to c2 then
3: Return Similarity
4: end if
5: commonParent ← findCommonParent(c1, c2)
{commonParent is the most specific concept that covers
both c1 and c2}
6: N1 ← findDistance(commonParent, c1)
7: N2 ← findDistance(commonParent, c2) {N1 & N2 are
the number of links between the concept and parent concept}
8: if (commonParent == c1) or (commonParent == c2) then
9: Similarity ← Similarity ∗ m(N1+N2)
10: else
11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2)
12: end if
13: Return Similarity
Relative distance between nodes c1 and c2 is estimated in the
following way. Starting from c1, the tree is traversed to reach c2.
At each hop, the similarity decreases since the concepts are getting
farther away from each other. However, based on our intuitions,
not all hops decrease the similarity equally.
Let m represent the factor for hopping from a child to a parent
and n represent the factor for hopping from a sibling to another
sibling. Since hopping from a node to its grandparent counts as
two parent hops, the discount factor of moving from a node to its
grandparent is m2
. According to the above intuitions, our constants
should be in the form m > n > m2
where the value of m and n
should be between zero and one. Algorithm 2 shows the distance
calculation.
According to the algorithm, firstly the similarity is initialized
with the value of one (line 1). If the concepts are equal to each other
then, similarity will be one (lines 2-4). Otherwise, we compute the
common parent of the two nodes and the distance of each concept
to the common parent without considering the sibling (lines 5-7).
If one of the concepts is equal to the common parent, then there
is no sibling relation between the concepts. For each level, we
multiply the similarity by m and do not consider the sibling factor
in the similarity estimation. As a result, we decrease the similarity
at each level with the rate of m (line9). Otherwise, there has to be
a sibling relation. This means that we have to consider the effect of
n when measuring similarity. Recall that we have counted N1+N2
edges between the concepts. Since there is a sibling relation, two of
these edges constitute the sibling relation. Hence, when calculating
the effect of the parent relation, we use N1+N2 −2 edges (line 11).
Some similarity estimations related to the taxonomy in Figure 2
are given in Table 2. In this example, m is taken as 2/3 and n is
taken as 4/7.
Table 2: Sample similarity estimation over sample taxonomy
Similarity(ReddishColor, Rose) = 1 ∗ (2/3) = 0.6666667
Similarity(Red, Rose) = 1 ∗ (4/7) = 0.5714286
Similarity(AnyW ineColor,Rose) = 1 ∗ (2/3)2
= 0.44444445
Similarity(W hite,Rose) = 1 ∗ (2/3) ∗ (4/7) = 0.3809524
For all semantic similarity metrics in our architecture, the 
taxonomy for features is held in the shared ontology. In order to evaluate
the similarity of feature vector, we firstly estimate the similarity for
feature one by one and take the average sum of these similarities.
Then the result is equal to the average semantic similarity of the
entire feature vector.
6. DEVELOPED SYSTEM
We have implemented our architecture in Java. To ease testing
of the system, the consumer agent has a user interface that allows
us to enter various requests. The producer agent is fully automated
and the learning and service offering operations work as explained
before. In this section, we explain the implementation details of the
developed system.
We use OWL [11] as our ontology language and JENA as our
ontology reasoner. The shared ontology is the modified version of
the Wine Ontology [19]. It includes the description of wine as a
concept and different types of wine. All participants of the 
negotiation use this ontology for understanding each other. According
to the ontology, seven properties make up the wine concept. The
consumer agent and the producer agent obtain the possible values
for the these properties by querying the ontology. Thus, all 
possible values for the components of the wine concept such as color,
body, sugar and so on can be reached by both agents. Also a 
variety of wine types are described in this ontology such as Burgundy,
Chardonnay, CheninBlanc and so on. Intuitively, any wine type 
described in the ontology also represents a wine concept. This allows
us to consider instances of Chardonnay wine as instances of Wine
class.
In addition to wine description, the hierarchical information of
some features can be inferred from the ontology. For instance,
we can represent the information Europe Continent covers 
Western Country. Western Country covers French Region, which covers
some territories such as Loire, Bordeaux and so on. This 
hierarchical information is used in estimation of semantic similarity. In this
part, some reasoning can be made such as if a concept X covers Y
and Y covers Z, then concept X covers Z. For example, Europe
Continent covers Bordeaux.
1306 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
For some features such as body, flavor and sugar, there is no
hierarchical information, but their values are semantically leveled.
When that is the case, we give the reasonable similarity values for
these features. For example, the body can be light, medium, or
strong. In this case, we assume that light is 0.66 similar to medium
but only 0.33 to strong.
WineStock Ontology is the producer"s inventory and describes
a product class as WineProduct. This class is necessary for the
producer to record the wines that it sells. Ontology involves the 
individuals of this class. The individuals represent available services
that the producer owns. We have prepared two separate WineStock
ontologies for testing. In the first ontology, there are 19 available
wine products and in the second ontology, there are 50 products.
7. PERFORMANCE EVALUATION
We evaluate the performance of the proposed systems in respect
to learning technique they used, DCEA and ID3, by comparing
them with the CEA, RO (for random offering), and SCR (offering
based on current request only).
We apply a variety of scenarios on this dataset in order to see the
performance differences. Each test scenario contains a list of 
preferences for the user and number of matches from the product list.
Table 3 shows these preferences and availability of those products
in the inventory for first five scenarios. Note that these preferences
are internal to the consumer and the producer tries to learn these
during negotiation.
Table 3: Availability of wines in different test scenarios
ID Preference of consumer Availability (out of 19)
1 Dry wine 15
2 Red and dry wine 8
3 Red, dry and moderate wine 4
4 Red and strong wine 2
5 Red or rose, and strong 3
7.1 Comparison of Learning Algorithms
In comparison of learning algorithms, we use the five scenarios
in Table 3. Here, first we use Tversky"s similarity measure. With
these test cases, we are interested in finding the number of 
iterations that are required for the producer to generate an acceptable
offer for the consumer. Since the performance also depends on the
initial request, we repeat our experiments with different initial 
requests. Consequently, for each case, we run the algorithms five
times with several variations of the initial requests. In each 
experiment, we count the number of iterations that were needed to reach
an agreement. We take the average of these numbers in order to
evaluate these systems fairly. As is customary, we test each 
algorithm with the same initial requests.
Table 4 compares the approaches using different learning 
algorithm. When the large parts of inventory is compatible with the
customer"s preferences as in the first test case, the performance of
all techniques are nearly same (e.g., Scenario 1). As the number of
compatible services drops, RO performs poorly as expected. The
second worst method is SCR since it only considers the customer"s
most recent request and does not learn from previous requests.
CEA gives the best results when it can generate an answer but 
cannot handle the cases containing disjunctive preferences, such as the
one in Scenario 5. ID3 and DCEA achieve the best results. Their
performance is comparable and they can handle all cases including
Scenario 5.
Table 4: Comparison of learning algorithms in terms of average
number of interactions
Run DCEA SCR RO CEA ID3
Scenario 1: 1.2 1.4 1.2 1.2 1.2
Scenario 2: 1.4 1.4 2.6 1.4 1.4
Scenario 3: 1.4 1.8 4.4 1.4 1.4
Scenario 4: 2.2 2.8 9.6 1.8 2
Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8
Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56
7.2 Comparison of Similarity Metrics
To compare the similarity metrics that were explained in 
Section 5, we fix the learning algorithm to DCEA. In addition to the
scenarios shown in Table 3, we add following five new scenarios
considering the hierarchical information.
• The customer wants to buy wine whose winery is located in
California and whose grape is a type of white grape. 
Moreover, the winery of the wine should not be expensive. There
are only four products meeting these conditions.
• The customer wants to buy wine whose color is red or rose
and grape type is red grape. In addition, the location of wine
should be in Europe. The sweetness degree is wished to be
dry or off dry. The flavor should be delicate or moderate
where the body should be medium or light. Furthermore, the
winery of the wine should be an expensive winery. There are
two products meeting all these requirements.
• The customer wants to buy moderate rose wine, which is 
located around French Region. The category of winery should
be Moderate Winery. There is only one product meeting
these requirements.
• The customer wants to buy expensive red wine, which is 
located around California Region or cheap white wine, which
is located in around Texas Region. There are five available
products.
• The customer wants to buy delicate white wine whose 
producer in the category of Expensive Winery. There are two
available products.
The first seven scenarios are tested with the first dataset that 
contains a total of 19 services and the last three scenarios are tested
with the second dataset that contains 50 services.
Table 5 gives the performance evaluation in terms of the number
of interactions needed to reach a consensus. Tversky"s metric gives
the worst results since it does not consider the semantic similarity.
Lin"s performance are better than Tversky but worse than others.
Wu Palmer"s metric and RP similarity measure nearly give the same
performance and better than others. When the results are examined,
considering semantic closeness increases the performance.
8. DISCUSSION
We review the recent literature in comparison to our work. Tama
et al. [16] propose a new approach based on ontology for 
negotiation. According to their approach, the negotiation protocols used
in e-commerce can be modeled as ontologies. Thus, the agents can
perform negotiation protocol by using this shared ontology without
the need of being hard coded of negotiation protocol details. While
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1307
Table 5: Comparison of similarity metrics in terms of number
of interactions
Run Tversky Lin Wu Palmer RP
Scenario 1: 1.2 1.2 1 1
Scenario 2: 1.4 1.4 1.6 1.6
Scenario 3: 1.4 1.8 2 2
Scenario 4: 2.2 1 1.2 1.2
Scenario 5: 2 1.6 1.6 1.6
Scenario 6: 5 3.8 2.4 2.6
Scenario 7: 3.2 1.2 1 1
Scenario 8: 5.6 2 2 2.2
Scenario 9: 2.6 2.2 2.2 2.6
Scenario 10: 4.4 2 2 1.8
Average of all cases: 2.9 1.82 1.7 1.76
Tama et al. model the negotiation protocol using ontologies, we
have instead modeled the service to be negotiated. Further, we have
built a system with which negotiation preferences can be learned.
Sadri et al. study negotiation in the context of resource 
allocation [14]. Agents have limited resources and need to require 
missing resources from other agents. A mechanism which is based on
dialogue sequences among agents is proposed as a solution. The
mechanism relies on observe-think-action agent cycle. These 
dialogues include offering resources, resource exchanges and offering
alternative resource. Each agent in the system plans its actions to
reach a goal state. Contrary to our approach, Sadri et al."s study is
not concerned with learning preferences of each other.
Brzostowski and Kowalczyk propose an approach to select an 
appropriate negotiation partner by investigating previous multi-attribute
negotiations [1]. For achieving this, they use case-based reasoning.
Their approach is probabilistic since the behavior of the partners
can change at each iteration. In our approach, we are interested in
negotiation the content of the service. After the consumer and 
producer agree on the service, price-oriented negotiation mechanisms
can be used to agree on the price.
Fatima et al. study the factors that affect the negotiation such as
preferences, deadline, price and so on, since the agent who 
develops a strategy against its opponent should consider all of them [5].
In their approach, the goal of the seller agent is to sell the service
for the highest possible price whereas the goal of the buyer agent
is to buy the good with the lowest possible price. Time interval 
affects these agents differently. Compared to Fatima et al. our focus
is different. While they study the effect of time on negotiation, our
focus is on learning preferences for a successful negotiation.
Faratin et al. propose a multi-issue negotiation mechanism, where
the service variables for the negotiation such as price, quality of
the service, and so on are considered traded-offs against each other
(i.e., higher price for earlier delivery) [4]. They generate a 
heuristic model for trade-offs including fuzzy similarity estimation and a
hill-climbing exploration for possibly acceptable offers. Although
we address a similar problem, we learn the preferences of the 
customer by the help of inductive learning and generate counter-offers
in accordance with these learned preferences. Faratin et al. only
use the last offer made by the consumer in calculating the 
similarity for choosing counter offer. Unlike them, we also take into 
account the previous requests of the consumer. In their experiments,
Faratin et al. assume that the weights for service variables are fixed
a priori. On the contrary, we learn these preferences over time.
In our future work, we plan to integrate ontology reasoning into
the learning algorithm so that hierarchical information can be learned
from subsumption hierarchy of relations. Further, by using 
relationships among features, the producer can discover new 
knowledge from the existing knowledge. These are interesting directions
that we will pursue in our future work.
9. REFERENCES
[1] J. Brzostowski and R. Kowalczyk. On possibilistic
case-based reasoning for selecting partners for
multi-attribute agent negotiation. In Proceedings of the 4th
Intl. Joint Conference on Autonomous Agents and
MultiAgent Systems (AAMAS), pages 273-278, 2005.
[2] L. Busch and I. Horstman. A comment on issue-by-issue
negotiations. Games and Economic Behavior, 19:144-148,
1997.
[3] J. K. Debenham. Managing e-market negotiation in context
with a multiagent system. In Proceedings 21st International
Conference on Knowledge Based Systems and Applied
Artificial Intelligence, ES"2002:, 2002.
[4] P. Faratin, C. Sierra, and N. R. Jennings. Using similarity
criteria to make issue trade-offs in automated negotiations.
Artificial Intelligence, 142:205-237, 2002.
[5] S. Fatima, M. Wooldridge, and N. Jennings. Optimal agents
for multi-issue negotiation. In Proceeding of the 2nd Intl.
Joint Conference on Autonomous Agents and MultiAgent
Systems (AAMAS), pages 129-136, 2003.
[6] C. Giraud-Carrier. A note on the utility of incremental
learning. AI Communications, 13(4):215-223, 2000.
[7] T.-P. Hong and S.-S. Tseng. Splitting and merging version
spaces to learn disjunctive concepts. IEEE Transactions on
Knowledge and Data Engineering, 11(5):813-815, 1999.
[8] D. Lin. An information-theoretic definition of similarity. In
Proc. 15th International Conf. on Machine Learning, pages
296-304. Morgan Kaufmann, San Francisco, CA, 1998.
[9] P. Maes, R. H. Guttman, and A. G. Moukas. Agents that buy
and sell. Communications of the ACM, 42(3):81-91, 1999.
[10] T. M. Mitchell. Machine Learning. McGraw Hill, NY, 1997.
[11] OWL. OWL: Web ontology language guide, 2003.
http://www.w3.org/TR/2003/CR-owl-guide-20030818/.
[12] S. K. Pal and S. C. K. Shiu. Foundations of Soft Case-Based
Reasoning. John Wiley & Sons, New Jersey, 2004.
[13] J. R. Quinlan. Induction of decision trees. Machine Learning,
1(1):81-106, 1986.
[14] F. Sadri, F. Toni, and P. Torroni. Dialogues for negotiation:
Agent varieties and dialogue sequences. In ATAL 2001,
Revised Papers, volume 2333 of LNAI, pages 405-421.
Springer-Verlag, 2002.
[15] M. P. Singh. Value-oriented electronic commerce. IEEE
Internet Computing, 3(3):6-7, 1999.
[16] V. Tamma, S. Phelps, I. Dickinson, and M. Wooldridge.
Ontologies for supporting negotiation in e-commerce.
Engineering Applications of Artificial Intelligence,
18:223-236, 2005.
[17] A. Tversky. Features of similarity. Psychological Review,
84(4):327-352, 1977.
[18] P. E. Utgoff. Incremental induction of decision trees.
Machine Learning, 4:161-186, 1989.
[19] Wine, 2003. 
http://www.w3.org/TR/2003/CR-owl-guide20030818/wine.rdf.
[20] Z. Wu and M. Palmer. Verb semantics and lexical selection.
In 32nd. Annual Meeting of the Association for
Computational Linguistics, pages 133 -138, 1994.
1308 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Letting loose a SPIDER on a network of POMDPs:
Generating quality guaranteed policies
Pradeep Varakantham, Janusz Marecki, Yuichi Yabu∗
, Milind Tambe, Makoto Yokoo∗
University of Southern California, Los Angeles, CA 90089, {varakant, marecki, tambe}@usc.edu
∗ Dept. of Intelligent Systems, Kyushu University, Fukuoka, 812-8581 Japan, yokoo@is.kyushu-u.ac.jp
ABSTRACT
Distributed Partially Observable Markov Decision Problems 
(Distributed POMDPs) are a popular approach for modeling multi-agent
systems acting in uncertain domains. Given the significant 
complexity of solving distributed POMDPs, particularly as we scale
up the numbers of agents, one popular approach has focused on
approximate solutions. Though this approach is efficient, the 
algorithms within this approach do not provide any guarantees on 
solution quality. A second less popular approach focuses on global
optimality, but typical results are available only for two agents,
and also at considerable computational cost. This paper overcomes
the limitations of both these approaches by providing SPIDER, a
novel combination of three key features for policy generation in 
distributed POMDPs: (i) it exploits agent interaction structure given
a network of agents (i.e. allowing easier scale-up to larger number
of agents); (ii) it uses a combination of heuristics to speedup policy
search; and (iii) it allows quality guaranteed approximations, 
allowing a systematic tradeoff of solution quality for time. 
Experimental results show orders of magnitude improvement in performance
when compared with previous global optimal algorithms.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
IntelligenceMulti-agent Systems
General Terms
Algorithms, Theory
1. INTRODUCTION
Distributed Partially Observable Markov Decision Problems 
(Distributed POMDPs) are emerging as a popular approach for 
modeling sequential decision making in teams operating under 
uncertainty [9, 4, 1, 2, 13]. The uncertainty arises on account of 
nondeterminism in the outcomes of actions and because the world state
may only be partially (or incorrectly) observable. Unfortunately, as
shown by Bernstein et al. [3], the problem of finding the optimal
joint policy for general distributed POMDPs is NEXP-Complete.
Researchers have attempted two different types of approaches
towards solving these models. The first category consists of highly
efficient approximate techniques, that may not reach globally 
optimal solutions [2, 9, 11]. The key problem with these techniques
has been their inability to provide any guarantees on the quality
of the solution. In contrast, the second less popular category of 
approaches has focused on a global optimal result [13, 5, 10]. Though
these approaches obtain optimal solutions, they typically consider
only two agents. Furthermore, they fail to exploit structure in the
interactions of the agents and hence are severely hampered with
respect to scalability when considering more than two agents.
To address these problems with the existing approaches, we 
propose approximate techniques that provide guarantees on the 
quality of the solution while focussing on a network of more than two
agents. We first propose the basic SPIDER (Search for Policies
In Distributed EnviRonments) algorithm. There are two key novel
features in SPIDER: (i) it is a branch and bound heuristic search
technique that uses a MDP-based heuristic function to search for an
optimal joint policy; (ii) it exploits network structure of agents by
organizing agents into a Depth First Search (DFS) pseudo tree and
takes advantage of the independence in the different branches of the
DFS tree. We then provide three enhancements to improve the 
efficiency of the basic SPIDER algorithm while providing guarantees
on the quality of the solution. The first enhancement uses 
abstractions for speedup, but does not sacrifice solution quality. In 
particular, it initially performs branch and bound search on abstract 
policies and then extends to complete policies. The second 
enhancement obtains speedups by sacrificing solution quality, but within
an input parameter that provides the tolerable expected value 
difference from the optimal solution. The third enhancement is again
based on bounding the search for efficiency, however with a 
tolerance parameter that is provided as a percentage of optimal.
We experimented with the sensor network domain presented in
Nair et al. [10], a domain representative of an important class of
problems with networks of agents working in uncertain 
environments. In our experiments, we illustrate that SPIDER dominates
an existing global optimal approach called GOA [10], the only
known global optimal algorithm with demonstrated experimental
results for more than two agents. Furthermore, we demonstrate
that abstraction improves the performance of SPIDER significantly
(while providing optimal solutions). We finally demonstrate a key
feature of SPIDER: by utilizing the approximation enhancements
it enables principled tradeoffs in run-time versus solution quality.
822
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
2. DOMAIN: DISTRIBUTED SENSOR NETS
Distributed sensor networks are a large, important class of 
domains that motivate our work. This paper focuses on a set of target
tracking problems that arise in certain types of sensor networks [6]
first introduced in [10]. Figure 1 shows a specific problem instance
within this type consisting of three sensors. Here, each sensor node
can scan in one of four directions: North, South, East or West (see
Figure 1). To track a target and obtain associated reward, two 
sensors with overlapping scanning areas must coordinate by scanning
the same area simultaneously. In Figure 1, to track a target in 
Loc11, sensor1 needs to scan ‘East" and sensor2 needs to scan ‘West"
simultaneously. Thus, sensors have to act in a coordinated fashion.
We assume that there are two independent targets and that each
target"s movement is uncertain and unaffected by the sensor agents.
Based on the area it is scanning, each sensor receives observations
that can have false positives and false negatives. The sensors" 
observations and transitions are independent of each other"s actions
e.g.the observations that sensor1 receives are independent of 
sensor2"s actions. Each agent incurs a cost for scanning whether the
target is present or not, but no cost if it turns off. Given the sensors"
observational uncertainty, the targets" uncertain transitions and the
distributed nature of the sensor nodes, these sensor nets provide a
useful domains for applying distributed POMDP models.
Figure 1: A 3-chain sensor configuration
3. BACKGROUND
3.1 Model: Network Distributed POMDP
The ND-POMDP model was introduced in [10], motivated by
domains such as the sensor networks introduced in Section 2. It is
defined as the tuple S, A, P, Ω, O, R, b , where S = ×1≤i≤nSi ×
Su is the set of world states. Si refers to the set of local states of
agent i and Su is the set of unaffectable states. Unaffectable state
refers to that part of the world state that cannot be affected by the
agents" actions, e.g. environmental factors like target locations that
no agent can control. A = ×1≤i≤nAi is the set of joint actions,
where Ai is the set of action for agent i.
ND-POMDP assumes transition independence, where the 
transition function is defined as P(s, a, s ) = Pu(su, su) · 1≤i≤n
Pi(si, su, ai, si), where a = a1, . . . , an is the joint action 
performed in state s = s1, . . . , sn, su and s = s1, . . . , sn, su is
the resulting state.
Ω = ×1≤i≤nΩi is the set of joint observations where Ωi is
the set of observations for agents i. Observational independence
is assumed in ND-POMDPs i.e., the joint observation function is
defined as O(s, a, ω) = 1≤i≤n Oi(si, su, ai, ωi), where s =
s1, . . . , sn, su is the world state that results from the agents 
performing a = a1, . . . , an in the previous state, and
ω = ω1, . . . , ωn ∈ Ω is the observation received in state s. This
implies that each agent"s observation depends only on the 
unaffectable state, its local action and on its resulting local state.
The reward function, R, is defined as
R(s, a) = l Rl(sl1, . . . , slr, su, al1, . . . , alr ), where each l
could refer to any sub-group of agents and r = |l|. Based on
the reward function, an interaction hypergraph is constructed. A
hyper-link, l, exists between a subset of agents for all Rl that 
comprise R. The interaction hypergraph is defined as G = (Ag, E),
where the agents, Ag, are the vertices and E = {l|l ⊆ Ag ∧
Rl is a component of R} are the edges.
The initial belief state (distribution over the initial state), b, is
defined as b(s) = bu(su) · 1≤i≤n bi(si), where bu and bi refer
to the distribution over initial unaffectable state and agent i"s initial
belief state, respectively. The goal in ND-POMDP is to compute
the joint policy π = π1, . . . , πn that maximizes team"s expected
reward over a finite horizon T starting from the belief state b.
An ND-POMDP is similar to an n-ary Distributed Constraint
Optimization Problem (DCOP)[8, 12] where the variable at each
node represents the policy selected by an individual agent, πi with
the domain of the variable being the set of all local policies, Πi.
The reward component Rl where |l| = 1 can be thought of as a
local constraint while the reward component Rl where l > 1 
corresponds to a non-local constraint in the constraint graph.
3.2 Algorithm: Global Optimal Algorithm (GOA)
In previous work, GOA has been defined as a global optimal
algorithm for ND-POMDPs [10]. We will use GOA in our 
experimental comparisons, since GOA is a state-of-the-art global optimal
algorithm, and in fact the only one with experimental results 
available for networks of more than two agents. GOA borrows from a
global optimal DCOP algorithm called DPOP[12]. GOA"s message
passing follows that of DPOP. The first phase is the UTIL 
propagation, where the utility messages, in this case values of policies,
are passed up from the leaves to the root. Value for a policy at an
agent is defined as the sum of best response values from its 
children and the joint policy reward associated with the parent policy.
Thus, given a policy for a parent node, GOA requires an agent to
iterate through all its policies, finding the best response policy and
returning the value to the parent - while at the parent node, to find
the best policy, an agent requires its children to return their best
responses to each of its policies. This UTIL propagation process
is repeated at each level in the tree, until the root exhausts all its
policies. In the second phase of VALUE propagation, where the
optimal policies are passed down from the root till the leaves.
GOA takes advantage of the local interactions in the interaction
graph, by pruning out unnecessary joint policy evaluations 
(associated with nodes not connected directly in the tree). Since the
interaction graph captures all the reward interactions among agents
and as this algorithm iterates through all the relevant joint policy
evaluations, this algorithm yields a globally optimal solution.
4. SPIDER
As mentioned in Section 3.1, an ND-POMDP can be treated as a
DCOP, where the goal is to compute a joint policy that maximizes
the overall joint reward. The brute-force technique for computing
an optimal policy would be to examine the expected values for all
possible joint policies. The key idea in SPIDER is to avoid 
computation of expected values for the entire space of joint policies, by
utilizing upper bounds on the expected values of policies and the
interaction structure of the agents.
Akin to some of the algorithms for DCOP [8, 12], SPIDER has a
pre-processing step that constructs a DFS tree corresponding to the
given interaction structure. Note that these DFS trees are pseudo
trees [12] that allow links between ancestors and children. We 
employ the Maximum Constrained Node (MCN) heuristic used in the
DCOP algorithm, ADOPT [8], however other heuristics (such as
MLSP heuristic from [7]) can also be employed. MCN heuristic
tries to place agents with more number of constraints at the top of
the tree. This tree governs how the search for the optimal joint 
polThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823
icy proceeds in SPIDER. The algorithms presented in this paper are
easily extendable to hyper-trees, however for expository purposes,
we assume binary trees.
SPIDER is an algorithm for centralized planning and distributed
execution in distributed POMDPs. In this paper, we employ the
following notation to denote policies and expected values:
Ancestors(i) ⇒ agents from i to the root (not including i).
Tree(i) ⇒ agents in the sub-tree (not including i) for which i is
the root.
πroot+
⇒ joint policy of all agents.
πi+
⇒ joint policy of all agents in Tree(i) ∪ i.
πi−
⇒ joint policy of agents that are in Ancestors(i).
πi ⇒ policy of the ith agent.
ˆv[πi, πi−
] ⇒ upper bound on the expected value for πi+
given πi
and policies of ancestor agents i.e. πi−
.
ˆvj[πi, πi−
] ⇒ upper bound on the expected value for πi+
from the
jth child.
v[πi, πi−
] ⇒ expected value for πi given policies of ancestor agents,
πi−
.
v[πi+
, πi−
] ⇒ expected value for πi+
given policies of ancestor
agents, πi−
.
vj[πi+
, πi−
] ⇒ expected value for πi+
from the jth child.
Figure 2: Execution of SPIDER, an example
4.1 Outline of SPIDER
SPIDER is based on the idea of branch and bound search, where
the nodes in the search tree represent partial/complete joint 
policies. Figure 2 shows an example search tree for the SPIDER 
algorithm, using an example of the three agent chain. Before SPIDER
begins its search we create a DFS tree (i.e. pseudo tree) from the
three agent chain, with the middle agent as the root of this tree.
SPIDER exploits the structure of this DFS tree while engaging in
its search. Note that in our example figure, each agent is assigned
a policy with T=2. Thus, each rounded rectange (search tree node)
indicates a partial/complete joint policy, a rectangle indicates an
agent and the ovals internal to an agent show its policy. Heuristic
or actual expected value for a joint policy is indicated in the top
right corner of the rounded rectangle. If the number is italicized
and underlined, it implies that the actual expected value of the joint
policy is provided.
SPIDER begins with no policy assigned to any of the agents
(shown in the level 1 of the search tree). Level 2 of the search tree
indicates that the joint policies are sorted based on upper bounds
computed for root agent"s policies. Level 3 shows one SPIDER
search node with a complete joint policy (a policy assigned to each
of the agents). The expected value for this joint policy is used to
prune out the nodes in level 2 (the ones with upper bounds < 234)
When creating policies for each non-leaf agent i, SPIDER 
potentially performs two steps:
1. Obtaining upper bounds and sorting: In this step, agent i
computes upper bounds on the expected values, ˆv[πi, πi−
] of the
joint policies πi+
corresponding to each of its policy πi and fixed
ancestor policies. An MDP based heuristic is used to compute these
upper bounds on the expected values. Detailed description about
this MDP heuristic is provided in Section 4.2. All policies of agent
i, Πi are then sorted based on these upper bounds (also referred to
as heuristic values henceforth) in descending order. Exploration of
these policies (in step 2 below) are performed in this descending
order. As indicated in the level 2 of the search tree (of Figure 2), all
the joint policies are sorted based on the heuristic values, indicated
in the top right corner of each joint policy. The intuition behind
sorting and then exploring policies in descending order of upper
bounds, is that the policies with higher upper bounds could yield
joint policies with higher expected values.
2. Exploration and Pruning: Exploration implies computing
the best response joint policy πi+,∗
corresponding to fixed 
ancestor policies of agent i, πi−
. This is performed by iterating through
all policies of agent i i.e. Πi and summing two quantities for each
policy: (i) the best response for all of i"s children (obtained by 
performing steps 1 and 2 at each of the child nodes); (ii) the expected
value obtained by i for fixed policies of ancestors. Thus, 
exploration of a policy πi yields actual expected value of a joint policy,
πi+
represented as v[πi+
, πi−
]. The policy with the highest 
expected value is the best response policy.
Pruning refers to avoiding exploring all policies (or computing
expected values) at agent i by using the current best expected value,
vmax
[πi+
, πi−
]. Henceforth, this vmax
[πi+
, πi−
] will be referred
to as threshold. A policy, πi need not be explored if the upper
bound for that policy, ˆv[πi, πi−
] is less than the threshold. This is
because the expected value for the best joint policy attainable for
that policy will be less than the threshold.
On the other hand, when considering a leaf agent, SPIDER 
computes the best response policy (and consequently its expected value)
corresponding to fixed policies of its ancestors, πi−
. This is 
accomplished by computing expected values for each of the policies 
(corresponding to fixed policies of ancestors) and selecting the highest
expected value policy. In Figure 2, SPIDER assigns best response
policies to leaf agents at level 3. The policy for the left leaf agent is
to perform action East at each time step in the policy, while the
policy for the right leaf agent is to perform Off at each time step.
These best response policies from the leaf agents yield an actual
expected value of 234 for the complete joint policy.
Algorithm 1 provides the pseudo code for SPIDER. This 
algorithm outputs the best joint policy, πi+,∗
(with an expected value
greater than threshold) for the agents in Tree(i). Lines 3-8 
compute the best response policy of a leaf agent i, while lines 9-23
computes the best response joint policy for agents in Tree(i). This
best response computation for a non-leaf agent i includes: (a) 
Sorting of policies (in descending order) based on heuristic values on
line 11; (b) Computing best response policies at each of the 
children for fixed policies of agent i in lines 16-20; and (c) Maintaining
824 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Algorithm 1 SPIDER(i, πi−
, threshold)
1: πi+,∗ ← null
2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi)
3: if IS-LEAF(i) then
4: for all πi ∈ Πi do
5: v[πi, πi−] ← JOINT-REWARD (πi, πi−)
6: if v[πi, πi−] > threshold then
7: πi+,∗ ← πi
8: threshold ← v[πi, πi−]
9: else
10: children ← CHILDREN (i)
11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−)
12: for all πi ∈ ˆΠi do
13: ˜πi+ ← πi
14: if ˆv[πi, πi−] < threshold then
15: Go to line 12
16: for all j ∈ children do
17: jThres ← threshold − v[πi, πi−]−
Σk∈children,k=j ˆvk[πi, πi−]
18: πj+,∗ ← SPIDER(j, πi πi−, jThres)
19: ˜πi+ ← ˜πi+ πj+,∗
20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−]
21: if v[˜πi+, πi−] > threshold then
22: threshold ← v[˜πi+, πi−]
23: πi+,∗ ← ˜πi+
24: return πi+,∗
Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi−
)
1: children ← CHILDREN (i)
2: ˆΠi ← null /* Stores the sorted list */
3: for all πi ∈ Πi do
4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−)
5: for all j ∈ children do
6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−)
7: ˆv[πi, πi−]
+
← ˆvj[πi, πi−]
8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi)
9: return ˆΠi
best expected value, joint policy in lines 21-23.
Algorithm 2 provides the pseudo code for sorting policies based
on the upper bounds on the expected values of joint policies. 
Expected value for an agent i consists of two parts: value obtained
from ancestors and value obtained from its children. Line 4 
computes the expected value obtained from ancestors of the agent 
(using JOINT-REWARD function), while lines 5-7 compute the 
heuristic value from the children. The sum of these two parts yields an
upper bound on the expected value for agent i, and line 8 of the
algorithm sorts the policies based on these upper bounds.
4.2 MDP based heuristic function
The heuristic function quickly provides an upper bound on the
expected value obtainable from the agents in Tree(i). The 
subtree of agents is a distributed POMDP in itself and the idea here
is to construct a centralized MDP corresponding to the (sub-tree)
distributed POMDP and obtain the expected value of the optimal
policy for this centralized MDP. To reiterate this in terms of the
agents in DFS tree interaction structure, we assume full 
observability for the agents in Tree(i) and for fixed policies of the agents in
{Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+
, πi−
] .
We use the following notation for presenting the equations for
computing upper bounds/heuristic values (for agents i and k):
Let Ei−
denote the set of links between agents in {Ancestors(i)∪
i} and Tree(i), Ei+
denote the set of links between agents in
Tree(i). Also, if l ∈ Ei−
, then l1 is the agent in {Ancestors(i)∪
i} and l2 is the agent in Tree(i), that l connects together. We first
compact the standard notation:
ot
k =Ok(st+1
k , st+1
u , πk(ωt
k), ωt+1
k ) (1)
pt
k =Pk(st
k, st
u, πk(ωt
k), st+1
k ) · ot
k
pt
u =P(st
u, st+1
u )
st
l = st
l1
, st
l2
, st
u ; ωt
l = ωt
l1
, ωt
l2
rt
l =Rl(st
l , πl1
(ωt
l1
), πl2
(ωt
l2
))
vt
l =V t
πl
(st
l , st
u, ωt
l1
, ωt
l2
)
Depending on the location of agent k in the agent tree we have the following
cases:
IF k ∈ {Ancestors(i) ∪ i}, ˆpt
k = pt
k, (2)
IF k ∈ Tree(i), ˆpt
k = Pk(st
k, st
u, πk(ωt
k), st+1
k )
IF l ∈ Ei−
, ˆrt
l = max
{al2
}
Rl(st
l , πl1
(ωt
l1
), al2
)
IF l ∈ Ei+
, ˆrt
l = max
{al1
,al2
}
Rl(st
l , al1
, al2
)
The value function for an agent i executing the joint policy πi+
at
time η − 1 is provided by the equation:
V η−1
πi+ (sη−1
, ωη−1
) = l∈Ei− vη−1
l + l∈Ei+ vη−1
l (3)
where vη−1
l = rη−1
l + ω
η
l
,sη pη−1
l1
pη−1
l2
pη−1
u vη
l
Algorithm 3 UPPER-BOUND (i, j, πj−
)
1: val ← 0
2: for all l ∈ Ej− ∪ Ej+ do
3: if l ∈ Ej− then πl1
← φ
4: for all s0
l do
5: val
+
← startBel[s0
l ]· UPPER-BOUND-TIME
(i, s0
l , j, πl1
, )
6: return val
Algorithm 4 UPPER-BOUND-TIME (i, st
l , j, πl1 , ωt
l1
)
1: maxV al ← −∞
2: for all al1
, al2
do
3: if l ∈ Ei− and l ∈ Ej− then al1
← πl1
(ωt
l1
)
4: val ← GET-REWARD(st
l , al1
, al2
)
5: if t < πi.horizon − 1 then
6: for all st+1
l , ωt+1
l1
do
7: futV al←pt
u ˆpt
l1
ˆpt
l2
8: futV al
∗
← UPPER-BOUND-TIME(st+1
l , j, πl1
, ωt
l1
ωt+1
l1
)
9: val
+
← futV al
10: if val > maxV al then maxV al ← val
11: return maxV al
Upper bound on the expected value for a link is computed by
modifying the equation 3 to reflect the full observability 
assumption. This involves removing the observational probability term
for agents in Tree(i) and maximizing the future value ˆvη
l over the
actions of those agents (in Tree(i)). Thus, the equation for the
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825
computation of the upper bound on a link l, is as follows:
IF l ∈ Ei−
, ˆvη−1
l =ˆrη−1
l + max
al2
ω
η
l1
,s
η
l
ˆpη−1
l1
ˆpη−1
l2
pη−1
u ˆvη
l
IF l ∈ Ei+
, ˆvη−1
l =ˆrη−1
l + max
al1
,al2
s
η
l
ˆpη−1
l1
ˆpη−1
l2
pη−1
u ˆvη
l
Algorithm 3 and Algorithm 4 provide the algorithm for computing
upper bound for child j of agent i, using the equations descirbed
above. While Algorithm 4 computes the upper bound on a link
given the starting state, Algorithm 3 sums the upper bound values
computed over each of the links in Ei−
∪ Ei+
.
4.3 Abstraction
Algorithm 5 SPIDER-ABS(i, πi−
, threshold)
1: πi+,∗ ← null
2: Πi ← GET-POLICIES (<>, 1)
3: if IS-LEAF(i) then
4: for all πi ∈ Πi do
5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−)
6: absHeuristic
∗
← (timeHorizon − πi.horizon)
7: if πi.horizon = timeHorizon and πi.absNodes = 0 then
8: v[πi, πi−] ← JOINT-REWARD (πi, πi−)
9: if v[πi, πi−] > threshold then
10: πi+,∗ ← πi; threshold ← v[πi, πi−]
11: else if v[πi, πi−] + absHeuristic > threshold then
12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1)
13: Πi
+
← INSERT-SORTED-POLICIES ( ˆΠi)
14: REMOVE(πi)
15: else
16: children ← CHILDREN (i)
17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−)
18: for all πi ∈ Πi do
19: ˜πi+ ← πi
20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−)
21: absHeuristic
∗
← (timeHorizon − πi.horizon)
22: if πi.horizon = timeHorizon and πi.absNodes = 0 then
23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then
24: Go to line 19
25: for all j ∈ children do
26: jThres ← threshold − v[πi, πi−]−
Σk∈children,k=j ˆvk[πi, πi−]
27: πj+,∗ ← SPIDER(j, πi πi−, jThres)
28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−]
29: if v[˜πi+, πi−] > threshold then
30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+
31: else if ˆv[πi+, πi−] + absHeuristic > threshold then
32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1)
33: Πi
+
← INSERT-SORTED-POLICIES (ˆΠi)
34: REMOVE(πi)
35: return πi+,∗
In SPIDER, the exploration/pruning phase can only begin after
the heuristic (or upper bound) computation and sorting for the 
policies has ended. We provide an approach to possibly circumvent the
exploration of a group of policies based on heuristic computation
for one abstract policy, thus leading to an improvement in runtime
performance (without loss in solution quality). The important steps
in this technique are defining the abstract policy and how heuristic
values are computated for the abstract policies. In this paper, we
propose two types of abstraction:
1. Horizon Based Abstraction (HBA): Here, the abstract policy is
defined as a shorter horizon policy. It represents a group of longer
horizon policies that have the same actions as the abstract policy
for times less than or equal to the horizon of the abstract policy.
In Figure 3(a), a T=1 abstract policy that performs East action,
represents a group of T=2 policies, that perform East in the first
time step.
For HBA, there are two parts to heuristic computation:
(a) Computing the upper bound for the horizon of the abstract 
policy. This is same as the heuristic computation defined by the 
GETHEURISTIC() algorithm for SPIDER, however with a shorter time
horizon (horizon of the abstract policy).
(b) Computing the maximum possible reward that can be 
accumulated in one time step (using GET-ABS-HEURISTIC()) and 
multiplying it by the number of time steps to time horizon. This 
maximum possible reward (for one time step) is obtained by iterating
through all the actions of all the agents in Tree(i) and computing
the maximum joint reward for any joint action.
Sum of (a) and (b) is the heuristic value for a HBA abstract policy.
2. Node Based Abstraction (NBA): Here an abstract policy is 
obtained by not associating actions to certain nodes of the policy tree.
Unlike in HBA, this implies multiple levels of abstraction. This is
illustrated in Figure 3(b), where there are T=2 policies that do not
have an action for observation ‘TP". These incomplete T=2 
policies are abstractions for T=2 complete policies. Increased levels of
abstraction leads to faster computation of a complete joint policy,
πroot+
and also to shorter heuristic computation and exploration,
pruning phases. For NBA, the heuristic computation is similar to
that of a normal policy, except in cases where there is no action
associated with policy nodes. In such cases, the immediate reward
is taken as Rmax (maximum reward for any action).
We combine both the abstraction techniques mentioned above
into one technique, SPIDER-ABS. Algorithm 5 provides the 
algorithm for this abstraction technique. For computing optimal joint
policy with SPIDER-ABS, a non-leaf agent i initially examines all
abstract T=1 policies (line 2) and sorts them based on abstract 
policy heuristic computations (line 17). The abstraction horizon is
gradually increased and these abstract policies are then explored
in descending order of heuristic values and ones that have heuristic
values less than the threshold are pruned (lines 23-24). Exploration
in SPIDER-ABS has the same definition as in SPIDER if the policy
being explored has a horizon of policy computation which is equal
to the actual time horizon and if all the nodes of the policy have an
action associated with them (lines 25-30). However, if those 
conditions are not met, then it is substituted by a group of policies that it
represents (using EXTEND-POLICY () function) (lines 31-32).
EXTEND-POLICY() function is also responsible for 
initializing the horizon and absNodes of a policy. absNodes 
represents the number of nodes at the last level in the policy tree,
that do not have an action assigned to them. If πi.absNodes =
|Ωi|πi.horizon−1
(i.e. total number of policy nodes possible at
πi.horizon) , then πi.absNodes is set to zero and πi.horizon is
increased by 1. Otherwise, πi.absNodes is increased by 1. Thus,
this function combines both HBA and NBA by using the policy
variables, horizon and absNodes. Before substituting the abstract
policy with a group of policies, those policies are sorted based on
heuristic values (line 33). Similar type of abstraction based best
response computation is adopted at leaf agents (lines 3-14).
4.4 Value ApproXimation (VAX)
In this section, we present an approximate enhancement to 
SPIDER called VAX. The input to this technique is an approximation
parameter , which determines the difference from the optimal 
solution quality. This approximation parameter is used at each agent
for pruning out joint policies. The pruning mechanism in SPIDER
and SPIDER-Abs dictates that a joint policy be pruned only if the
threshold is exactly greater than the heuristic value. However, the
826 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction)
idea in this technique is to prune out joint a policy if the following
condition is satisfied: threshold + > ˆv[πi
, πi−
]. Apart from the
pruning condition, VAX is the same as SPIDER/SPIDER-ABS.
In the example of Figure 2, if the heuristic value for the second
joint policy (or second search tree node) in level 2 were 238 instead
of 232, then that policy could not be be pruned using SPIDER or
SPIDER-Abs. However, in VAX with an approximation parameter
of 5, the joint policy in consideration would also be pruned. This is
because the threshold (234) at that juncture plus the approximation
parameter (5), i.e. 239 would have been greater than the heuristic
value for that joint policy (238). It can be noted from the example
(just discussed) that this kind of pruning can lead to fewer 
explorations and hence lead to an improvement in the overall run-time
performance. However, this can entail a sacrifice in the quality of
the solution because this technique can prune out a candidate 
optimal solution. A bound on the error introduced by this approximate
algorithm as a function of , is provided by Proposition 3.
4.5 Percentage ApproXimation (PAX)
In this section, we present the second approximation 
enhancement over SPIDER called PAX. Input to this technique is a 
parameter, δ that represents the minimum percentage of the optimal
solution quality that is desired. Output of this technique is a policy
with an expected value that is at least δ% of the optimal solution
quality. A policy is pruned if the following condition is satisfied:
threshold > δ
100
ˆv[πi
, πi−
]. Like in VAX, the only difference 
between PAX and SPIDER/SPIDER-ABS is this pruning condition.
Again in Figure 2, if the heuristic value for the second search
tree node in level 2 were 238 instead of 232, then PAX with an 
input parameter of 98% would be able to prune that search tree node
(since 98
100
∗238 < 234). This type of pruning leads to fewer 
explorations and hence an improvement in run-time performance, while
potentially leading to a loss in quality of the solution. Proposition 4
provides the bound on quality loss.
4.6 Theoretical Results
PROPOSITION 1. Heuristic provided using the centralized MDP
heuristic is admissible.
Proof. For the value provided by the heuristic to be admissible,
it should be an over estimate of the expected value for a joint policy.
Thus, we need to show that: For l ∈ Ei+
∪ Ei−
: ˆvt
l ≥ vt
l (refer to
notation in Section 4.2)
We use mathematical induction on t to prove this.
Base case: t = T − 1. Irrespective of whether l ∈ Ei−
or l ∈
Ei+
, ˆrt
l is computed by maximizing over all actions of the agents
in Tree(i), while rt
l is computed for fixed policies of the same
agents. Hence, ˆrt
l ≥ rt
l and also ˆvt
l ≥ vt
l .
Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.
We now have to prove that the proposition holds for t = η − 1.
We show the proof for l ∈ Ei−
and similar reasoning can be
adopted to prove for l ∈ Ei+
. The heuristic value function for
l ∈ Ei−
is provided by the following equation:
ˆvη−1
l =ˆrη−1
l + max
al2
ω
η
l1
,s
η
l
ˆpη−1
l1
ˆpη−1
l2
pη−1
u ˆvη
l
Rewriting the RHS and using Eqn 2 (in Section 4.2)
=ˆrη−1
l + max
al2
ω
η
l1
,s
η
l
pη−1
u pη−1
l1
ˆpη−1
l2
ˆvη
l
=ˆrη−1
l +
ω
η
l1
,s
η
l
pη−1
u pη−1
l1
max
al2
ˆpη−1
l2
ˆvη
l
Since maxal2
ˆpη−1
l2
ˆvη
l ≥ ωl2
oη−1
l2
ˆpη−1
l2
ˆvη
l and pη−1
l2
= oη−1
l2
ˆpη−1
l2
≥ˆrη−1
l +
ω
η
l1
,s
η
l
pη−1
u pη−1
l1
ωl2
pη−1
l2
ˆvη
l
Since ˆvη
l ≥ vη
l (from the assumption)
≥ˆrη−1
l +
ω
η
l1
,s
η
l
pη−1
u pη−1
l1
ωl2
pη−1
l2
vη
l
Since ˆrη−1
l ≥ rη−1
l (by definition)
≥rη−1
l +
ω
η
l1
,s
η
l
pη−1
u pη−1
l1
ωl2
pη−1
l2
vη
l
=rη−1
l +
(ω
η
l
,s
η
l
)
pη−1
u pη−1
l1
pη−1
l2
vη
l = vη−1
l
Thus proved.
PROPOSITION 2. SPIDER provides an optimal solution.
Proof. SPIDER examines all possible joint policies given the
interaction structure of the agents. The only exception being when
a joint policy is pruned based on the heuristic value. Thus, as long
as a candidate optimal policy is not pruned, SPIDER will return an
optimal policy. As proved in Proposition 1, the expected value for
a joint policy is always an upper bound. Hence when a joint policy
is pruned, it cannot be an optimal solution.
PROPOSITION 3. Error bound on the solution quality for VAX
(implemented over SPIDER-ABS) with an approximation 
parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827
Proof. We prove this proposition using mathematical induction
on the depth of the DFS tree.
Base case: depth = 1 (i.e. one node). Best response is 
computed by iterating through all policies, Πk. A policy,πk is pruned
if ˆv[πk, πk−
] < threshold + . Thus the best response policy
computed by VAX would be at most away from the optimal best
response. Hence the proposition holds for the base case.
Assumption: Proposition holds for d, where 1 ≤ depth ≤ d.
We now have to prove that the proposition holds for d + 1.
Without loss of generality, lets assume that the root node of this
tree has k children. Each of this children is of depth ≤ d, and hence
from the assumption, the error introduced in kth child is ρk , where
ρk is the number of leaf nodes in kth child of the root. Therefore,
ρ = k ρk, where ρ is the number of leaf nodes in the tree.
In SPIDER-ABS, threshold at the root agent, thresspider =
k v[πk+
, πk−
]. However, with VAX the threshold at the root
agent will be (in the worst case), threshvax = k v[πk+
, πk−
]−
k ρk . Hence, with VAX a joint policy is pruned at the root
agent if ˆv[πroot, πroot−
] < threshvax + ⇒ ˆv[πroot, πroot−
] <
threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤
threshspider − ρ . Hence proved.
PROPOSITION 4. For PAX (implemented over SPIDER-ABS) with
an input parameter of δ, the solution quality is at least δ
100
v[πroot+,∗
],
where v[πroot+,∗
] denotes the optimal solution quality.
Proof. We prove this proposition using mathematical induction
on the depth of the DFS tree.
Base case: depth = 1 (i.e. one node). Best response is 
computed by iterating through all policies, Πk. A policy,πk is pruned
if δ
100
ˆv[πk, πk−
] < threshold. Thus the best response policy
computed by PAX would be at least δ
100
times the optimal best
response. Hence the proposition holds for the base case.
Assumption: Proposition holds for d, where 1 ≤ depth ≤ d.
We now have to prove that the proposition holds for d + 1.
Without loss of generality, lets assume that the root node of
this tree has k children. Each of this children is of depth ≤ d,
and hence from the assumption, the solution quality in the kth
child is at least δ
100
v[πk+,∗
, πk−
] for PAX. With SPIDER-ABS,
a joint policy is pruned at the root agent if ˆv[πroot, πroot−
] <
k v[πk+,∗
, πk−
]. However with PAX, a joint policy is pruned if
δ
100
ˆv[πroot, πroot−
] < k
δ
100
v[πk+,∗
, πk−
] ⇒ ˆv[πroot, πroot−
] <
k v[πk+,∗
, πk−
]. Since the pruning condition at the root agent in
PAX is the same as the one in SPIDER-ABS, there is no error 
introduced at the root agent and all the error is introduced in the 
children. Thus, overall solution quality is at least δ
100
of the optimal
solution. Hence proved.
5. EXPERIMENTAL RESULTS
All our experiments were conducted on the sensor network 
domain from Section 2. The five network configurations employed
are shown in Figure 4. Algorithms that we experimented with
are GOA, SPIDER, SPIDER-ABS, PAX and VAX. We compare
against GOA because it is the only global optimal algorithm that
considers more than two agents. We performed two sets of 
experiments: (i) firstly, we compared the run-time performance of the
above algorithms and (ii) secondly, we experimented with PAX and
VAX to study the tradeoff between run-time and solution quality.
Experiments were terminated after 10000 seconds1
.
Figure 5(a) provides run-time comparisons between the optimal
algorithms GOA, SPIDER, SPIDER-Abs and the approximate 
algorithms, PAX ( of 30) and VAX(δ of 80). X-axis denotes the
1
Machine specs for all experiments: Intel Xeon 3.6 GHZ processor,
2GB RAM
sensor network configuration used, while Y-axis indicates the 
runtime (on a log-scale). The time horizon of policy computation was
3. For each configuration (3-chain, 4-chain, 4-star and 5-star), there
are five bars indicating the time taken by GOA, SPIDER, 
SPIDERAbs, PAX and VAX. GOA did not terminate within the time limit
for 4-star and 5-star configurations. SPIDER-Abs dominated the
SPIDER and GOA for all the configurations. For instance, in the 
3chain configuration, SPIDER-ABS provides 230-fold speedup over
GOA and 2-fold speedup over SPIDER and for the 4-chain 
configuration it provides 58-fold speedup over GOA and 2-fold speedup
over SPIDER. The two approximation approaches, VAX and PAX
provided further improvement in performance over SPIDER-Abs.
For instance, in the 5-star configuration VAX provides a 15-fold
speedup and PAX provides a 8-fold speedup over SPIDER-Abs.
Figures 5(b) provides a comparison of the solution quality 
obtained using the different algorithms for the problems tested in 
Figure 5(a). X-axis denotes the sensor network configuration while
Y-axis indicates the solution quality. Since GOA, SPIDER, and
SPIDER-Abs are all global optimal algorithms, the solution 
quality is the same for all those algorithms. For 5-P configuration,
the global optimal algorithms did not terminate within the limit of
10000 seconds, so the bar for optimal quality indicates an upper
bound on the optimal solution quality. With both the 
approximations, we obtained a solution quality that was close to the optimal
solution quality. In 3-chain and 4-star configurations, it is 
remarkable that both PAX and VAX obtained almost the same actual 
quality as the global optimal algorithms, despite the approximation 
parameter and δ. For other configurations as well, the loss in quality
was less than 20% of the optimal solution quality.
Figure 5(c) provides the time to solution with PAX (for 
varying epsilons). X-axis denotes the approximation parameter, δ 
(percentage to optimal) used, while Y-axis denotes the time taken to
compute the solution (on a log-scale). The time horizon for all
the configurations was 4. As δ was decreased from 70 to 30, the
time to solution decreased drastically. For instance, in the 3-chain
case there was a total speedup of 170-fold when the δ was changed
from 70 to 30. Interestingly, even with a low δ of 30%, the actual
solution quality remained equal to the one obtained at 70%.
Figure 5(d) provides the time to solution for all the 
configurations with VAX (for varying epsilons). X-axis denotes the 
approximation parameter, used, while Y-axis denotes the time taken to
compute the solution (on a log-scale). The time horizon for all the
configurations was 4. As was increased, the time to solution 
decreased drastically. For instance, in the 4-star case there was a total
speedup of 73-fold when the was changed from 60 to 140. Again,
the actual solution quality did not change with varying epsilon.
Figure 4: Sensor network configurations
828 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 5: Comparison of GOA, SPIDER, SPIDER-Abs and VAX for T = 3 on (a) Runtime and (b) Solution quality; (c) Time to solution for PAX
with varying percentage to optimal for T=4 (d) Time to solution for VAX with varying epsilon for T=4
6. SUMMARY AND RELATED WORK
This paper presents four algorithms SPIDER, SPIDER-ABS, PAX
and VAX that provide a novel combination of features for 
policy search in distributed POMDPs: (i) exploiting agent interaction
structure given a network of agents (i.e. easier scale-up to larger
number of agents); (ii) using branch and bound search with an MDP
based heuristic function; (iii) utilizing abstraction to improve 
runtime performance without sacrificing solution quality; (iv) 
providing a priori percentage bounds on quality of solutions using PAX;
and (v) providing expected value bounds on the quality of solutions
using VAX. These features allow for systematic tradeoff of solution
quality for run-time in networks of agents operating under 
uncertainty. Experimental results show orders of magnitude 
improvement in performance over previous global optimal algorithms.
Researchers have typically employed two types of techniques
for solving distributed POMDPs. The first set of techniques 
compute global optimal solutions. Hansen et al. [5] present an 
algorithm based on dynamic programming and iterated elimination of
dominant policies, that provides optimal solutions for distributed
POMDPs. Szer et al. [13] provide an optimal heuristic search
method for solving Decentralized POMDPs. This algorithm is based
on the combination of a classical heuristic search algorithm, A∗
and
decentralized control theory. The key differences between SPIDER
and MAA* are: (a) Enhancements to SPIDER (VAX and PAX)
provide for quality guaranteed approximations, while MAA* is a
global optimal algorithm and hence involves significant 
computational complexity; (b) Due to MAA*"s inability to exploit 
interaction structure, it was illustrated only with two agents. However,
SPIDER has been illustrated for networks of agents; and (c) 
SPIDER explores the joint policy one agent at a time, while MAA* 
expands it one time step at a time (simultaneously for all the agents).
The second set of techniques seek approximate policies. 
EmeryMontemerlo et al. [4] approximate POSGs as a series of one-step
Bayesian games using heuristics to approximate future value, 
trading off limited lookahead for computational efficiency, resulting in
locally optimal policies (with respect to the selected heuristic). Nair
et al. [9]"s JESP algorithm uses dynamic programming to reach a
local optimum solution for finite horizon decentralized POMDPs.
Peshkin et al. [11] and Bernstein et al. [2] are examples of policy
search techniques that search for locally optimal policies. Though
all the above techniques improve the efficiency of policy 
computation considerably, they are unable to provide error bounds on the
quality of the solution. This aspect of quality bounds differentiates
SPIDER from all the above techniques.
Acknowledgements. This material is based upon work 
supported by the Defense Advanced Research Projects Agency (DARPA),
through the Department of the Interior, NBC, Acquisition Services
Division under Contract No. NBCHD030010. The views and 
conclusions contained in this document are those of the authors, and
should not be interpreted as representing the official policies, either
expressed or implied, of the Defense Advanced Research Projects
Agency or the U.S. Government.
7. REFERENCES
[1] R. Becker, S. Zilberstein, V. Lesser, and C.V. Goldman.
Solving transition independent decentralized Markov
decision processes. JAIR, 22:423-455, 2004.
[2] D. S. Bernstein, E.A. Hansen, and S. Zilberstein. Bounded
policy iteration for decentralized POMDPs. In IJCAI, 2005.
[3] D. S. Bernstein, S. Zilberstein, and N. Immerman. The
complexity of decentralized control of MDPs. In UAI, 2000.
[4] R. Emery-Montemerlo, G. Gordon, J. Schneider, and
S. Thrun. Approximate solutions for partially observable
stochastic games with common payoffs. In AAMAS, 2004.
[5] E. Hansen, D. Bernstein, and S. Zilberstein. Dynamic
programming for partially observable stochastic games. In
AAAI, 2004.
[6] V. Lesser, C. Ortiz, and M. Tambe. Distributed sensor nets:
A multiagent perspective. Kluwer, 2003.
[7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce, and
P. Varakantham. Taking dcop to the real world : Efficient
complete solutions for distributed event scheduling. In
AAMAS, 2004.
[8] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo. An
asynchronous complete method for distributed constraint
optimization. In AAMAS, 2003.
[9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe, and S. Marsella.
Taming decentralized POMDPs: Towards efficient policy
computation for multiagent settings. In IJCAI, 2003.
[10] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.
Networked distributed POMDPs: A synthesis of distributed
constraint optimization and POMDPs. In AAAI, 2005.
[11] L. Peshkin, N. Meuleau, K.-E. Kim, and L. Kaelbling.
Learning to cooperate via policy search. In UAI, 2000.
[12] A. Petcu and B. Faltings. A scalable method for multiagent
constraint optimization. In IJCAI, 2005.
[13] D. Szer, F. Charpillet, and S. Zilberstein. MAA*: A heuristic
search algorithm for solving decentralized POMDPs. In
IJCAI, 2005.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 829
Operational Semantics of Multiagent Interactions
Juan M. Serrano
University Rey Juan Carlos
C/Tulipan S/N
Madrid, Spain
juanmanuel.serrano@urjc.es
Sergio Saugar
University Rey Juan Carlos
C/Tulipan S/N
Madrid, Spain
sergio.saugar@urjc.es
ABSTRACT
The social stance advocated by institutional frameworks and
most multi-agent system methodologies has resulted in a
wide spectrum of organizational and communicative 
abstractions which have found currency in several programming
frameworks and software platforms. Still, these tools and
frameworks are designed to support a limited range of 
interaction capabilities that constrain developers to a fixed set
of particular, pre-defined abstractions. The main hypothesis
motivating this paper is that the variety of multi-agent 
interaction mechanisms - both, organizational and 
communicative, share a common semantic core. In the realm of software
architectures, the paper proposes a connector-based model
of multi-agent interactions which attempts to identify the
essential structure underlying multi-agent interactions. 
Furthermore, the paper also provides this model with a formal
execution semantics which describes the dynamics of social
interactions. The proposed model is intended as the 
abstract machine of an organizational programming language
which allows programmers to accommodate an open set of
interaction mechanisms.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
Intelligence-multi-agent systems
General Terms
Languages, Theory, Design
1. INTRODUCTION
The suitability of agent-based computing to manage the
complex patterns of interactions naturally occurring in the
development of large scale, open systems, has become one
of its major assets over the last few years [26, 24, 15]. 
Particularly, the organizational or social stance advocated by
institutional frameworks [2] and most multi-agent system
(MAS) methodologies [26, 10], provides an excellent basis to
deal with the complexity and dynamism of the interactions
among system components. This approach has resulted in
a wide spectrum of organizational and communicative 
abstractions, such as institutions, normative positions, power
relationships, organizations, groups, scenes, dialogue games,
communicative actions (CAs), etc., to effectively model the
interaction space of MAS. This wealth of computational 
abstractions has found currency in several programming 
frameworks and software platforms (AMELI [9], MadKit [13], 
INGENIAS toolkit [18], etc.), which leverage multi-agent 
middlewares built upon raw ACL-based interaction mechanism
[14], and minimize the gap between organizational 
metamodels and target implementation languages.
Still, these tools and frameworks are designed to support
a limited range of interaction capabilities that constrain 
developers to a fixed set of particular, pre-defined abstractions.
The main hypothesis motivating this paper is that the 
variety of multi-agent interaction mechanisms - both, 
organizational and communicative, share a common semantic core.
This paper thus focuses on the fundamental building blocks
of multi-agent interactions: those which may be composed,
extended or refined in order to define more complex 
organizational or communicative types of interactions.
Its first goal is to carry out a principled analysis of 
multiagent interactions, departing from general features commonly
ascribed to agent-based computing: autonomy, situatedness
and sociality [26]. To approach this issue, we draw on the
notion of connector, put forward within the field of software
architectures [1, 17]. The outcome of this analysis will be a
connector-based model of multi-agent interactions between
autonomous social and situated components, i.e. agents, 
attempting to identify their essential structure. Furthermore,
the paper also provides this model with a formal 
execution semantics which describes the dynamics of multi-agent
(or social) interactions. Structural Operational Semantics
(SOS)[21], a common technique to specify the operational
semantics of programming languages, is used for this 
purpose.
The paper is structured as follows: first, the major entities
and relationships which constitute the structure of social
interactions are introduced. Next, the dynamics of social
interactions will show how these entities and relationships
evolve. Last, relevant work in the literature is discussed
889
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
with respect to the proposal, limitations are addressed, and
current and future work is described.
2. SOCIAL INTERACTION STRUCTURE
From an architectural point of view, interactions between
software components are embodied in software connectors:
first-class entities defined on the basis of the different roles
played by software components and the protocols that 
regulate their behaviour [1]. The roles of a connector represent
its participants, such as the caller and callee roles of an
RPC connector, or the sender and receiver roles in a 
message passing connector. The attachment operation binds a
component to the role of a given connector.
The analysis of social interactions introduced in this 
section gives rise to a new kind of social connector. It refines the
generic model in several respects, attending to the features
commonly ascribed to agent-based computing:
• According to the autonomy feature, we may 
distinguish a first kind of participant (i.e. role) in a 
social interaction, so-called agents. Basically, agents are
those software components which will be regarded as
autonomous within the scope of the interaction1
.
• A second group of participants, so-called 
environmental resources, may be identified from the situatedness
feature. Unlike agents, resources represent those 
nonautonomous components whose state may be 
externally controlled by other components (agents or 
resources) within the interaction. Moreover, the 
participation of resources in an interaction is not 
mandatory.
• Last, according to the sociality of agents, the 
specification of social connector protocols - the glue linking
agents among themselves and with resources, will rely
on normative concepts such as permissions, obligations
and empowerments [23].
Besides agents, resources and social protocols, two other
kinds of entities are of major relevance in our analysis of 
social interactions: actions, which represent the way in which
agents alter the environmental and social state of the 
interaction; and events, which represent the changes in the
interaction resulting from the performance of actions or the
activity of environmental resources.
In the following, we describe the basic entities involved in
social interactions. Each kind of entity T will be specified as
a record type T l1 : T1, . . . ln : Tn , possibly followed by
a number of invariants, definitions, and the actions affecting
their state. Instances or values v of a record type T will be
represented as v = v1, . . . , vn : T. The type SetT 
represents a collection of values drawn from type T. The type
QueueT represents a queue of values v : T waiting to be 
processed. The value v in the expression [v| ] : Queue[T] 
represents the head of the queue. The type Enum {v1, . . . , vn}
1
Note that we think of the autonomy feature in a relative,
rather than absolute, perspective. Basically, this means that
software components counting as agents in a social 
interaction may behave non-autonomously in other contexts, e.g.
in their interactions through human-user interfaces. This
conceptualization of agenthood resembles the way in which
objects are understood in CORBA: as any kind of software
component (C, Prolog, Cobol, etc.) attached to an ORB.
represents an enumeration type whose values are v1, . . . ,
vn. Given some value v : T, the term vl
refers to the value
of the field l of a record type T. Given some labels l1, l2,
. . . , the expression vl1,l2,...
is syntactic sugar for ((vl1
)l2
) . . ..
The special term nil will be used to represent the absence
of proper value for an optional field, so that vl
= nil will be
true in those cases and false otherwise. The formal model
will be illustrated with several examples drawn from the 
design of a virtual organization to aid in the management of
university courses.
2.1 Social Interactions
Social interactions shall be considered as composite 
connectors [17], structured in terms of a tree of nested 
subinteractions. Let"s consider an interaction representing a
university course (e.g. on data structures). On the one
hand, this interaction is actually a complex one, made up
of lower-level interactions. For instance, within the scope
of the course agents will participate in programming 
assignment groups, lectures, tutoring meetings, examinations and
so on. Assignment groups, in turn, may hold a number of
assignment submissions and test requests interactions. A
test request may also be regarded as a complex interaction,
ultimately decomposed in the atomic, or bottom-level 
interactions represented by communicative actions (e.g. 
request, agree, refuse, . . . ). On the other hand, courses are
run within the scope of a particular degree (e.g. computer
science), a higher-level interaction. Traversing upwards from
a degree to its ancestors, we find its faculty, the university
and, finally, the multi-agent community or agent society.
The community is thus the top-level interaction which 
subsumes any other kind of multi-agent interaction2
.
The organizational and communicative interaction types
identified above clearly differ in many ways. However, we
may identify four major components in all of them: the
participating agents, the resources that agents manipulate,
the protocol regulating the agent activities and the 
subinteraction space. Accordingly, we may specify the type I
of social interactions, ranged over by the meta-variable i, as
follows:
I state : SI, ini : A, mem : Set A, env : Set R,
sub : Set I, prot : P, ch : CH
def. : (1) icontext = i1 ⇔ i ∈ isub
1
inv. : (2) iini = nil ⇔ icontext = nil
act. : setUp, join, create, destroy
where the member and environment fields represent the
agents (A) and local resources (R) participating in the 
interaction; the sub-interaction field, its set of inner interactions;
and the protocol field the rules that govern the interaction
(P). The event channel, to be described in the next 
section, allows the dispatching of local events to external 
interactions. The context of some interaction is defined as its
super-interaction (def. 1), so that the context of the 
toplevel interaction is nil.
The type SI Enum {open, closing, closed} represents
the possible execution states of the interaction. Any 
interaction, but the top-level one, is set up within the context of
another interaction by an initiator agent. The initiator is
2
In the context of this application, a one-to-one mapping
between human users and software components attached to
the community as agents would be a right choice.
890 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
thus a mandatory feature for any interaction different to the
community (inv. 2). The life-cycle of the interaction begins
in the open state. Its sets of agent and resource participants,
initially empty, vary as agents join and leave the interaction,
and as they create and destroy resources from its local 
environment. Eventually, the interaction may come to an end
(according to the protocol"s rules), or be explicitly closed by
some agent, thus prematurely disabling the activity of its
participants. The transient closing state will be described
in the next section.
2.2 Agents
Components attach themselves as agents in social 
interactions with the purpose of achieving something. The purpose
declared by some agent when it joins an interaction shall be
regarded as the institutional goal that it purports to satisfy
within that context3
. The types of agents participating in
a given interaction are primarily identified from their 
purposes. For instance, students are those agents participating
in a course who purport to obtain a certificate in the course"s
subject. Other members of the course include lecturers and
teaching assistants.
The type A of agents, ranged over by meta-variable a, is
defined as follows:
A state : SA, player : A, purp : F, att : Queue ACT ,
ev : Queue E, obl : Set O
def. : (3) acontext = i ⇔ a ∈ imem
(4) a1 ∈ aroles ⇔ aplayer
1 = a
(5) i ∈ apartIn ⇔ a1 ∈ imem ∧ a1 ∈ aroles
act. : see
where the purpose is represented as a well-formed boolean
formula, of a generic type F, which evaluates to true if the
purpose is satisfied and false otherwise. The context of some
agent is defined as the interaction in which it participates
(def. 3).
The type SA Enum {playing, leaving, succ, unsuc}
represents the execution state of the agent. Its life-cycle 
begins in the playing state when its player agent joins the 
interaction, or some software component is attached as an agent
to the multi-agent system (in this latter case, the player
value is nil). The derived roles and partIn features 
represent the roles played by the agent and the contexts in which
these roles are played (def. 4, 5)4
. An agent may play roles
at interactions within or outside the scope of its context. For
instance, students of a course are played by student agents
belonging to the (undergraduate) degree, whereas lecturers
may be played by teachers of a given department and the
assistant role may be played by students of a Ph.D degree
(both, the department and the Ph.D. degrees, are modelled
as sub-interactions of the faculty).
Components will normally attempt to perform different
actions (e.g. to set up sub-interactions) in order to satisfy
their purposes within some interaction. Moreover, 
components need to be aware of the current state of the interaction,
so that they will also be capable of observing certain events
from the interaction. Both, the visibility of the interaction
3
Thus, it may or may not correspond to actual internal
goals or intentions of the component.
4
Free variables in the antecedents/consequents of 
implications shall be understood as universally/existentially 
quantified.
and the attempts of members, are subject to the rules 
governing the interaction. The attempts and events fields of
the agent structure represent the queues of attempts to 
execute some actions (ACT ), and the events (E) received by
the agent which have not been observed yet. An agent may
update its event queue by seeing the state of some entity
of the community. The last field of the structure represents
the obligations (O) of agents, to be described later.
Eventually, the participation of some agent in the 
interaction will be over. This may either happen when certain
conditions are met (specified by the protocol rules), or when
the agent takes the explicit decision of leaving the 
interaction. In either case, the final state of the agent will be
successful if its purpose was satisfied; unsuccessful 
otherwise. The transient leaving state will be described in the
next section.
2.3 Resources
Resources are software components which may represent
different types of non-autonomous informational or 
computational entities. For instance, objectives, topics, 
assignments, grades and exams are different kinds of informational
resources created by lecturers and assistants in the context
of the course interaction. Students may also create programs
to satisfy the requirements of some assignment. Other types
of computational resources put at the disposal of students
by teachers include compilers and interpreters.
The type R of resources, ranged over by meta-variable r,
can be specified by the following record type:
R cr : A, owners : Set A, op : Set OP
def. : (6) rcontext = i ⇔ r ∈ ienv
act. : take, share, give, invoke
Essentially, resources can be regarded as objects deployed
in a social setting. This means that resources are created,
accessed and manipulated by agents in a social interaction
context (def. 6), according to the rules specified by its 
protocol. The mandatory feature creator represents the agent
who created this resource. Moreover, resources may have
owners. The ownership relationship between members and
resources is considered as a normative device aimed at the
simplification of the protocol"s rules that govern the 
interaction of agents and the environment. Members may gain
ownership of some resource by taking it, and grant 
ownership to other agents by giving or sharing their own 
properties. For instance, the ownership of programs may be shared
by several students if the assignment can be performed by
groups of two or more students.
The last operations feature represents the interface of the
resource, consisting of a set of operations. A resource is
structured around several public operations that 
participants may invoke, in accordance to the rules specified by the
interaction"s protocol. The set of operations of a resource
makes up its interface.
2.4 Protocols
The protocol of any interaction is made up of the rules
which govern its overall state and dynamics. The present
specification abstracts away the particular formalism used to
specify these rules, and focuses instead on several 
requirements concerning the structure and interface of protocols.
Accordingly, the type P of protocols, ranged over by 
metaThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 891
variable p, is defined as follows5
:
P emp : A × ACT → Boolean,
perm : A × ACT → Boolean,
obl :→ Set (A × Set O × Set E),
monitor : E → Set A,
finish :→ Boolean,
over : A → Boolean
def. : (7) pcontext = i ⇔ p = iprot
inv. : (8) pfinish() ∧ s ∈ pcontext,sub ⇒ sprot,finish()
(9) pfinish() ∧ a ∈ pcontext,mem ⇒ pover(a)
(10) pover(a) ∧ ai ∈ aroles ⇒ acontext,prot,over
i (ai)
(11) αadd ∪ {a} ⊆ pmonitor( a, α, )
act. : Close, Leave
We demand from protocols four major kinds of functions.
Firstly, protocols shall include rules to identify the 
empowerments and permissions of any agent attempting to alter
the state of the interaction (e.g. its members, the 
environment, etc.) through the execution of some action (e.g. join,
create, etc.). Empowerments shall be regarded as the 
institutional capabilities which some agent possesses in order
to satisfy its purpose. Corresponding rules, encapsulated
by the empowered function field, shall allow to determine
whether some agent is capable to perform a given action
over the interaction. Empowerments may only be exercised
under certain circumstances - that permissions specify. 
Permission rules shall allow to determine whether the attempt
of an empowered agent to perform some particular action
is satisfied or not (cf. permitted field). For instance, the
course"s protocol specifies that the agents empowered to
join the interaction as students are those students of the
degree who have payed the fee established for the course"s
subject, and own the certificates corresponding to its 
prerequisite subjects. Permission rules, in turn, specify that those
students may only join the course in the admission stage.
Hence, even if some student has paid the fee, the attempt
to join the course will fail if the course has not entered the
corresponding stage6
.
Secondly, protocols shall allow to determine the 
obligations of agents towards the interaction. Obligations 
represent a normative device of social enforcement, fully 
compatible with the autonomy of agents, used to bias their 
behaviour in a certain direction. These kinds of rules shall
allow to determine whether some agent must perform an 
action of a given type, as well as if some obligation was fulfilled,
violated or needs to be revoked. The function obligations of
the protocol structure thus identifies the agents whose 
obligation set must be updated. Moreover, it returns for each
agent a collection of events representing the changes in the
obligation set. For instance, the course"s protocol 
establishes that members of departments must join the course as
teachers whenever they are assigned to the course"s subject.
Thirdly, the protocol shall allow to specify monitoring
rules for the different events originating within the 
interaction. Corresponding rules shall establish the set of agents
that must be awared of some event. For instance, this 
func5
The formalization assumes that protocol"s functions 
implicitly recieve as input the interaction being regulated.
6
The hasPaidFee relationship between (degree) students
and subject resources is represented by an additional,
application-dependent field of the agent structure for this
kind of roles. Similarly, the admission stage is an additional
boolean field of the structure for school interactions. The
generic types I, A, R and P are thus extendable.
tionality is exploited by teachers in order to monitor the
enrollment of students to the course.
Last, the protocol shall allow to control the state of the 
interaction as well as the states of its members. Corresponding
rules identify the conditions under which some interaction
will be automatically finished, and whether the participation
of some member agent will be automatically over. Thus, the
function field finish returns true if the regulated interaction
must finish its execution. If so happens, a well-defined set of
protocols must ensure that its sub-interactions and members
are finished as well (inv. 8,9). Similarly, the function over 
returns true if the participation of the specified member must
be over. Well-formed protocols must ensure the consistency
between these functions across playing roles (inv. 10)7
. For
instance, the course"s protocol establishes that the 
participation of students is over when they gain ownership of the
course"s certificate or the chances to get it are exhausted.
It also establishes that the course must be finished when
the admission stage has passed and all the students finished
their participation.
3. SOCIAL INTERACTION DYNAMICS
The dynamics of the multi-agent community is influenced
by the external actions executed by software components
and the protocols governing their interactions. This section
focuses on the dynamics resulting from a particular kind of
external action: the attempt of some component, attached
to the community as an agent, to execute a given (internal)
action. The description of other external actions concerning
agents (e.g. observe the events from its event queue, enter
or exit from the community) and resources (e.g. a timer
resource may signal the pass of time) will be skipped.
The processing of some attempt may give rise to changes
in the scope of the target interaction, such as the 
instantiation of new participants (agents or resources) or the 
setting up of new sub-interactions. These resulting events
may cause further changes in the state of other interactions
(the target one included), namely, in its execution state as
well as in the execution state, obligations and visibility of
their members. This section will also describe the way in
which these events are processed. The resulting dynamics
described bellow allows for actions and events corresponding
to different agents and interactions to be processed 
simultaneously. Due to lack of space, we only include some of the
operational rules that formalise the execution semantics.
3.1 Attempt processing
An attempt is defined by the structure AT T perf :
A, act : ACT , where the performer represents the agent
in charge of executing the specified action. This action is
intended to alter the state of some target interaction 
(possibly, the performer"s context itself), and notify a collection
of addressees of the changes resulting from a successful 
execution. Accordingly, the type ACT of actions, ranged over
by meta-variable α, is specified as follows:
ACT state : SACT , target : I, add : Set A
def. : (12) αperf = a ⇔ α ∈ aatt
7
The close and leave actions update the finish and over 
function fields as explained in the next section. Additional 
actions, such as permit, forbid, empower, etc., to update other
protocol"s fields are yet to be identified in future work.
892 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
where: the performer is formally defined as the agent who
stores the action in its queue of attempts, and the state field
represents the current phase of processing. This process
goes through four major phases, as specified by the 
enumeration type SACT Enum {emp, perm, exec} : 
empowerment checking, permission checking and action execution,
described in the sequel.
3.1.1 Empowerment checking
The post-condition of an attempt consists of inserting the
action in the queue of attempts of the specified performer.
As rule 1 specifies8
, this will only be possible if the 
performer is empowered to execute that action according to the
rules that govern the state of the target interaction. If this
condition is not met, the attempt will simply be ignored.
Moreover, the performer agent must be in the playing state
(this pre-condition is also required for any rule concerning
the processing of attempts). If these pre-conditions are 
satisfied the rule is fired and the processing of the action 
continues in the permission checking stage. For instance, when
the software component attached as a student in a degree 
attempts to join as a student the course in which some subject
is teached, the empowerment rules of the course interaction
are checked. If the (degree) student has passed the course"s
prerequisite subjects the join action will be inserted in its
queue of attempts and considered for execution.
αtarget,prot,emp(a, α)
a = playing, , , qACT , ,
a,α :AT T
−→ playing, , , qACT , ,
(1)
W here : (α )state
= perm
(qACT ) = insert(α , qACT )
3.1.2 Permissions checking
The processing of the action resumes when the possible
preceding actions in the performer"s queue of attempts are
fully processed and removed from the queue. Moreover,
there should be no pending events to be processed in the
interaction, for these events may cause the member or the
interaction to be finished (as will be shortly explained in the
next sub-section). If these conditions are met the 
permissions to execute the given action (and notify the specified
addressees) are checked (e.g. it will be checked whether the
student paid the fee for the course"s subject). If the protocol
of the target interaction grants permission, the processing
of the attempt moves to the action execution stage (rule 2).
Otherwise, the action is discharged and removed from the
queue. Unlike unempowered attempts, a forbidden one will
cause an event to be generated and transfered to the event
channel for further processing.
αstate = perm ∧ acontext,ch,in,ev = ∅ ∧ αtarget,prot,perm(a, α)
a = playing, , , [α| ], , −→ playing, , , [α | ], ,
(2)
W here : (α )state
= exec
8
Labels of record instances are omitted to allow for more
compact specifications. Moreover, note that record updates
in where clauses only affect the specified fields.
3.1.3 Action execution
The transitions fired in this stage are classified 
according to the different types of actions to be executed. The
intended effects of some actions may directly be achieved
in a single step, while others will required an indirect 
approach and possibly several execution steps. Actions of the
first kind are constructive ones such as set up and join.
The second group of actions include those, such as close and
leave, whose effects are indirectly achieved by updating the
interaction protocol.
As an example of constructive action, let"s consider the
execution of a set up action, whose type is defined as 
follows9
:
SetUp ACT · new : I
inv. : (13) αnew,mem = αnew,res = αnew,sub = ∅
(14) αnew,state = open
where the new field represents the new interaction to be
initiated. Its sets of participants (agents and resources) and
sub-interactions must be empty (inv. 13) and its state must
be open (inv. 14). The setting up of the new interaction may
thus affect its protocol and possible application-dependent
fields (e.g. the subject of a course interaction). According
to rule 3, the outcome of the execution is threefold: firstly,
the performer"s attempt queue is updated so that the 
executing action is removed; secondly, the new interaction is
added to the target"s set of sub-interactions (moreover, its
initiator field is set to the performer agent); last, the event
representing this change (which includes a description of the
change, the agent that caused it and the action performed)
is inserted in the output port of the target"s event channel.
αstate = exec ∧ α : SetUp ∧ αnew = i
a = playing, , , [α|qACT ], , −→ playing, , , qACT , ,
αtarget = open, , , , , sI , c −→ open, , , , , sI ∪ i , c
(3)
W here : (i )ini
= a
(c )out,ev
= insert( a, α, sub(αtarget
, i ) , cout,ev
)
Let"s consider now the case of a close action. This action
represents an attempt by the performer to force some 
interaction to finish, thus bypassing its current protocol rules
(those concerning the finish function). The way to achieve
this effect is to cause an update on the protocol so that the
finish function returns true afterwards10
. Accordingly, we
may specify this type of action as follows:
Close ACT · upd : (→ Bool) → (→ Bool)
inv. : (15) αtarget,state = open
(16) αtarget,context = nil
(17) αupd(αtarget,prot,finish)()
where the inherited target field represents the interaction
to be closed (which must be open and different to the 
topinteraction, according to invariants 15 and 16) and the new
9
The resulting type consists of the fields of the ACT record
extended with an additional new field.
10
This strategy is also followed in the definition of leave and
may also be used in the definition of other types of actions
such as fire, permit, forbid, etc.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 893
update field represents a proper higher-order function to 
update the target"s protocol (inv. 17). The transition which
models the execution of this action, specified by rule 4, 
defines two effects in the target interaction: its protocol is
updated and the event representing this change is inserted
in its output port. This event will actually trigger the 
closing process of the interaction as described in the next 
subsection.
αstate = exec ∧ α : Close
a = playing, , , [α|qACT ], , −→ playing, , , qACT , ,
αtarget = open, , , , , p, c −→ open, , , , , p , c
(4)
W here : (p )finish
= αupd
(pfinish
)
(c )out,ev
= insert( a, α, finish(αtarget
) , cout,ev
)
3.2 Event Processing
The processing of events is encapsulated in the event 
channels of interactions. Channels, ranged over by meta-variable
c, are defined by two input and output ports, according to
the following definition:
CH out : OutP, in : InP
inv. : (18) ccontext ∈ cout,disp( , , finish(ccontext) )
(19) ccontext ∈ cout,disp( , , over(a) )
(20) ccontext,sub ⊆ cout,disp(closing(ccontext))
(21) apartsIn ⊆ cout,disp(leaving(a))
(22) ccontext ∈ cout,disp(closed(i))
(23) {ccontext, aplayer,context} ⊆ cout,disp(left(a))
OutP ev : Queue E, disp : E → Set I, int : Set I, ag : Set A
InP ev : Queue E, stage : Enum {int, mem, obl}, ag : Set A
The output port stores and processes the events originated
within the scope of the channel"s interaction. Its first 
purpose is to dispatch the local events to the agents 
identified by the protocol"s monitoring function. Moreover, since
these events may influence the results of the finishing, over
and obligation functions of certain protocols, they will also
be dispatched to the input ports of the interactions 
identified through a dispatching function - whose invariants will
be explained later on. Thus, input ports serve as a 
coordination mechanism which activate the re-evaluation of the
above functios whenever some event is received11
. 
Accordingly, the processing of some event goes through four major
stages: event dispatching, interaction state update, member
state update and obligations update. The first one takes place
in the output port of the interaction in which the event 
originated, whereas the other ones execute in separate control
threads associated to the input ports of the interactions to
which the event was dispatched.
3.2.1 Event dispatching
The processing of some event stored in the output port is
triggered when all its preceding events have been dispatched.
As a first step, the auxiliary int and ag fields are initialised
11
Alternatively, we may have assumed that interactions are
fully aware of any change in the multi-agent community. In
this scenario, interactions would trigger themselves without
requiring any explicit notification. On the contrary, we 
adhere to the more realistic assumption of limited awareness.
with the returned values of the dispatching and protocol"s
monitoring functions, respectively (rule 5). Then, additional
rules simply iterate over these collections until all agents and
interactions have been notified (i.e., both sets are empty).
Last, the event is removed from the queue and the auxiliary
fields are re-set to nil.
The dispatching function shall identify the set of 
interactions (possibly, empty) that may be affected by the event
(which may include the channel"s interaction itself)12
. For
instance, according to the finishing rule of university courses
mentioned in the last section, the event representing the
end of the admission stage, originated within the scope of
the school interaction, will be dispatched to every course of
the school"s degrees. Concerning the monitoring function,
according to invariant 11 of protocols, if the event is 
generated as the result of an action performance, the agents
to be notified will include the performer and addressees of
that action. Thus, according to the monitoring rule of 
university courses, if a student of some degree joins a certain
course and specifies a colleague as addressee of that action,
the course"s teachers and itself will also be notified of the
successful execution.
ccontext,state
s = open ∧ ccontext,prot,monitor
s = mon
cs = [e| ], d, nil, nil , −→ [e| ], , d(e), mon(e) ,
(5)
3.2.2 Interaction state update
Input port activity is triggered when a new event is 
received. Irrespective of the kind of incoming event, the first
processing action is to check whether the channel"s 
interaction must be finished. Thus, the dispatching of the finish
event resulting from a close action (inv. 18) serves as a 
trigger of the closing procedure. If the interaction has not to
be finished, the input port stage field is set to the member
state update stage and the auxiliary ag field is initialised to
the interaction members. Otherwise, we can consider two
possible scenarios. In the first one, the interaction has no
members and no sub-interactions. In this case, the 
interaction can be inmediately closed down. As rule 6 shows,
the interaction is closed, removed from the context"s set of
sub-interactions and a closed event is inserted in its output
channel. According to invariant 22, this event will be later
inserted to its input channel to allow for further treatment.
cin,ev
1 = ∅ ∧ cin,stage
1 = int ∧ pfinish()
, , , , {i} ∪ sI , , c −→ , , , , sI , , c
i = , , ∅, , ∅, p, c1 −→ closed, , , , , ,
(6)
W here : (c )out,ev
= insert(closed(i), cout,ev
)
In the second scenario, the interaction has some member
or sub-interaction. In this case, clean-up is required prior to
the disposal of the interaction (e.g. if the admission period
ends and no student has matriculated for the course, 
teachers has to be finished before finishing the course itself). As
rule 7 shows, the interaction is moved to the transient 
closing state and a corresponding event is inserted in the output
port. According to invariant 20, the closing event will be
dispatched to every sub-interaction in order to activate its
closing procedure (guaranteed by invariant 8). Moreover,
12
This is essentially determined by the protocol rules of these
interactions. The way in which the dispatching function is
initialised and updated is out of the scope of this paper.
894 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
the stage and ag fields are properly initialised so that the
process goes on in the next member state update stage. This
stage will further initiate the leaving process of the members
(according to invariant 9).
cin,ev = ∅ ∧ cin,stage = int ∧ pfinish() ∧ (sA = ∅ ∨ sI = ∅)
i = open, , sA, , sI , p, c −→ closing, , sA, , sI , p, c
(7)
W here : (c )out,ev
= insert(closing(i), cout,ev
)
(c )in,stage
= mem
(c )in,ag
= sA
Eventually, every member will leave the interaction and
every sub-interaction will be closed. Corresponding events
will be received by the interaction (according to invariants
23 and 22) so that the conditions of the first scenario will
hold.
3.2.3 Member state update
This stage simply iterates over the members of the 
interaction to check whether they must be finished according to
the protocol"s over function. When all members have been
checked, the stage field will be set to the next obligation
update stage and the auxiliary ag field will be initalised
with the agents identified by the protocol"s obligation 
update function.
If some member has to end its participation in the 
interaction and it is not playing any role, it will be inmediately
abandoned (successfully or unsuccessfully, according to the
satisfaction of its purpose). The corresponding event will
be forwarded to its interaction and to the interaction of its
player agent to account for further changes (inv. 23). 
Otherwise, the member enters the transient leaving state, thus
preventing any action performance. Then, it waits for the
completion of the leaving procedures of its played roles, 
triggered by proper dispatching of the leaving event (inv. 21).
3.2.4 Obligations update
In this stage, the obligations of agents (not necessaryly
members of the interaction) towards the interaction are 
updated accordingly. When all the identified agents have been
updated, the event is removed from the input queue and
the stage field is set back to the interaction state update.
For instance, when a course interaction receives an event
representing the assignment of some department member to
its subject, an obligation to join the course as a teacher is
created for that member. Moreover, the event representing
this change is added to the output channel of the department
interaction.
4. DISCUSSION
This paper has attempted to expose a possible 
semantic core underlying the wide spectrum of interaction types
between autonomous, social and situated software 
components. In the realm of software architectures, this core has
been formalised as an operational model of social 
connectors, intended to describe both the basic structure and 
dynamics of multi-agent interactions, from the largest (the
agent society itself) down to the smallest ones 
(communicative actions). Thus, top-level interactions may represent the
kind of agent-web pursued by large-scale initiatives such as
the Agentcities/openNet one [25]. Large-scale interactions,
modelling complex aggregates of agent interactions such as
those represented by e-institutions or virtual organizations
[2, 26], are also amenable to be conceptualised as 
particular kinds of first-level social interactions. The last 
levels of the interaction tree may represent small-scale 
multiagent interactions such as those represented by interaction
protocols [11], dialogue games [16], or scenes [2]. Finally,
bottom-level interactions may represent communicative 
actions. From this perspective, the member types of a CA
include the speaker and possibly many listeners. The 
purpose of the speaker coincides with the illocutionary purpose
of the CA [22], whereas the purpose of any listener is to 
declare that it (actually, the software component) successfully
processed the meaning of the CA.
The analysis of social interactions put forward in this 
paper draws upon current proposals of the literature in 
several general respects, such as the institutional and 
organizational character of multi-agent systems [2, 26, 10, 7] and the
normative perspective on multi-agent protocols [12, 23, 20].
These proposals as well as others focusing in relevant 
abstractions such as power relationships, contracts, trust and
reputation mechanisms in organizational settings, etc., could
be further exploited in order to characterize more accurately
the organizational character of some multi-agent 
interactions. Similarly, the conceptualization of communicative
actions as atomic interactions may similarly benefit from
public semantics of communicative actions such as the one
introduced in [3]. Last, the abstract model of protocols may
be refined taking into account existing operational models
of norms [12, 6]. These analyses shall result in new 
organizational and communicative abstractions obtained through
a refinement and/or extension of the general model of 
social interactions. Thus, the proposed model is not intended
to capture every organizational or communicative feature of
multi-agent interactions, but to reveal their roots in basic
interaction mechanisms. In turn, this would allow for the
exploitation of common formalisms, particularly concerning
protocols.
Unlike the development of individual agents, which has
greatly benefited from the design of several agent 
programming languages [4], societal features of multi-agent systems
are mostly implemented in terms of visual modelling [8, 18]
and a fixed set of interaction abstractions. We argue that
the current field of multi-agent system programming may
greatly benefit from multi-agent programming languages that
allow programmers to accommodate an open set of 
interaction mechanisms. The model of social interactions put
forward in this paper is intended as the abstract machine
of a language of this type. This abstract machine would
be independent of particular agent architectures and 
languages (i.e. software components may be programmed in a
BDI language such as Jason [5] or in a non-agent oriented
language).
On top of the presented execution semantics, current and
future work aims at the specification of the type system [19]
which allows to program the abstract machine, the 
specification of the corresponding surface syntaxes (both textual
and visual) and the design and implementation of a virtual
machine over existing middleware technologies such as FIPA
platforms or Web services. We also plan to study particular
refinements and limitations to the proposed model, 
particularly with respect to the dispatching of events, semantics
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 895
of obligations, dynamic updates of protocols and rule 
formalisms. In this latter aspect, we plan to investigate the
use of Answer Set Programming to specify the rules of 
protocols, attending to the role that incompleteness (rules may
only specify either necessary or sufficient conditions, for 
instance), explicit negation (e.g. prohibitions) and defaults
play in this domain.
5. ACKNOWLEDGMENTS
The authors thank anonymous reviewers for their 
comments and suggestions. Research sponsored by the Spanish
Ministry of Science and Education (MEC), project 
TIN200615455-C03-03.
6. REFERENCES
[1] R. Allen and D. Garlan. A Formal Basis for
Architectural Connection. ACM Transactions on
Software Engineering and Methodology, 6(3):213-249,
June 1997.
[2] J. L. Arcos, M. Esteva, P. Noriega, J. A. Rodr´ıguez,
and C. Sierra. Engineering open environments with
electronic institutions. Journal on Engineering
Applications of Artificial Intelligence, 18(2):191-204,
2005.
[3] G. Boella, R. Damiano, J. Hulstijn, and L. W. N.
van der Torre. Role-based semantics for agent
communication: embedding of the "mental attitudes"
and "social commitments" semantics. In AAMAS,
pages 688-690, 2006.
[4] R. H. Bordini, L. Braubach, M. Dastani, A. E. F.
Seghrouchni, J. J. G. Sanz, J. Leite, G. O"Hare,
A. Pokahr, and A. Ricci. A survey of programming
languages and platforms for multi-agent systems.
Informatica, 30:33-44, 2006.
[5] R. H. Bordini, J. F. H¨ubner, and R. Vieira. Jason and
the golden fleece of agent-oriented programming. In
R. H. Bordini, D. M., J. Dix, and
A. El Fallah Seghrouchni, editors, Multi-Agent
Programming: Languages, Platforms and Applications,
chapter 1. Springer-Verlag, 2005.
[6] O. Cliffe, M. D. Vos, and J. A. Padget. Specifying and
analysing agent-based social institutions using answer
set programming. In EUMAS, pages 476-477, 2005.
[7] V. Dignum, J. V´azquez-Salceda, and F. Dignum.
Omni: Introducing social structure, norms and
ontologies into agent organizations. In R. Bordini,
M. Dastani, J. Dix, and A. Seghrouchni, editors,
Programming Multi-Agent Systems Second
International Workshop ProMAS 2004, volume 3346
of LNAI, pages 181-198. Springer, 2005.
[8] M. Esteva, D. de la Cruz, and C. Sierra. ISLANDER:
an electronic institutions editor. In M. Gini, T. Ishida,
C. Castelfranchi, and W. L. Johnson, editors,
Proceedings of the First International Joint
Conference on Autonomous Agents and Multiagent
Systems (AAMAS"02), pages 1045-1052. ACM Press,
July 2002.
[9] M. Esteva, B. Rosell, J. A. Rodr´ıguez-Aguilar, and
J. L. Arcos. AMELI: An agent-based middleware for
electronic institutions. In Proceedings of the Third
International Joint Conference on Autonomous Agents
and Multiagent Systems, volume 1, pages 236-243,
2004.
[10] J. Ferber, O. Gutknecht, and F. Michel. From agents
to organizations: An organizational view of
multi-agent systems. In AOSE, pages 214-230, 2003.
[11] Foundation for Intelligent Physical Agents. FIPA
Interaction Protocol Library Specification.
http://www.fipa.org/repository/ips.html, 2003.
[12] A. Garc´ıa-Camino, J. A. Rodr´ıguez-Aguilar, C. Sierra,
and W. Vasconcelos. Norm-oriented programming of
electronic institutions. In AAMAS, pages 670-672,
2006.
[13] O. Gutknecht and J. Ferber. The MadKit agent
platform architecture. Lecture Notes in Computer
Science, 1887:48-55, 2001.
[14] JADE. The JADE project home page.
http://jade.cselt.it, 2005.
[15] M. Luck, P. McBurney, O. Shehory, and S. Willmott.
Agent Technology: Computing as Interaction - A
Roadmap for Agent-Based Computing. AgentLink III,
2005.
[16] P. McBurney and S. Parsons. A formal framework for
inter-agent dialogues. In J. P. M¨uller, E. Andre,
S. Sen, and C. Frasson, editors, Proceedings of the
Fifth International Conference on Autonomous
Agents, pages 178-179, Montreal, Canada, May 2001.
ACM Press.
[17] N. R. Mehta, N. Medvidovic, and S. Phadke. Towards
a taxonomy of software connectors. In Proceedings of
the 22nd International Conference on Software
Engineering, pages 178-187. ACM Press, June 2000.
[18] J. Pav´on and J. G´omez-Sanz. Agent oriented software
engineering with ingenias. In V. Marik, J. Muller, and
M. Pechoucek, editors, Proceedings of the 3rd
International Central and Eastern European
Conference on Multi-Agent Systems. Springer Verlag,
2003.
[19] B. C. Pierce. Types and Programming Languages. The
MIT Press, Cambridge, MA, 2002.
[20] J. Pitt, L. Kamara, M. Sergot, and A. Artikis. Voting
in multi-agent systems. Feb. 27 2006.
[21] G. Plotkin. A structural approach to operational
semantics. Technical Report DAIMI FN-19, Aarhus
University, Sept. 1981.
[22] J. Searle. Speech Acts. Cambridge University Press,
1969.
[23] M. Sergot. A computational theory of normative
positions. ACM Transactions on Computational Logic,
2(4):581-622, Oct. 2001.
[24] M. P. Singh. Agent-based abstractions for software
development. In F. Bergenti, M.-P. Gleizes, and
F. Zambonelli, editors, Methodologies and Software
Engineering for Agent Systems, chapter 1, pages 5-18.
Kluwer, 2004.
[25] S. Willmot and al. Agentcities / opennet testbed.
http://x-opennet.net, 2004.
[26] F. Zambonelli, N. R. Jennings, and M. Wooldridge.
Developing multiagent systems: The Gaia
methodology. ACM Transactions on Software
Engineering and Methodology, 12(3):317-370, July
2003.
896 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
A Randomized Method for the Shapley Value
for the Voting Game
Shaheen S. Fatima
Department of
Computer Science,
University of Liverpool
Liverpool L69 3BX, UK.
shaheen@csc.liv.ac.uk
Michael Wooldridge
Department of
Computer Science,
University of Liverpool
Liverpool L69 3BX, UK.
mjw@csc.liv.ac.uk
Nicholas R. Jennings
School of Electronics and
Computer Science
University of Southampton
Southampton SO17 1BJ, UK.
nrj@ecs.soton.ac.uk
ABSTRACT
The Shapley value is one of the key solution concepts for 
coalition games. Its main advantage is that it provides a unique and fair
solution, but its main problem is that, for many coalition games,
the Shapley value cannot be determined in polynomial time. In
particular, the problem of finding this value for the voting game is
known to be #P-complete in the general case. However, in this 
paper, we show that there are some specific voting games for which
the problem is computationally tractable. For other general voting
games, we overcome the problem of computational complexity by
presenting a new randomized method for determining the 
approximate Shapley value. The time complexity of this method is linear
in the number of players. We also show, through empirical studies,
that the percentage error for the proposed method is always less
than 20% and, in most cases, less than 5%.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems
General Terms
Algorithms, Design, Theory
1. INTRODUCTION
Coalition formation, a key form of interaction in multi-agent 
systems, is the process of joining together two or more agents so as
to achieve goals that individuals on their own cannot, or to achieve
them more efficiently [1, 11, 14, 13]. Often, in such situations,
there is more than one possible coalition and a player"s payoff 
depends on which one it joins. Given this, a key problem is to ensure
that none of the parties in a coalition has any incentive to break
away from it and join another coalition (i.e., the coalitions should
be stable). However, in many cases there may be more than one
solution (i.e., a stable coalition). In such cases, it becomes difficult
to select a single solution from among the possible ones, especially
if the parties are self-interested (i.e., they have different preferences
over stable coalitions).
In this context, cooperative game theory deals with the 
problem of coalition formation and offers a number of solution 
concepts that possess desirable properties like stability, fair division
of joint gains, and uniqueness [16, 14]. Cooperative game theory
differs from its non-cooperative counterpart in that for the former
the players are allowed to form binding agreements and so there is
a strong incentive to work together to receive the largest total 
payoff. Also, unlike non-cooperative game theory, cooperative game
theory does not specify a game through a description of the 
strategic environment (including the order of players" moves and the set
of actions at each move) and the resulting payoffs, but, instead, it
reduces this collection of data to the coalitional form where each
coalition is represented by a single real number: there are no 
actions, moves, or individual payoffs. The chief advantage of this
approach, at least in multiple-player environments, is its practical
usefulness. Thus, many more real-life situations fit more easily into
a coalitional form game, whose structure is more tractable than that
of a non-cooperative game, whether that be in normal or extensive
form and it is for this reason that we focus on such forms in this
paper.
Given these observations, a number of multiagent systems 
researchers have used and extended cooperative game-theoretic 
solutions to facilitate automated coalition formation [20, 21, 18]. 
Moreover, in this work, one of the most extensively studied solution 
concepts is the Shapley value [19]. A player"s Shapley value gives an
indication of its prospects of playing the game - the higher the
Shapley value, the better its prospects. The main advantage of the
Shapley value is that it provides a solution that is both unique and
fair (see Section 2.1 for a discussion of the property of fairness).
However, while these are both desirable properties, the Shapley
value has one major drawback: for many coalition games, it 
cannot be determined in polynomial time. For instance, finding this
value for the weighted voting game is, in general, #P-complete [6].
A problem is #P-hard if solving it is as hard as counting 
satisfying assignments of propositional logic formulae [15, p442]. Since
#P-completeness thus subsumes NP-completeness, this implies that
computing the Shapley value for the weighted voting game will be
intractable in general. In other words, it is practically infeasible to
try to compute the exact Shapley value. However, the voting game
has practical relevance to multi-agent systems as it is an important
means of reaching consensus between multiple agents. Hence, our
objective is to overcome the computational complexity of finding
the Shapley value for this game. Specifically, we first show that
there are some specific voting games for which the exact value can
959
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
be computed in polynomial time. By identifying such games, we
show, for the first tme, when it is feasible to find the exact value and
when it is not. For the computationally complex voting games, we
present a new randomised method, along the lines of Monte-Carlo
simulation, for computing the approximate Shapley value.
The computational complexity of such games has typically been
tackled using two main approaches. The first is to use 
generating functions [3]. This method trades time complexity for 
storage space. The second uses an approximation technique based on
Monte Carlo simulation [12, 7]. However the method we propose is
more general than either of these (see Section 6 for details). 
Moreover, no work has previously analysed the approximation error. The
approximation error relates to how close the approximate is to the
true Shapley value. Specifically, it is the difference between the true
and the approximate Shapley value. It is important to determine
this error because the performance of an approximation method is
evaluated in terms of two criteria: its time complexity, and its 
approximation error. Thus, our contribution lies in also in providing,
for the first time, an analysis of the percentage error in the 
approximate Shapley value. This analysis is carried out empirically.
Our experiments show that the error is always less than 20%,
and in most cases it is under 5%. Finally, our method has time
complexity linear in the number of players and it does not require
any arrays (i.e., it is economical in terms of both computing time
and storage space). Given this, and the fact that software agents
have limited computational resources and therefore cannot 
compute the true Shapley value, our results are especially relevant to
such resource bounded agents.
The rest of the paper is organised as follows. Section 2 defines
the Shapley value and describes the weighted voting game. In 
Section 3 we describe voting games whose Shapley value can be found
in polynomial time. In Section 4, we present a randomized method
for finding the approximate Shapley value and analyse its 
performance in Section 5. Section 6 discusses related literature. Finally,
Section 7 concludes.
2. BACKGROUND
We begin by introducing coalition games and the Shapley value and
then define the weighted voting game. A coalition game is a game
where groups of players (coalitions) may enforce cooperative 
behaviour between their members. Hence the game is a competition
between coalitions of players, rather than between individual 
players.
Depending on how the players measure utility, coalition game
theory is split into two parts. If the players measure utility or the
payoff in the same units and there is a means of exchange of utility
such as side payments, we say the game has transferable utility;
otherwise it has non-transferable utility. More formally, a coalition
game with transferable utility, N, v , consists of:
1. a finite set (N = {1, 2, . . . , n}) of players and
2. a function (v) that associates with every non-empty subset S
of N (i.e., a coalition) a real number v(S) (the worth of S).
For each coalition S, the number v(S) is the total payoff that is
available for division among the members of S (i.e., the set of joint
actions that coalition S can take consists of all possible divisions
of v(S) among the members of S). Coalition games with 
nontransferable payoffs differ from ones with transferable payoffs in
the following way. For the former, each coalition is associated with
a set of payoff vectors that is not necessarily the set of all possible
divisions of some fixed amount. The focus of this paper is on the
weighted voting game (described in Section 2.1) which is a game
with transferable payoffs.
Thus, in either case, the players will only join a coalition if they
expect to gain from it. Here, the players are allowed to form 
binding agreements, and so there is strong incentive to work together to
receive the largest total payoff. The problem then is how to split the
total payoff between or among the players. In this context, Shapley
[19] constructed a solution using an axiomatic approach. Shapley
defined a value for games to be a function that assigns to a game
(N, v), a number ϕi(N, v) for each i in N. This function satisfies
three axioms [17]:
1. Symmetry. This axiom requires that the names of players
play no role in determining the value.
2. Carrier. This axiom requires that the sum of ϕi(N, v) for all
players i in any carrier C equal v(C). A carrier C is a subset
of N such that v(S) = v(S ∩ C) for any subset of players
S ⊂ N.
3. Additivity. This axiom specifies how the values of different
games must be related to one another. It requires that for
any games ϕi(N, v) and ϕi(N, v ), ϕi(N, v)+ϕi(N, v ) =
ϕi(N, v + v ) for all i in N.
Shapley showed that there is a unique function that satisfies these
three axioms.
Shapley viewed this value as an index for measuring the power
of players in a game. Like a price index or other market indices, the
value uses averages (or weighted averages in some of its 
generalizations) to aggregate the power of players in their various cooperation
opportunities. Alternatively, one can think of the Shapley value as
a measure of the utility of risk neutral players in a game.
We first introduce some notation and then define the Shapley
value. Let S denote the set N − {i} and fi : S → 2N−{i}
be
a random variable that takes its values in the set of all subsets of
N − {i}, and has the probability distribution function (g) defined
as:
g(fi(S) = S) =
|S|!(n − |S| − 1)!
n!
The random variable fi is interpreted as the random choice of a
coalition that player i joins. Then, a player"s Shapley value is 
defined in terms of its marginal contribution. Thus, the marginal 
contribution of player i to coalition S with i /∈ S is a function Δiv that
is defined as follows:
Δiv(S) = v(S ∪ {i}) − v(S)
Thus a player"s marginal contribution to a coalition S is the 
increase in the value of S as a result of i joining it.
DEFINITION 1. The Shapley value (ϕi) of the game N, v for
player i is the expectation (E) of its marginal contribution to a
coalition that is chosen randomly:
ϕi(N, v) = E[Δiv ◦ fi] (1)
The Shapley value is interpreted as follows. Suppose that all
the players are arranged in some order, all orderings being equally
likely. Then ϕi(N, v) is the expected marginal contribution, over
all orderings, of player i to the set of players who precede him.
The method for finding a player"s Shapley value depends on the
definition of the value function (v). This function is different for
different games, but here we focus specifically on the weighted 
voting game for the reasons outlined in Section 1.
960 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
2.1 The weighted voting game
We adopt the definition of the voting game given in [6]. Thus, there
is a set of n players that may, for example, represent shareholders
in a company or members in a parliament. The weighted voting
game is then a game G = N, v in which:
v(S) =
j
1 if w(S) ≥ q
0 otherwise
for some q ∈ IR+ and wi ∈ IRN
+ , where:
w(S) =
X
i∈S
wi
for any coalition S. Thus wi is the number of votes that player i
has and q is the number of votes needed to win the game (i.e., the
quota).
Note that for this game (denoted q; w1, . . . , wn ), a player"s
marginal contribution is either zero or one. This is because the
value of any coalition is either zero or one. A coalition with value
zero is called a losing coalition and with value one a winning
coalition. If a player"s entry to a coalition changes it from losing to
winning, then the player"s marginal contribution for that coalition
is one; otherwise it is zero.
The main advantage of the Shapley value is that it gives a 
solution that is both unique and fair. The property of uniqueness
is desirable because it leaves no ambiguity. The property of 
fairness relates to how the gains from cooperation are split between
the members of a coalition. In this case, a player"s Shapley value
is proportional to the contribution it makes as a member of a 
coalition; the more contribution it makes, the higher its value. Thus,
from a player"s perspective, both uniqueness and fairness are 
desirable properties.
3. VOTING GAMES WITH POLYNOMIAL
TIME SOLUTIONS
Here we describe those voting games for which the Shapley value
can be determined in polynomial time. This is achieved using the
direct enumeration approach (i.e., listing all possible coalitions and
finding a player"s marginal contribution to each of them). We 
characterise such games in terms of the number of players and their
weights.
3.1 All players have equal weight
Consider the game q; j, . . . , j with m parties. Each party has j
votes. If q ≤ j, then there would be no need for the players to form
a coalition. On the other hand, if q = mj (m = |N| is the number
of players), only the grand coalition is possible. The interesting
games are those for which the quota (q) satisfies the constraint:
(j + 1) ≤ q ≤ j(m − 1). For these games, the value of a coalition
is one if the weight of the coalition is greater than or equal to q,
otherwise it is zero.
Let ϕ denote the Shapley value for a player. Consider any one
player. This player can join a coalition as the ith member where
1 ≤ i ≤ m. However, the marginal contribution of the player is 1
only if it joins a coalition as the q/j th member. In all other cases,
its marginal contribution is zero. Thus, the Shapley value for each
player ϕ = 1/m. Since ϕ requires one division operation, it can
be found in constant time (i.e., O(1)).
3.2 A single large party
Consider a game in which there are two types of players: large
(with weight wl > ws) and small (with weight ws). There is one
large player and m small ones. The quota for this game is q; i.e., we
have a game of the form q; wl, ws, ws, . . . , ws . The total number
of players is (m + 1). The value of a coalition is one if the weight
of the coalition is greater than or equal to q, otherwise it is zero.
Let ϕl denote the Shapley value for the large player and ϕs that for
each small player.
We first consider ws = 1 and then ws > 1. The smallest 
possible value for q is wl + 1. This is because, if q ≤ wl, then the
large party can win the election on its own without the need for
a coalition. Thus, the quota for the game satisfies the constraint
wl + 1 ≤ q ≤ m + wl − 1. Also, the lower and upper limits for
wl are 2 and (q − 1) respectively. The lower limit is 2 because
the weight of the large party has to be greater than each small one.
Furthermore, the weight of the large party cannot be greater than
q, since in that case, there would be no need for the large party
to form a coalition. Recall that for our voting game, a player"s
marginal contribution to a coalition can only be zero or one.
Consider the large party. This party can join a coalition as the
ith member where 1 ≤ i ≤ (m + 1). However, the marginal
contribution of the large party is one if it joins a coalition as the
ith member where (q − wl) ≤ i < q. In all the remaining cases,
its marginal contribution is zero. Thus, out of the total (m + 1)
possible cases, its marginal contribution is one in wl cases. Hence,
the Shapley value of the large party is: ϕl = wl/(m + 1). In the
same way, we obtain the Shapley value of the large party for the
general case where ws > 1 as:
ϕl = wl/ws /(m + 1)
Now consider a small player. We know that the sum of the 
Shapley values of all the m+1 players is one. Also, since the small 
parties have equal weights, their Shapley values are the same. Hence,
we get:
ϕs =
1 − ϕl
m
Thus, both ϕl and ϕs can be computed in constant time. This
is because both require a constant number of basic operations 
(addition, subtraction, multiplication, and division). In the same way,
the Shapley value for a voting game with a single large party and
multiple small parties can be determined in constant time.
3.3 Multiple large and small parties
We now consider a voting game that has two player types: large
and small (as in Section 3.2), but now there are multiple large and
multiple small parties. The set of parties consists of ml large 
parties and ms small parties. The weight of each large party is wl and
that of each small one is ws, where ws < wl. We show the 
computational tractability for this game by considering the following four
possible scenarios:
S1 q ≤ mlwl and q ≤ msws
S2 q ≤ mlwl and q ≥ msws
S3 q ≥ mlwl and q ≥ msws
S4 q ≥ mlwl and q ≤ msws
For the first scenario, consider a large player. In order to determine
the Shapley value for this player, we need to consider the number
of all possible coalitions that give it a marginal contribution of one.
It is possible for the marginal contribution of this player to be one if
it joins a coalition in which the number of large players is between
zero and (q −1)/wl. In other words, there are (q −1)/wl +1 such
cases and we now consider each of them.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 961
Consider a coalition such that when the large player joins in,
there are i large players and (q − iwl − 1)/ws small players 
already in it, and the remaining players join after the large player.
Such a coalition gives the large player unit marginal contribution.
Let C2
l (i, q) denote the number of all such coalitions. To begin,
consider the case i = 0:
C2
l (0, q) = C
„
ms,
q − 1
ws
«
× FACTORIAL
„
q − 1
ws
«
×
FACTORIAL
„
ml + ms −
q − 1
ws
− 1
«
where C(y, x) denotes the number of possible combinations of x
items from a set of y items. For i = 1, we get:
C2
l (1, q) = C(ml, 1) × C
„
ms,
q − wl − 1
ws
«
×
FACTORIAL
„
q − wl − 1
ws
«
×
FACTORIAL
„
ml + ms −
q − wl − 1
ws
− 1
«
In general, for i > 1, we get:
C2
l (i, q) = C(ml, i) × C
„
ms,
q − iwl − 1
ws
«
×
FACTORIAL
„
q − iwl − 1
ws
«
×
FACTORIAL
„
ml + ms −
q − wl − 1
ws
− 1
«
Thus the large player"s Shapley value is:
ϕl =
q−1
wlX
i=0
C2
l (i, q)/FACTORIAL(ml + ms)
For a given i, the time to find C2
l (i, q) is O(T) where
T = (mlms(q − iwl − 1)(ml + ms))/ws
Hence, the time to find the Shapley value is O(T q/wl).
In the same way, a small player"s Shapley value is:
ϕs =
q−1
wsX
i=0
C2
s (i, q)/FACTORIAL(ml + ms)
and can be found in time O(T q/ws). Likewise, the remaining three
scenarios (S2 to S4) can be shown to have the same time 
complexity.
3.4 Three player types
We now consider a voting game that has three player types: 1, 2,
and 3. The set of parties consists of m1 players of type 1 (each
with weight w1), m2 players of type 2 (each with weight w2), and
m3 players of type 3 (each with weight w3).
For this voting game, consider a player of type 1. It is possible
for the marginal contribution of this player to be one if it joins a
coalition in which the number of type 1 players is between zero
and (q − 1)/w1. In other words, there are (q − 1)/w1 + 1 such
cases and we now consider each of them.
Consider a coalition such that when the type 1 player joins in,
there are i type 1 players already in it. The remaining players join
after the type 1 player. Let C3
l (i, q) denote the number of all such
coalitions that give a marginal contribution of one to the type 1
player where:
C3
1 (i, q) =
q−1
w1X
i=0
q−iw1−1
w2X
j=0
C2
1 (j, q − iw1)
Therefore the Shapley value of the type 1 player is:
ϕ1 =
q−1
w1X
i=0
C3
1 (i, q)/FACTORIAL(m1 + m2 + m3)
The time complexity of finding this value is O(T q2
/w1w2) where:
T = (
3Y
i=1
mi)(q − iwl − 1)(
3X
i=1
mi)/(w2 + w3)
Likewise, for the other two player types (2 and 3).
Thus, we have identified games for which the exact Shapley
value can be easily determined. However, the computational 
complexity of the above direct enumeration method increases with the
number of player types. For a voting game with more than three
player types, the time complexity of the above method is a 
polynomial of degree four or more. To deal with such situations, therefore,
the following section presents a faster randomised method for 
finding the approximate Shapley value.
4. FINDING THE APPROXIMATE SHAPLEY
VALUE
We first give a brief introduction to randomized algorithms and
then present our randomized method for finding the approximate
Shapley value. Randomized algorithms are the most commonly
used approach for finding approximate solutions to 
computationally hard problems. A randomized algorithm is an algorithm that,
during some of its steps, performs random choices [2]. The 
random steps performed by the algorithm imply that by executing the
algorithm several times with the same input we are not guaranteed
to find the same solution. Now, since such algorithms generate 
approximate solutions, their performance is evaluated in terms of two
criteria: their time complexity, and their error of approximation.
The approximation error refers to the difference between the 
exact solution and its approximation. Against this background, we
present a randomized method for finding the approximate Shapley
value and empirically evaluate its error.
We first describe the general voting game and then present our
randomized algorithm. In its general form, a voting game has more
than two types of players. Let wi denote the weight of player
i. Thus, for m players and for quota q the game is of the form
q; w1, w2, . . . , wm . The weights are specified in terms of a 
probability distribution function. For such a game, we want to find the
approximate Shapley value.
We let P denote a population of players. The players" weights
in this population are defined by a probability distribution function.
Irrespective of the actual probability distribution function, let μ be
the mean weight for the population of players and ν the variance in
the players" weights. From this population of players we randomly
draw samples and find the sum of the players" weights in the sample
using the following rule from Sampling Theory (see [8] p425):
If w1, w2, . . . , wn is a random sample of size n drawn
from any distribution with mean μ and variance ν, then
the sample sum has an approximate Normal 
distribution with mean nμ and variance ν
n
(the larger the n the
better the approximation).
962 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
R-SHAPLEYVALUE (P, μ, ν, q, wi)
P: Population of players
μ: Mean weight of the population P
ν: Variance in the weights for poulation P
q: Quota for the voting game
wi: Player i"s weight
1. Ti ← 0; a ← q − wi; b ← q −
2. For X from 1 to m repeatedly do the following
2.1. Select a random sample SX of size X from the
population P
2.2. Evaluate expected marginal contribution (ΔX
i )
of player i to SX as:
ΔX
i ← 1√
(2πν/X)
R b
a
e−X
(x−Xμ)2
2ν dx
2.3. Ti ← Ti + ΔX
i
3. Evaluate Shapley value of player i as:
ϕi ← Ti/m
Table 1: Randomized algorithm to find the Shapley value for
player i.
We know from Definition 1, that the Shapley value for a player is
the expectation (E) of its marginal contribution to a coalition that is
chosen randomly. We use this rule to determine the Shapley value
as follows.
For player i with weight wi, let ϕi denote the Shapley value. Let
X denote the size of a random sample drawn from a population
in which the individual player weights have any distribution. The
marginal contribution of player i to this random sample is one if the
total weight of the X players in the sample is greater than or equal
to a = q −wi but less than b = q − (where is an inifinitesimally
small quantity). Otherwise, its marginal contribution is zero. Thus,
the expected marginal contribution of player i (denoted ΔX
i ) to the
sample coalition is the area under the curve defined by N(Xμ, ν
X
)
in the interval [a, b]. This area is shown as the region B in Figure 1
(the dotted line in the figure is Xμ). Hence we get:
ΔX
i =
1
p
(2πν/X)
Z b
a
e−X
(x−Xμ)2
2ν dx (2)
and the Shapley value is:
ϕi =
1
m
mX
X=1
ΔX
i (3)
The above steps are described in Table 1. In more detail, Step
1 does the initialization. In Step 2, we vary X between 1 and m
and repeatedly do the following. In Step 2.1, we randomly select a
sample SX of size X from the population P. Player i"s marginal
contribution to the random coalition SX is found in Step 2.2. The
average marginal contribution is found in Step 3 - and this is the
Shapley value for player i.
THEOREM 1. The time complexity of the proposed randomized
method is linear in the number of players.
PROOF. As per Equation 3, ΔX
i must be computed m times.
This is done in the for loop of Step 2 in Table 1. Hence, the time
complexity of computing a player"s Shapley value is O(m).
The following section analyses the approximation error for the 
proposed method.
5. PERFORMANCE OF THE RANDOMIZED
METHOD
We first derive the formula for measuring the error in the 
approximate Shapley value and then conduct experiments for evaluating
this error in a wide range of settings. However, before doing so, we
introduce the idea of error.
The concept of error relates to a measurement made of a 
quantity which has an accepted value [22, 4]. Obviously, it cannot be
determined exactly how far off a measurement is from the accepted
value; if this could be done, it would be possible to just give a more
accurate, corrected value. Thus, error has to do with uncertainty in
measurements that nothing can be done about. If a measurement is
repeated, the values obtained will differ and none of the results can
be preferred over the others. However, although it is not possible
to do anything about such error, it can be characterized.
As described in Section 4, we make measurements on samples
that are drawn randomly from a given population (P) of players.
Now, there are statistical errors associated with sampling which
are unavoidable and must be lived with. Hence, if the result of a
measurement is to have meaning it cannot consist of the measured
value alone. An indication of how accurate the result is must be
included also. Thus, the result of any physical measurement has
two essential components:
1. a numerical value giving the best estimate possible of the
quantity measured, and
2. the degree of uncertainty associated with this estimated value.
For example, if the estimate of a quantity is x and the uncertainty
is e(x) the quantity would lie in x ± e(x).
For sampling experiments, the standard error is by far the most
common way of characterising uncertainty [22]. Given this, the
following section defines this error and uses it to evaluate the 
performance of the proposed randomized method.
5.1 Approximation error
The accuracy of the above randomized method depends on its 
sampling error which is defined as follows [22, 4]:
DEFINITION 2. The sampling error (or standard error) is 
defined as the standard deviation for a set of measurements divided
by the square root of the number of measurements.
To this end, let e(σX
) be the sampling error in the sum of the
weights for a sample of size X drawn from the distribution N(Xμ, ν
X
)
where:
e(σX
) =
p
(ν/X)/
p
(X)
=
p
(ν)/X (4)
Let e(ΔX
i ) denote the error in the marginal contribution for player
i (given in Equation 2). This error is obtained by propagating the
error in Equation 4 to Equation 2. In Equation 2, a and b are the
lower and upper limits for the sum of the players" weights for a
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 963
b
C
B
a − e (σX )
a
A
)X(σeb+ Sum of weights
Z1 Z2
Figure 1: A normal distribution for the sum of players" weights
in a coalition of size X.
40
60
80
100
0
50
100
0
5
10
15
20
25
QuotaWeight
PercentageerrorintheShapleyvalue
Figure 2: Performance of the randomized method for m = 10
players.
coalition of size X. Since the error in this sum is e(σX
), the actual
values of a and b lie in the interval a ± e(σX
) and b ± e(σX
)
respectively. Hence, the error in Equation 2 is either the probability
that the sum lies between the limits a − e(σX
) and a (i.e., the area
under the curve defined by N(Xμ, ν
X
) between a − e(σX
) and a,
which is the shaded region A in Figure 1) or the probability that the
sum of weights lies between the limits b and b+e(σX
) (i.e., the area
under the curve defined by N(Xμ, ν
X
) between b and b + e(σX
),
which is the shaded region C in Figure 1). More specifically, the
error is the maximum of these two probabilities:
e(ΔX
i ) =
1
p
(2πν/X)
× MAX
„Z a
a−e(σX )
e−X
(x−Xμ)2
2ν dx,
Z b+e(σX
)
b
e−X
(x−Xμ)2
2ν dx
«
On the basis of the above error, we find the error in the Shapley
value by using the following standard error propagation rules [22]:
R1 If x and y are two random variables with errors e(x) and e(y)
respectively, then the error in the random variable z = x + y
is given by:
e(z) = e(x) + e(y)
R2 If x is a random variable with error e(x) and z = kx where
0
100
200
300
400
500
0
100
200
300
400
500
0
5
10
15
20
25
QuotaWeight
PercentageerrorintheShapleyvalue
Figure 3: Performance of the randomized method for m = 50
players.
the constant k has no error, then the error in z is:
e(z) = |k|e(x)
Using the above rules, the error in the Shapley value (given in
Equation 3) is obtained by propagating the error in Equation 4 to
all coalitions between the sizes X = 1 and X = m. Let e(ϕi)
denote this error where:
e(ϕi) =
1
m
mX
X=1
e(ΔX
i )
We analyze the performance of our method in terms of the 
percentage error PE in the approximate Shapley value which is defined
as follows:
PE = 100 × e(ϕi)/ϕi (5)
5.2 Experimental Results
We now compute the percentage error in the Shapley value using
the above equation for PE. Since this error depends on the 
parameters of the voting game, we evaluate it in a range of settings by
systematically varying the parameters of the voting game.
In particular, we conduct experiments in the following setting.
For a player with weight w, the percentage error in a player"s 
Shapley value depends on the following five parameters (see Equation 3):
1. The number of parties (m).
2. The mean weight (μ).
3. The variance in the player"s weights (ν).
4. The quota for the voting game (q).
5. The given player"s weight (w).
We fix μ = 10 and ν = 1. This is because, for the normal
distribution, μ = 10 ensures that for almost all the players the
weight is positive, and ν = 1 is used most commonly in statistical
experiments (ν can be higher or lower but PE is increasing in 
νsee Equations 4 and 5). We then vary m, q, and w as follows. We
vary m between 5 and 100 (since beyond 100 we found that the
error is close to zero), for each m we vary q between 4μ and mμ
(we impose these limits because they ensure that the size of the
964 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
0
200
400
600
800
1000
0
200
400
600
800
1000
0
5
10
15
20
25
QuotaWeight
PercentageerrorintheShapleyvalue
Figure 4: Performance of the randomized method for m = 100
players.
winning coalition is more than one and less than m - see Section 3
for details), and for each q, we vary w between 1 and q−1 (because
a winning coalition must contain at least two players). The results
of these experiments are shown in Figures 2, 3, and 4. As seen in
the figures, the maximum PE is around 20% and in most cases it is
below 5%.
We now analyse the effect of the three parameters: w, q, and m
on the percentage error in more detail.
- Effect of w. The PE depends on e(σX
) because, in 
Equation 5, the limits of integration depend on e(σX
). The 
interval over which the first integration in Equation 5 is done is
a − a + e(σX
) = e(σX
), and the interval over which the
second one is done is b + e(σX
) − b = e(σX
). Thus, the 
interval is the same for both integrations and it is independent
of wi. Note that each of the two functions that are integrated
in Equation 5 are the same as the function that is integrated
in Equation 2. Only the limits of the integration are different.
Also, the interval over which the integration for the marginal
contribution of Equation 2 is done is b − a = wi − (see
Figure 1). The error in the marginal contribution is either the
area of the shaded region A (between a − e(σX
) and a) in
Figure 1, or the shaded area C (between b and b + e(σX
)).
As per Equation 5, it is the maximum of these two areas.
Since e(σX
) is independent of wi, as wi increases, e(σX
)
remains unchanged. However, the area of the unshaded 
region B increases. Hence, as wi increases, the error in the
marginal contribution decreases and PE also decreases.
- Effect of q. For a given q, the Shapley value for player i is
as given in Equation 3. We know that, for a sample of size
X, the sum of the players" weights is distributed normally
with mean Xμ and variance ν/X. Since 99% of a normal
distribution lies within two standard deviations of its mean
[8], player i"s marginal contribution to a sample of size X is
almost zero if:
a < Xμ + 2
p
ν/X or b > Xμ − 2
p
ν/X
This is because the three regions A, B, and C (in Figure 1)
lie either to the right of Z2 or to the left of Z1. However,
player i"s marginal contribution is greater than zero for those
X for which the following constraint is satisfied:
Xμ − 2
p
ν/X < a < b < Xμ + 2
p
ν/X
For this constraint, the three regions A, B, and C lie 
somewhere between Z1 and Z2. Since a = q −wi and b = q − ,
Equation 6 can also be written as:
Xμ − 2
p
ν/X < q − wi < q − < Xμ + 2
p
ν/X
The smallest X that satisfies the constraint in Equation 6
strictly increases with q. As X increases, the error in sum
of weights in a sample (i.e., e(σX
) =
p
(ν)/X) decreases.
Consequently, the error in a player"s marginal contribution
(see Equation 5) also decreases. This implies that as q 
increases, the error in the marginal contribution (and 
consequently the error in the Shapley value) decreases.
- Effect of m. It is clear from Equation 4 that the error e(σX
)
is highest for X = 1 and it decreases with X. Hence, for
small m, e(σ1
) has a significant effect on PE. But as m 
increases, the effect of e(σ1
) on PE decreases and, as a result,
PE decreases.
6. RELATED WORK
In order to overcome the computational complexity of finding the
Shapley value, two main approaches have been proposed in the
literature. One approach is to use generating functions [3]. This
method is an exact procedure that overcomes the problem of time
complexity, but its storage requirements are substantial - it requires
huge arrays. It also has the limitation (not shared by other 
approaches) that it can only be applied to games with integer weights
and quotas.
The other method uses an approximation technique based on
Monte Carlo simulation. In [12], for instance, the Shapley value is
computed by considering a random sample from a large population
of players. The method we propose differs from this in that they 
define the Shapley value by treating a player"s number of swings (if a
player can change a losing coalition to a winning one, then, for the
player, the coalition is counted as a swing) as a random variable,
while we treat the players" weights as random variables. In [12],
however, the question remains how to get the number of swings
from the definition of a voting game and what is the time 
complexity of doing this. Since the voting game is defined in terms of the
players" weights and the number of swings are obtained from these
weights, our method corresponds more closely to the definition of
the voting game. Our method also differs from [7] in that while [7]
presents a method for the case where all the players" weights are
distributed normally, our method applies to any type of distribution
for these weights. Thus, as stated in Section 1, our method is more
general than [3, 12, 7]. Also, unlike all the above mentioned work,
we provide an analysis of the performance of our method in terms
of the percentage error in the approximate Shapley value.
A method for finding the Shapley value was also proposed in
[5]. This method gives the exact Shapley value, but its time 
complexity is exponential. Furthermore, the method can be used only
if the game is represented in a specific form (viz., the multi-issue
representation), not otherwise. Finally, [9, 10] present a 
polynomial time method for finding the Shapley value. This method can
be used if the coalition game is represented as a marginal 
contribution net. Furthermore, they assume that the Shapley value of
a component of a given coalition game is given by an oracle, and
on the basis of this assumption aggregate these values to find the
value for the overall game. In contrast, our method is independent
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 965
of the representation and gives an approximate Shapley value in
linear time, without the need for an oracle.
7. CONCLUSIONS AND FUTURE WORK
Coalition formation is an important form of interaction in 
multiagent systems. An important issue in such work is for the agents to
decide how to split the gains from cooperation between the 
members of a coalition. In this context, cooperative game theory offers
a solution concept called the Shapley value. The main advantage of
the Shapley value is that it provides a solution that is both unique
and fair. However, its main problem is that, for many coalition
games, the Shapley value cannot be determined in polynomial time.
In particular, the problem of finding this value for the voting game
is #P-complete. Although this problem is, in general #P-complete,
we show that there are some specific voting games for which the
Shapley value can be determined in polynomial time and 
characterise such games. By doing so, we have shown when it is 
computationally feasible to find the exact Shapley value. For other complex
voting games, we presented a new randomized method for 
determining the approximate Shapley value. The time complexity of the
proposed method is linear in the number of players. We analysed
the performance of this method in terms of the percentage error in
the approximate Shapley value.
Our experiments show that the percentage error in the Shapley
value is at most 20. Furthermore, in most cases, the error is less
than 5%. Finally, we analyse the effect of the different parameters
of the voting game on this error. Our study shows that the error
decreases as
1. a player"s weight increases,
2. the quota increases, and
3. the number of players increases.
Given the fact that software agents have limited computational 
resources and therefore cannot compute the true Shapley value, our
results are especially relevant to such resource bounded agents. In
future, we will explore the problem of determining the Shapley
value for other commonly occurring coalition games like the 
production economy and the market economy.
8. REFERENCES
[1] R. Aumann. Acceptable points in general cooperative
n-person games. In Contributions to theTheory of Games
volume IV. Princeton University Press, 1959.
[2] G. Ausiello, P. Crescenzi, G. Gambosi, V. Kann,
A. Marchetti-Spaccamela, and M. Protasi. Complexity and
approximation: Combinatorial optimization problems and
their approximability properties. Springer, 2003.
[3] J. M. Bilbao, J. R. Fernandez, A. J. Losada, and J. J. Lopez.
Generating functions for computing power indices
efficiently. Top 8, 2:191-213, 2000.
[4] P. Bork, H. Grote, D. Notz, and M. Regler. Data Analysis
Techniques in High Energy Physics Experiments. Cambridge
University Press, 1993.
[5] V. Conitzer and T. Sandholm. Computing Shapley values,
manipulating value division schemes, and checking core
membership in multi-issue domains. In Proceedings of the
National Conference on Artificial Intelligence, pages
219-225, San Jose, California, 2004.
[6] X. Deng and C. H. Papadimitriou. On the complexity of
cooperative solution concepts. Mathematics of Operations
Research, 19(2):257-266, 1994.
[7] S. S. Fatima, M. Wooldridge, and N. R. Jennings. An
analysis of the shapley value and its uncertainty for the
voting game. In Proc. 7th Int. Workshop on Agent Mediated
Electronic Commerce, pages 39-52, 2005.
[8] A. Francis. Advanced Level Statistics. Stanley Thornes
Publishers, 1979.
[9] S. Ieong and Y. Shoham. Marginal contribution nets: A
compact representation scheme for coalitional games. In
Proceedings of the Sixth ACM Conference on Electronic
Commerce, pages 193-202, Vancouver, Canada, 2005.
[10] S. Ieong and Y. Shoham. Multi-attribute coalition games. In
Proceedings of the Seventh ACM Conference on Electronic
Commerce, pages 170-179, Ann Arbor, Michigan, 2006.
[11] J. P. Kahan and A. Rapoport. Theories of Coalition
Formation. Lawrence Erlbaum Associates Publishers, 1984.
[12] I. Mann and L. S. Shapley. Values for large games iv:
Evaluating the electoral college exactly. Technical report,
The RAND Corporation, Santa Monica, 1962.
[13] A. MasColell, M. Whinston, and J. R. Green.
Microeconomic Theory. Oxford University Press, 1995.
[14] M. J. Osborne and A. Rubinstein. A Course in Game Theory.
The MIT Press, 1994.
[15] C. H. Papadimitriou. Computational Complexity. Addison
Wesley Longman, 1994.
[16] A. Rapoport. N-person Game Theory : Concepts and
Applications. Dover Publications, Mineola, NY, 2001.
[17] A. E. Roth. Introduction to the shapley value. In A. E. Roth,
editor, The Shapley value, pages 1-27. University of
Cambridge Press, Cambridge, 1988.
[18] T. Sandholm and V. Lesser. Coalitions among
computationally bounded agents. Artificial Intelligence
Journal, 94(1):99-137, 1997.
[19] L. S. Shapley. A value for n person games. In A. E. Roth,
editor, The Shapley value, pages 31-40. University of
Cambridge Press, Cambridge, 1988.
[20] O. Shehory and S. Kraus. A kernel-oriented model for
coalition-formation in general environments: Implemetation
and results. In In Proceedings of the National Conference on
Artificial Intelligence (AAAI-96), pages 131-140, 1996.
[21] O. Shehory and S. Kraus. Methods for task allocation via
agent coalition formation. Artificial Intelligence Journal,
101(2):165-200, 1998.
[22] J. R. Taylor. An introduction to error analysis: The study of
uncertainties in physical measurements. University Science
Books, 1982.
966 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
An Efficient Heuristic Approach for Security Against
Multiple Adversaries
Praveen Paruchuri, Jonathan P. Pearce,
Milind Tambe, Fernando Ordonez
University of Southern California
Los Angeles, CA 90089
{paruchur, jppearce, tambe, fordon}@usc.edu
Sarit Kraus
Bar-Ilan University
Ramat-Gan 52900, Israel
sarit@cs.biu.ac.il
ABSTRACT
In adversarial multiagent domains, security, commonly defined as
the ability to deal with intentional threats from other agents, is a
critical issue. This paper focuses on domains where these threats
come from unknown adversaries. These domains can be modeled
as Bayesian games; much work has been done on finding equilibria
for such games. However, it is often the case in multiagent security
domains that one agent can commit to a mixed strategy which its
adversaries observe before choosing their own strategies. In this
case, the agent can maximize reward by finding an optimal 
strategy, without requiring equilibrium. Previous work has shown this
problem of optimal strategy selection to be NP-hard. Therefore,
we present a heuristic called ASAP, with three key advantages to
address the problem. First, ASAP searches for the highest-reward
strategy, rather than a Bayes-Nash equilibrium, allowing it to find
feasible strategies that exploit the natural first-mover advantage of
the game. Second, it provides strategies which are simple to 
understand, represent, and implement. Third, it operates directly on the
compact, Bayesian game representation, without requiring 
conversion to normal form. We provide an efficient Mixed Integer Linear
Program (MILP) implementation for ASAP, along with 
experimental results illustrating significant speedups and higher rewards over
other approaches.
Categories and Subject Descriptors
I.2.11 [Computing Methodologies]: Artificial Intelligence: 
Distributed Artificial Intelligence - Intelligent Agents
General Terms
Security, Design, Theory
1. INTRODUCTION
In many multiagent domains, agents must act in order to 
provide security against attacks by adversaries. A common issue that
agents face in such security domains is uncertainty about the 
adversaries they may be facing. For example, a security robot may
need to make a choice about which areas to patrol, and how often
[16]. However, it will not know in advance exactly where a robber
will choose to strike. A team of unmanned aerial vehicles (UAVs)
[1] monitoring a region undergoing a humanitarian crisis may also
need to choose a patrolling policy. They must make this decision
without knowing in advance whether terrorists or other adversaries
may be waiting to disrupt the mission at a given location. It may
indeed be possible to model the motivations of types of adversaries
the agent or agent team is likely to face in order to target these 
adversaries more closely. However, in both cases, the security robot
or UAV team will not know exactly which kinds of adversaries may
be active on any given day.
A common approach for choosing a policy for agents in such
scenarios is to model the scenarios as Bayesian games. A Bayesian
game is a game in which agents may belong to one or more types;
the type of an agent determines its possible actions and payoffs.
The distribution of adversary types that an agent will face may
be known or inferred from historical data. Usually, these games
are analyzed according to the solution concept of a Bayes-Nash
equilibrium, an extension of the Nash equilibrium for Bayesian
games. However, in many settings, a Nash or Bayes-Nash 
equilibrium is not an appropriate solution concept, since it assumes that
the agents" strategies are chosen simultaneously [5].
In some settings, one player can (or must) commit to a strategy
before the other players choose their strategies. These scenarios are
known as Stackelberg games [6]. In a Stackelberg game, a leader
commits to a strategy first, and then a follower (or group of 
followers) selfishly optimize their own rewards, considering the action
chosen by the leader. For example, the security agent (leader) must
first commit to a strategy for patrolling various areas. This strategy
could be a mixed strategy in order to be unpredictable to the 
robbers (followers). The robbers, after observing the pattern of patrols
over time, can then choose their strategy (which location to rob).
Often, the leader in a Stackelberg game can attain a higher 
reward than if the strategies were chosen simultaneously. To see the
advantage of being the leader in a Stackelberg game, consider a
simple game with the payoff table as shown in Table 1. The leader
is the row player and the follower is the column player. Here, the
leader"s payoff is listed first.
1 2 3
1 5,5 0,0 3,10
2 0,0 2,2 5,0
Table 1: Payoff table for example normal form game.
The only Nash equilibrium for this game is when the leader plays
2 and the follower plays 2 which gives the leader a payoff of 2.
311
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
However, if the leader commits to a uniform mixed strategy of
playing 1 and 2 with equal (0.5) probability, the follower"s best
response is to play 3 to get an expected payoff of 5 (10 and 0 with
equal probability). The leader"s payoff would then be 4 (3 and 5
with equal probability). In this case, the leader now has an 
incentive to deviate and choose a pure strategy of 2 (to get a payoff of
5). However, this would cause the follower to deviate to strategy
2 as well, resulting in the Nash equilibrium. Thus, by committing
to a strategy that is observed by the follower, and by avoiding the
temptation to deviate, the leader manages to obtain a reward higher
than that of the best Nash equilibrium.
The problem of choosing an optimal strategy for the leader to
commit to in a Stackelberg game is analyzed in [5] and found to
be NP-hard in the case of a Bayesian game with multiple types of
followers. Thus, efficient heuristic techniques for choosing 
highreward strategies in these games is an important open issue. 
Methods for finding optimal leader strategies for non-Bayesian games
[5] can be applied to this problem by converting the Bayesian game
into a normal-form game by the Harsanyi transformation [8]. If, on
the other hand, we wish to compute the highest-reward Nash 
equilibrium, new methods using mixed-integer linear programs (MILPs)
[17] may be used, since the highest-reward Bayes-Nash 
equilibrium is equivalent to the corresponding Nash equilibrium in the
transformed game. However, by transforming the game, the 
compact structure of the Bayesian game is lost. In addition, since the
Nash equilibrium assumes a simultaneous choice of strategies, the
advantages of being the leader are not considered.
This paper introduces an efficient heuristic method for 
approximating the optimal leader strategy for security domains, known as
ASAP (Agent Security via Approximate Policies). This method has
three key advantages. First, it directly searches for an optimal 
strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing
it to find high-reward non-equilibrium strategies like the one in the
above example. Second, it generates policies with a support which
can be expressed as a uniform distribution over a multiset of fixed
size as proposed in [12]. This allows for policies that are simple
to understand and represent [12], as well as a tunable parameter
(the size of the multiset) that controls the simplicity of the policy.
Third, the method allows for a Bayes-Nash game to be expressed
compactly without conversion to a normal-form game, allowing for
large speedups over existing Nash methods such as [17] and [11].
The rest of the paper is organized as follows. In Section 2 we
fully describe the patrolling domain and its properties. Section 3
introduces the Bayesian game, the Harsanyi transformation, and
existing methods for finding an optimal leader"s strategy in a 
Stackelberg game. Then, in Section 4 the ASAP algorithm is presented
for normal-form games, and in Section 5 we show how it can be
adapted to the structure of Bayesian games with uncertain 
adversaries. Experimental results showing higher reward and faster 
policy computation over existing Nash methods are shown in Section
6, and we conclude with a discussion of related work in Section 7.
2. THE PATROLLING DOMAIN
In most security patrolling domains, the security agents (like
UAVs [1] or security robots [16]) cannot feasibly patrol all areas all
the time. Instead, they must choose a policy by which they patrol
various routes at different times, taking into account factors such as
the likelihood of crime in different areas, possible targets for crime,
and the security agents" own resources (number of security agents,
amount of available time, fuel, etc.). It is usually beneficial for
this policy to be nondeterministic so that robbers cannot safely rob
certain locations, knowing that they will be safe from the security
agents [14]. To demonstrate the utility of our algorithm, we use a
simplified version of such a domain, expressed as a game.
The most basic version of our game consists of two players: the
security agent (the leader) and the robber (the follower) in a world
consisting of m houses, 1 . . . m. The security agent"s set of pure
strategies consists of possible routes of d houses to patrol (in an
order). The security agent can choose a mixed strategy so that the
robber will be unsure of exactly where the security agent may 
patrol, but the robber will know the mixed strategy the security agent
has chosen. For example, the robber can observe over time how
often the security agent patrols each area. With this knowledge, the
robber must choose a single house to rob. We assume that the 
robber generally takes a long time to rob a house. If the house chosen
by the robber is not on the security agent"s route, then the robber
successfully robs it. Otherwise, if it is on the security agent"s route,
then the earlier the house is on the route, the easier it is for the
security agent to catch the robber before he finishes robbing it.
We model the payoffs for this game with the following variables:
• vl,x: value of the goods in house l to the security agent.
• vl,q: value of the goods in house l to the robber.
• cx: reward to the security agent of catching the robber.
• cq: cost to the robber of getting caught.
• pl: probability that the security agent can catch the robber at
the lth house in the patrol (pl < pl ⇐⇒ l < l).
The security agent"s set of possible pure strategies (patrol routes)
is denoted by X and includes all d-tuples i =< w1, w2, ..., wd >
with w1 . . . wd = 1 . . . m where no two elements are equal (the
agent is not allowed to return to the same house). The robber"s
set of possible pure strategies (houses to rob) is denoted by Q and
includes all integers j = 1 . . . m. The payoffs (security agent,
robber) for pure strategies i, j are:
• −vl,x, vl,q, for j = l /∈ i.
• plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.
With this structure it is possible to model many different types
of robbers who have differing motivations; for example, one robber
may have a lower cost of getting caught than another, or may value
the goods in the various houses differently. If the distribution of
different robber types is known or inferred from historical data,
then the game can be modeled as a Bayesian game [6].
3. BAYESIAN GAMES
A Bayesian game contains a set of N agents, and each agent n
must be one of a given set of types θn. For our patrolling domain,
we have two agents, the security agent and the robber. θ1 is the set
of security agent types and θ2 is the set of robber types. Since there
is only one type of security agent, θ1 contains only one element.
During the game, the robber knows its type but the security agent
does not know the robber"s type. For each agent (the security agent
or the robber) n, there is a set of strategies σn and a utility function
un : θ1 × θ2 × σ1 × σ2 → .
A Bayesian game can be transformed into a normal-form game
using the Harsanyi transformation [8]. Once this is done, new,
linear-program (LP)-based methods for finding high-reward 
strategies for normal-form games [5] can be used to find a strategy in the
transformed game; this strategy can then be used for the Bayesian
game. While methods exist for finding Bayes-Nash equilibria 
directly, without the Harsanyi transformation [10], they find only a
312 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
single equilibrium in the general case, which may not be of high
reward. Recent work [17] has led to efficient mixed-integer linear
program techniques to find the best Nash equilibrium for a given
agent. However, these techniques do require a normal-form game,
and so to compare the policies given by ASAP against the optimal
policy, as well as against the highest-reward Nash equilibrium, we
must apply these techniques to the Harsanyi-transformed matrix.
The next two subsections elaborate on how this is done.
3.1 Harsanyi Transformation
The first step in solving Bayesian games is to apply the Harsanyi
transformation [8] that converts the Bayesian game into a normal
form game. Given that the Harsanyi transformation is a standard
concept in game theory, we explain it briefly through a simple 
example in our patrolling domain without introducing the 
mathematical formulations. Let us assume there are two robber types a and
b in the Bayesian game. Robber a will be active with probability
α, and robber b will be active with probability 1 − α. The rules
described in Section 2 allow us to construct simple payoff tables.
Assume that there are two houses in the world (1 and 2) and
hence there are two patrol routes (pure strategies) for the agent:
{1,2} and {2,1}. The robber can rob either house 1 or house 2
and hence he has two strategies (denoted as 1l, 2l for robber type
l). Since there are two types assumed (denoted as a and b), we
construct two payoff tables (shown in Table 2) corresponding to
the security agent playing a separate game with each of the two
robber types with probabilities α and 1 − α. First, consider robber
type a. Borrowing the notation from the domain section, we assign
the following values to the variables: v1,x = v1,q = 3/4, v2,x =
v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Using these
values we construct a base payoff table as the payoff for the game
against robber type a. For example, if the security agent chooses
route {1,2} when robber a is active, and robber a chooses house 1,
the robber receives a reward of -1 (for being caught) and the agent
receives a reward of 0.5 for catching the robber. The payoffs for the
game against robber type b are constructed using different values.
Security agent: {1,2} {2,1}
Robber a
1a -1, .5 -.375, .125
2a -.125, -.125 -1, .5
Robber b
1b -.9, .6 -.275, .225
2b -.025, -.025 -.9, .6
Table 2: Payoff tables: Security Agent vs Robbers a and b
Using the Harsanyi technique involves introducing a chance node,
that determines the robber"s type, thus transforming the security
agent"s incomplete information regarding the robber into imperfect
information [3]. The Bayesian equilibrium of the game is then 
precisely the Nash equilibrium of the imperfect information game. The
transformed, normal-form game is shown in Table 3. In the 
transformed game, the security agent is the column player, and the set
of all robber types together is the row player. Suppose that robber
type a robs house 1 and robber type b robs house 2, while the 
security agent chooses patrol {1,2}. Then, the security agent and the
robber receive an expected payoff corresponding to their payoffs
from the agent encountering robber a at house 1 with probability α
and robber b at house 2 with probability 1 − α.
3.2 Finding an Optimal Strategy
Although a Nash equilibrium is the standard solution concept for
games in which agents choose strategies simultaneously, in our 
security domain, the security agent (the leader) can gain an advantage
by committing to a mixed strategy in advance. Since the followers
(the robbers) will know the leader"s strategy, the optimal response
for the followers will be a pure strategy. Given the common 
assumption, taken in [5], in the case where followers are indifferent,
they will choose the strategy that benefits the leader, there must
exist a guaranteed optimal strategy for the leader [5].
From the Bayesian game in Table 2, we constructed the Harsanyi
transformed bimatrix in Table 3. The strategies for each player 
(security agent or robber) in the transformed game correspond to all
combinations of possible strategies taken by each of that player"s
types. Therefore, we denote X = σθ1
1 = σ1 and Q = σθ2
2 as the
index sets of the security agent and robbers" pure strategies 
respectively, with R and C as the corresponding payoff matrices. Rij is
the reward of the security agent and Cij is the reward of the 
robbers when the security agent takes pure strategy i and the robbers
take pure strategy j. A mixed strategy for the security agent is a
probability distribution over its set of pure strategies and will be
represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0
and
P
pxi = 1. Here, pxi is the probability that the security agent
will choose its ith pure strategy.
The optimal mixed strategy for the security agent can be found
in time polynomial in the number of rows in the normal form game
using the following linear program formulation from [5].
For every possible pure strategy j by the follower (the set of all
robber types),
max
P
i∈X pxiRij
s.t. ∀j ∈ Q,
P
i∈σ1
pxiCij ≥
P
i∈σ1
pxiCij
P
i∈X pxi = 1
∀i∈X , pxi >= 0
(1)
Then, for all feasible follower strategies j, choose the one that 
maximizes
P
i∈X pxiRij, the reward for the security agent (leader).
The pxi variables give the optimal strategy for the security agent.
Note that while this method is polynomial in the number of rows
in the transformed, normal-form game, the number of rows 
increases exponentially with the number of robber types. Using this
method for a Bayesian game thus requires running |σ2||θ2|

separate linear programs. This is no surprise, since finding the leader"s
optimal strategy in a Bayesian Stackelberg game is NP-hard [5].
4. HEURISTIC APPROACHES
Given that finding the optimal strategy for the leader is NP-hard,
we provide a heuristic approach. In this heuristic we limit the 
possible mixed strategies of the leader to select actions with 
probabilities that are integer multiples of 1/k for a predetermined integer
k. Previous work [14] has shown that strategies with high entropy
are beneficial for security applications when opponents" utilities
are completely unknown. In our domain, if utilities are not 
considered, this method will result in uniform-distribution strategies.
One advantage of such strategies is that they are compact to 
represent (as fractions) and simple to understand; therefore they can
be efficiently implemented by real organizations. We aim to 
maintain the advantage provided by simple strategies for our security
application problem, incorporating the effect of the robbers" 
rewards on the security agent"s rewards. Thus, the ASAP heuristic
will produce strategies which are k-uniform. A mixed strategy is
denoted k-uniform if it is a uniform distribution on a multiset S of
pure strategies with |S| = k. A multiset is a set whose elements
may be repeated multiple times; thus, for example, the mixed 
strategy corresponding to the multiset {1, 1, 2} would take strategy 1
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313
{1,2} {2,1}
{1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α)
{1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α)
{2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α)
{2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α)
Table 3: Harsanyi Transformed Payoff Table
with probability 2/3 and strategy 2 with probability 1/3. ASAP 
allows the size of the multiset to be chosen in order to balance the
complexity of the strategy reached with the goal that the identified
strategy will yield a high reward.
Another advantage of the ASAP heuristic is that it operates 
directly on the compact Bayesian representation, without requiring
the Harsanyi transformation. This is because the different follower
(robber) types are independent of each other. Hence, evaluating
the leader strategy against a Harsanyi-transformed game matrix
is equivalent to evaluating against each of the game matrices for
the individual follower types. This independence property is 
exploited in ASAP to yield a decomposition scheme. Note that the LP
method introduced by [5] to compute optimal Stackelberg policies
is unlikely to be decomposable into a small number of games as it
was shown to be NP-hard for Bayes-Nash problems. Finally, note
that ASAP requires the solution of only one optimization problem,
rather than solving a series of problems as in the LP method of [5].
For a single follower type, the algorithm works the following
way. Given a particular k, for each possible mixed strategy x for the
leader that corresponds to a multiset of size k, evaluate the leader"s
payoff from x when the follower plays a reward-maximizing pure
strategy. We then take the mixed strategy with the highest payoff.
We need only to consider the reward-maximizing pure 
strategies of the followers (robbers), since for a given fixed strategy x
of the security agent, each robber type faces a problem with fixed
linear rewards. If a mixed strategy is optimal for the robber, then
so are all the pure strategies in the support of that mixed strategy.
Note also that because we limit the leader"s strategies to take on
discrete values, the assumption from Section 3.2 that the followers
will break ties in the leader"s favor is not significant, since ties will
be unlikely to arise. This is because, in domains where rewards are
drawn from any random distribution, the probability of a follower
having more than one pure optimal response to a given leader 
strategy approaches zero, and the leader will have only a finite number
of possible mixed strategies.
Our approach to characterize the optimal strategy for the security
agent makes use of properties of linear programming. We briefly
outline these results here for completeness, for detailed discussion
and proofs see one of many references on the topic, such as [2].
Every linear programming problem, such as:
max cT
x | Ax = b, x ≥ 0,
has an associated dual linear program, in this case:
min bT
y | AT
y ≥ c.
These primal/dual pairs of problems satisfy weak duality: For any x
and y primal and dual feasible solutions respectively, cT
x ≤ bT
y.
Thus a pair of feasible solutions is optimal if cT
x = bT
y, and
the problems are said to satisfy strong duality. In fact if a linear
program is feasible and has a bounded optimal solution, then the
dual is also feasible and there is a pair x∗
, y∗
that satisfies cT
x∗
=
bT
y∗
. These optimal solutions are characterized with the following
optimality conditions (as defined in [2]):
• primal feasibility: Ax = b, x ≥ 0
• dual feasibility: AT
y ≥ c
• complementary slackness: xi(AT
y − c)i = 0 for all i.
Note that this last condition implies that
cT
x = xT
AT
y = bT
y,
which proves optimality for primal dual feasible solutions x and y.
In the following subsections, we first define the problem in its
most intuititive form as a mixed-integer quadratic program (MIQP),
and then show how this problem can be converted into a 
mixedinteger linear program (MILP).
4.1 Mixed-Integer Quadratic Program
We begin with the case of a single type of follower. Let the
leader be the row player and the follower the column player. We
denote by x the vector of strategies of the leader and q the vector
of strategies of the follower. We also denote X and Q the index
sets of the leader and follower"s pure strategies, respectively. The
payoff matrices R and C correspond to: Rij is the reward of the
leader and Cij is the reward of the follower when the leader takes
pure strategy i and the follower takes pure strategy j. Let k be the
size of the multiset.
We first fix the policy of the leader to some k-uniform policy
x. The value xi is the number of times pure strategy i is used in
the k-uniform policy, which is selected with probability xi/k. We
formulate the optimization problem the follower solves to find its
optimal response to x as the following linear program:
max
X
j∈Q
X
i∈X
1
k
Cijxi qj
s.t.
P
j∈Q qj = 1
q ≥ 0.
(2)
The objective function maximizes the follower"s expected reward
given x, while the constraints make feasible any mixed strategy q
for the follower. The dual to this linear programming problem is
the following:
min a
s.t. a ≥
X
i∈X
1
k
Cijxi j ∈ Q. (3)
From strong duality and complementary slackness we obtain that
the follower"s maximum reward value, a, is the value of every pure
strategy with qj > 0, that is in the support of the optimal mixed
strategy. Therefore each of these pure strategies is optimal. 
Optimal solutions to the follower"s problem are characterized by linear
programming optimality conditions: primal feasibility constraints
in (2), dual feasibility constraints in (3), and complementary 
slackness
qj a −
X
i∈X
1
k
Cijxi
!
= 0 j ∈ Q.
These conditions must be included in the problem solved by the
leader in order to consider only best responses by the follower to
the k-uniform policy x.
314 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
The leader seeks the k-uniform solution x that maximizes its
own payoff, given that the follower uses an optimal response q(x).
Therefore the leader solves the following integer problem:
max
X
i∈X
X
j∈Q
1
k
Rijq(x)j xi
s.t.
P
i∈X xi = k
xi ∈ {0, 1, . . . , k}.
(4)
Problem (4) maximizes the leader"s reward with the follower"s best
response (qj for fixed leader"s policy x and hence denoted q(x)j)
by selecting a uniform policy from a multiset of constant size k. We
complete this problem by including the characterization of q(x)
through linear programming optimality conditions. To simplify
writing the complementary slackness conditions, we will constrain
q(x) to be only optimal pure strategies by just considering integer
solutions of q(x). The leader"s problem becomes:
maxx,q
X
i∈X
X
j∈Q
1
k
Rijxiqj
s.t.
P
i xi = kP
j∈Q qj = 1
0 ≤ (a −
P
i∈X
1
k
Cijxi) ≤ (1 − qj)M
xi ∈ {0, 1, ...., k}
qj ∈ {0, 1}.
(5)
Here, the constant M is some large number. The first and fourth
constraints enforce a k-uniform policy for the leader, and the 
second and fifth constraints enforce a feasible pure strategy for the
follower. The third constraint enforces dual feasibility of the 
follower"s problem (leftmost inequality) and the complementary 
slackness constraint for an optimal pure strategy q for the follower 
(rightmost inequality). In fact, since only one pure strategy can be 
selected by the follower, say qh = 1, this last constraint enforces that
a =
P
i∈X
1
k
Cihxi imposing no additional constraint for all other
pure strategies which have qj = 0.
We conclude this subsection noting that Problem (5) is an 
integer program with a non-convex quadratic objective in general,
as the matrix R need not be positive-semi-definite. Efficient 
solution methods for non-linear, non-convex integer problems remains
a challenging research question. In the next section we show a 
reformulation of this problem as a linear integer programming 
problem, for which a number of efficient commercial solvers exist.
4.2 Mixed-Integer Linear Program
We can linearize the quadratic program of Problem 5 through the
change of variables zij = xiqj, obtaining the following problem
maxq,z
P
i∈X
P
j∈Q
1
k
Rijzij
s.t.
P
i∈X
P
j∈Q zij = k
P
j∈Q zij ≤ k
kqj ≤
P
i∈X zij ≤ k
P
j∈Q qj = 1
0 ≤ (a −
P
i∈X
1
k
Cij(
P
h∈Q zih)) ≤ (1 − qj)M
zij ∈ {0, 1, ...., k}
qj ∈ {0, 1}
(6)
PROPOSITION 1. Problems (5) and (6) are equivalent.
Proof: Consider x, q a feasible solution of (5). We will show
that q, zij = xiqj is a feasible solution of (6) of same objective
function value. The equivalence of the objective functions, and
constraints 4, 6 and 7 of (6) are satisfied by construction. The fact
that
P
j∈Q zij = xi as
P
j∈Q qj = 1 explains constraints 1, 2, and
5 of (6). Constraint 3 of (6) is satisfied because
P
i∈X zij = kqj.
Let us now consider q, z feasible for (6). We will show that q and
xi =
P
j∈Q zij are feasible for (5) with the same objective value.
In fact all constraints of (5) are readily satisfied by construction. To
see that the objectives match, notice that if qh = 1 then the third
constraint in (6) implies that
P
i∈X zih = k, which means that
zij = 0 for all i ∈ X and all j = h. Therefore,
xiqj =
X
l∈Q
zilqj = zihqj = zij.
This last equality is because both are 0 when j = h. This shows
that the transformation preserves the objective function value, 
completing the proof.
Given this transformation to a mixed-integer linear program (MILP),
we now show how we can apply our decomposition technique on
the MILP to obtain significant speedups for Bayesian games with
multiple follower types.
5. DECOMPOSITION FOR MULTIPLE 
ADVERSARIES
The MILP developed in the previous section handles only one
follower. Since our security scenario contains multiple follower
(robber) types, we change the response function for the follower
from a pure strategy into a weighted combination over various pure
follower strategies where the weights are probabilities of 
occurrence of each of the follower types.
5.1 Decomposed MIQP
To admit multiple adversaries in our framework, we modify the
notation defined in the previous section to reason about multiple
follower types. We denote by x the vector of strategies of the leader
and ql
the vector of strategies of follower l, with L denoting the 
index set of follower types. We also denote by X and Q the index
sets of leader and follower l"s pure strategies, respectively. We also
index the payoff matrices on each follower l, considering the 
matrices Rl
and Cl
.
Using this modified notation, we characterize the optimal 
solution of follower l"s problem given the leaders k-uniform policy x,
with the following optimality conditions:
X
j∈Q
ql
j = 1
al
−
X
i∈X
1
k
Cl
ijxi ≥ 0
ql
j(al
−
X
i∈X
1
k
Cl
ijxi) = 0
ql
j ≥ 0
Again, considering only optimal pure strategies for follower l"s
problem we can linearize the complementarity constraint above.
We incorporate these constraints on the leader"s problem that 
selects the optimal k-uniform policy. Therefore, given a priori 
probabilities pl
, with l ∈ L of facing each follower, the leader solves
the following problem:
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315
maxx,q
X
i∈X
X
l∈L
X
j∈Q
pl
k
Rl
ijxiql
j
s.t.
P
i xi = kP
j∈Q ql
j = 1
0 ≤ (al
−
P
i∈X
1
k
Cl
ijxi) ≤ (1 − ql
j)M
xi ∈ {0, 1, ...., k}
ql
j ∈ {0, 1}.
(7)
Problem (7) for a Bayesian game with multiple follower types
is indeed equivalent to Problem (5) on the payoff matrix obtained
from the Harsanyi transformation of the game. In fact, every pure
strategy j in Problem (5) corresponds to a sequence of pure 
strategies jl, one for each follower l ∈ L. This means that qj = 1 if
and only if ql
jl
= 1 for all l ∈ L. In addition, given the a 
priori probabilities pl
of facing player l, the reward in the Harsanyi
transformation payoff table is Rij =
P
l∈L pl
Rl
ijl
. The same 
relation holds between C and Cl
. These relations between a pure
strategy in the equivalent normal form game and pure strategies in
the individual games with each followers are key in showing these
problems are equivalent.
5.2 Decomposed MILP
We can linearize the quadratic programming problem 7 through
the change of variables zl
ij = xiql
j, obtaining the following 
problem
maxq,z
P
i∈X
P
l∈L
P
j∈Q
pl
k
Rl
ijzl
ij
s.t.
P
i∈X
P
j∈Q zl
ij = k
P
j∈Q zl
ij ≤ k
kql
j ≤
P
i∈X zl
ij ≤ k
P
j∈Q ql
j = 1
0 ≤ (al
−
P
i∈X
1
k
Cl
ij(
P
h∈Q zl
ih)) ≤ (1 − ql
j)M
P
j∈Q zl
ij =
P
j∈Q z1
ij
zl
ij ∈ {0, 1, ...., k}
ql
j ∈ {0, 1}
(8)
PROPOSITION 2. Problems (7) and (8) are equivalent.
Proof: Consider x, ql
, al
with l ∈ L a feasible solution of (7).
We will show that ql
, al
, zl
ij = xiql
j is a feasible solution of (8)
of same objective function value. The equivalence of the objective
functions, and constraints 4, 7 and 8 of (8) are satisfied by 
construction. The fact that
P
j∈Q zl
ij = xi as
P
j∈Q ql
j = 1 explains
constraints 1, 2, 5 and 6 of (8). Constraint 3 of (8) is satisfied 
because
P
i∈X zl
ij = kql
j.
Lets now consider ql
, zl
, al
feasible for (8). We will show that
ql
, al
and xi =
P
j∈Q z1
ij are feasible for (7) with the same 
objective value. In fact all constraints of (7) are readily satisfied by
construction. To see that the objectives match, notice for each l
one ql
j must equal 1 and the rest equal 0. Let us say that ql
jl
= 1,
then the third constraint in (8) implies that
P
i∈X zl
ijl
= k, which
means that zl
ij = 0 for all i ∈ X and all j = jl. In particular this
implies that
xi =
X
j∈Q
z1
ij = z1
ij1
= zl
ijl
,
the last equality from constraint 6 of (8). Therefore xiql
j = zl
ijl
ql
j =
zl
ij. This last equality is because both are 0 when j = jl. 
Effectively, constraint 6 ensures that all the adversaries are calculating
their best responses against a particular fixed policy of the agent.
This shows that the transformation preserves the objective function
value, completing the proof.
We can therefore solve this equivalent linear integer program
with efficient integer programming packages which can handle 
problems with thousands of integer variables. We implemented the 
decomposed MILP and the results are shown in the following section.
6. EXPERIMENTAL RESULTS
The patrolling domain and the payoffs for the associated game
are detailed in Sections 2 and 3. We performed experiments for this
game in worlds of three and four houses with patrols consisting of
two houses. The description given in Section 2 is used to generate
a base case for both the security agent and robber payoff functions.
The payoff tables for additional robber types are constructed and
added to the game by adding a random distribution of varying size
to the payoffs in the base case. All games are normalized so that,
for each robber type, the minimum and maximum payoffs to the
security agent and robber are 0 and 1, respectively.
Using the data generated, we performed the experiments using
four methods for generating the security agent"s strategy:
• uniform randomization
• ASAP
• the multiple linear programs method from [5] (to find the true
optimal strategy)
• the highest reward Bayes-Nash equilibrium, found using the
MIP-Nash algorithm [17]
The last three methods were applied using CPLEX 8.1. Because
the last two methods are designed for normal-form games rather
than Bayesian games, the games were first converted using the
Harsanyi transformation [8]. The uniform randomization method is
simply choosing a uniform random policy over all possible patrol
routes. We use this method as a simple baseline to measure the 
performance of our heuristics. We anticipated that the uniform policy
would perform reasonably well since maximum-entropy policies
have been shown to be effective in multiagent security domains
[14]. The highest-reward Bayes-Nash equilibria were used in order
to demonstrate the higher reward gained by looking for an optimal
policy rather than an equilibria in Stackelberg games such as our
security domain.
Based on our experiments we present three sets of graphs to
demonstrate (1) the runtime of ASAP compared to other common
methods for finding a strategy, (2) the reward guaranteed by ASAP
compared to other methods, and (3) the effect of varying the 
parameter k, the size of the multiset, on the performance of ASAP.
In the first two sets of graphs, ASAP is run using a multiset of
80 elements; in the third set this number is varied. The first set of
graphs, shown in Figure 1 shows the runtime graphs for three-house
(left column) and four-house (right column) domains. Each of the
three rows of graphs corresponds to a different randomly-generated
scenario. The x-axis shows the number of robber types the 
security agent faces and the y-axis of the graph shows the runtime in
seconds. All experiments that were not concluded in 30 minutes
(1800 seconds) were cut off. The runtime for the uniform policy
is always negligible irrespective of the number of adversaries and
hence is not shown.
The ASAP algorithm clearly outperforms the optimal, 
multipleLP method as well as the MIP-Nash algorithm for finding the 
highestreward Bayes-Nash equilibrium with respect to runtime. For a
316 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 1: Runtimes for various algorithms on problems of 3
and 4 houses.
domain of three houses, the optimal method cannot reach a 
solution for more than seven robber types, and for four houses it 
cannot solve for more than six types within the cutoff time in any of
the three scenarios. MIP-Nash solves for even fewer robber types
within the cutoff time. On the other hand, ASAP runs much faster,
and is able to solve for at least 20 adversaries for the three-house
scenarios and for at least 12 adversaries in the four-house 
scenarios within the cutoff time. The runtime of ASAP does not increase
strictly with the number of robber types for each scenario, but in
general, the addition of more types increases the runtime required.
The second set of graphs, Figure 2, shows the reward to the patrol
agent given by each method for three scenarios in the three-house
(left column) and four-house (right column) domains. This reward
is the utility received by the security agent in the patrolling game,
and not as a percentage of the optimal reward, since it was not 
possible to obtain the optimal reward as the number of robber types
increased. The uniform policy consistently provides the lowest 
reward in both domains; while the optimal method of course 
produces the optimal reward. The ASAP method remains consistently
close to the optimal, even as the number of robber types increases.
The highest-reward Bayes-Nash equilibria, provided by the 
MIPNash method, produced rewards higher than the uniform method,
but lower than ASAP. This difference clearly illustrates the gains in
the patrolling domain from committing to a strategy as the leader
in a Stackelberg game, rather than playing a standard Bayes-Nash
strategy.
The third set of graphs, shown in Figure 3 shows the effect of the
multiset size on runtime in seconds (left column) and reward (right
column), again expressed as the reward received by the security
agent in the patrolling game, and not a percentage of the optimal
Figure 2: Reward for various algorithms on problems of 3 and
4 houses.
reward. Results here are for the three-house domain. The trend is
that as as the multiset size is increased, the runtime and reward level
both increase. Not surprisingly, the reward increases monotonically
as the multiset size increases, but what is interesting is that there is
relatively little benefit to using a large multiset in this domain. In
all cases, the reward given by a multiset of 10 elements was within
at least 96% of the reward given by an 80-element multiset. The
runtime does not always increase strictly with the multiset size;
indeed in one example (scenario 2 with 20 robber types), using a
multiset of 10 elements took 1228 seconds, while using 80 elements
only took 617 seconds. In general, runtime should increase since a
larger multiset means a larger domain for the variables in the MILP,
and thus a larger search space. However, an increase in the number
of variables can sometimes allow for a policy to be constructed
more quickly due to more flexibility in the problem.
7. SUMMARY AND RELATED WORK
This paper focuses on security for agents patrolling in hostile 
environments. In these environments, intentional threats are caused
by adversaries about whom the security patrolling agents have 
incomplete information. Specifically, we deal with situations where
the adversaries" actions and payoffs are known but the exact 
adversary type is unknown to the security agent. Agents acting in the
real world quite frequently have such incomplete information about
other agents. Bayesian games have been a popular choice to model
such incomplete information games [3]. The Gala toolkit is one
method for defining such games [9] without requiring the game to
be represented in normal form via the Harsanyi transformation [8];
Gala"s guarantees are focused on fully competitive games. Much
work has been done on finding optimal Bayes-Nash equilbria for
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317
Figure 3: Reward for ASAP using multisets of 10, 30, and 80
elements
subclasses of Bayesian games, finding single Bayes-Nash 
equilibria for general Bayesian games [10] or approximate Bayes-Nash
equilibria [18]. Less attention has been paid to finding the optimal
strategy to commit to in a Bayesian game (the Stackelberg scenario
[15]). However, the complexity of this problem was shown to be
NP-hard in the general case [5], which also provides algorithms for
this problem in the non-Bayesian case.
Therefore, we present a heuristic called ASAP, with three key
advantages towards addressing this problem. First, ASAP searches
for the highest reward strategy, rather than a Bayes-Nash 
equilibrium, allowing it to find feasible strategies that exploit the 
natural first-mover advantage of the game. Second, it provides 
strategies which are simple to understand, represent, and implement.
Third, it operates directly on the compact, Bayesian game 
representation, without requiring conversion to normal form. We provide
an efficient Mixed Integer Linear Program (MILP) implementation
for ASAP, along with experimental results illustrating significant
speedups and higher rewards over other approaches.
Our k-uniform strategies are similar to the k-uniform strategies
of [12]. While that work provides epsilon error-bounds based on
the k-uniform strategies, their solution concept is still that of a
Nash equilibrium, and they do not provide efficient algorithms for
obtaining such k-uniform strategies. This contrasts with ASAP,
where our emphasis is on a highly efficient heuristic approach that
is not focused on equilibrium solutions.
Finally the patrolling problem which motivated our work has 
recently received growing attention from the multiagent community
due to its wide range of applications [4, 13]. However most of this
work is focused on either limiting energy consumption involved in
patrolling [7] or optimizing on criteria like the length of the path
traveled [4, 13], without reasoning about any explicit model of an
adversary[14].
Acknowledgments : This research is supported by the United States
Department of Homeland Security through Center for Risk and Economic
Analysis of Terrorism Events (CREATE). It is also supported by the 
Defense Advanced Research Projects Agency (DARPA), through the 
Department of the Interior, NBC, Acquisition Services Division, under Contract
No. NBCHD030010. Sarit Kraus is also affiliated with UMIACS.
8. REFERENCES
[1] R. W. Beard and T. McLain. Multiple UAV cooperative
search under collision avoidance and limited range
communication constraints. In IEEE CDC, 2003.
[2] D. Bertsimas and J. Tsitsiklis. Introduction to Linear
Optimization. Athena Scientific, 1997.
[3] J. Brynielsson and S. Arnborg. Bayesian games for threat
prediction and situation analysis. In FUSION, 2004.
[4] Y. Chevaleyre. Theoretical analysis of multi-agent patrolling
problem. In AAMAS, 2004.
[5] V. Conitzer and T. Sandholm. Choosing the best strategy to
commit to. In ACM Conference on Electronic Commerce,
2006.
[6] D. Fudenberg and J. Tirole. Game Theory. MIT Press, 1991.
[7] C. Gui and P. Mohapatra. Virtual patrol: A new power
conservation design for surveillance using sensor networks.
In IPSN, 2005.
[8] J. C. Harsanyi and R. Selten. A generalized Nash solution for
two-person bargaining games with incomplete information.
Management Science, 18(5):80-106, 1972.
[9] D. Koller and A. Pfeffer. Generating and solving imperfect
information games. In IJCAI, pages 1185-1193, 1995.
[10] D. Koller and A. Pfeffer. Representations and solutions for
game-theoretic problems. Artificial Intelligence,
94(1):167-215, 1997.
[11] C. Lemke and J. Howson. Equilibrium points of bimatrix
games. Journal of the Society for Industrial and Applied
Mathematics, 12:413-423, 1964.
[12] R. J. Lipton, E. Markakis, and A. Mehta. Playing large
games using simple strategies. In ACM Conference on
Electronic Commerce, 2003.
[13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.
Multi-agent patrolling: an empirical analysis on alternative
architectures. In MABS, 2002.
[14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus. Security
in multiagent systems by policy randomization. In AAMAS,
2006.
[15] T. Roughgarden. Stackelberg scheduling strategies. In ACM
Symposium on TOC, 2001.
[16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.
Patrolling in a stochastic environment. In 10th Intl.
Command and Control Research Symp., 2005.
[17] T. Sandholm, A. Gilpin, and V. Conitzer. Mixed-integer
programming methods for finding nash equilibria. In AAAI,
2005.
[18] S. Singh, V. Soni, and M. Wellman. Computing approximate
Bayes-Nash equilibria with tree-games of incomplete
information. In ACM Conference on Electronic Commerce,
2004.
318 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Searching for Joint Gains in Automated Negotiations
Based on Multi-criteria Decision Making Theory
Quoc Bao Vo
School of Computer Science and IT
RMIT University, Australia
vqbao@cs.rmit.edu.au
Lin Padgham
School of Computer Science and IT
RMIT University, Australia
linpa@cs.rmit.edu.au
ABSTRACT
It is well established by conflict theorists and others that successful
negotiation should incorporate creating value as well as 
claiming value. Joint improvements that bring benefits to all parties
can be realised by (i) identifying attributes that are not of direct
conflict between the parties, (ii) tradeoffs on attributes that are 
valued differently by different parties, and (iii) searching for values
within attributes that could bring more gains to one party while not
incurring too much loss on the other party. In this paper we 
propose an approach for maximising joint gains in automated 
negotiations by formulating the negotiation problem as a multi-criteria
decision making problem and taking advantage of several 
optimisation techniques introduced by operations researchers and conflict
theorists. We use a mediator to protect the negotiating parties from
unnecessary disclosure of information to their opponent, while also
allowing an objective calculation of maximum joint gains. We 
separate out attributes that take a finite set of values (simple attributes)
from those with continuous values, and we show that for simple
attributes, the mediator can determine the Pareto-optimal values.
In addition we show that if none of the simple attributes strongly
dominates the other simple attributes, then truth telling is an 
equilibrium strategy for negotiators during the optimisation of simple
attributes. We also describe an approach for improving joint gains
on non-simple attributes, by moving the parties in a series of steps,
towards the Pareto-optimal frontier.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems;
K.4.4 [Computers and Society]: Electronic Commerce
General Terms
Algorithms, Design
1. INTRODUCTION
Given that negotiation is perhaps one of the oldest activities in
the history of human communication, it"s perhaps surprising that
conducted experiments on negotiations have shown that negotiators
more often than not reach inefficient compromises [1, 21]. Raiffa
[17] and Sebenius [20] provide analyses on the negotiators" failure
to achieve efficient agreements in practice and their unwillingness
to disclose private information due to strategic reasons. According
to conflict theorists Lax and Sebenius [13], most negotiation 
actually involves both integrative and distributive bargaining which
they refer to as creating value and claiming value. They argue
that negotiation necessarily includes both cooperative and 
competitive elements, and that these elements exist in tension. Negotiators
face a dilemma in deciding whether to pursue a cooperative or a
competitive strategy at a particular time during a negotiation. They
refer to this problem as the Negotiator"s Dilemma.
We argue that the Negotiator"s Dilemma is essentially 
informationbased, due to the private information held by the agents. Such
private information contains both the information that implies the
agent"s bottom lines (or, her walk-away positions) and the 
information that enforces her bargaining strength. For instance, when
bargaining to sell a house to a potential buyer, the seller would
try to hide her actual reserve price as much as possible for she
hopes to reach an agreement at a much higher price than her 
reserve price. On the other hand, the outside options available to her
(e.g. other buyers who have expressed genuine interest with fairly
good offers) consist in the information that improves her 
bargaining strength about which she would like to convey to her opponent.
But at the same time, her opponent is well aware of the fact that it
is her incentive to boost her bargaining strength and thus will not
accept every information she sends out unless it is substantiated by
evidence.
Coming back to the Negotiator"s Dilemma, it"s not always 
possible to separate the integrative bargaining process from the 
distributive bargaining process. In fact, more often than not, the two 
processes interplay with each other making information manipulation
become part of the integrative bargaining process. This is because a
negotiator could use the information about his opponent"s interests
against her during the distributive negotiation process. That is, a
negotiator may refuse to concede on an important conflicting issue
by claiming that he has made a major concession (on another 
issue) to meet his opponent"s interests even though the concession he
made could be insignificant to him. For instance, few buyers would
start a bargaining with a dealer over a deal for a notebook computer
by declaring that he is most interested in an extended warranty for
the item and therefore prepared to pay a high price to get such an
extended warranty.
Negotiation Support Systems (NSSs) and negotiating software
508
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
agents (NSAs) have been introduced either to assist humans in
making decisions or to enable automated negotiation to allow 
computer processes to engage in meaningful negotiation to reach 
agreements (see, for instance, [14, 15, 19, 6, 5]). However, because of
the Negotiator"s Dilemma and given even bargaining power and
incomplete information, the following two undesirable situations
often arise: (i) negotiators reach inefficient compromises, or (ii)
negotiators engage in a deadlock situation in which both 
negotiators refuse to act upon with incomplete information and at the same
time do not want to disclose more information.
In this paper, we argue for the role of a mediator to resolve the
above two issues. The mediator thus plays two roles in a 
negotiation: (i) to encourage cooperative behaviour among the negotiators,
and (ii) to absorb the information disclosure by the negotiators to
prevent negotiators from using uncertainty and private information
as a strategic device. To take advantage of existing results in 
negotiation analysis and operations research (OR) literatures [18], we
employ multi-criteria decision making (MCDM) theory to allow
the negotiation problem to be represented and analysed. Section 2
provides background on MCDM theory and the negotiation 
framework. Section 3 formulates the problem. In Section 4, we discuss
our approach to integrative negotiation. Section 5 discusses the 
future work with some concluding remarks.
2. BACKGROUND
2.1 Multi-criteria decision making theory
Let A denote the set of feasible alternatives available to a 
decision maker M. As an act, or decision, a in A may involve 
multiple aspects, we usually describe the alternatives a with a set of
attributes j; (j = 1, . . . , m). (Attributes are also referred to as 
issues, or decision variables.) A typical decision maker also has 
several objectives X1, . . . , Xk. We assume that Xi, (i = 1, . . . , k),
maps the alternatives to real numbers. Thus, a tuple (x1, . . . , xk) =
(X1(a), . . . , Xk(a)) denotes the consequence of the act a to the
decision maker M. By definition, objectives are statements that
delineate the desires of a decision maker. Thus, M wishes to 
maximise his objectives. However, as discussed thoroughly by Keeney
and Raiffa [8], it is quite likely that a decision maker"s objectives
will conflict with each other in that the improved achievement with
one objective can only be accomplished at the expense of another.
For instance, most businesses and public services have objectives
like minimise cost and maximise the quality of services. Since
better services can often only be attained for a price, these 
objectives conflict.
Due to the conflicting nature of a decision maker"s objectives, M
usually has to settle at a compromise solution. That is, he may have
to choose an act a ∈ A that does not optimise every objective. This
is the topic of the multi-criteria decision making theory. Part of the
solution to this problem is that M has to try to identify the Pareto
frontier in the consequence space {(X1(a), . . . , Xk(a))}a∈A.
DEFINITION 1. (Dominant)
Let x = (x1, . . . , xk) and x = (x1, . . . , xk) be two 
consequences. x dominates x iff xi > xi for all i, and the inequality is
strict for at least one i.
The Pareto frontier in a consequence space then consists of all
consequences that are not dominated by any other consequence.
This is illustrated in Fig. 1 in which an alternative consists of two
attributes d1 and d2 and the decision maker tries to maximise the
two objectives X1 and X2. A decision a ∈ A whose consequence
does not lie on the Pareto frontier is inefficient. While the Pareto
1x
d2
a (X (a),X (a))
d1
1
x2
2
Alternative spaceA
Pareto frontier
Consequence space
optimal consequenc
Figure 1: The Pareto frontier
frontier allows M to avoid taking inefficient decisions, M still has
to decide which of the efficient consequences on the Pareto frontier
is most preferred by him.
MCDM theorists introduce a mechanism to allow the objective
components of consequences to be normalised to the payoff 
valuations for the objectives. Consequences can then be ordered: if the
gains in satisfaction brought about by C1 (in comparison to C2)
equals to the losses in satisfaction brought about by C1 (in 
comparison to C2), then the two consequences C1 and C2 are considered
indifferent. M can now construct the set of indifference curves1
in
the consequence space (the dashed curves in Fig. 1). The most 
preferred indifference curve that intersects with the Pareto frontier is
in focus: its intersection with the Pareto frontier is the sought after
consequence (i.e., the optimal consequence in Fig. 1).
2.2 A negotiation framework
A multi-agent negotiation framework consists of:
1. A set of two negotiating agents N = {1, 2}.
2. A set of attributes Att = {α1, . . . , αm} characterising the 
issues the agents are negotiating over. Each attribute α can take a
value from the set V alα;
3. A set of alternative outcomes O. An outcome o ∈ O is 
represented by an assignment of values to the corresponding attributes
in Att.
4. Agents" utility: Based on the theory of multiple-criteria decision
making [8], we define the agents" utility as follows:
• Objectives: Agent i has a set of ni objectives, or interests;
denoted by j (j = 1, . . . , ni). To measure how much an 
outcome o fulfills an objective j to an agent i, we use objective
functions: for each agent i, we define i"s interests using the
objective vector function fi = [fij ] : O → Rni
.
• Value functions: Instead of directly evaluating an outcome o,
agent i looks at how much his objectives are fulfilled and will
make a valuation based on these more basic criteria. Thus,
for each agent i, there is a value function σi : Rni
→ R.
In particular, Raiffa [17] shows how to systematically 
construct an additive value function to each party involved in a
negotiation.
• Utility: Now, given an outcome o ∈ O, an agent i is able
to determine its value, i.e., σi(fi(o)). However, a 
negotiation infrastructure is usually required to facilitate negotiation.
This might involve other mechanisms and factors/parties, e.g.,
a mediator, a legal institution, participation fees, etc. The
standard way to implement such a thing is to allow money
1
In fact, given the k-dimensional space, these should be called 
indifference surfaces. However, we will not bog down to that level of
details.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 509
and side-payments. In this paper, we ignore those side-effects
and assume that agent i"s utility function ui is normalised so
that ui : O → [0, 1].
EXAMPLE 1. There are two agents, A and B. Agent A has
a task T that needs to be done and also 100 units of a resource
R. Agent B has the capacity to perform task T and would like to
obtain at least 10 and at most 20 units of the resource R. Agent B is
indifferent on any amount between 10 and 20 units of the resource
R. The objective functions for both agents A and B are cost and
revenue. And they both aim at minimising costs while maximising
revenues. Having T done generates for A a revenue rA,T while
doing T incurs a cost cB,T to B. Agent B obtains a revenue rB,R
for each unit of the resource R while providing each unit of the
resource R costs agent A cA,R.
Assuming that money transfer between agents is possible, the set
Att then contains three attributes:
• T, taking values from the set {0, 1}, indicates whether the
task T is assigned to agent B;
• R, taking values from the set of non-negative integer, 
indicates the amount of resource R being allocated to agent B;
and
• MT, taking values from R, indicates the payment p to be
transferred from A to B.
Consider the outcome o = [T = 1, R = k, MT = p], i.e., the
task T is assigned to B, and A allocates to B with k units of the
resource R, and A transfers p dollars to B. Then, costA(o) =
k.cA,R + p and revA(o) = rA,T ; and costB(o) = cB,T and
revA(o) =
j
k.rB,R + p if 10 ≤ k ≤ 20
p otherwise.
And, σi(costi(o), revi(o)) = revi(o) − costi(o), (i = A, B).
3. PROBLEM FORMALISATION
Consider Example 1, assume that rA,T = $150 and cB,T =
$100 and rB,R = $10 and cA,R = $7. That is, the revenues 
generated for A exceeds the costs incurred to B to do task T, and B
values resource R more highly than the cost for A to provide it.
The optimal solution to this problem scenario is to assign task T to
agent B and to allocate 20 units of resource R (i.e., the maximal
amount of resource R required by agent B) from agent A to agent
B. This outcome regarding the resource and task allocation 
problems leaves payoffs of $10 to agent A and $100 to agent B.2
Any
other outcome would leave at least one of the agents worse off. In
other words, the presented outcome is Pareto-efficient and should
be part of the solution outcome for this problem scenario.
However, as the agents still have to bargain over the amount of
money transfer p, neither agent would be willing to disclose their
respective costs and revenues regarding the task T and the resource
R. As a consequence, agents often do not achieve the optimal 
outcome presented above in practice. To address this issue, we 
introduce a mediator to help the agents discover better agreements than
the ones they might try to settle on. Note that this problem is 
essentially the problem of searching for joint gains in a multilateral
negotiation in which the involved parties hold strategic information,
i.e., the integrative part in a negotiation. In order to help facilitate
this process, we introduce the role of a neutral mediator. Before
formalising the decision problems faced by the mediator and the
2
Certainly, without money transfer to compensate agent A, this
outcome is not a fair one.
negotiating agents, we discuss the properties of the solution 
outcomes to be achieved by the mediator. In a negotiation setting, the
two typical design goals would be:
• Efficiency: Avoid the agents from settling on an outcome that
is not Pareto-optimal; and
• Fairness: Avoid agreements that give the most of the gains
to a subset of agents while leaving the rest with too little.
The above goals are axiomatised in Nash"s seminal work [16] on
cooperative negotiation games. Essentially, Nash advocates for the
following properties to be satisfied by solution to the bilateral 
negotiation problem: (i) it produces only Pareto-optimal outcomes; (ii)
it is invariant to affine transformation (to the consequence space);
(iii) it is symmetric; and (iv) it is independent from irrelevant 
alternatives. A solution satisfying Nash"s axioms is called a Nash
bargaining solution.
It then turns out that, by taking the negotiators" utilities as its
objectives the mediator itself faces a multi-criteria decision making
problem. The issues faced by the mediator are: (i) the mediator
requires access to the negotiators" utility functions, and (ii) 
making (fair) tradeoffs between different agents" utilities. Our methods
allow the agents to repeatedly interact with the mediator so that a
Nash solution outcome could be found by the parties.
Informally, the problem faced by both the mediator and the 
negotiators is construction of the indifference curves. Why are the
indifference curves so important?
• To the negotiators, knowing the options available along 
indifference curves opens up opportunities to reach more 
efficient outcomes. For instance, consider an agent A who is
presenting his opponent with an offer θA which she refuses
to accept. Rather than having to concede, A could look at
his indifference curve going through θA and choose another
proposal θA. To him, θA and θA are indifferent but θA could
give some gains to B and thus will be more acceptable to B.
In other words, the outcome θA is more efficient than θA to
these two negotiators.
• To the mediator, constructing indifference curves requires a
measure of fairness between the negotiators. The mediator
needs to determine how much utility it needs to take away
from the other negotiators to give a particular negotiator a
specific gain G (in utility).
In order to search for integrative solutions within the outcome
space O, we characterise the relationship between the agents over
the set of attributes Att. As the agents hold different objectives and
have different capacities, it may be the case that changing between
two values of a specific attribute implies different shifts in utility
of the agents. However, the problem of finding the exact 
Paretooptimal set3
is NP-hard [2].
Our approach is thus to solve this optimisation problem in two
steps. In the first steps, the more manageable attributes will be
solved. These are attributes that take a finite set of values. The
result of this step would be a subset of outcomes that contains the
Pareto-optimal set. In the second step, we employ an iterative 
procedure that allows the mediator to interact with the negotiators to
find joint improvements that move towards a Pareto-optimal 
outcome. This approach will not work unless the attributes from Att
3
The Pareto-optimal set is the set of outcomes whose consequences
(in the consequence space) correspond to the Pareto frontier.
510 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
are independent. Most works on multi-attribute, or multi-issue, 
negotiation (e.g. [17]) assume that the attributes or the issues are 
independent, resulting in an additive value function for each agent.4
ASSUMPTION 1. Let i ∈ N and S ⊆ Att. Denote by ¯S the
set Att \ S. Assume that vS and vS are two assignments of values
to the attributes of S and v1
¯S, v2
¯S are two arbitrary value 
assignments to the attributes of ¯S, then (ui([vS, v1
¯S]) − ui([vS, v2
¯S])) =
(ui([vS, v1
¯S])−ui([vS, v2
¯S])). That is, the utility function of agent i
will be defined on the attributes from S independently of any value
assignment to other attributes.
4. MEDIATOR-BASED BILATERAL 
NEGOTIATIONS
As discussed by Lax and Sebenius [13], under incomplete 
information the tension between creating and claiming values is the
primary cause of inefficient outcomes. This can be seen most 
easily in negotiations involving two negotiators; during the 
distributive phase of the negotiation, the two negotiators"s objectives are
directly opposing each other. We will now formally characterise
this relationship between negotiators by defining the opposition 
between two negotiating parties. The following exposition will be
mainly reproduced from [9].
Assuming for the moment that all attributes from Att take values
from the set of real numbers R, i.e., V alj ⊆ R for all j ∈ Att. We
further assume that the set O = ×j∈AttV alj of feasible outcomes
is defined by constraints that all parties must obey and O is convex.
Now, an outcome o ∈ O is just a point in the m-dimensional space
of real numbers. Then, the questions are: (i) from the point of view
of an agent i, is o already the best outcome for i? (ii) if o is not
the best outcome for i then is there another outcome o such that o
gives i a better utility than o and o does not cause a utility loss to
the other agent j in comparison to o?
The above questions can be answered by looking at the directions
of improvement of the negotiating parties at o, i.e., the directions
in the outcome space O into which their utilities increase at point
o. Under the assumption that the parties" utility functions ui are
differentiable concave, the set of all directions of improvement for
a party at a point o can be defined in terms of his most preferred,
or gradient, direction at that point. When the gradient direction
∇ui(o) of agent i at point o is outright opposing to the gradient
direction ∇uj (o) of agent j at point o then the two parties strongly
disagree at o and no joint improvements can be achieved for i and
j in the locality surrounding o.
Since opposition between the two parties can vary considerably
over the outcome space (with one pair of outcomes considered
highly antagonistic and another pair being highly cooperative), we
need to describe the local properties of the relationship. We begin
with the opposition at any point of the outcome space Rm
. The
following definition is reproduced from [9]:
DEFINITION 2. 1. The parties are in local strict opposition
at a point x ∈ Rm
iff for all points x ∈ Rm
that are 
sufficiently close to x (i.e., for some > 0 such that
∀x x −x < ), an increase of one utility can be achieved
only at the expense of a decrease of the other utility.
2. The parties are in local non-strict opposition at a point x ∈
Rm
iff they are not in local strict opposition at x, i.e., iff it is
possible for both parties to raise their utilities by moving an
infinitesimal distance from x.
4
Klein et al. [10] explore several implications of complex contracts
in which attributes are possibly inter-dependent.
3. The parties are in local weak opposition at a point x ∈ Rm
iff ∇u1(x).∇u2(x) ≥ 0, i.e., iff the gradients at x of the two
utility functions form an acute or right angle.
4. The parties are in local strong opposition at a point x ∈ Rm
iff ∇u1(x).∇u2(x) < 0, i.e., iff the gradients at x form an
obtuse angle.
5. The parties are in global strict (nonstrict, weak, strong) 
opposition iff for every x ∈ Rm
they are in local strict 
(nonstrict, weak, strong) opposition.
Global strict and nonstrict oppositions are complementary cases.
Essentially, under global strict opposition the whole outcome space
O becomes the Pareto-optimal set as at no point in O can the 
negotiating parties make a joint improvement, i.e., every point in O
is a Pareto-efficient outcome. In other words, under global strict
opposition the outcome space O can be flattened out into a single
line such that for each pair of outcomes x, y ∈ O, u1(x) < u1(y)
iff u2(x) > u2(y), i.e., at every point in O, the gradient of the two
utility functions point to two different ends of the line.
Intuitively, global strict opposition implies that there is no way to
obtain joint improvements for both agents. As a consequence, the
negotiation degenerates to a distributive negotiation, i.e., the 
negotiating parties should try to claim as much shares from the 
negotiation issues as possible while the mediator should aim for the 
fairness of the division. On the other hand, global nonstrict opposition
allows room for joint improvements and all parties might be better
off trying to realise the potential gains by reaching Pareto-efficient
agreements. Weak and strong oppositions indicate different levels
of opposition. The weaker the opposition, the more potential gains
can be realised making cooperation the better strategy to employ
during negotiation. On the other hand, stronger opposition 
suggests that the negotiating parties tend to behave strategically 
leading to misrepresentation of their respective objectives and utility
functions and making joint gains more difficult to realise.
We have been temporarily making the assumption that the 
outcome space O is the subset of Rm
. In many real-world 
negotiations, this assumption would be too restrictive. We will continue
our exposition by lifting this restriction and allowing discrete 
attributes. However, as most negotiations involve only discrete 
issues with a bounded number of options, we will assume that each
attribute takes values either from a finite set or from the set of real
numbers R. In the rest of the paper, we will refer to attributes whose
values are from finite sets as simple attributes and attributes whose
values are from R as continuous attributes. The notions of local
oppositions, i.e., strict, nonstrict, weak and strong, are not 
applicable to outcome spaces that contain simple attributes and nor are the
notions of global weak and strong oppositions. However, the 
notions of global strict and nonstrict oppositions can be generalised
for outcome spaces that contain simple attributes.
DEFINITION 3. Given an outcome space O, the parties are in
global strict opposition iff ∀x, y ∈ O, u1(x) < u1(y) iff u2(x) >
u2(y).
The parties are in global nonstrict opposition if they are not in
global strict opposition.
4.1 Optimisation on simple attributes
In order to extract the optimal values for a subset of attributes,
in the first step of this optimisation process the mediator requests
the negotiators to submit their respective utility functions over the
set of simple attributes. Let Simp ⊆ Att denote the set of all 
simple attributes from Att. Note that, due to Assumption 1, agent i"s
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 511
utility function can be characterised as follows:
ui([vSimp, vSimp]) = wi
1 ∗ ui,1([vSimp]) + wi
2 ∗ ui,2([vSimp]),
where Simp = Att \ Simp, and ui,1 and ui,2 are the utility 
components of ui over the sets of attributes Simp and Simp, respectively,
and 0 < wi
1, wi
2 < 1 and wi
1 + wi
2 = 1.
As attributes are independent of each other regarding the agents"
utility functions, the optimisation problem over the attributes from
Simp can be carried out by fixing ui([vSimp]) to a constant C,
and then search for the optimal values within the set of attributes
Simp. Now, how does the mediator determine the optimal values
for the attributes in Simp? Several well-known optimisation 
strategies could be applicable here:
• The utilitarian solution: The sum of the agents" utilities are
maximised. Thus, the optimal values are the solution of the
following optimisation problem:
arg max
v∈V alSimp
X
i∈N
ui(v)
• The Nash solution: The product of the agents" utilities are
maximised. Thus, the optimal values are the solution of the
following optimisation problem:
arg max
v∈V alSimp
Y
i∈N
ui(v)
• The egalitarian solution (aka. the maximin solution): The
utility of the agent with minimum utility is maximised. Thus,
the optimal values are the solution of the following 
optimisation problem:
arg max
v∈V alSimp
min
i∈N
ui(v)
The question now is of course whether a negotiator has the 
incentive to misrepresent his utility function. First of all, recall that the
agents" utility functions are bounded, i.e., ∀o ∈ O.0 ≤ ui(o) ≤ 1.
Thus, the agents have no incentive to overstate their utility 
regarding an outcome o: If o is the most preferred outcome to an agent
i then he already assigns the maximal utility to o. On the other
hand, if o is not the most preferred outcome to i then by 
overstating the utility he assigns to o, the agent i runs the risk of having
to settle on an agreement which would give him less payoffs than
he is supposed to receive. However, agents do have an incentive
to understate their utility if the final settlement will be based on the
above solutions alone. Essentially, the mechanism to avoid an agent
to understate his utility regarding particular outcomes is to 
guarantee a certain measure of fairness for the final settlement. That is,
the agents lose the incentive to be dishonest to obtain gains from
taking advantage of the known solutions to determine the 
settlement outcome for they would be offset by the fairness maintenance
mechanism. Firsts, we state an easy lemma.
LEMMA 1. When Simp contains one single attributes, the agents
have the incentive to understate their utility functions regarding
outcomes that are not attractive to them.
By way of illustration, consider the set Simp containing only one
attribute that could take values from the finite set {A, B, C, D}.
Assume that negotiator 1 assigns utilities of 0.4, 0.7, 0.9, and 1
to A, B, C, and D, respectively. Assume also that negotiator 2
assigns utilities of 1, 0.9, 0.7, and 0.4 to A, B, C, and D, 
respectively. If agent 1 misrepresents his utility function to the mediator
by reporting utility 0 for all values A, B and C and utility 1 for
value D then the agent 2 who plays honestly in his report to the
mediator will obtain the worst outcome D given any of the above
solutions. Note that agent 1 doesn"t need to know agent 2"s utility
function, nor does he need to know the strategy employed by agent
2. As long as he knows that the mediator is going to employ one of
the above three solutions, then the above misrepresentation is the
dominant strategy for this game.
However, when the set Simp contains more than one attribute
and none of the attributes strongly dominate the other attributes
then the above problem disminishes by itself thanks to the 
integrative solution. We of course have to define clearly what it means
for an attribute to strongly dominate other attributes. Intuitively, if
most of an agent"s utility concentrates on one of the attributes then
this attribute strongly dominates other attributes. We again appeal
to the Assumption 1 on additivity of utility functions to achieve a
measure of fairness within this negotiation setting. Due to 
Assumption 1, we can characterise agent i"s utility component over the set
of attributes Simp by the following equation:
ui,1([vSimp]) =
X
j∈Simp
wi
j ∗ ui,j([vj]) (1)
where
P
j∈Simp wj = 1.
Then, an attribute ∈ Simp strongly dominates the rest of the
attributes in Simp (for agent i) iff wi
>
P
j∈(Simp− ) wi
j . Attribute
is said to be strongly dominant (for agent i) wrt. the set of simple
attributes Simp.
The following theorem shows that if the set of attributes Simp
does not contain a strongly dominant attribute then the negotiators
have no incentive to be dishonest.
THEOREM 1. Given a negotiation framework, if for every agent
the set of simple attributes doesn"t contain a strongly dominant 
attribute, then truth-telling is an equilibrium strategy for the 
negotiators during the optimisation of simple attributes.
So far, we have been concentrating on the efficiency issue while
leaving the fairness issue aside. A fair framework does not only
support a more satisfactory distribution of utility among the agents,
but also often a good measure to prevent misrepresentation of 
private information by the agents. Of the three solutions presented
above, the utilitarian solution does not support fairness. On the
other hand, Nash [16] proves that the Nash solution satisfies the
above four axioms for the cooperative bargaining games and is 
considered a fair solution. The egalitarian solution is another 
mechanism to achieve fairness by essentially helping the worst off. The
problem with these solutions, as discussed earlier, is that they are
vulnerable to strategic behaviours when one of the attributes strongly
dominates the rest of attributes.
However, there is yet another solution that aims to guarantee 
fairness, the minimax solution. That is, the utility of the agent with
maximum utility is minimised. It"s obvious that the minimax 
solution produces inefficient outcomes. However, to get around this
problem (given that the Pareto-optimal set can be tractably 
computed), we can apply this solution over the Pareto-optimal set only.
Let POSet ⊆ V alSimp be the Pareto-optimal subset of the simple
outcomes, the minimax solution is defined to be the solution of the
following optimisation problem.
arg min
v∈P OSet
max
i∈N
ui(v)
While overall efficiency often suffers under a minimax solution,
i.e., the sum of all agents" utilities are often lower than under other
solutions, it can be shown that the minimax solution is less 
vulnerable to manipulation.
512 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
THEOREM 2. Given a negotiation framework, under the 
minimax solution, if the negotiators are uncertain about their 
opponents" preferences then truth-telling is an equilibrium strategy for
the negotiators during the optimisation of simple attributes.
That is, even when there is only one single simple attribute, if an
agent is uncertain whether the other agent"s most preferred 
resolution is also his own most preferred resolution then he should opt for
truth-telling as the optimal strategy.
4.2 Optimisation on continuous attributes
When the attributes take values from infinite sets, we assume that
they are continuous. This is similar to the common practice in 
operations research in which linear programming solutions/techniques
are applied to integer programming problems.
We denote the number of continuous attributes by k, i.e., Att =
Simp ∪ Simp and |Simp| = k. Then, the outcome space O can be
represented as follows: O = (
Q
j∈Simp V alj) × (
Q
l∈Simp V all),
where
Q
l∈Simp V all ⊆ Rk
is the continuous component of O. Let
Oc
denote the set
Q
l∈Simp V all. We"ll refer to Oc
as the feasible
set and assume that Oc
is closed and convex. After carrying out
the optimisation over the set of simple attributes, we are able to 
assign the optimal values to the simple attributes from Simp. Thus,
we reduce the original problem to the problem of searching for 
optimal (and fair) outcomes within the feasible set Oc
. Recall that,
by Assumption 1, we can characterise agent i"s utility function as
follows:
ui([v∗
Simp, vSimp]) = C + wi
2 ∗ ui,2([vSimp]),
where C is the constant wi
1 ∗ ui,1([v∗
Simp]) and v∗
Simp denotes the
optimal values of the simple attributes in Simp. Hence, without loss
of generality (albeit with a blatant abuse of notation), we can take
the agent i"s utility function as ui : Rk
→ [0, 1]. Accordingly we
will also take the set of outcomes under consideration by the agents
to be the feasible set Oc
. We now state another assumption to be
used in this section:
ASSUMPTION 2. The negotiators" utility functions can be 
described by continuously differentiable and concave functions ui :
Rk
→ [0, 1], (i = 1, 2).
It should be emphasised that we do not assume that agents 
explicitly know their utility functions. For the method to be described
in the following to work, we only assume that the agents know the
relevant information, e.g. at certain point within the feasible set Oc
,
the gradient direction of their own utility functions and some 
section of their respective indifference curves. Assume that a tentative
agreement (which is a point x ∈ Rk
) is currently on the table, the
process for the agents to jointly improve this agreement in order to
reach a Pareto-optimal agreement can be described as follows. The
mediator asks the negotiators to discretely submit their respective
gradient directions at x, i.e., ∇u1(x) and ∇u2(x).
Note that the goal of the process to be described here is to search
for agreements that are more efficient than the tentative agreement
currently on the table. That is, we are searching for points x within
the feasible set Oc
such that moving to x from the current tentative
agreement x brings more gains to at least one of the agents while
not hurting any of the agents. Due to the assumption made above,
i.e. the feasible set Oc
is bounded, the conditions for an alternative
x ∈ Oc
to be efficient vary depending on the position of x. The
following results are proved in [9]:
Let B(x) = 0 denote the equation of the boundary of Oc
, 
defining x ∈ Oc
iff B(x) ≥ 0. An alternative x∗
∈ Oc
is efficient iff,
either
A. x∗
is in the interior of Oc
and the parties are in local strict 
opposition at x∗
, i.e.,
∇u1(x∗
) = −γ∇u2(x∗
) (2)
where γ > 0; or
B. x∗
is on the boundary of Oc
, and for some α, β ≥ 0:
α∇u1(x∗
) + β∇u2(x∗
) = ∇B(x∗
) (3)
We are now interested in answering the following questions:
(i) What is the initial tentative agreement x0?
(ii) How to find the more efficient agreement xh+1, given the
current tentative agreement xh?
4.2.1 Determining a fair initial tentative agreement
It should be emphasised that the choice of the initial tentative
agreement affects the fairness of the final agreement to be reached
by the presented method. For instance, if the initial tentative 
agreement x0 is chosen to be the most preferred alternative to one of
the agents then it is also a Pareto-optimal outcome, making it 
impossible to find any joint improvement from x0. However, if x0
will then be chosen to be the final settlement and if x0 turns out
to be the worst alternative to the other agent then this outcome is a
very unfair one. Thus, it"s important that the choice of the initial
tentative agreement be sensibly made.
Ehtamo et al [3] present several methods to choose the initial 
tentative agreement (called reference point in their paper). However,
their goal is to approximate the Pareto-optimal set by 
systematically choosing a set of reference points. Once an (approximate)
Pareto-optimal set is generated, it is left to the negotiators to decide
which of the generated Pareto-optimal outcomes to be chosen as
the final settlement. That is, distributive negotiation will then be
required to settle the issue.
We, on the other hand, are interested in a fair initial tentative
agreement which is not necessarily efficient. Improving a given
tentative agreement to yield a Pareto-optimal agreement is 
considered in the next section. For each attribute j ∈ Simp, an agent i will
be asked to discretely submit three values (from the set V alj): the
most preferred value, denoted by pvi,j, the least preferred value,
denoted by wvi,j, and a value that gives i an approximately 
average payoff, denoted by avi,j. (Note that this is possible 
because the set V alj is bounded.) If pv1,j and pv2,j are sufficiently
close, i.e., |pv1,j − pv2,j| < Δ for some pre-defined Δ > 0,
then pv1,j and pv2,j are chosen to be the two core values, 
denoted by cv1 and cv2. Otherwise, between the two values pv1,j
and av1,j, we eliminate the one that is closer to wv2,j, the 
remaining value is denoted by cv1. Similarly, we obtain cv2 from the
two values pv2,j and av2,j. If cv1 = cv2 then cv1 is selected as
the initial value for the attribute j as part of the initial tentative
agreement. Otherwise, without loss of generality, we assume that
cv1 < cv2. The mediator selects randomly p values mv1, . . . , mvp
from the open interval (cv1, cv2), where p ≥ 1. The mediator then
asks the agents to submit their valuations over the set of values
{cv1, cv2, mv1, . . . , mvp}. The value whose the two valuations of
two agents are closest is selected as the initial value for the attribute
j as part of the initial tentative agreement.
The above procedure guarantees that the agents do not gain by
behaving strategically. By performing the above procedure on 
every attribute j ∈ Simp, we are able to identify the initial tentative
agreement x0 such that x0 ∈ Oc
. The next step is to compute
a new tentative agreement from an existing tentative agreement so
that the new one would be more efficient than the existing one.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 513
4.2.2 Computing new tentative agreement
Our procedure is a combination of the method of jointly 
improving direction introduced by Ehtamo et al [4] and a method we 
propose in the coming section. Basically, the idea is to see how strong
the opposition the parties are in. If the two parties are in (local)
weak opposition at the current tentative agreement xh, i.e., their
improving directions at xh are close to each other, then the 
compromise direction proposed by Ehtamo et al [4] is likely to point to
a better agreement for both agents. However, if the two parties are
in local strong opposition at the current point xh then it"s unclear
whether the compromise direction would really not hurt one of the
agents whilst bringing some benefit to the other.
We will first review the method proposed by Ehtamo et al [4]
to compute the compromise direction for a group of negotiators at
a given point x ∈ Oc
. Ehtamo et al define a a function T(x)
that describes the mediator"s choice for a compromise direction at
x. For the case of two-party negotiations, the following bisecting
function, denoted by T BS
, can be defined over the interior set of Oc
.
Note that the closed set Oc
contains two disjoint subsets: Oc
=
Oc
0 ∪Oc
B , where Oc
0 denotes the set of interior points of Oc
and Oc
B
denotes the boundary of Oc
. The bisecting compromise is defined
by a function T BS
: Oc
0 → R2
,
T BS
(x) =
∇u1(x)
∇u1(x)
+
∇u2(x)
∇u2(x)
, x ∈ Oc
0. (4)
Given the current tentative agreement xh (h ≥ 0), the mediator
has to choose a point xh+1 along d = T(xh) so that all parties
gain. Ehtamo et al then define a mechanism to generate a sequence
of points and prove that when the generated sequence is bounded
and when all generated points (from the sequence) belong to the
interior set Oc
0 then the sequence converges to a weakly 
Paretooptimal agreement [4, pp. 59-60].5
As the above mechanism does not work at the boundary points
of Oc
, we will introduce a procedure that works everywhere in an
alternative space Oc
. Let x ∈ Oc
and let θ(x) denote the angle
between the gradients ∇u1(x) and ∇u2(x) at x. That is,
θ(x) = arccos(
∇u1(x).∇u2(x)
∇u1(x) . ∇u2(x)
)
From Definition 2, it is obvious that the two parties are in local
strict opposition (at x) iff θ(x) = π, and they are in local strong
opposition iff π ≥ θ(x) > π/2, and they are in local weak 
opposition iff π/2 ≥ θ(x) ≥ 0. Note also that the two vectors ∇u1(x)
and ∇u2(x) define a hyperplane, denoted by h∇(x), in the 
kdimensional space Rk
. Furthermore, there are two indifference
curves of agents 1 and 2 going through point x, denoted by IC1(x)
and IC2(x), respectively. Let hT1(x) and hT2(x) denote the 
tangent hyperplanes to the indifference curves IC1(x) and IC2(x),
respectively, at point x. The planes hT1(x) and hT2(x) intersect
h∇(x) in the lines IS1(x) and IS2(x), respectively. Note that
given a line L(x) going through the point x, there are two (unit)
vectors from x along L(x) pointing to two opposite directions, 
denoted by L+
(x) and L−
(x).
We can now informally explain our solution to the problem of
searching for joint gains. When it isn"t possible to obtain a 
compromise direction for joint improvements at a point x ∈ Oc
either
because the compromise vector points to the space outside of the
feasible set Oc
or because the two parties are in local strong 
opposition at x, we will consider to move along the indifference curve of
one party while trying to improve the utility of the other party. As
5
Let S be the set of alternatives, x∗
is weakly Pareto optimal if
there is no x ∈ S such that ui(x) > ui(x∗
) for all agents i.
the mediator does not know the indifference curves of the parties,
he has to use the tangent hyperplanes to the indifference curves of
the parties at point x. Note that the tangent hyperplane to a curve
is a useful approximation of the curve in the immediate vicinity of
the point of tangency, x.
We are now describing an iteration step to reach the next tentative
agreement xh+1 from the current tentative agreement xh ∈ Oc
. A
vector v whose tail is xh is said to be bounded in Oc
if ∃λ > 0
such that xh +λv ∈ Oc
. To start, the mediator asks the negotiators
for their gradients ∇u1(xh) and ∇u2(xh), respectively, at xh.
1. If xh is a Pareto-optimal outcome according to equation 2 or
equation 3, then the process is terminated.
2. If 1 ≥ ∇u1(xh).∇u2(xh) > 0 and the vector T BS
(xh) is
bounded in Oc
then the mediator chooses the compromise
improving direction d = T BS
(xh) and apply the method
described by Ehtamo et al [4] to generate the next tentative
agreement xh+1.
3. Otherwise, among the four vectors ISσ
i (xh), i = 1, 2 and
σ = +/−, the mediator chooses the vector that (i) is bounded
in Oc
, and (ii) is closest to the gradient of the other agent,
∇uj (xh)(j = i). Denote this vector by T G(xh). That is,
we will be searching for a point on the indifference curve of
agent i, ICi(xh), while trying to improve the utility of agent
j. Note that when xh is an interior point of Oc
then the 
situation is symmetric for the two agents 1 and 2, and the mediator
has the choice of either finding a point on IC1(xh) to 
improve the utility of agent 2, or finding a point on IC2(xh) to
improve the utility of agent 1. To decide on which choice to
make, the mediator has to compute the distribution of gains
throughout the whole process to avoid giving more gains to
one agent than to the other. Now, the point xh+1 to be 
generated lies somewhere on the intersection of ICi(xh) and the
hyperplane defined by ∇ui(xh) and T G(xh). This 
intersection is approximated by T G(xh). Thus, the sought 
after point xh+1 can be generated by first finding a point yh
along the direction of T G(xh) and then move from yh to the
same direction of ∇ui(xh) until we intersect with ICi(xh).
Mathematically, let ζ and ξ denote the vectors T G(xh) and
∇ui(xh), respectively, xh+1 is the solution to the following
optimisation problem:
max
λ1,λ2∈L
uj(xh + λ1ζ + λ2ξ)
s.t. xh+λ1ζ+λ2ξ ∈ Oc
, and ui(xh+λ1ζ+λ2ξ) = ui(xh),
where L is a suitable interval of positive real numbers; e.g.,
L = {λ|λ > 0}, or L = {λ|a < λ ≤ b}, 0 ≤ a < b.
Given an initial tentative agreement x0, the method described
above allows a sequence of tentative agreements x1, x2, . . . to be
iteratively generated. The iteration stops whenever a weakly Pareto
optimal agreement is reached.
THEOREM 3. If the sequence of agreements generated by the
above method is bounded then the method converges to a point
x∗
∈ Oc
that is weakly Pareto optimal.
5. CONCLUSION AND FUTURE WORK
In this paper we have established a framework for negotiation
that is based on MCDM theory for representing the agents" 
objectives and utilities. The focus of the paper is on integrative 
negotiation in which agents aim to maximise joint gains, or create value.
514 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
We have introduced a mediator into the negotiation in order to 
allow negotiators to disclose information about their utilities, 
without providing this information to their opponents. Furthermore, the
mediator also works toward the goal of achieving fairness of the
negotiation outcome.
That is, the approach that we describe aims for both efficiency, in
the sense that it produces Pareto optimal outcomes (i.e. no aspect
can be improved for one of the parties without worsening the 
outcome for another party), and also for fairness, which chooses 
optimal solutions which distribute gains amongst the agents in some
appropriate manner. We have developed a two step process for 
addressing the NP-hard problem of finding a solution for a set of 
integrative attributes, which is within the Pareto-optimal set for those
attributes. For simple attributes (i.e. those which have a finite set
of values) we use known optimisation techniques to find a 
Paretooptimal solution. In order to discourage agents from 
misrepresenting their utilities to gain an advantage, we look for solutions that
are least vulnerable to manipulation. We have shown that as long
as one of the simple attributes does not strongly dominate the 
others, then truth telling is an equilibrium strategy for the negotiators
during the stage of optimising simple attributes. For non-simple 
attributes we propose a mechanism that provides stepwise 
improvements to move the proposed solution in the direction of a 
Paretooptimal solution.
The approach presented in this paper is similar to the ideas 
behind negotiation analysis [18]. Ehtamo et al [4] presents an 
approach to searching for joint gains in multi-party negotiations. The
relation of their approach to our approach is discussed in the 
preceding section. Lai et al [12] provide an alternative approach to
integrative negotiation. While their approach was clearly described
for the case of two-issue negotiations, the generalisation to 
negotiations with more than two issues is not entirely clear.
Zhang et at [22] discuss the use of integrative negotiation in
agent organisations. They assume that agents are honest. Their
main result is an experiment showing that in some situations, agents"
cooperativeness may not bring the most benefits to the organisation
as a whole, while giving no explanation. Jonker et al [7] consider
an approach to multi-attribute negotiation without the use of a 
mediator. Thus, their approach can be considered a complement of
ours. Their experimental results show that agents can reach 
Paretooptimal outcomes using their approach.
The details of the approach have currently been shown only for
bilateral negotiation, and while we believe they are generalisable to
multiple negotiators, this work remains to be done. There is also
future work to be done in more fully characterising the outcomes
of the determination of values for the non-simple attributes. In 
order to provide a complete framework we are also working on the
distributive phase using the mediator.
Acknowledgement
The authors acknowledge financial support by ARC Dicovery Grant
(2006-2009, grant DP0663147) and DEST IAP grant (2004-2006,
grant CG040014). The authors would like to thank Lawrence 
Cavedon and the RMIT Agents research group for their helpful 
comments and suggestions.
6. REFERENCES
[1] F. Alemi, P. Fos, and W. Lacorte. A demonstration of
methods for studying negotiations between physicians and
health care managers. Decision Science, 21:633-641, 1990.
[2] M. Ehrgott. Multicriteria Optimization. Springer-Verlag,
Berlin, 2000.
[3] H. Ehtamo, R. P. Hamalainen, P. Heiskanen, J. Teich,
M. Verkama, and S. Zionts. Generating pareto solutions in a
two-party setting: Constraint proposal methods.
Management Science, 45(12):1697-1709, 1999.
[4] H. Ehtamo, E. Kettunen, and R. P. Hmlinen. Searching for
joint gains in multi-party negotiations. European Journal of
Operational Research, 130:54-69, 2001.
[5] P. Faratin. Automated Service Negotiation Between
Autonomous Computational Agents. PhD thesis, University
of London, 2000.
[6] A. Foroughi. Minimizing negotiation process losses with
computerized negotiation support systems. The Journal of
Applied Business Research, 14(4):15-26, 1998.
[7] C. M. Jonker, V. Robu, and J. Treur. An agent architecture
for multi-attribute negotiation using incomplete preference
information. J. Autonomous Agents and Multi-Agent
Systems, (to appear).
[8] R. L. Keeney and H. Raiffa. Decisions with Multiple
Objectives: Preferences and Value Trade-Offs. John Wiley
and Sons, Inc., New York, 1976.
[9] G. Kersten and S. Noronha. Rational agents, contract curves,
and non-efficient compromises. IEEE Systems, Man, and
Cybernetics, 28(3):326-338, 1998.
[10] M. Klein, P. Faratin, H. Sayama, and Y. Bar-Yam. Protocols
for negotiating complex contracts. IEEE Intelligent Systems,
18(6):32-38, 2003.
[11] S. Kraus, J. Wilkenfeld, and G. Zlotkin. Multiagent
negotiation under time constraints. Artificial Intelligence
Journal, 75(2):297-345, 1995.
[12] G. Lai, C. Li, and K. Sycara. Efficient multi-attribute
negotiation with incomplete information. Group Decision
and Negotiation, 15:511-528, 2006.
[13] D. Lax and J. Sebenius. The manager as negotiator: The
negotiator"s dilemma: Creating and claiming value, 2nd ed.
In S. Goldberg, F. Sander & N. Rogers, editors, Dispute
Resolution, 2nd ed., pages 49-62. Little Brown & Co., 1992.
[14] M. Lomuscio and N. Jennings. A classification scheme for
negotiation in electronic commerce. In Agent-Mediated
Electronic Commerce: A European Agentlink Perspective.
Springer-Verlag, 2001.
[15] R. Maes and A. Moukas. Agents that buy and sell.
Communications of the ACM, 42(3):81-91, 1999.
[16] J. Nash. Two-person cooperative games. Econometrica,
21(1):128-140, April 1953.
[17] H. Raiffa. The Art and Science of Negotiation. Harvard
University Press, Cambridge, USA, 1982.
[18] H. Raiffa, J. Richardson, and D. Metcalfe. Negotiation
Analysis: The Science and Art of Collaborative Decision
Making. Belknap Press, Cambridge, MA, 2002.
[19] T. Sandholm. Agents in electronic commerce: Component
technologies for automated negotiation and coalition
formation. JAAMAS, 3(1):73-96, 2000.
[20] J. Sebenius. Negotiation analysis: A characterization and
review. Management Science, 38(1):18-38, 1992.
[21] L. Weingart, E. Hyder, and M. Pietrula. Knowledge matters:
The effect of tactical descriptions on negotiation behavior
and outcome. Tech. Report, CMU, 1995.
[22] X. Zhang, V. R. Lesser, and T. Wagner. Integrative
negotiation among agents situated in organizations. IEEE
Trans. on Systems, Man, and Cybernetics, Part C,
36(1):19-30, 2006.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 515
On the relevance of utterances in formal inter-agent
dialogues
Simon Parsons1
Peter McBurney2
1
Department of Computer & Information Science
Brooklyn College, City University of New York
Brooklyn NY 11210 USA
{parsons,sklar}@sci.brooklyn.cuny.edu
Elizabeth Sklar1
Michael Wooldridge2
2
Department of Computer Science
University of Liverpool
Liverpool L69 7ZF UK
{p.j.mcburney,m.j.wooldridge}@csc.liv.ac.uk
ABSTRACT
Work on argumentation-based dialogue has defined 
frameworks within which dialogues can be carried out, established
protocols that govern dialogues, and studied different 
properties of dialogues. This work has established the space in
which agents are permitted to interact through dialogues.
Recently, there has been increasing interest in the 
mechanisms agents might use to choose how to act - the 
rhetorical manoeuvring that they use to navigate through the space
defined by the rules of the dialogue. Key in such 
considerations is the idea of relevance, since a usual requirement is
that agents stay focussed on the subject of the dialogue and
only make relevant remarks. Here we study several notions
of relevance, showing how they can be related to both the
rules for carrying out dialogues and to rhetorical 
manoeuvring.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
Intelligence: Coherence & co-ordination; languages & 
structures; multiagent systems.
General Terms
Design, languages, theory.
1. INTRODUCTION
Finding ways for agents to reach agreements in 
multiagent systems is an area of active research. One mechanism
for achieving agreement is through the use of argumentation
- where one agent tries to convince another agent of 
something during the course of some dialogue. Early examples of
argumentation-based approaches to multiagent agreement
include the work of Dignum et al. [7], Kraus [14], 
Parsons and Jennings [16], Reed [23], Schroeder et al. [25] and
Sycara [26].
The work of Walton and Krabbe [27], popularised in the
multiagent systems community by Reed [23], has been 
particularly influential in the field of argumentation-based 
dialogue. This work influenced the field in a number of ways,
perhaps most deeply in framing multi-agent interactions as
dialogue games in the tradition of Hamblin [13]. Viewing 
dialogues in this way, as in [2, 21], provides a powerful 
framework for analysing the formal properties of dialogues, and
for identifying suitable protocols under which dialogues can
be conducted [18, 20]. The dialogue game view overlaps with
work on conversation policies (see, for example, [6, 10]), but
differs in considering the entire dialogue rather than dialogue
segments.
In this paper, we extend the work of [18] by considering
the role of relevance - the relationship between utterances
in a dialogue. Relevance is a topic of increasing interest
in argumentation-based dialogue because it relates to the
scope that an agent has for applying strategic manoeuvering
to obtain the outcomes that it requires [19, 22, 24]. Our
work identifes the limits on such rhetorical manoeuvering,
showing when it can and cannot have an effect.
2. BACKGROUND
We begin by introducing the formal system of 
argumentation that underpins our approach, as well as the 
corresponding terminology and notation, all taken from [2, 8, 17].
A dialogue is a sequence of messages passed between two
or more members of a set of agents A. An agent α maintains
a knowledge base, Σα, containing formulas of a propositional
language L and having no deductive closure. Agent α also
maintains the set of its past utterances, called the 
commitment store, CSα. We refer to this as an agent"s public
knowledge, since it contains information that is shared with
other agents. In contrast, the contents of Σα are private
to α.
Note that in the description that follows, we assume that
is the classical inference relation, that ≡ stands for logical
equivalence, and we use Δ to denote all the information
available to an agent. Thus in a dialogue between two agents
α and β, Δα = Σα ∪ CSα ∪ CSβ, so the commitment store
CSα can be loosely thought of as a subset of Δα consisting of
the assertions that have been made public. In some dialogue
games, such as those in [18] anything in CSα is either in Σα
or can be derived from it. In other dialogue games, such as
1006
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
those in [2], CSα may contain things that cannot be derived
from Σα.
Definition 2.1. An argument A is a pair (S, p) where p
is a formula of L and S a subset of Δ such that (i) S is
consistent; (ii) S p; and (iii) S is minimal, so no proper
subset of S satisfying both (1) and (2) exists.
S is called the support of A, written S = Support(A) and p
is the conclusion of A, written p = Conclusion(A). Thus we
talk of p being supported by the argument (S, p).
In general, since Δ may be inconsistent, arguments in
A(Δ), the set of all arguments which can be made from Δ,
may conflict, and we make this idea precise with the notion
of undercutting:
Definition 2.2. Let A1 and A2 be arguments in A(Δ).
A1 undercuts A2 iff ∃¬p ∈ Support(A2) such that p ≡
Conclusion(A1).
In other words, an argument is undercut if and only if there
is another argument which has as its conclusion the negation
of an element of the support for the first argument.
To capture the fact that some beliefs are more strongly
held than others, we assume that any set of beliefs has a
preference order over it. We consider all information 
available to an agent, Δ, to be stratified into non-overlapping
subsets Δ1, . . . , Δn such that beliefs in Δi are all equally
preferred and are preferred over elements in Δj where i > j.
The preference level of a nonempty subset S ⊂ Δ, where
different elements s ∈ S may belong to different layers Δi,
is valued at the highest numbered layer which has a member
in S and is referred to as level(S). In other words, S is only
as strong as its weakest member. Note that the strength of
a belief as used in this context is a separate concept from
the notion of support discussed earlier.
Definition 2.3. Let A1 and A2 be arguments in A(Δ).
A1 is preferred to A2 according to Pref , A1
Pref
A2, iff
level(Support(A1)) > level(Support(A2)). If A1 is 
preferred to A2, we say that A1 is stronger than A2.
We can now define the argumentation system we will use:
Definition 2.4. An argumentation system is a triple:
A(Δ), Undercut, Pref
such that:
• A(Δ) is a set of the arguments built from Δ,
• Undercut is a binary relation representing the defeat
relationship between arguments, Undercut ⊆ A(Δ) ×
A(Δ), and
• Pref is a pre-ordering on A(Δ) × A(Δ).
The preference order makes it possible to distinguish 
different types of relations between arguments:
Definition 2.5. Let A1, A2 be two arguments of A(Δ).
• If A2 undercuts A1 then A1 defends itself against A2
iff A1
Pref
A2. Otherwise, A1 does not defend itself.
• A set of arguments A defends A1 iff for every A2 that
undercuts A1, where A1 does not defend itself against
A2, then there is some A3 ∈ A such that A3 undercuts
A2 and A2 does not defend itself against A3.
We write AUndercut,Pref to denote the set of all non-undercut
arguments and arguments defending themselves against all
their undercutting arguments. The set A(Δ) of acceptable
arguments of the argumentation system
A(Δ), Undercut, Pref
is [1] the least fixpoint of a function F:
A ⊆ A(Δ)
F(A) = {(S, p) ∈ A(Δ) | (S, p) is defended by A}
Definition 2.6. The set of acceptable arguments for an
argumentation system A(Δ), Undercut, Pref is recursively
defined as:
A(Δ) =
[
Fi≥0(∅)
= AUndercut,Pref ∪
h[
Fi≥1(AUndercut,Pref )
i
An argument is acceptable if it is a member of the acceptable
set, and a proposition is acceptable if it is the conclusion of
an acceptable argument.
An acceptable argument is one which is, in some sense,
proven since all the arguments which might undermine it
are themselves undermined.
Definition 2.7. If there is an acceptable argument for a
proposition p, then the status of p is accepted, while if there
is not an acceptable argument for p, the status of p is not
accepted.
Argument A is said to affect the status of another argument
A if changing the status of A will change the status of A .
3. DIALOGUES
Systems like those described in [2, 18], lay down sets of
locutions that agents can make to put forward propositions
and the arguments that support them, and protocols that 
define precisely which locutions can be made at which points
in the dialogue. We are not concerned with such a level
of detail here. Instead we are interested in the interplay
between arguments that agents put forth. As a result, we
will consider only that agents are allowed to put forward
arguments. We do not discuss the detail of the mechanism
that is used to put these arguments forward - we just 
assume that arguments of the form (S, p) are inserted into
an agent"s commitment store where they are then visible to
other agents.
We then have a typical definition of a dialogue:
Definition 3.1. A dialogue D is a sequence of moves:
m1, m2, . . . , mn.
A given move mi is a pair α, Ai where Ai is an argument
that α places into its commitment store CSα.
Moves in an argumentation-based dialogue typically attack
moves that have been made previously. While, in general,
a dialogue can include moves that undercut several 
arguments, in the remainder of this paper, we will only consider
dialogues that put forward moves that undercut at most
one argument. For now we place no additional constraints
on the moves that make up a dialogue. Later we will see
how different restrictions on moves lead to different kinds of
dialogue.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1007
The sequence of arguments put forward in the dialogue
is determined by the agents who are taking part in the 
dialogue, but they are usually not completely free to choose
what arguments they make. As indicated earlier, their choice
is typically limited by a protocol. If we write the sequence
of n moves m1, m2, . . . , mn as mn, and denote the empty 
sequence as m0, then we can define a profocol in the following
way:
Definition 3.2. A protocol P is a function on a sequence
of moves mi in a dialogue D that, for all i ≥ 0, identifies
a set of possible moves Mi+1 from which the mi+1th move
may be drawn:
P : mi → Mi+1
In other words, for our purposes here, at every point in
a dialogue, a protocol determines a set of possible moves
that agents may make as part of the dialogue. If a dialogue
D always picks its moves m from the set M identified by
protocol P, then D is said to conform to P.
Even if a dialogue conforms to a protocol, it is typically
the case that the agent engaging in the dialogue has to make
a choice of move - it has to choose which of the moves in M
to make. This excercise of choice is what we refer to as an
agent"s use of rhetoric (in its oratorical sense of influencing
the thought and conduct of an audience). Some of our
results will give a sense of how much scope an agent has to
exercise rhetoric under different protocols.
As arguments are placed into commitment stores, and
hence become public, agents can determine the relationships
between them. In general, after several moves in a 
dialogue, some arguments will undercut others. We will denote
the set of arguments {A1, A2, . . . , Aj} asserted after moves
m1, m2, . . . , mj of a dialogue to be Aj - the relationship of
the arguments in Aj can be described as an argumentation
graph, similar to those described in, for example, [3, 4, 9]:
Definition 3.3. An argumentation graph AG over a set
of arguments A is a directed graph (V, E) such that every
vertex v, v ∈ V denotes one argument A ∈ A, every 
argument A is denoted by one vertex v, and every directed edge
e ∈ E from v to v denotes that v undercuts v .
We will use the term argument graph as a synonym for 
argumentation graph.
Note that we do not require that the argumentation graph
is connected. In other words the notion of an argumentation
graph allows for the representation of arguments that do
not relate, by undercutting or being undercut, to any other
arguments (we will come back to this point very shortly).
We adapt some standard graph theoretic notions in order
to describe various aspects of the argumentation graph. If
there is an edge e from vertex v to vertex v , then v is said
to be the parent of v and v is said to be the child of v.
In a reversal of the usual notion, we define a root of an
argumentation graph1
as follows:
Definition 3.4. A root of an argumentation graph AG =
(V, E) is a node v ∈ V that has no children.
Thus a root of a graph is a node to which directed edges
may be connected, but from which no directed edges 
connect to other nodes. Thus a root is a node representing an
1
Note that we talk of a root rather than the root - as defined,
an argumentation graph need not be a tree.
v v"
Figure 1: An example argument graph
argument that is undercut, but which itself does no 
undercutting. Similarly:
Definition 3.5. A leaf of an argumentation graph AG =
(V, E) is a node v ∈ V that has no parents.
Thus a leaf in an argumentation graph represents an 
argument that undercuts another argument, but does no 
undercutting. Thus in Figure 1, v is a root, and v is a leaf. The
reason for the reversal of the usual notions of root and leaf
is that, as we shall see, we will consider dialogues to 
construct argumentation graphs from the roots (in our sense)
to the leaves. The reversal of the terminology means that it
matches the natural process of tree construction.
Since, as described above, argumentation graphs are 
allowed to be not connected (in the usual graph theory sense),
it is helpful to distinguish nodes that are connected to other
nodes, in particular to the root of the tree. We say that node
v is connected to node v if and only if there is a path from
v to v . Since edges represent undercut relations, the notion
of connectedness between nodes captures the influence that
one argument may have on another:
Proposition 3.1. Given an argumentation graph AG, if
there is any argument A, denoted by node v that affects the
status of another argument A , denoted by v , then v is 
connected to v . The converse does not hold.
Proof. Given Definitions 2.5 and 2.6, the only ways in
which A can affect the status of A is if A either undercuts
A , or if A undercuts some argument A that undercuts A ,
or if A undercuts some A that undercuts some A that
undercuts A , and so on. In all such cases, a sequence of
undercut relations relates the two arguments, and if they are
both in an argumentation graph, this means that they are
connected.
Since the notion of path ignores the direction of the 
directed arcs, nodes v and v are connected whether the edge
between them runs from v to v or vice versa. Since A only
undercuts A if the edge runs from v to v , we cannot infer
that A will affect the status of A from information about
whether or not they are connected.
The reason that we need the concept of the argumentation
graph is that the properties of the argumentation graph tell
us something about the set of arguments A the graph 
represents. When that set of arguments is constructed through a
dialogue, there is a relationship between the structure of the
argumentation graph and the protocol that governs the 
dialogue. It is the extent of the relationship between structure
and protocol that is the main subject of this paper. To study
this relationship, we need to establish a correspondence 
between a dialogue and an argumentation graph. Given the
definitions we have so far, this is simple:
Definition 3.6. A dialogue D, consisting of a sequence
of moves mn, and an argument graph AG = (V, E) 
correspond to one another iff ∀m ∈ mn, the argument Ai that
1008 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
is advanced at move mi is represented by exactly one node
v ∈ V , and ∀v ∈ V , v represents exactly one argument Ai
that has been advanced by a move m ∈ mn.
Thus a dialogue corresponds to an argumentation graph if
and only if every argument made in the dialogue corresponds
to a node in the graph, and every node in the graph 
corresponds to an argument made in the dialogue. This 
one-toone correspondence allows us to consider each node v in the
graph to have an index i which is the index of the move in
the dialogue that put forward the argument which that node
represents. Thus we can, for example, refer to the third
node in the argumentation graph, meaning the node that
represents the argument put forward in the third move of
the dialogue.
4. RELEVANCE
Most work on dialogues is concerned with what we might
call coherent dialogues, that is dialogues in which the 
participants are, as in the work of Walton and Krabbe [27],
focused on resolving some question through the dialogue2
To capture this coherence, it seems we need a notion of 
relevance to constrain the statements made by agents. Here
we study three notions of relevance:
Definition 4.1. Consider a dialogue D, consisting of a
sequence of moves mi, with a corresponding argument graph
AG. The move mi+1, i > 1, is said to be relevant if one or
more of the following hold:
R1 Making mi+1 will change the status of the argument
denoted by the first node of AG.
R2 Making mi+1 will add a node vi+1 that is connected to
the first node of AG.
R3 Making mi+1 will add a node vi+1 that is connected to
the last node to be added to AG.
R2-relevance is the form of relevance defined by [3] in their
study of strategic and tactical reasoning3
. R1-relevance was
suggested by the notion used in [15], and though it differs
somewhat from that suggested there, we believe it captures
the essence of its predecessor.
Note that we only define relevance for the second move
of the dialogue onwards because the first move is taken to
identify the subject of the dialogue, that is, the central 
question that the dialogue is intended to answer, and hence it
must be relevant to the dialogue, no matter what it is. In
assuming this, we focus our attention on the same kind of
dialogues as [18].
We can think of relevance as enforcing a form of 
parsimony on a dialogue - it prevents agents from making 
statements that do not bear on the current state of the dialogue.
This promotes efficiency, in the sense of limiting the 
number of moves in the dialogue, and, as in [15], prevents agents
revealing information that they might better keep hidden.
Another form of parsimony is to insist that agents are not
allowed to put forward arguments that will be undercut by
arguments that have already been made during the dialogue.
We therefore distinguish such arguments.
2
See [11, 12] for examples of dialogues where this is not the case.
3
We consider such reasoning sub-types of rhetoric.
Definition 4.2. Consider a dialogue D, consisting of a
sequence of moves mi, with a corresponding argument graph
AG. The move mi+1 and the argument it puts forward,
Ai+1, are both said to be pre-empted, if Ai+1 is undercut by
some A ∈ Ai.
We use the term pre-empted because if such an argument
is put forward, it can seem as though another agent 
anticipated the argument being made, and already made an
argument that would render it useless. In the rest of this
paper, we will only deal with protocols that permit moves
that are relevant, in any of the senses introduced above, and
are not allowed to be pre-empted. We call such protocols 
basic protocols, and dialogues carried out under such protocols
basic dialogues.
The argument graph of a basic dialogue is somewhat 
restricted.
Proposition 4.1. Consider a basic dialogue D. The 
argumentation graph AG that corresponds to D is a tree with
a single root.
Proof. Recall that Definition 3.3 requires only that AG
be a directed graph. To show that it is a tree, we have to
show that it is acyclic and connected.
That the graph is connected follows from the construction
of the graph under a protocol that enforces relevance. If the
notion of relevance is R3, each move adds a node that is
connected to the previous node. If the notion of relevance is
R2, then every move adds a node that is connected to the
root, and thus is connected to some node in the graph. If the
notion of relevance is R1, then every move has to change the
status of the argument denoted by the root. Proposition 3.1
tells us that to affect the status of an argument A , the node
v representing the argument A that is effecting the change
has to be connected to v , the node representing A , and so
it follows that every new node added as a result of an 
R1relevant move will be connected to the argumentation graph.
Thus AG is connected.
Since a basic dialogue does not allow moves that are 
preempted, every edge that is added during construction is 
directed from the node that is added to one already in the graph
(thus denoting that the argument A denoted by the added
node, v, undercuts the argument A denoted by the node to
which the connection is made, v , rather than the other way
around). Since every edge that is added is directed from the
new node to the rest of the graph, there can be no cycles.
Thus AG is a tree.
To show that AG has a single root, consider its 
construction from the initial node. After m1 the graph has one node,
v1 that is both a root and a leaf. After m2, the graph is two
nodes connected by an edge, and v1 is now a root and not a
leaf. v2 is a leaf and not a root. However the third node is
added, the argument earlier in this proof demonstrates that
there will be a directed edge from it to some other node, 
making it a leaf. Thus v1 will always be the only root. The ruling
out of pre-empted moves means that v1 will never cease to
be a root, and so the argumentation graph will always have
one root.
Since every argumentation graph constructed by a basic 
dialogue is a tree with a single root, this means that the first
node of every argumentation graph is the root.
Although these results are straightforward to obtain, they
allow us to show how the notions of relevance are related.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1009
Proposition 4.2. Consider a basic dialogue D, 
consisting of a sequence of moves mi, with a corresponding 
argument graph AG.
1. Every move mi+1 that is R1-relevant is R2-relevant.
The converse does not hold.
2. Every move mi+1 that is R3-relevant is R2-relevant.
The converse does not hold.
3. Not every move mi+1 that is R1-relevant is R3-relevant,
and not every move mi+1 that is R3-relevant is 
R1relevant
Proof. For 1, consider how move mi+1 can satisfy R1.
Proposition 3.1 tells us that if Ai+1 can change the status
of the argument denoted by the root v1 (which, as observed
above, is the first node) of AG, then vi+1 must be connected
to the root. This is precisely what is required to satisfy R2,
and the relatiosnhip is proved to hold.
To see that the converse does not hold, we have to consider
what it takes to change the status of r (since Proposition 3.1
tells us that connectedness is not enough to ensure a change
of status - if it did, R1 and R2 relevance would coincide).
For mi+1 to change the status of the root, it will have to (1)
make the argument A represented by r either unacceptable,
if it were acceptable before the move, or (2) acceptable if it
were unacceptable before the move. Given the definition of
acceptability, it can achieve (1) either by directly 
undercutting the argument represented by r, in which case vi+1 will
be directly connected to r by some edge, or by undercutting
some argument A that is part of the set of non-undercut
arguments defending A. In the latter case, vi+1 will be 
directly connected to the node representing A and by 
Proposition 4.1 to r. To achieve (2), vi+1 will have to undercut
an argument A that is either currently undercutting A, or
is undercutting an argument that would otherwise defend A.
Now, further consider that mi+1 puts forward an argument
Ai+1 that undercuts the argument denoted by some node v ,
but this latter argument defends itself against Ai+1. In such
a case, the set of acceptable arguments will not change, and
so the status of Ar will not change. Thus a move that is
R2-relevant need not be R1-relevant.
For 2, consider that mi+1 can satisfy R3 simply by adding
a node that is connected to vi, the last node to be added
to AG. By Proposition 4.1, it is connected to r and so is
R2-relevant.
To see that the converse does not hold, consider that an
R2-relevant move can connect to any node in AG.
The first part of 3 follows by a similar argument to that we
just used - an R1-relevant move does not have to connect to
vi, just to some v that is part of the graph - and the second
part follows since a move that is R3-relevant may introduce
an argument Ai+1 that undercuts the argument Ai put 
forward by the previous move (and so vi+1 is connected to vi),
but finds that Ai defends itself against Ai+1, preventing a
change of status at the root.
What is most interesting is not so much the results but
why they hold, since this reveals some aspects of the 
interplay between relevance and the structure of argument
graphs. For example, to restate a case from the proof of
Proposition 4.2, a move that is R3-relevant by definition has
to add a node to the argument graph that is connected to the
last node that was added. Since a move that is R2-relevant
can add a node that connects anywhere on an argument
graph, any move that is R3-relevant will be R2-relevant,
but the converse does not hold.
It turns out that we can exploit the interplay between
structure and relevance that Propositions 4.1 and 4.2 have
started to illuminate to establish relationships between the
protocols that govern dialogues and the argument graphs
constructed during such dialogues. To do this we need to
define protocols in such a way that they refer to the structure
of the graph. We have:
Definition 4.3. A protocol is single-path if all dialogues
that conform to it construct argument graphs that have only
one branch.
Proposition 4.3. A basic protocol P is single-path if, for
all i, the set of permitted moves Mi at move i are all 
R3relevant. The converse does not hold.
Proof. R3-relevance requires that every node added to
the argument graph be connected to the previous node. 
Starting from the first node this recursively constructs a tree with
just one branch, and the relationship holds. The converse
does not hold because even if one or more moves in the 
protocol are R1- or R2-relevant, it may be the case that, because
of an agent"s rhetorical choice or because of its knowledge,
every argument that is chosen to be put forward will 
undercut the previous argument and so the argument graph is a
one-branch tree.
Looking for more complex kinds of protocol that construct
more complex kinds of argument graph, it is an obvious
move to turn to:
Definition 4.4. A basic protocol is multi-path if all 
dialogues that conform to it can construct argument graphs that
are trees.
But, on reflection, since any graph with only one branch is
also a tree:
Proposition 4.4. Any single-path protocol is an instance
of a multi-path protocol.
and, furthermore:
Proposition 4.5. Any basic protocol P is multi-path.
Proof. Immediate from Proposition 4.1
So the notion of a multi-path protocol does not have much
traction. As a result we distinguish multi-path protocols
that permit dialogues that can construct trees that have
more than one branch as bushy protocols. We then have:
Proposition 4.6. A basic protocol P is bushy if, for some
i, the set of permitted moves Mi at move i are all R1- or
R2-relevant.
Proof. From Proposition 4.3 we know that if all moves
are R3-relevant then we"ll get a tree with one branch, and
from Proposition 4.1 we know that all basic protocols will
build an argument graph that is a tree, so providing we 
exclude R3-relevant moves, we will get protocols that can build
multi-branch trees.
1010 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Of course, since, by Proposition 4.2, any move that is 
R3relevant is R2-relevant and can quite possibly be R1-relevant
(all that Proposition 4.2 tells us is that there is no 
guarantee that it will be), all that Proposition 4.6 tells us is that
dialogues that conform to bushy protocols may have more
than one branch. All we can do is to identify a bound on
the number of branches:
Proposition 4.7. Consider a basic dialogue D that 
includes m moves that are not R3-relevant, and has a 
corresponding argumentation graph AG. The number of branches
in AG is less than or equal to m + 1.
Proof. Since it must connect a node to the last node
added to AG, an R3-relevant move can only extend an 
existing branch. Since they do not have the same restriction,
R1 and R2-relevant moves may create a new branch by 
connecting to a node that is not the last node added. Every
such move could create a new branch, and if they do, we
will have m branches. If there were R3-relevant moves 
before any of these new-branch-creating moves, then these m
branches are in addition to the initial branch created by the
R3-relevant moves, and we have a maximum of m + 1 
possible branches.
We distinguish bushy protocols from multi-path protocols,
and hence R1- and R2-relevance from R3-relevance, because
of the kinds of dialogue that R3-relevance enforces. In a 
dialogue in which all moves must be R3-relevant, the 
argumentation graph has a single branch - the dialogue consists of
a sequence of arguments each of which undercuts the 
previous one and the last move to be made is the one that settles
the dialogue. This, as we will see next, means that such a
dialogue only allows a subset of all the moves that would
otherwise be possible.
5. COMPLETENESS
The above discussion of the difference between dialogues
carried out under single-path and bushy protocols brings us
to the consideration of what [18] called predeterminism,
but we now prefer to describe using the term 
completeness. The idea of predeterminism, as described in [18],
captures the notion that, under some circumstances, the 
result of a dialogue can be established without actually having
the dialogue - the agents have sufficiently little room for
rhetorical manoeuver that were one able to see the contents
of all the Σi of all the αi ∈ A, one would be able to 
identify the outcome of any dialogue on a given subject4
. We
develop this idea by considering how the argument graphs
constructed by dialogues under different protocols compare
to benchmark complete dialogues. We start by developing
ideas of what complete might mean. One reasonable 
definition is that:
Definition 5.1. A basic dialogue D between the set of
agents A with a corresponding argumentation graph AG is
topic-complete if no agent can construct an argument A that
undercuts any argument A represented by a node in AG.
The argumentation graph constructed by a topic-complete
dialogue is called a topic-complete argumentation graph and
is denoted AG(D)T .
4
Assuming that the Σi do not change during the dialogue, which is
the usual assumption in this kind of dialogue.
A dialogue is topic-complete when no agent can add 
anything that is directly connected to the subject of the 
dialogue. Some protocols will prevent agents from making
moves even though the dialogue is not topic-complete. To
distinguish such cases we have:
Definition 5.2. A basic dialogue D between the set of
agents A with a corresponding argumentation graph AG is
protocol-complete under a protocol P if no agent can make
a move that adds a node to the argumentation graph that is
permitted by P.
The argumentation graph constructed by a protocol-complete
dialogue is called a protocol-complete argumentation graph
and is denoted AG(D)P . Clearly:
Proposition 5.1. Any dialogue D under a basic protocol
P is protocol-complete if it is topic-complete. The converse
does not hold in general.
Proof. If D is topic-complete, no agent can make a move
that will extend the argumentation graph. This means that
no agent can make a move that is permitted by a basic 
protocol, and so D is also protocol complete.
The converse does not hold since some basic dialogues 
(under a protocol that only permits R3-relevant moves, for 
example) will not permit certain moves (like the addition of
a node that connects to the root of the argumentation graph
after more than two moves) that would be allowed in a 
topiccomplete dialogue.
Corollary 5.1. For a basic dialogue D, AG(D)P is a
sub-graph of AG(D)T .
Obviously, from the definition of a sub-graph, the converse
of Corollary 5.1 does not hold in general.
The important distinction between topic- and 
protocolcompleteness is that the former is determined purely by the
state of the dialogue - as captured by the argumentation
graph - and is thus independent of the protocol, while the
latter is determined entirely by the protocol. Any time that
a dialogue ends in a state of protocol-completeness rather
than topic completeness, it is ending when agents still have
things to say but can"t because the protocol won"t allow
them to.
With these definitions of completeness, our task is to 
relate topic-completeness - the property that ensures that
agents can say everything that they have to say in a dialogue
that is, in some sense, important - to the notions of 
relevance we have developed - which determine what agents
are allowed to say. When we need very specific conditions to
make protocol-complete dialogues topic-complete, it means
that agents have lots of room for rhetorical maneouver when
those conditions are not in force. That is there are many
ways they can bring dialogues to a close before everything
that can be said has been said. Where few conditions are
required, or conditions are absent, then dialogues between
agents with the same knowledge will always play out the
same way, and rhetoric has no place. We have:
Proposition 5.2. A protocol-complete basic dialogue D
under a protocol which only allows R3-relevant moves will
be topic-complete only when AG(D)T has a single branch
in which the nodes are labelled in increasing order from the
root.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1011
Proof. Given what we know about R3-relevance, the 
condition on AG(D)P having a single branch is obvious. This
is not a sufficient condition on its own because certain 
protocols may prevent - through additional restrictions, like
strict turn-taking in a multi-party dialogue - all the nodes
in AG(D)T , which is not subject to such restrictions, being
added to the graph. Only when AG(D)T includes the nodes
in the exact order that the corresponding arguments are put
forward is it necessary that a topic-complete argumentation
graph be constructed.
Given Proposition 5.1, these are the conditions under which
dialogues conducted under the notion of R3-relevance will
always be predetermined, and given how restrictive the 
conditions are, such dialogues seem to have plenty of room for
rhetoric to play a part.
To find similar conditions for dialogues composed of 
R1and R2-relevant moves, we first need to distinguish between
them. We can do this in terms of the structure of the 
argumentation graph:
Proposition 5.3. Consider a basic dialogue D, with 
argumentation graph AG which has root r denoting an 
argument A. If argument A , denoted by node v is an an 
R2relevant move m, m is not R1-relevant if and only if:
1. there are two nodes v and v on the path between v
and r, and the argument denoted by v defends itself
against the argument denoted by v ; or
2. there is an argument A , denoted by node v , that 
affects the status of A, and the path from v to r has one
or more nodes in common with the path from v to r.
Proof. For the first condition, consider that since AG is
a tree, v is connected to r. Thus there is a series of undercut
relations between A and A , and this corrresponds to a path
through AG. If this path is the only branch in the tree, then
A will affect the status of A unless the chain of affect
is broken by an undercut that can"t change the status of the
undercut argument because the latter defends itself.
For the second condition, as for the first, the only way
that A cannot affect the status of A is if something is 
blocking its influence. If this is not due to defending against,
it must be because there is some node u on the path that
represents an argument whose status is fixed somehow, and
that must mean that there is another chain of undercut 
relations, another branch of the tree, that is incident at u. Since
this second branch denotes another chain of arguments, and
these affect the status of the argument denoted by u, they
must also affect the status of A. Any of these are the A in
the condition.
So an R2-relevant move m is not R1-relevant if either its
effect is blocked because an argument upstream is not strong
enough, or because there is another line of argument that
is currently determining the status of the argument at the
root. This, in turn, means that if the effect is not due to
defending against, then there is an alternative move that
is R1-relevant - a move that undercuts A in the second
condition above5
. We can now show
5
Though whether the agent in question can make such a move is
another question.
Proposition 5.4. A protocol-complete basic dialogue D
will always be topic-complete under a protocol which only 
includes R2-relevant moves and allows every R2-relevant move
to be made.
The restriction on R2-relevant rules is exactly that for 
topiccompleteness, so a dialogue that has only R2-relevant moves
will continue until every argument that any agent can make
has been put forward. Given this, and what we revealed
about R1-relevance in Proposition 5.3, we can see that:
Proposition 5.5. A protocol-complete basic dialogue D
under a protocol which only includes R1-relevant moves will
be topic-complete if AG(D)T :
1. includes no path with adjacent nodes v, denoting A,
and v , denoting A , such that A undercuts A and A
is stronger that A; and
2. is such that the nodes in every branch have consecutive
indices and no node with degree greater than two is an
odd number of arcs from a leaf node.
Proof. The first condition rules out the first condition
in Proposition 5.3, and the second deals with the situation
that leads to the second condition in Proposition 5.3. The
second condition ensures that each branch is constructed in
full before any new branch is added, and when a new branch
is added, the argument that is undercut as part of the 
addition will be acceptable, and so the addition will change the
status of the argument denoted by that node, and hence the
root. With these conditions, every move required to 
construct AG(D)T will be permitted and so the dialogue will be
topic-complete when every move has been completed.
The second part of this result only identifies one possible
way to ensure that the second condition in Proposition 5.3
is met, so the converse of this result does not hold.
However, what we have is sufficient to answer the 
question about predetermination that we started with. For 
dialogues to be predetermined, every move that is R2-relevant
must be made. In such cases every dialogue is topic 
complete. If we do not require that all R2-relevant moves are
made, then there is some room for rhetoric - the way in
which alternative lines of argument are presented becomes
an issue. If moves are forced to be R3-relevant, then there
is considerable room for rhetorical play.
6. SUMMARY
This paper has studied the different ideas of relevance in
argumentation-based dialogue, identifying the relationship
between these ideas, and showing how they can impact the
extent to which the way that agents choose moves in a 
dialogue - what some authors have called the strategy and
tactics of a dialogue. This extends existing work on 
relvance, such as [3, 15] by showing how different notions of
relevance can have an effect on the outcome of a dialogue,
in particular when they render the outcome predetermined.
This connection extends the work of [18] which considered
dialogue outcome, but stopped short of identifying the 
conditions under which it is predetermined.
There are two ways we are currently trying to extend this
work, both of which will generalise the results and extend its
applicability. First, we want to relax the restrictions that
1012 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
we have imposed, the exclusion of moves that attack 
several arguments (without which the argument graph can be
mulitply-connected) and the exclusion of pre-empted moves,
without which the argument graph can have cycles. 
Second, we want to extend the ideas of relevance to cope with
moves that do not only add undercutting arguments, but
also supporting arguments, thus taking account of bipolar
argumentation frameworks [5].
Acknowledgments
The authors are grateful for financial support received from
the EC, through project IST-FP6-002307, and from the NSF
under grants REC-02-19347 and NSF IIS-0329037. They are
also grateful to Peter Stone for a question, now several years
old, which this paper has finally answered.
7. REFERENCES
[1] L. Amgoud and C. Cayrol. On the acceptability of
arguments in preference-based argumentation
framework. In Proceedings of the 14th Conference on
Uncertainty in Artificial Intelligence, pages 1-7, 1998.
[2] L. Amgoud, S. Parsons, and N. Maudet. Arguments,
dialogue, and negotiation. In W. Horn, editor,
Proceedings of the Fourteenth European Conference on
Artificial Intelligence, pages 338-342, Berlin,
Germany, 2000. IOS Press.
[3] J. Bentahar, M. Mbarki, and B. Moulin. Strategic and
tactic reasoning for communicating agents. In
N. Maudet, I. Rahwan, and S. Parsons, editors,
Proceedings of the Third Workshop on Argumentation
in Muliagent Systems, Hakodate, Japan, 2006.
[4] P. Besnard and A. Hunter. A logic-based theory of
deductive arguments. Artificial Intelligence,
128:203-235, 2001.
[5] C. Cayrol, C. Devred, and M.-C. Lagasquie-Schiex.
Handling controversial arguments in bipolar
argumentation frameworks. In P. E. Dunne and
T. J. M. Bench-Capon, editors, Computational Models
of Argument: Proceedings of COMMA 2006, pages
261-272. IOS Press, 2006.
[6] B. Chaib-Draa and F. Dignum. Trends in agent
communication language. Computational Intelligence,
18(2):89-101, 2002.
[7] F. Dignum, B. Dunin-K¸eplicz, and R. Verbrugge.
Agent theory for team formation by dialogue. In
C. Castelfranchi and Y. Lesp´erance, editors, Seventh
Workshop on Agent Theories, Architectures, and
Languages, pages 141-156, Boston, USA, 2000.
[8] P. M. Dung. On the acceptability of arguments and its
fundamental role in nonmonotonic reasoning, logic
programming and n-person games. Artificial
Intelligence, 77:321-357, 1995.
[9] P. M. Dung, R. A. Kowalski, and F. Toni. Dialectic
proof procedures for assumption-based, admissable
argumentation. Artificial Intelligence, 170(2):114-159,
2006.
[10] R. A. Flores and R. C. Kremer. To commit or not to
commit. Computational Intelligence, 18(2):120-173,
2002.
[11] D. M. Gabbay and J. Woods. More on
non-cooperation in Dialogue Logic. Logic Journal of
the IGPL, 9(2):321-339, 2001.
[12] D. M. Gabbay and J. Woods. Non-cooperation in
Dialogue Logic. Synthese, 127(1-2):161-186, 2001.
[13] C. L. Hamblin. Mathematical models of dialogue.
Theoria, 37:130-155, 1971.
[14] S. Kraus, K. Sycara, and A. Evenchik. Reaching
agreements through argumentation: a logical model
and implementation. Artificial Intelligence,
104(1-2):1-69, 1998.
[15] N. Oren, T. J. Norman, and A. Preece. Loose lips sink
ships: A heuristic for argumentation. In N. Maudet,
I. Rahwan, and S. Parsons, editors, Proceedings of the
Third Workshop on Argumentation in Muliagent
Systems, Hakodate, Japan, 2006.
[16] S. Parsons and N. R. Jennings. Negotiation through
argumentation - a preliminary report. In Proceedings
of Second International Conference on Multi-Agent
Systems, pages 267-274, 1996.
[17] S. Parsons, M. Wooldridge, and L. Amgoud. An
analysis of formal inter-agent dialogues. In 1st
International Conference on Autonomous Agents and
Multi-Agent Systems. ACM Press, 2002.
[18] S. Parsons, M. Wooldridge, and L. Amgoud. On the
outcomes of formal inter-agent dialogues. In 2nd
International Conference on Autonomous Agents and
Multi-Agent Systems. ACM Press, 2003.
[19] H. Prakken. On dialogue systems with speech acts,
arguments, and counterarguments. In Proceedings of
the Seventh European Workshop on Logic in Artificial
Intelligence, Berlin, Germany, 2000. Springer Verlag.
[20] H. Prakken. Relating protocols for dynamic dispute
with logics for defeasible argumentation. Synthese,
127:187-219, 2001.
[21] H. Prakken and G. Sartor. Modelling reasoning with
precedents in a formal dialogue game. Artificial
Intelligence and Law, 6:231-287, 1998.
[22] I. Rahwan, P. McBurney, and E. Sonenberg. Towards
a theory of negotiation strategy. In I. Rahwan,
P. Moraitis, and C. Reed, editors, Proceedings of the
1st International Workshop on Argumentation in
Multiagent Systems, New York, NY, 2004.
[23] C. Reed. Dialogue frames in agent communications. In
Y. Demazeau, editor, Proceedings of the Third
International Conference on Multi-Agent Systems,
pages 246-253. IEEE Press, 1998.
[24] M. Rovatsos, I. Rahwan, F. Fisher, and G. Weiss.
Adaptive strategies for practical argument-based
negotiation. In I. Rahwan, P. Moraitis, and C. Reed,
editors, Proceedings of the 1st International Workshop
on Argumentation in Multiagent Systems, New York,
NY, 2004.
[25] M. Schroeder, D. A. Plewe, and A. Raab. Ultima
ratio: should Hamlet kill Claudius. In Proceedings of
the 2nd International Conference on Autonomous
Agents, pages 467-468, 1998.
[26] K. Sycara. Argumentation: Planning other agents"
plans. In Proceedings of the Eleventh Joint Conference
on Artificial Intelligence, pages 517-523, 1989.
[27] D. N. Walton and E. C. W. Krabbe. Commitment in
Dialogue: Basic Concepts of Interpersonal Reasoning.
State University of New York Press, Albany, NY,
USA, 1995.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1013
On the Benefits of Cheating by Self-Interested Agents in
Vehicular Networks∗
Raz Lin and Sarit Kraus
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
{linraz,sarit}@cs.biu.ac.il
Yuval Shavitt
School of Electrical Engineering
Tel-Aviv University, Israel
shavitt@eng.tau.ac.il
ABSTRACT
As more and more cars are equipped with GPS and Wi-Fi
transmitters, it becomes easier to design systems that will
allow cars to interact autonomously with each other, e.g.,
regarding traffic on the roads. Indeed, car manufacturers
are already equipping their cars with such devices. Though,
currently these systems are a proprietary, we envision a 
natural evolution where agent applications will be developed
for vehicular systems, e.g., to improve car routing in dense
urban areas. Nonetheless, this new technology and agent 
applications may lead to the emergence of self-interested car
owners, who will care more about their own welfare than the
social welfare of their peers. These car owners will try to
manipulate their agents such that they transmit false data
to their peers. Using a simulation environment, which 
models a real transportation network in a large city, we 
demonstrate the benefits achieved by self-interested agents if no
counter-measures are implemented.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
Intelligence-Intelligent agents
General Terms
Experimentation
1. INTRODUCTION
As technology advances, more and more cars are being
equipped with devices, which enable them to act as 
autonomous agents. An important advancement in this 
respect is the introduction of ad-hoc communication networks
(such as Wi-Fi), which enable the exchange of information
between cars, e.g., for locating road congestions [1] and 
optimal routes [15] or improving traffic safety [2].
Vehicle-To-Vehicle (V2V) communication is already 
onboard by some car manufactures, enabling the 
collaboration between different cars on the road. For example, GM"s
proprietary algorithm [6], called the threat assessment 
algorithm, constantly calculates, in real time, other vehicles"
positions and speeds, and enables messaging other cars when
a collision is imminent; Also, Honda has began testing its
system in which vehicles talk with each other and with the
highway system itself [7].
In this paper, we investigate the attraction of being a 
selfish agent in vehicular networks. That is, we investigate the
benefits achieved by car owners, who tamper with on-board
devices and incorporate their own self-interested agents in
them, which act for their benefit. We build on the notion
of Gossip Networks, introduced by Shavitt and Shay [15], in
which the agents can obtain road congestion information by
gossiping with peer agents using ad-hoc communication.
We recognize two typical behaviors that the self-interested
agents could embark upon, in the context of vehicular 
networks. In the first behavior, described in Section 4, the
objective of the self-interested agents is to maximize their
own utility, expressed by their average journey duration on
the road. This situation can be modeled in real life by car
owners, whose aim is to reach their destination as fast as 
possible, and would like to have their way free of other cars. To
this end they will let their agents cheat the other agents, by
injecting false information into the network. This is achieved
by reporting heavy traffic values for the roads on their route
to other agents in the network in the hope of making the
other agents believe that the route is jammed, and causing
them to choose a different route.
The second type of behavior, described in Section 5, is
modeled by the self-interested agents" objective to cause
disorder in the network, more than they are interested in
maximizing their own utility. This kind of behavior could
be generated, for example, by vandalism or terrorists, who
aim to cause as much mayhem in the network as possible.
We note that the introduction of self-interested agents to
the network, would most probably motivate other agents to
try and detect these agents in order to minimize their effect.
This is similar, though in a different context, to the problem
introduced by Lamport et al. [8] as the Byzantine Generals
Problem. However, the introduction of mechanisms to deal
with self-interested agents is costly and time consuming. In
this paper we focus mainly on the attractiveness of selfish
behavior by these agents, while we also provide some insights
327
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
into the possibility of detecting self-interested agents and
minimizing their effect.
To demonstrate the benefits achieved by self-interested
agents, we have used a simulation environment, which 
models the transportation network in a central part of a large
real city. The simulation environment is further described in
Section 3. Our simulations provide insights to the benefits
of self-interested agents cheating. Our findings can motivate
future research in this field in order to minimize the effect
of selfish-agents.
The rest of this paper is organized as follows. In Section 2
we review related work in the field of self-interested agents
and V2V communications. We continue and formally 
describe our environment and simulation settings in Section 3.
Sections 4 and 5 describe the different behaviors of the 
selfinterested agents and our findings. Finally, we conclude the
paper with open questions and future research directions.
2. RELATED WORK
In their seminal paper, Lamport et al. [8] describe the
Byzantine Generals problem, in which processors need to
handle malfunctioning components that give conflicting 
information to different parts of the system. They also present
a model in which not all agents are connected, and thus an
agent cannot send a message to all the other agents. Dolev et
al. [5] has built on this problem and has analyzed the number
of faulty agents that can be tolerated in order to eventually
reach the right conclusion about true data. Similar work
is presented by Minsky et al. [11], who discuss techniques
for constructing gossip protocols that are resilient to up to
t malicious host failures. As opposed to the above works,
our work focuses on vehicular networks, in which the agents
are constantly roaming the network and exchanging data.
Also, the domain of transportation networks introduces 
dynamic data, as the load of the roads is subject to change. In
addition, the system in transportation networks has a 
feedback mechanism, since the load in the roads depends on the
reports and the movement of the agents themselves.
Malkhi et al. [10] present a gossip algorithm for 
propagating information in a network of processors, in the presence
of malicious parties. Their algorithm prevents the spreading
of spurious gossip and diffuses genuine data. This is done
in time, which is logarithmic in the number of processes
and linear in the number of corrupt parties. Nevertheless,
their work assumes that the network is static and also that
the agents are static (they discuss a network of processors).
This is not true for transportation networks. For example,
in our model, agents might gossip about heavy traffic load
of a specific road, which is currently jammed, yet this 
information might be false several minutes later, leaving the
agents to speculate whether the spreading agents are indeed
malicious or not. In addition, as the agents are constantly
moving, each agent cannot choose with whom he interacts
and exchanges data.
In the context of analyzing the data and deciding whether
the data is true or not, researchers have focused on 
distributed reputation systems or decision mechanisms to decide
whether or not to share data.
Yu and Singh [18] build a social network of agents" 
reputations. Every agent keeps a list of its neighbors, which can
be changed over time, and computes the trustworthiness of
other agents by updating the current values of testimonies
obtained from reliable referral chains. After a bad 
experience with another agent every agent decreases the rating of
the "bad" agent and propagates this bad experience 
throughout the network so that other agents can update their 
ratings accordingly. This approach might be implemented in
our domain to allow gossip agents to identify self-interested
agents and thus minimize their effect. However, the 
implementation of such a mechanism is an expensive addition
to the infrastructure of autonomous agents in 
transportation networks. This is mainly due to the dynamic nature of
the list of neighbors in transportation networks. Thus, not
only does it require maintaining the neighbors" list, since the
neighbors change frequently, but it is also harder to build a
good reputation system.
Leckie et al. [9] focus on the issue of when to share 
information between the agents in the network. Their domain
involves monitoring distributed sensors. Each agent 
monitors a subset of the sensors and evaluates a hypothesis based
on the local measurements of its sensors. If the agent 
believes that a hypothesis is sufficient likely he exchanges this
information with the other agents. In their domain, the
goal of all the agents is to reach a global consensus about
the likelihood of the hypothesis. In our domain, however, as
the agents constantly move, they have many samples, which
they exchange with each other. Also, the data might also
vary (e.g., a road might be reported as jammed, but a few
minutes later it could be free), thus making it harder to
decide whether to trust the agent, who sent the data. 
Moreover, the agent might lie only about a subset of its samples,
thus making it even harder to detect his cheating.
Some work has been done in the context of gossip networks
or transportation networks regarding the spreading of data
and its dissemination.
Datta et al. [4] focus on information dissemination in
mobile ad-hoc networks (MANET). They propose an 
autonomous gossiping algorithm for an infrastructure-less 
mobile ad-hoc networking environment. Their autonomous 
gossiping algorithm uses a greedy mechanism to spread data
items in the network. The data items are spread to 
immediate neighbors that are interested in the information, and
avoid ones that are not interested. The decision which node
is interested in the information is made by the data item
itself, using heuristics. However, their work concentrates on
the movement of the data itself, and not on the agents who
propagate the data. This is different from our scenario in
which each agent maintains the data it has gathered, while
the agent itself roams the road and is responsible (and has
the capabilities) for spreading the data to other agents in
the network.
Das et al. [3] propose a cooperative strategy for content
delivery in vehicular networks. In their domain, peers 
download a file from a mesh and exchange pieces of the file among
themselves. We, on the other hand, are interested in 
vehicular networks in which there is no rule forcing the agents to
cooperate among themselves.
Shibata et al. [16] propose a method for cars to 
cooperatively and autonomously collect traffic jam statistics to
estimate arrival time to destinations for each car. The 
communication is based on IEEE 802.11, without using a fixed
infrastructure on the ground. While we use the same 
domain, we focus on a different problem. Shibata et al. [16]
mainly focus on efficiently broadcasting the data between
agents (e.g., avoid duplicates and communication overhead),
as we focus on the case where agents are not cooperative in
328 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
nature, and on how selfish agents affect other agents and the
network load.
Wang et al. [17] also assert, in the context of wireless 
networks, that individual agents are likely to do what is most
beneficial for their owners, and will act selfishly. They design
a protocol for communication in networks in which all agents
are selfish. Their protocol motivates every agent to 
maximize its profit only when it behaves truthfully (a mechanism
of incentive compatibility). However, the domain of wireless
networks is quite different from the domain of 
transportation networks. In the wireless network, the wireless terminal
is required to contribute its local resources to transmit data.
Thus, Wang et al. [17] use a payment mechanism, which 
attaches costs to terminals when transmitting data, and thus
enables them to maximize their utility when transmitting
data, instead of acting selfishly. Unlike this, in the context
of transportation networks, constructing such a mechanism
is not quite a straightforward task, as self-interested agents
and regular gossip agents might incur the same cost when
transmitting data. The difference between the two types of
agents only exists regarding the credibility of the data they
exchange.
In the next section, we will describe our transportation
network model and gossiping between the agents. We will
also describe the different agents in our system.
3. MODEL AND SIMULATIONS
We first describe the formal transportation network model,
and then we describe the simulations designs.
3.1 Formal Model
Following Shavitt and Shay [15] and Parshani [13], the
transportation network is represented by a directed graph
G(V, E), where V is the set of vertices representing 
junctions, and E is the set of edges, representing roads. An edge
e ∈ E is associated with a weight w > 0, which specifies
the time it takes to traverse the road associated with that
edge. The roads" weights vary in time according to the 
network (traffic) load. Each car, which is associated with an
autonomous agent, is given a pair of origin and destination
points (vertices). A journey is defined as the (not 
necessarily simple) path taken by an agent between the origin vertex
and the destination vertex. We assume that there is always
a path between a source and a destination. A journey length
is defined as the sum of all weights of the edges constituting
this path. Every agent has to travel between its origin and
destination points and aims to minimize its journey length.
Initially, agents are ignorant about the state of the roads.
Regular agents are only capable of gathering information
about the roads as they traverse them. However, we assume
that some agents have means of inter-vehicle 
communication (e.g., IEEE 802.11) with a given communication range,
which enables them to communicate with other agents with
the same device. Those agents are referred to as gossip
agents. Since the communication range is limited, the 
exchange of information using gossiping is done in one of two
ways: (a) between gossip agents passing one another, or (b)
between gossip agents located at the same junction. We 
assume that each agent stores the most recent information it
has received or gathered around the edges in the network.
A subset of the gossip agents are those agents who are 
selfinterested and manipulate the devices for their own benefit.
We will refer to these agents as self-interested agents. A
detailed description of their behavior is given in Sections 4
and 5.
3.2 Simulation Design
Building on [13], the network in our simulations replicates
a central part of a large city, and consists of 50 junctions
and 150 roads, which are approximately the number of main
streets in the city. Each simulation consists of 6 iterations.
The basic time unit of the iteration is a step, which 
equivalents to about 30 seconds. Each iteration simulates six hours
of movements. The average number of cars passing through
the network during the iteration is about 70,000 and the 
average number of cars in the network at a specific time unit
is about 3,500 cars. In each iteration the same agents are
used with the same origin and destination points, whereas
the data collected in earlier iterations is preserved in the
future iterations (referred to as the history of the agent).
This allows us to simulate somewhat a daily routine in the
transportation network (e.g., a working week).
Each of the experiments that we describe below is run
with 5 different traffic scenarios. Each such traffic scenario
differs from one another by the initial load of the roads and
the designated routes of the agents (cars) in the network.
For each such scenario 5 simulations are run, creating a total
of 25 simulations for each experiment.
It has been shown by Parshani et al. [13, 14] that the 
information propagation in the network is very efficient when
the percentage of gossiping agents is 10% or more. Yet, due
to congestion caused by too many cars rushing to what is
reported as the less congested part of the network 20-30%
of gossiping agents leads to the most efficient routing results
in their experiments. Thus, in our simulation, we focus only
on simulations in which the percentage of gossip agents is
20%.
The simulations were done with different percentages of
self-interested agents. To gain statistical significance we ran
each simulation with changes in the set of the gossip agents,
and the set of the self-interested agents.
In order to gain a similar ordinal scale, the results were
normalized. The normalized values were calculated by 
comparing each agent"s result to his results when the same 
scenario was run with no self-interested agents. This was done
for all of the iterations. Using the normalized values enabled
us to see how worse (or better) each agent would perform
compared to the basic setting. For example, if an average
journey length of a certain agent in iteration 1 with no 
selfinterested agent was 50, and the length was 60 in the same
scenario and iteration in which self-interested agents were
involved, then the normalized value for that agent would be
60/50 = 1.2.
More details regarding the simulations are described in
Sections 4 and 5.
4. SPREADING LIES, MAXIMIZING 
UTILITY
In the first set of experiments we investigated the benefits
achieved by the self-interested agents, whose aim was to 
minimize their own journey length. The self-interested agents
adopted a cheating approach, in which they sent false data
to their peers.
In this section we first describe the simulations with the
self-interested agents. Then, we model the scenario as a
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 329
game with two types of agents, and prove that the 
equilibrium result can only be achieved when there is no efficient
exchange of gossiping information in the network.
4.1 Modeling the Self-Interested Agents" 
Behavior
While the gossip agents gather data and send it to other
agents, the self-interested agents" behavior is modeled as
follows:
1. Calculate the shortest path from origin to destination.
2. Communicate the following data to other agents:
(a) If the road is not in the agent"s route - send the
true data about it (e.g., data about roads it has
received from other agents)
(b) For all roads in the agent"s route, which the agent
has not yet traversed, send a random high weight.
Basically, the self-interested agent acts the same as the 
gossip agent. It collects data regarding the weight of the roads
(either by traversing the road or by getting the data from
other agents) and sends the data it has collected to other
agents. However, the self-interested agent acts differently
when the road is in its route. Since the agent"s goal is to
reach its destination as fast as possible, the agent will falsely
report that all the roads in its route are heavily congested.
This is in order to free the path for itself, by making other
agents recalculate their paths, this time without including
roads on the self-interested agent"s route. To this end, for
all the roads in its route, which the agent has not yet passed,
the agent generates a random weight, which is above the 
average weight of the roads in the network. It then associates
these new weights with the roads in its route and sends them
to the other agents.
While an agent can also divert cars from its route by
falsely reporting congested roads in parallel to its route as
free, this behavior is not very likely since other agents, 
attempting to use the roads, will find the mistake within a
short time and spread the true congestion on the road. On
the other hand, if an agent manages to persuade other agents
not to use a road, it will be harder for them to detect that
the said roads are not congested.
In addition, to avoid being influenced by its own lies and
other lies spreading in the network, all self-interested agents
will ignore data received about roads with heavy traffic (note
that data about roads that are not heavily traffic will not
be ignored)1
.
In the next subsection we describe the simulation results,
involving the self-interested agents.
4.2 Simulation Results
To test the benefits of cheating by the self-interested agents
we ran several experiments. In the first set of experiments,
we created a scenario, in which a small group of self-interested
agents spread lies on the same route, and tested its 
effect on the journey length of all the agents in the network.
1
In other simulations we have run, in which there had been
several real congestions in the network, we indeed saw that
even when the roads are jammed, the self-interested agents
were less affected if they ignored all reported heavy traffic,
since by such they also discarded all lies roaming the network
Table 1: Normalized journey length values, 
selfinterested agents with the same route
Iteration Self-Interested Gossip - Gossip - Regular
Number Agents SR Others Agents
1 1.38 1.27 1.06 1.06
2 0.95 1.56 1.18 1.14
3 1.00 1.86 1.28 1.17
4 1.06 2.93 1.35 1.16
5 1.13 2.00 1.40 1.17
6 1.08 2.02 1.43 1.18
Thus, several cars, which had the same origin and 
destination points, were designated as self-interested agents. In this
simulation, we selected only 6 agents to be part of the group
of the self-interested agents, as we wanted to investigate the
effect achieved by only a small number of agents.
In each simulation in this experiment, 6 different agents
were randomly chosen to be part of the group of self-interested
agents, as described above. In addition, one road, on the
route of these agents, was randomly selected to be partially
blocked, letting only one car go through that road at each
time step. About 8,000 agents were randomly selected as
regular gossip agents, and the other 32,000 agents were 
designated as regular agents.
We analyzed the average journey length of the self-interested
agents as opposed to the average journey length of other
regular gossip agents traveling along the same route. Table
1 summarizes the normalized results for the self-interested
agents, the gossip agents (those having the same origin and
destination points as the self-interested agents, denoted 
Gossip - SR, and all other gossip agents, denoted Gossip - 
Others) and the regular agents, as a function of the iteration
number.
We can see from the results that the first time the 
selfinterested agents traveled the route while spreading the false
data about the roads did not help them (using the paired
t-test we show that those agents had significantly lower 
journey lengths in the scenario in which they did not spread any
lies, with p < 0.01). This is mainly due to the fact that
the lies do not bypass the self-interested agent and reach
other cars that are ahead of the self-interested car on the
same route. Thus, spreading the lies in the first iteration
does not help the self-interested agent to free the route he
is about to travel, in the first iteration.
Only when the self-interested agents had repeated their
journey in the next iteration (iteration 2) did it help them
significantly (p = 0.04). The reason for this is that other 
gossip agents received this data and used it to recalculate their
shortest path, thus avoiding entrance to the roads, for which
the self-interested agents had spread false information about
congestion. It is also interesting to note the large value
attained by the self-interested agents in the first iteration.
This is mainly due to several self-interested agents, who 
entered the jammed road. This situation occurred since the
self-interested agents ignored all heavy traffic data, and thus
ignored the fact that the road was jammed. As they started
spreading lies about this road, more cars shifted from this
route, thus making the road free for the future iterations.
However, we also recall that the self-interested agents 
ignore all information about the heavy traffic roads. Thus,
330 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Table 2: Normalized journey length values, 
spreading lies for a beneficiary agent
Iteration Beneficiary Gossip - Gossip - Regular
Number Agent SR Others Agents
1 1.10 1.05 0.94 1.11
2 1.09 1.14 0.99 1.14
3 1.04 1.19 1.02 1.14
4 1.03 1.26 1.03 1.14
5 1.05 1.32 1.05 1.12
6 0.92 1.40 1.06 1.11
when the network becomes congested, more self-interested
cars are affected, since they might enter jammed roads,
which they would otherwise not have entered. This can be
seen, for example, in iterations 4-6, in which the normalized
value of the self-interested agents increased above 1.00. 
Using the paired t-test to compare these values with the values
achieved by these agents when no lies are used, we see that
there is no significant difference between the two scenarios.
As opposed to the gossip agents, we can see how little
effect the self-interested agents have on the regular agents.
As compared to the gossip agents on the same route that
have traveled as much as 193% more, when self-interested
agents are introduced, the average journey length for the
regular agents has only increased by about 15%. This result
is even lower than the effect on other gossip agents in the
entire network.
Since we noticed that cheating by the self-interested agents
does not benefit them in the first iteration, we devised 
another set of experiments. In the second set of experiments,
the self-interested agents have the objective to help another
agent, who is supposed to enter the network some time 
after the self-interested agent entered. We refer to the latter
agent as the beneficiary agent. Just like a self-interested
agent, the beneficiary agent also ignores all data regarding
heavy traffic. In real-life this can be modeled, for 
example, by a husband, who would like to help his wife find a
faster route to her destination. Table 2 summarizes the 
normalized values for the different agents. As in the first set
of experiments, 5 simulations were run for each scenario,
with a total of 25 simulations. In each of these simulation
one agent was randomly selected as a self-interested agent,
and then another agent, with the same origin as the 
selfinterested agent, was randomly selected as the beneficiary
agent. The other 8,000 and 32,000 agents were designated
as regular gossip agents and regular agents, respectively.
We can see that as the number of iterations advances, the
lower the normalized value for the beneficiary agent. In this
scenario, just like the previous one, in the first iterations,
not only does the beneficiary agent not avoid the jammed
roads, since he ignores all heavy traffic, he also does not
benefit from the lies spread by the self-interested agent. This
is due to the fact that the lies are not yet incorporated by
other gossip agents. Thus, if we compare the average journey
length in the first iteration when lies are spread and when
there are no lies, the average is significantly lower when there
are no lies (p < 0.03). On the other hand, if we compare
the average journey length in all of the iterations, there is
no significant difference between the two settings. Still, in
most of the iterations, the average journey length of the
beneficiary agent is longer than in the case when no lies are
spread.
We can also see the impact on the other agents in the
system. While the gossip agents, which are not on the
route of the beneficiary agent, virtually are not affected by
the self-interested agent, those on the route and the 
regular agents are affected and have higher normalized values.
That is, even with just one self-interested car, we can see
that both the gossip agents that follow the same route as
the lies spread by the self-interested agents, and other 
regular agents, increase their journey length by more than 14%.
In our third set of experiments we examined a setting in
which there was an increasing number of agents, and the
agents did not necessarily have the same origin and 
destination points. To model this we randomly selected 
selfinterested agents, whose objective was to minimize their
average journey length, assuming the cars were repeating
their journeys (that is, more than one iteration was made).
As opposed to the first set of experiments, in this set the
self-interested agents were selected randomly, and we did
not enforce the constraint that they will all have the same
origin and destination points.
As in the previous sets of experiments we ran 5 
different simulations per scenario. In each simulation 11 runs
were made, each run with different numbers of self-interested
agents: 0 (no self-interested agents), 1, 2, 4, 8, and 16. Each
agent adopted the behavior modeled in Section 4.1. Figure
1 shows the normalized value achieved by the self-interested
agents as a function of their number. The figure shows these
values for iterations 2-6. The first iteration is not shown 
intentionally, as we assume repeated journeys. Also, we have
seen in the previous set of experiments and we have 
provided explanations as to why the self-interested agents do
not gain much from their behavior in the first iteration.
0.955
0.96
0.965
0.97
0.975
0.98
0.985
0.99
0.995
1
1.005
1.01
1.015
1.02
1.025
1.03
0 1 2 3 4 5 6 7 8 9 10111213141516
Self-Interested Agents Number
NormalizedValue
Iteration 2 Iteration 3 Iteration 4
Iteration 5 Iteration 6
Figure 1: Self-interested agents normalized values as
a function of the number of self-interested agents.
Using these simulations we examined what the threshold
could be for the number of randomly selected self-interested
agents in order to allow themselves to benefit from their
selfish behavior. We can see that up to 8 self-interested
agents, the average normalized value is below 1. That is,
they benefit from their malicious behavior. In the case of
one self-interested agent there is a significant difference 
between the average journey length of when the agent spread
lies and when no lies are spread (p < 0.001), while when
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 331
there are 2, 4, 8 and 16 self-interested agents there is no
significance difference. Yet, as the number of self-interested
agents increases, the normalized value also increases. In
such cases, the normalized value is larger than 1, and the
self-interested agents journey length becomes significantly
higher than their journey length, in cases where there are
no self-interested agents in the system.
In the next subsection we analyze the scenario as a game
and show that when in equilibrium the exchange of gossiping
between the agents becomes inefficient.
4.3 When Gossiping is Inefficient
We continued and modeled our scenario as a game, in 
order to find the equilibrium. There are two possible types for
the agents: (a) regular gossip agents, and (b) self-interested
agents. Each of these agents is a representative of its group,
and thus all agents in the same group have similar behavior.
We note that the advantage of using gossiping in 
transportation networks is to allow the agents to detect anomalies
in the network (e.g., traffic jams) and to quickly adapt to
them by recalculating their routes [14]. We also assume that
the objective of the self-interested agents is to minimize their
own journey length, thus they spread lies on their routes, as
described in Section 4.1. We also assume that sophisticated
methods for identifying the self-interested agents or 
managing reputation are not used. This is mainly due to the 
complexity of incorporating and maintaining such mechanisms,
as well as due to the dynamics of the network, in which 
interactions between different agents are frequent, agents may
leave the network, and data about the road might change as
time progresses (e.g., a road might be reported by a regular
gossip agent as free at a given time, yet it may currently be
jammed due to heavy traffic on the road).
Let Tavg be the average time it takes to traverse an edge
in the transportation network (that is, the average load of
an edge). Let Tmax be the maximum time it takes to 
traverse an edge. We will investigate the game, in which the
self-interested and the regular gossip agents can choose the
following actions. The self-interested agents can choose how
much to lie, that is, they can choose to spread how long (not
necessarily the true duration) it takes to traverse certain
roads. Since the objective of the self-interested agents is to
spread messages as though some roads are jammed, the 
traversal time they report is obviously larger than the average
time. We denote the time the self-interested agents spread
as Ts, such that Tavg ≤ Ts ≤ Tmax. Motivated by the 
results of the simulations we have described above, we saw that
the agents are less affected if they discard the heavy traffic
values. Thus, the regular gossip cars, attempting to 
mitigate the effect of the liars, can choose a strategy to ignore
abnormal congestion values above a certain threshold, Tg.
Obviously, Tavg ≤ Tg ≤ Tmax. In order to prevent the 
gossip agents from detecting the lies and just discarding those
values, the self-interested agents send lies in a given range,
[Ts, Tmax], with an inverse geometric distribution, that is,
the higher the T value, the higher its frequency.
Now we construct the utility functions for each type of
agents, which is defined by the values of Ts and Tg. If the
self-interested agents spread traversal times higher than or
equal to the regular gossip cars" threshold, they will not
benefit from those lies. Thus, the utility value of the 
selfinterested agents in this case is 0. On the other hand, if the
self-interested agents spread traversal time which is lower
than the threshold, they will gain a positive utility value.
From the regular gossip agents point-of-view, if they accept
messages from the self-interested agents, then they 
incorporate the lies in their calculation, thus they will lose utility
points. On the other hand, if they discard the false values
the self-interested agents send, that is, they do not 
incorporate the lies, they will gain utility values. Formally, we use
us
to denote the utility of the self-interested agents and ug
to denote the utility of the regular gossip agents. We also
denote the strategy profile in the game as {Ts, Tg}. The
utility functions are defined as:
us
=
0 if Ts ≥ Tg
Ts − Tavg + 1 if Ts < Tg
(1)
ug
=
Tg − Tavg if Ts ≥ Tg
Ts − Tg if Ts < Tg
(2)
We are interested in finding the Nash equilibrium. We
recall from [12], that the Nash equilibrium is a strategy 
profile, where no player has anything to gain by deviating from
his strategy, given that the other agent follows his strategy
profile. Formally, let (S, u) denote the game, where S is
the set of strategy profiles and u is the set of utility 
functions. When each agent i ∈ {regular gossip, self-interested}
chooses a strategy Ti resulting in a strategy profile T =
(Ts, Tg) then agent i obtains a utility of ui
(T). A strategy
profile T∗
∈ S is a Nash equilibrium if no deviation in the
strategy by any single agent is profitable, that is, if for all i,
ui
(T∗
) ≥ ui
(Ti, T∗
−i). That is, (Ts, Tg) is a Nash equilibrium
if the self-interested agents have no other value Ts such that
us
(Ts, Tg) > us
(Ts, Tg), and similarly for the gossip agents.
We now have the following theorem.
Theorem 4.1. (Tavg, Tavg) is the only Nash equilibrium.
Proof. First we will show that (Tavg, Tavg) is a Nash 
equilibrium. Assume, by contradiction, that the gossip agents
choose another value Tg > Tavg. Thus, ug
(Tavg, Tg ) =
Tavg − Tg < 0. On the other hand, ug
(Tavg, Tavg) = 0.
Thus, the regular gossip agents have no incentive to 
deviate from this strategy. The self-interested agents also have
no incentive to deviate from this strategy. By 
contradiction, again assume that the self-interested agents choose
another value Ts > Tavg. Thus, us
(Ts , Tavg) = 0, while
us
(Tavg, Tavg) = 0.
We will now show that the above solution is unique. We
will show that any other tuple (Ts, Tg), such that Tavg <
Tg ≤ Tmax and Tavg < Ts ≤ Tmax is not a Nash equilibrium.
We have three cases. In the first Tavg < Tg < Ts ≤ Tmax.
Thus, us
(Ts, Tg) = 0 and ug
(Ts, Tg) = Tg − Tavg. In this
case, the regular gossip agents have an incentive to deviate
and choose another strategy Tg + 1, since by doing so they
increase their own utility: ug
(Ts, Tg + 1) = Tg + 1 − Tavg.
In the second case we have Tavg < Ts < Tg ≤ Tmax. Thus,
ug
(Ts, Tg) = Ts − Tg < 0. Also, the regular gossip agents
have an incentive to deviate and choose another strategy
Tg −1, in which their utility value is higher: ug
(Ts, Tg −1) =
Ts − Tg + 1.
In the last case we have Tavg < Ts = Tg ≤ Tmax. Thus,
us
(Ts, Tg) = Ts − Tg = 0. In this case, the self-interested
agents have an incentive to deviate and choose another 
strategy Tg − 1, in which their utility value is higher: us
(Tg −
1, Tg) = Tg − 1 − Tavg + 1 = Tg − Tavg > 0.
332 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Table 3: Normalized journey length values for the
first iteration
Self-Interested Self-Interested Gossip Regular
Agents Number Agents Agents Agents
1 0.98 1.01 1.05
2 1.09 1.02 1.05
4 1.07 1.02 1.05
8 1.06 1.04 1.05
16 1.03 1.08 1.06
32 1.07 1.17 1.08
50 1.12 1.28 1.1
64 1.14 1.4 1.13
80 1.15 1.5 1.14
100 1.17 1.63 1.16
Table 4: Normalized journey length values for all
iterations
Self-Interested Self-Interested Gossip Regular
Agents Number Agents Agents Agents
1 0.98 1.02 1.06
2 1.0 1.04 1.07
4 1.0 1.08 1.07
8 1.01 1.33 1.11
16 1.02 1.89 1.17
32 1.06 2.46 1.25
50 1.13 2.24 1.29
64 1.21 2.2 1.32
80 1.21 2.13 1.27
100 1.26 2.11 1.27
The above theorem proves that the equilibrium point is
reached only when the self-interested agents send the time
to traverse certain edges equals the average time, and on
the other hand the regular gossip agents discard all data 
regarding roads that are associated with an average time or
higher. Thus, for this equilibrium point the exchange of 
gossiping information between agents is inefficient, as the gossip
agents are unable to detect any anomalies in the network.
In the next section we describe another scenario for the
self-interested agents, in which they are not concerned with
their own utility, but rather interested in maximizing the
average journey length of other gossip agents.
5. SPREADING LIES, CAUSING CHAOS
Another possible behavior that can be adopted by 
selfinterested agents is characterized by their goal to cause 
disorder in the network. This can be achieved, for example, by
maximizing the average journey length of all agents, even at
the cost of maximizing their own journey length.
To understand the vulnerability of the gossip based 
transportation support system, we ran 5 different simulations for
each scenario. In each simulation different agents were 
randomly chosen (using a uniform distribution) to act as 
gossip agents, among them self-interested agents were chosen.
Each self-interested agent behaved in the same manner as
described in Section 4.1.
Every simulation consisted of 11 runs with each run 
comprising different numbers of self-interested agents: 0 (no 
selfinterested agents), 1, 2, 4, 8, 16, 32, 50, 64, 80 and 100.
Also, in each run the number of self-interested agents was
increased incrementally. For example: the run with 50 
selfinterested agents consisted of all the self-interested agents
that were used in the run with 32 self-interested agents, but
with an additional 18 self-interested agents.
Tables 3 and 4 summarize the normalized journey length
for the self-interested agents, the regular gossip agents and
the regular (non-gossip) agents. Table 3 summarizes the
data for the first iteration and Table 4 summarizes the data
for the average of all iterations. Figure 2 demonstrates
the changes in the normalized values for the regular gossip
agents and the regular agents, as a function of the iteration
number. Similar to the results in our first set of experiments,
described in Section 4.2, we can see that randomly selected
self-interested agents who follow different randomly selected
routes do not benefit from their malicious behavior (that is,
their average journey length does not decrease). However,
when only one self-interested agent is involved, it does 
benefit from the malicious behavior, even in the first iteration.
The results also indicate that the regular gossip agents are
more sensitive to malicious behavior than regular 
agentsthe average journey length for the gossip agents increases
significantly (e.g., with 32 self-interested agents the average
journey length for the gossip agents was 146% higher than
in the setting with no self-interested agents at all, as 
opposed to an increase of only 25% for the regular agents). In
contrast, these results also indicate that the self-interested
agents do not succeed in causing a significant load in the
network by their malicious behavior.
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
1 2 3 4 5 6
Iteration Number
NormalizedValue
32 self-interested agents, gossip agents normalized value
100 self-interested agents, gossip agents normalized value
32 self-interested agents, regular agents normalized value
100 self-interested agents, regular agents normalized value
Figure 2: Gossip and regular agents normalized 
values, as a function of the iteration.
Since the goal of the self-interested agents in this case is
to cause disorder in the network rather than use the lies for
their own benefits, the question arises as to why would the
behavior of the self-interested agents be to send lies about
their routes only. Furthermore, we hypothesize that if they
all send lies about the same major roads the damage they
might inflict on the entire network would be larger that had
each of them sent lies about its own route. To examine this
hypothesis, we designed another set of experiments. In this
set of experiments, all the self-interested agents spread lies
about the same 13 main roads in the network. However, the
results show quite a smaller impact on other gossip and 
reguThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 333
Table 5: Normalized journey length values for all
iterations. Network with congestions.
Self-Interested Self-Interested Gossip Regular
Agents Number Agents Agents Agents
1 1.04 1.02 1.22
2 1.06 1.04 1.22
4 1.04 1.06 1.23
8 1.07 1.15 1.26
16 1.09 1.55 1.39
32 1.12 2.25 1.56
50 1.24 2.25 1.60
64 1.28 2.47 1.63
80 1.50 2.41 1.64
100 1.69 2.61 1.75
lar agents in the network. The average normalized value for
the gossip agents in these simulations was only about 1.07,
as opposed to 1.7 in the original scenario. When analyzing
the results we saw that although the false data was spread,
it did not cause other gossip cars to change their route. The
main reason was that the lies were spread on roads that were
not on the route of the self-interested agents. Thus, it took
the data longer to reach agents on the main roads, and when
the agents reached the relevant roads this data was too old
to be incorporated in the other agents calculations.
We also examined the impact of sending lies in order to
cause chaos when there are already congestions in the 
network. To this end, we simulated a network in which 13 main
roads are jammed. The behavior of the self-interested agents
is as described in Section 4.1, and the self-interested agents
spread lies about their own route. The simulation results,
detailed in Table 5, show that there is a greater incentive
for the self-interested agents to cheat when the network is
already congested, as their cheating causes more damage
to the other agents in the network. For example, whereas
the average journey length of the regular agents increased
only by about 15% in the original scenario, in which the 
network was not congested, in this scenario the average journey
length of the agents had increased by about 60%.
6. CONCLUSIONS
In this paper we investigated the benefits achieved by
self-interested agents in vehicular networks. Using 
simulations we investigated two behaviors that might be taken
by self-interested agents: (a) trying to minimize their 
journey length, and (b) trying to cause chaos in the network.
Our simulations indicate that in both behaviors the 
selfinterested agents have only limited success achieving their
goal, even if no counter-measures are taken. This is in 
contrast to the greater impact inflicted by self-interested agents
in other domains (e.g., E-Commerce). Some reasons for this
are the special characteristics of vehicular networks and their
dynamic nature. While the self-interested agents spread lies,
they cannot choose which agents with whom they will 
interact. Also, by the time their lies reach other agents, they
might become irrelevant, as more recent data has reached
the same agents.
Motivated by the simulation results, future research in
this field will focus on modeling different behaviors of the
self-interested agents, which might cause more damage to
the network. Another research direction would be to find
ways of minimizing the effect of selfish-agents by using 
distributed reputation or other measures.
7. REFERENCES
[1] A. Bejan and R. Lawrence. Peer-to-peer cooperative
driving. In Proceedings of ISCIS, pages 259-264,
Orlando, USA, October 2002.
[2] I. Chisalita and N. Shahmehri. A novel architecture for
supporting vehicular communication. In Proceedings of
VTC, pages 1002-1006, Canada, September 2002.
[3] S. Das, A. Nandan, and G. Pau. Spawn: A swarming
protocol for vehicular ad-hoc wireless networks. In
Proceedings of VANET, pages 93-94, 2004.
[4] A. Datta, S. Quarteroni, and K. Aberer. Autonomous
gossiping: A self-organizing epidemic algorithm for
selective information dissemination in mobile ad-hoc
networks. In Proceedings of IC-SNW, pages 126-143,
Maison des Polytechniciens, Paris, France, June 2004.
[5] D. Dolev, R. Reischuk, and H. R. Strong. Early
stopping in byzantine agreement. JACM,
37(4):720-741, 1990.
[6] GM. Threat assessment algorithm.
http://www.nhtsa.dot.gov/people/injury/research/pub/
acas/acas-fieldtest/, 2000.
[7] Honda.
http://world.honda.com/news/2005/c050902.html.
[8] Lamport, Shostak, and Pease. The byzantine generals
problem. In Advances in Ultra-Dependable Distributed
Systems, N. Suri, C. J. Walter, and M. M. Hugue
(Eds.). IEEE Computer Society Press, 1982.
[9] C. Leckie and R. Kotagiri. Policies for sharing
distributed probabilistic beliefs. In Proceedings of
ACSC, pages 285-290, Adelaide, Australia, 2003.
[10] D. Malkhi, E. Pavlov, and Y. Sella. Gossip with
malicious parties. Technical report: 2003-9, School of
Computer Science and Engineering - The Hebrew
University of Jerusalem, Israel, March 2003.
[11] Y. M. Minsky and F. B. Schneider. Tolerating
malicious gossip. Distributed Computing, 16(1):49-68,
February 2003.
[12] M. J. Osborne and A. Rubinstein. A Course In Game
Theory. MIT Press, Cambridge MA, 1994.
[13] R. Parshani. Routing in gossip networks. Master"s
thesis, Department of Computer Science, Bar-Ilan
University, Ramat-Gan, Israel, October 2004.
[14] R. Parshani, S. Kraus, and Y. Shavitt. A study of
gossiping in transportation networks. Submitted for
publication, 2006.
[15] Y. Shavitt and A. Shay. Optimal routing in gossip
networks. IEEE Transactions on Vehicular
Technology, 54(4):1473-1487, July 2005.
[16] N. Shibata, T. Terauchi, T. Kitani, K. Yasumoto,
M. Ito, and T. Higashino. A method for sharing traffic
jam information using inter-vehicle communication. In
Proceedings of V2VCOM, USA, 2006.
[17] W. Wang, X.-Y. Li, and Y. Wang. Truthful multicast
routing in selfish wireless networks. In Proceedings of
MobiCom, pages 245-259, USA, 2004.
[18] B. Yu and M. P. Singh. A social mechanism of
reputation management in electronic communities. In
Proceedings of CIA, 2000.
334 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
A Multilateral Multi-issue Negotiation Protocol
Miniar Hemaissia
THALES Research &
Technology France
RD 128
F-91767 Palaiseau Cedex,
France
miniar.hemaissia@lip6.fr
Amal El Fallah
Seghrouchni
LIP6, University of Paris 6
8 rue du Capitaine Scott
F-75015 Paris, France
amal.elfallah@lip6.fr
Christophe Labreuche
and Juliette Mattioli
THALES Research &
Technology France
RD 128
F-91767 Palaiseau Cedex,
France
ABSTRACT
In this paper, we present a new protocol to address 
multilateral multi-issue negotiation in a cooperative context. We
consider complex dependencies between multiple issues by
modelling the preferences of the agents with a multi-criteria
decision aid tool, also enabling us to extract relevant 
information on a proposal assessment. This information is
used in the protocol to help in accelerating the search for
a consensus between the cooperative agents. In addition,
the negotiation procedure is defined in a crisis management
context where the common objective of our agents is also
considered in the preferences of a mediator agent.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Intelligent
agents, Multiagent systems
General Terms
Theory, Design, Experimentation
1. INTRODUCTION
Multi-issue negotiation protocols represent an important
field of study since negotiation problems in the real world
are often complex ones involving multiple issues. To date,
most of previous work in this area ([2, 3, 19, 13]) dealt 
almost exclusively with simple negotiations involving 
independent issues. However, real-world negotiation problems 
involve complex dependencies between multiple issues. When
one wants to buy a car, for example, the value of a given
car is highly dependent on its price, consumption, comfort
and so on. The addition of such interdependencies greatly
complicates the agents utility functions and classical 
utility functions, such as the weighted sum, are not sufficient
to model this kind of preferences. In [10, 9, 17, 14, 20], the
authors consider inter-dependencies between issues, most 
often defined with boolean values, except for [9], while we can
deal with continuous and discrete dependent issues thanks
to the modelling power of the Choquet integral. In [17],
the authors deal with bilateral negotiation while we are 
interested in a multilateral negotiation setting. Klein et al.
[10] present an approach similar to ours, using a mediator
too and information about the strength of the approval or
rejection that an agent makes during the negotiation. In
our protocol, we use more precise information to improve
the proposals thanks to the multi-criteria methodology and
tools used to model the preferences of our agents. Lin, in [14,
20], also presents a mediation service but using an 
evolutionary algorithm to reach optimal solutions and as explained
in [4], players in the evolutionary models need to repeatedly
interact with each other until the stable state is reached.
As the population size increases, the time it takes for the
population to stabilize also increases, resulting in excessive
computation, communication, and time overheads that can
become prohibitive, and for one-to-many and many-to-many
negotiations, the overheads become higher as the number of
players increases. In [9], the authors consider a non-linear
utility function by using constraints on the domain of the
issues and a mediation service to find a combination of bids
maximizing the social welfare. Our preference model, a 
nonlinear utility function too, is more complex than [9] one since
the Choquet integral takes into account the interactions and
the importance of each decision criteria/issue, not only the
dependencies between the values of the issues, to determine
the utility. We also use an iterative protocol enabling us to
find a solution even when no bid combination is possible.
In this paper, we propose a negotiation protocol suited for
multiple agents with complex preferences and taking into 
account, at the same time, multiple interdependent issues and
recommendations made by the agents to improve a proposal.
Moreover, the preferences of our agents are modelled using
a multi-criteria methodology and tools enabling us to take
into account information about the improvements that can
be made to a proposal, in order to help in accelerating the
search for a consensus between the agents. Therefore, we
propose a negotiation protocol consisting of solving our 
decision problem using a MAS with a multi-criteria decision
aiding modelling at the agent level and a cooperation-based
multilateral multi-issue negotiation protocol. This protocol
is studied under a non-cooperative approach and it is shown
943
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
that it has subgame perfect equilibria, provided that agents
behave rationally in the sense of von Neumann and 
Morgenstern. The approach proposed in this paper has been first
introduced and presented in [8]. In this paper, we present
our first experiments, with some noteworthy results, and a
more complex multi-agent system with representatives to
enable us to have a more robust system.
In Section 2, we present our application, a crisis 
management problem. Section 3 deals with the general aspect of the
proposed approach. The preference modelling is described
in sect. 4, whereas the motivations of our protocol are 
considered in sect. 5 and the agent/multiagent modelling in
sect. 6. Section 7 presents the formal modelling and 
properties of our protocol before presenting our first experiments
in sect. 8. Finally, in Section 9, we conclude and present
the future work.
2. CASE STUDY
This protocol is applied to a crisis management problem.
Crisis management is a relatively new field of management
and is composed of three types of activities: crisis 
prevention, operational preparedness and management of declared
crisis. The crisis prevention aims to bring the risk of crisis
to an acceptable level and, when possible, avoid that the 
crisis actually happens. The operational preparedness includes
strategic advanced planning, training and simulation to 
ensure availability, rapid mobilisation and deployment of 
resources to deal with possible emergencies. The management
of declared crisis is the response to - including the 
evacuation, search and rescue - and the recovery from the crisis by
minimising the effects of the crises, limiting the impact on
the community and environment and, on a longer term, by
bringing the community"s systems back to normal. In this
paper, we focus on the response part of the management of
declared crisis activity, and particularly on the evacuation
of the injured people in disaster situations. When a crisis
is declared, the plans defined during the operational 
preparedness activity are executed. For disasters, master plans
are executed. These plans are elaborated by the authorities
with the collaboration of civil protection agencies, police,
health services, non-governmental organizations, etc.
When a victim is found, several actions follow. First, a
rescue party is assigned to the victim who is examined and is
given first aid on the spot. Then, the victims can be placed
in an emergency centre on the ground called the medical
advanced post. For all victims, a sorter physician - 
generally a hospital physician - examines the seriousness of their
injuries and classifies the victims by pathology. The 
evacuation by emergency health transport if necessary can take
place after these clinical examinations and classifications.
Nowadays, to evacuate the injured people, the physicians
contact the emergency call centre to pass on the medical
assessments of the most urgent cases. The emergency call
centre then searches for available and appropriate spaces in
the hospitals to care for these victims. The physicians are
informed of the allocations, so they can proceed to the 
evacuations choosing the emergency health transports according
to the pathologies and the transport modes provided. In
this context, we can observe that the evacuation is based
on three important elements: the examination and 
classification of the victims, the search for an allocation and the
transport. In the case of the 11 March 2004 Madrid attacks,
for instance, some injured people did not receive the 
appropriate health care because, during the search for space, the
emergency call centre did not consider the transport 
constraints and, in particular, the traffic. Therefore, for a large
scale crisis management problem, there is a need to support
the emergency call centre and the physicians in the 
dispatching to take into account the hospitals and the transport 
constraints and availabilities.
3. PROPOSED APPROACH
To accept a proposal, an agent has to consider several 
issues such as, in the case of the crisis management problem,
the availabilities in terms of number of beds by unit, 
medical and surgical staffs, theatres and so on. Therefore, each
agent has its own preferences in correlation with its resource
constraints and other decision criteria such as, for the case
study, the level of congestion of a hospital. All the agents
also make decisions by taking into account the dependencies
between these decision criteria.
The first hypothesis of our approach is that there are 
several parties involved in and impacted by the decision, and
so they have to decide together according to their own 
constraints and decision criteria. Negotiation is the process by
which a group facing a conflict communicates with one 
another to try and come to a mutually acceptable agreement
or decision and so, the agents have to negotiate. The 
conflict we have to resolve is finding an acceptable solution for
all the parties by using a particular protocol. In our 
context, multilateral negotiation is a negotiation protocol type
that is the best suited for this type of problem : this type
of protocol enables the hospitals and the physicians to 
negotiate together. The negotiation also deals with multiple
issues. Moreover, an other hypothesis is that we are in a
cooperative context where all the parties have a common
objective which is to provide the best possible solution for
everyone. This implies the use of a negotiation protocol 
encouraging the parties involved to cooperate as satisfying its
preferences.
Taking into account these aspects, a Multi-Agent System
(MAS) seems to be a reliable method in the case of a 
distributed decision making process. Indeed, a MAS is a suitable
answer when the solution has to combine, at least, 
distribution features and reasoning capabilities. Another motivation
for using MAS lies in the fact that MAS is well known for
facilitating automated negotiation at the operative decision
making level in various applications.
Therefore, our approach consists of solving a multiparty
decision problem using a MAS with
• The preferences of the agents are modelled using a
multi-criteria decision aid tool, MYRIAD, also 
enabling us to consider multi-issue problems by evaluating
proposals on several criteria.
• A cooperation-based multilateral and multi-issue 
negotiation protocol.
4. THE PREFERENCE MODEL
We consider a problem where an agent has several decision
criteria, a set Nk = {1, . . . , nk} of criteria for each agent k
involved in the negotiation protocol. These decision criteria
enable the agents to evaluate the set of issues that are 
negotiated. The issues correspond directly or not to the decision
criteria. However, for the example of the crisis management
944 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
problem, the issues are the set of victims to dispatch 
between the hospitals. These issues are translated to decision
criteria enabling the hospital to evaluate its congestion and
so to an updated number of available beds, medical teams
and so on. In order to take into account the complexity that
exists between the criteria/issues, we use a multi-criteria 
decision aiding (MCDA) tool named MYRIAD [12] developed
at Thales for MCDA applications based on a two-additive
Choquet integral which is a good compromise between 
versatility and ease to understand and model the interactions
between decision criteria [6].
The set of the attributes of Nk is denoted by Xk
1 , . . . , Xk
nk
.
All the attributes are made commensurate thanks to the 
introduction of partial utility functions uk
i : Xk
i → [0, 1]. The
[0, 1] scale depicts the satisfaction of the agent k regarding
the values of the attributes. An option x is identified to an
element of Xk
= Xk
1 × · · · × Xk
nk
, with x = (x1, . . . , xnk ).
Then the overall assessment of x is given by
Uk(x) = Hk(uk
1 (x1), . . . , uh
nk
(xnk )) (1)
where Hk : [0, 1]nk → [0, 1] is the aggregation function. The
overall preference relation over Xk
is then
x y ⇐⇒ Uk(x) ≥ Uk(y) .
The two-additive Choquet integral is defined for
(z1, . . . , znk ) ∈ [0, 1]nk by [7]
Hk(z1, . . . , znk ) =
X
i∈Nk
0
@vk
i −
1
2
X
j=i
|Ik
i,j|
1
A zi
+
X
Ik
i,j >0
Ik
i,j zi ∧ zj +
X
Ii,j <0
|Ii,j| zi ∨ zj (2)
where vk
i is the relative importance of criterion i for agent
k and Ik
i,j is the interaction between criteria i and j, ∧ and
∨ denote the min and max functions respectively. Assume
that zi < zj. A positive interaction between criteria i and
j depicts complementarity between these criteria (positive
synergy) [7]. Hence, the lower score of z on criterion i 
conceals the positive effect of the better score on criterion j to
a larger extent on the overall evaluation than the impact of
the relative importance of the criteria taken independently
of the other ones. In other words, the score of z on criterion
j is penalized by the lower score on criterion i. Conversely, a
negative interaction between criteria i and j depicts 
substitutability between these criteria (negative synergy) [7]. The
score of z on criterion i is then saved by a better score on
criterion j.
In MYRIAD, we can also obtain some recommendations
corresponding to an indicator ωC (H, x) measuring the worth
to improve option x w.r.t. Hk on some criteria C ⊆ Nk as
follows
ωC (Hk, x)=
Z 1
0
Hk
`
(1 − τ)xC + τ, xNk\C
´
− Hk(x)
EC (τ, x)
dτ
where ((1−τ)xC +τ, xNk\C ) is the compound act that equals
(1 − τ)xi + τ if i ∈ C and equals xi if i ∈ Nk \ C. Moreover,
EC (τ, x) is the effort to go from the profile x to the profile
((1 − τ)xC + τ, xNk\C ). Function ωC (Hk, x) depicts the 
average improvement of Hk when the criteria of coalition A
range from xC to 1C divided by the average effort needed
for this improvement. We generally assume that EC is of
order 1, that is EC (τ, x) = τ
P
i∈C (1 − xi). The expression
of ωC (Hk, x) when Hk is a Choquet integral, is given in [11].
The agent is then recommended to improve of coalition C
for which ωC (Hk, x) is maximum. This recommendation is
very useful in a negotiation protocol since it helps the agents
to know what to do if they want an offer to be accepted while
not revealing their own preference model.
5. PROTOCOL MOTIVATIONS
For multi-issue problems, there are two approaches: a
complete package approach where the issues are 
negotiated simultaneously in opposition to the sequential approach
where the issues are negotiated one by one. When the issues
are dependant, then it is the best choice to bargain 
simultaneously over all issues [5]. Thus, the complete package is
the adopted approach so that an offer will be on the 
overall set of injured people while taking into account the other
decision criteria.
We have to consider that all the parties of the 
negotiation process have to agree on the decision since they are all
involved in and impacted by this decision and so an 
unanimous agreement is required in the protocol. In addition, no
party can leave the process until an agreement is reached,
i.e. a consensus achieved. This makes sense since a proposal
concerns all the parties. Moreover, we have to guarantee the
availability of the resources needed by the parties to ensure
that a proposal is realistic. To this end, the information
about these availabilities are used to determine admissible
proposals such that an offer cannot be made if one of the 
parties has not enough resources to execute/achieve it. At the
beginning of the negotiation, each party provides its 
maximum availabilities, this defining the constraints that have
to be satisfied for each offer submitted.
The negotiation has also to converge quickly on an 
unanimous agreement. We decided to introduce in the negotiation
protocol an incentive to cooperate taking into account the
passed negotiation time. This incentive is defined on the
basis of a time dependent penalty, the discounting factor as
in [18] or a time-dependent threshold. This penalty has to
be used in the accept/reject stage of our consensus 
procedure. In fact, in the case of a discounting factor, each party
will accept or reject an offer by evaluating the proposal 
using its utility function deducted from the discounting factor.
In the case of a time-dependent threshold, if the evaluation
is greater or equal to this threshold, the offer is accepted,
otherwise, in the next period, its threshold is reduced.
The use of a penalty is not enough alone since it does
not help in finding a solution. Some information about the
assessments of the parties involved in the negotiation are
needed. In particular, it would be helpful to know why an
offer has been rejected and/or what can be done to make
a proposal that would be accepted. MYRIAD provides an
analysis that determines the flaws an option, here a 
proposal. In particular, it gives this type of information: which
criteria of a proposal should be improved so as to reach the
highest possible overall evaluation [11]. As we use this tool
to model the parties involved in the negotiation, the 
information about the criteria to improve can be used by the
mediator to elaborate the proposals. We also consider that
the dual function can be used to take into account another
type of information: on which criteria of a proposal, no 
improvement is necessary so that the overall evaluation of a
proposal is still acceptable, do not decrease. Thus, all 
information is a constraint to be satisfied as much as possible by
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 945
Figure 1: An illustration of some system.
the parties to make a new proposal.
We are in a cooperative context and revealing one"s 
opinion on what can be improved is not prohibited, on the 
contrary, it is useful and recommended here seeing that it helps
in converging on an agreement. Therefore, when one of the
parties refuses an offer, some information will be 
communicated. In order to facilitate and speed up the negotiation,
we introduce a mediator. This specific entity is in charge
of making the proposals to the other parties in the system
by taking into account their public constraints (e.g. their
availabilities) and the recommendations they make. This
mediator can also be considered as the representative of
the general interest we can have, in some applications, such
as in the crisis management problem, the physician will be
the mediator and will also have some more information to
consider when making an offer (e.g. traffic state, transport
mode and time). Each party in a negotiation N, a 
negotiator, can also be a mediator of another negotiation N , this
party becoming the representative of N in the negotiation
N, as illustrated by fig. 1 what can also help in reducing
the communication time.
6. AGENTIFICATION
How the problem is transposed in a MAS problem is a
very important aspect when designing such a system. The
agentification has an influence upon the systems efficiency in
solving the problem. Therefore, in this section, we describe
the elements and constraints taken into account during the
modelling phase and for the model itself. However, for this
negotiation application, the modelling is quite natural when
one observes the negotiation protocol motivations and main
properties.
First of all, it seems obvious that there should be one agent
for each player of our multilateral multi-issue negotiation
protocol. The agents have the involved parties" information
and preferences. These agents are:
• Autonomous: they decide for themselves what, when
and under what conditions actions should be 
performed;
• Rational: they have a means-ends competence to fit
its decisions, according to its knowledge, preferences
and goal;
• Self-interested: they have their own interests which
may conflict with the interests of other agents.
Moreover, their preferences are modelled and a proposal
evaluated and analysed using MYRIAD. Each agent has
private information and can access public information as
knowledge.
In fact, there are two types of agents: the mediator type
for the agents corresponding to the mediator of our 
negotiation protocol, the delegated physician in our application,
and the negotiator type for the agents corresponding to
the other parties, the hospitals. The main behaviours that
an agent of type mediator needs to negotiate in our protocol
are the following:
• convert_improvements: converts the information 
given by the other agents involved in the negotiation
about the improvements to be done, into constraints
on the next proposal to be made;
• convert_no_decrease: converts the information given
by the other agents involved in the negotiation about
the points that should not be changed into constraints
on the next proposal to be made;
• construct_proposal: constructs a new proposal 
according to the constraints obtained with 
convert_improvements, convert_no_decrease and the agent 
preferences;
The main behaviours that an agent of type negotiator
needs to negotiate in our protocol are the following:
• convert_proposal: converts a proposal to a MYRIAD
option of the agent according to its preferences model
and its private data;
• convert_improvements_wc: converts the agent 
recommendations for the improvements of a MYRIAD 
option into general information on the proposal;
• convert_no_decrease_wc: converts the agent 
recommendations about the criteria that should not be 
changed in the MYRIAD option into general information
on the proposal;
In addition to these behaviours, there are, for the two types
of agents, access behaviours to MYRIAD functionalities
such as the evaluation and improvement functions:
• evaluate_option: evaluates the MYRIAD option 
obtained using the agent behaviour convert_proposal;
• improvements: gets the agent recommendations to 
improve a proposal from the MYRIAD option;
• no_decrease: gets the agent recommendations to not
change some criteria from the MYRIAD option;
Of course, before running the system with such agents, we
must have defined each party preferences model in 
MYRIAD. This model has to be part of the agent so that it could
be used to make the assessments and to retrieve the 
improvements. In addition to these behaviours, the communication
acts between the agents is as follows.
1. mediator agent communication acts:
946 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
m1 m
1
1
m
inform1 m
mediator negotiator
accept−proposal
l
1
accept−proposal
m−l
reject−proposal
propose
propose
Figure 2: The protocol diagram in AUML, and
where m is the number of negotiator agents and l
is the number of agents refusing current proposal.
(a) propose: sends a message containing a proposal
to all negotiator agents;
(b) inform: sends a message to all negotiator agents
to inform them that an agreement has been 
reached and containing the consensus outcome.
2. negotiator agent communication acts:
(a) accept-proposal: sends a message to the 
mediator agent containing the agent recommendations
to improve the proposal and obtained with
convert_improvements_wc;
(b) reject-proposal: sends a message to the 
mediator agent containing the agent recommendations
about the criteria that should not be changed and
obtained with convert_no_decrease_wc.
Such agents are interchangeable, in a case of failure, since
they all have the same properties and represent a user with
his preference model, not depending on the agent, but on the
model defined in MYRIAD. When the issues and the 
decision criteria are different from each other, the information
about the criteria improvement have to be pre-processed to
give some instructions on the directions to take and about
the negotiated issues. It is the same for the evaluation of a
proposal: each agent has to convert the information about
the issues to update its private information and to obtain
the values of each attribute of the decision criteria.
7. OUR PROTOCOL
Formally, we consider negotiations where a set of 
players A = {1, 2, . . . , m} and a player a are negotiating over
a set Q of size q. The player a is the protocol 
mediator, the mediator agent of the agentification. The 
utility/preference function of a player k ∈ A ∪ {a} is Uk, 
defined using MYRIAD, as presented in Section 4, with a set
Nk of criteria, Xk
an option, and so on. An offer is a 
vector P = (P1, P2, · · · , Pm), a partition of Q, in which Pk is
player k"s share of Q. We have P ∈ P where P is the set of
admissible proposals, a finite set. Note that P is determined
using all players general constraints on the proposals and Q.
Moreover, let ˜P denote a particular proposal defined as a"s
preferred proposal.
We also have the following notation: δk is the 
threshold decrease factor of player k, Φk : Pk → Xk
is player
k"s function to convert a proposal to an option and Ψk is
the function indicating which points P has to be improved,
with Ψk its dual function - on which points no 
improvement is necessary. Ψk is obtained using the dual function of
ωC (Hk, x):
eωC (Hk, x)=
Z 1
0
Hk(x) − Hk
`
τ xC , xNk\C
´
eEC (τ, x)
dτ
Where eEC (τ, x) is the cost/effort to go from (τxC , xNk\C )
to x.
In period t of our consensus procedure, player a proposes
an agreement P. All players k ∈ A respond to a by 
accepting or rejecting P. The responses are made simultaneously.
If all players k ∈ A accept the offer, the game ends. If any
player k rejects P, then the next period t+1 begins: player a
makes another proposal P by taking into account 
information provided by the players and the ones that have rejected
P apply a penalty. Therefore, our negotiation protocol can
be as follows:
Protocol P1.
• At the beginning, we set period t = 0
• a makes a proposal P ∈ P that has not been
proposed before.
• Wait that all players of A give their opinion
Yes or No to the player a. If all players
agree on P, this later is chosen. Otherwise
t is incremented and we go back to previous
point.
• If there is no more offer left from P, the
default offer ˜P will be chosen.
• The utility of players regarding a given 
offer decreases over time. More precisely, the
utility of player k ∈ A at period t regarding
offer P is Uk(Φk(Pk), t) = ft(Uk(Φk(Pk))),
where one can take for instance ft(x) =
x.(δk)t
or ft(x) = x − δk.t, as penalty 
function.
Lemma 1. Protocol P1 has at least one subgame perfect
equilibrium 1
.
Proof : Protocol P1 is first transformed in a game in 
extensive form. To this end, one shall specify the order in which
the responders A react to the offer P of a. However the
order in which the players answer has no influence on the
course of the game and in particular on their personal 
utility. Hence protocol P1 is strictly equivalent to a game in
1
A subgame perfect equilibrium is an equilibrium such that
players" strategies constitute a Nash equilibrium in every
subgame of the original game [18, 16]. A Nash equilibrium
is a set of strategies, one for each player, such that no player
has incentive to unilaterally change his/her action [15].
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 947
extensive form, considering any order of the players A. This
game is clearly finite since P is finite and each offer can
only be proposed once. Finally P1 corresponds to a game
with perfect information. We end the proof by using a 
classical result stating that any finite game in extensive form
with perfect information has at least one subgame perfect
equilibrium (see e.g. [16]).
Rational players (in the sense of von Neumann and 
Morgenstern) involved in protocol P1 will necessarily come up
with a subgame perfect equilibrium.
Example 1. Consider an example with A = {1, 2} and
P = {P1
, P2
, P3
} where the default offer is P1
. Assume
that ft(x) = x − 0.1 t. Consider the following table giving
the utilities at t = 0.
P1
P2
P3
a 1 0.8 0.7
1 0.1 0.7 0.5
2 0.1 0.3 0.8
It is easy to see that there is one single subgame perfect 
equilibrium for protocol P1 corresponding to these values. This
equilibrium consists of the following choices: first a proposes
P3
; player 1 rejects this offer; a proposes then P2
and both
players 1 and 2 accepts otherwise they are threatened to 
receive the worse offer P1
for them. Finally offer P2
is 
chosen. Option P1
is the best one for a but the two other players
vetoed it. It is interesting to point out that, even though a
prefers P2
to P3
, offer P3
is first proposed and this make
P2
being accepted. If a proposes P2
first, then the subgame
perfect equilibrium in this situation is P3
. To sum up, the
worse preferred options have to be proposed first in order to
get finally the best one. But this entails a waste of time.
Analysing the previous example, one sees that the game
outcome at the equilibrium is P2
that is not very attractive
for player 2. Option P3
seems more balanced since no player
judges it badly. It could be seen as a better solution as a
consensus among the agents.
In order to introduce this notion of balanceness in the
protocol, we introduce a condition under which a player will
be obliged to accept the proposal, reducing the autonomy
of the agents but for increasing rationality and cooperation.
More precisely if the utility of a player is larger than a given
threshold then acceptance is required. The threshold 
decreases over time so that players have to make more and
more concession. Therefore, the protocol becomes as 
follows.
Protocol P2.
• At the beginning we set period t = 0
• a makes a proposal P ∈ P that has not been
proposed before.
• Wait that all players of A give their opinion
Yes or No to the player a. A player k must
accept the offer if Uk(Φk(Pk)) ≥ ρk(t) where
ρk(t) tends to zero when t grows. 
Moreover there exists T such that for all t ≥ T,
ρk(t) = 0. If all players agree on P, this
later is chosen. Otherwise t is incremented
and we go back to previous point.
• If there is no more offer left from P, the
default offer ˜P will be chosen.
One can show exactly as in Lemma 1 that protocol P2
has at least one subgame perfect equilibrium. We expect
that protocol P2 provides a solution not to far from P ,
so it favours fairness among the players. Therefore, our
cooperation-based multilateral multi-issue protocol is the
following:
Protocol P.
• At the beginning we set period t = 0
• a makes a proposal P ∈ P that has not
been proposed before, considering Ψk(Pt
)
and Ψk(Pt
) for all players k ∈ A.
• Wait that all players of A give their opinion
(Yes , Ψk(Pt
)) or (No , Ψk(Pt
)) to the
player a. A player k must accept the offer
if Uk(Φk(Pk)) ≥ ρk(t) where ρk(t) tends to
zero when t grows. Moreover there exists
T such that for all t ≥ T, ρk(t) = 0. If
all players agree on P, this later is chosen.
Otherwise t is incremented and we go back
to previous point.
• If there is no more offer left from P, the
default offer ˜P will be chosen.
8. EXPERIMENTS
We developed a MAS using the widely used JADE agent
platform [1]. This MAS is designed to be as general as 
possible (e.g. a general framework to specialise according to the
application) and enable us to make some preliminary 
experiments. The experiments aim at verifying that our approach
gives solutions as close as possible to the Maximin solution
and in a small number of rounds and hopefully in a short
time since our context is highly cooperative. We defined
the two types of agents and their behaviours as introduced
in section 6. The agents and their behaviours correspond
to the main classes of our prototype, NegotiatorAgent
and NegotiatorBehaviour for the negotiator agents, and
MediatorAgent and MediatorBehaviour for the mediator
agent. These classes extend JADE classes and integrate
MYRIAD into the agents, reducing the amount of 
communications in the system. Some functionalities depending on
the application have to be implemented according to the 
application by extending these classes. In particular, all 
conversion parts of the agents have to be specified according to
the application since to convert a proposal into decision 
criteria, we need to know, first, this model and the correlations
between the proposals and this model.
First, to illustrate our protocol, we present a simple 
example of our dispatch problem. In this example, we have
three hospitals, H1, H2 and H3. Each hospital can receive
victims having a particular pathology in such a way that
H1 can receive patients with the pathology burn, surgery
or orthopedic, H2 can receive patients with the pathology
surgery, orthopedic or cardiology and H3 can receive 
patients with the pathology burn or cardiology. All the 
hospitals have similar decision criteria reflecting their preferences
on the level of congestion they can face for the overall 
hospital and the different services available, as briefly explained
for hospital H1 hereafter.
For hospital H1, the preference model, fig. 3, is composed
of five criteria. These criteria correspond to the preferences
on the pathologies the hospital can treat. In the case of
948 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 3: The H1 preference model in MYRIAD.
the pathology burn, the corresponding criterion, also named
burn as shown in fig. 3, represents the preferences of H1 
according to the value of Cburn which is the current capacity
of burn. Therefore, the utility function of this criterion 
represents a preference such that the more there are patients of
this pathology in the hospital, the less the hospital may 
satisfy them, and this with an initial capacity. In addition to
reflecting this kind of viewpoint, the aggregation function as
defined in MYRIAD introduces a veto on the criteria burn,
surgery, orthopedic and EReceipt, where EReceipt is the
criterion for the preferences about the capacity to receive a
number of patients at the same time.
In this simplified example, the physician have no 
particular preferences on the dispatch and the mediator agent
chooses a proposal randomly in a subset of the set of 
admissibility. This subset have to satisfy as much as possible the
recommendations made by the hospitals. To solve this 
problem, for this example, we decided to solve a linear problem
with the availability constraints and the recommendations
as linear constraints on the dispatch values. The set of 
admissibility is then obtained by solving this linear problem
by the use of Prolog. Moreover, only the recommendations
on how to improve a proposal are taken into account. The
problem to solve is then to dispatch to hospital H1, H2 and
H3, the set of victims composed of 5 victims with the 
pathology burn, 10 with surgery, 3 with orthopedic and 7 with
cardiology. The availabilities of the hospitals are as 
presented in the following table.
Available Overall burn surg. orthop. cardio.
H1 11 4 8 
10H2 25 - 3 4 10
H3 7 10 - - 3
We obtain a multiagent system with the mediator agent
and three agents of type negotiator for the three hospital
in the problem. The hospitals threshold are fixed 
approximatively to the level where an evaluation is considered as
good. To start, the negotiator agents send their 
availabilities. The mediator agent makes a proposal chosen randomly
in admissible set obtained with these availabilities as 
linear constraints. This proposal is the vector P0 = [[H1,burn,
3], [H1, surgery, 8], [H1, orthopaedic, 0], [H2, surgery, 2],
[H2, orthopaedic, 3], [H2, cardiology, 6], [H3, burn, 2], [H3,
cardiology, 1]] and the mediator sends propose(P0) to H1,
H2 and H3 for approval. Each negotiator agent evaluates
this proposal and answers back by accepting or rejecting P0:
• Agent H1 rejects this offer since its evaluation is very
far from the threshold (0.29, a bad score) and gives
a recommendation to improve burn and surgery by
sending the message 
reject_proposal([burn,surgery]);
• Agent H2 accepts this offer by sending the message
accept_proposal(), the proposal evaluation being 
good;
• Agent H3 accepts P0 by sending the message accept_
proposal(), the proposal evaluation being good.
Just with the recommendations provided by agent H1, the
mediator is able to make a new proposal by restricting the
value of burn and surgery. The new proposal obtained is
then P1 = [[H1,burn, 0], [H1, surgery, 8], [H1, orthopaedic,
1], [H2, surgery, 2], [H2, orthopaedic, 2], [H2, cardiology,
6], [H3, burn, 5], [H3, cardiology, 1]]. The mediator sends
propose(P1) the negotiator agents. H1, H2 and H3 answer
back by sending the message accept_proposal(), P1 being
evaluated with a high enough score to be acceptable, and
also considered as a good proposal when using the 
explanation function of MYRIAD. An agreement is reached with
P1. Note that the evaluation of P1 by H3 has decreased in
comparison with P0, but not enough to be rejected and that
this solution is the Pareto one, P∗
.
Other examples have been tested with the same settings:
issues in IN, three negotiator agents and the same mediator
agent, with no preference model but selecting randomly the
proposal. We obtained solutions either equal or close to the
Maximin solution, the distance from the standard deviation
being less than 0.0829, the evaluations not far from the ones
obtained with P∗
and with less than seven proposals made.
This shows us that we are able to solve this multi-issue 
multilateral negotiation problem in a simple and efficient way,
with solutions close to the Pareto solution.
9. CONCLUSION AND FUTURE WORK
This paper presents a new protocol to address 
multilateral multi-issue negotiation in a cooperative context. The
first main contribution is that we take into account complex
inter-dependencies between multiple issues with the use of
a complex preference modelling. This contribution is 
reinforced by the use of multi-issue negotiation in a multilateral
context. Our second contribution is the use of sharp 
recommendations in the protocol to help in accelerating the search
of a consensus between the cooperative agents and in finding
an optimal solution. We have also shown that the protocol
has subgame perfect equilibria and these equilibria converge
to the usual maximum solution. Moreover, we tested this
protocol in a crisis management context where the 
negotiation aim is where to evacuate a whole set of injured people
to predefined hospitals.
We have already developed a first MAS, in particular 
integrating MYRIAD, to test this protocol in order to know
more about its efficiency in terms of solution quality and
quickness in finding a consensus. This prototype enabled
us to solve some examples with our approach and the 
results we obtained are encouraging since we obtained quickly
good agreements, close to the Pareto solution, in the light
of the initial constraints of the problem: the availabilities.
We still have to improve our MAS by taking into account
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 949
the two types of recommendations and by adding a 
preference model to the mediator of our system. Moreover, a
comparative study has to be done in order to evaluate the
performance of our framework against the existing ones and
against some variations on the protocol.
10. ACKNOWLEDGEMENT
This work is partly funded by the ICIS research project
under the Dutch BSIK Program (BSIK 03024).
11. REFERENCES
[1] JADE. http://jade.tilab.com/.
[2] P. Faratin, C. Sierra, and N. R. Jennings. Using
similarity criteria to make issue trade-offs in
automated negotiations. Artificial Intelligence,
142(2):205-237, 2003.
[3] S. S. Fatima, M. Wooldridge, and N. R. Jennings.
Optimal negotiation of multiple issues in incomplete
information settings. In 3rd International Joint
Conference on Autonomous Agents and Multiagent
Systems (AAMAS"04), pages 1080-1087, New York,
USA, 2004.
[4] S. S. Fatima, M. Wooldridge, and N. R. Jennings. A
comparative study of game theoretic and evolutionary
models of bargaining for software agents. Artificial
Intelligence Review, 23:185-203, 2005.
[5] S. S. Fatima, M. Wooldridge, and N. R. Jennings. On
efficient procedures for multi-issue negotiation. In 8th
International Workshop on Agent-Mediated Electronic
Commerce(AMEC"06), pages 71-84, Hakodate, Japan,
2006.
[6] M. Grabisch. The application of fuzzy integrals in
multicriteria decision making. European J. of
Operational Research, 89:445-456, 1996.
[7] M. Grabisch, T. Murofushi, and M. Sugeno. Fuzzy
Measures and Integrals. Theory and Applications
(edited volume). Studies in Fuzziness. Physica Verlag,
2000.
[8] M. Hemaissia, A. El Fallah-Seghrouchni,
C. Labreuche, and J. Mattioli. Cooperation-based
multilateral multi-issue negotiation for crisis
management. In 2th International Workshop on
Rational, Robust and Secure Negotiation (RRS"06),
pages 77-95, Hakodate, Japan, May 2006.
[9] T. Ito, M. Klein, and H. Hattori. A negotiation
protocol for agents with nonlinear utility functions. In
AAAI, 2006.
[10] M. Klein, P. Faratin, H. Sayama, and Y. Bar-Yam.
Negotiating complex contracts. Group Decision and
Negotiation, 12:111-125, March 2003.
[11] C. Labreuche. Determination of the criteria to be
improved first in order to improve as much as possible
the overall evaluation. In IPMU 2004, pages 609-616,
Perugia, Italy, 2004.
[12] C. Labreuche and F. Le Hu´ed´e. MYRIAD: a tool suite
for MCDA. In EUSFLAT"05, pages 204-209,
Barcelona, Spain, 2005.
[13] R. Y. K. Lau. Towards genetically optimised
multi-agent multi-issue negotiations. In Proceedings of
the 38th Annual Hawaii International Conference on
System Sciences (HICSS"05), Big Island, Hawaii, 2005.
[14] R. J. Lin. Bilateral multi-issue contract negotiation for
task redistribution using a mediation service. In Agent
Mediated Electronic Commerce VI (AMEC"04), New
York, USA, 2004.
[15] J. F. Nash. Non cooperative games. Annals of
Mathematics, 54:286-295, 1951.
[16] G. Owen. Game Theory. Academic Press, New York,
1995.
[17] V. Robu, D. J. A. Somefun, and J. A. L. Poutr´e.
Modeling complex multi-issue negotiations using
utility graphs. In 4th International Joint Conference
on Autonomous agents and multiagent systems
(AAMAS"05), pages 280-287, 2005.
[18] A. Rubinstein. Perfect equilibrium in a bargaining
model. Econometrica, 50:97-109, jan 1982.
[19] L.-K. Soh and X. Li. Adaptive, confidence-based
multiagent negotiation strategy. In 3rd International
Joint Conference on Autonomous agents and
multiagent systems (AAMAS"04), pages 1048-1055,
Los Alamitos, CA, USA, 2004.
[20] H.-W. Tung and R. J. Lin. Automated contract
negotiation using a mediation service. In 7th IEEE
International Conference on E-Commerce Technology
(CEC"05), pages 374-377, Munich, Germany, 2005.
950 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
On Opportunistic Techniques for Solving Decentralized
Markov Decision Processes with Temporal Constraints
Janusz Marecki and Milind Tambe
Computer Science Department
University of Southern California
941 W 37th Place, Los Angeles, CA 90089
{marecki, tambe}@usc.edu
ABSTRACT
Decentralized Markov Decision Processes (DEC-MDPs) are a 
popular model of agent-coordination problems in domains with 
uncertainty and time constraints but very difficult to solve. In this
paper, we improve a state-of-the-art heuristic solution method for
DEC-MDPs, called OC-DEC-MDP, that has recently been shown
to scale up to larger DEC-MDPs. Our heuristic solution method,
called Value Function Propagation (VFP), combines two 
orthogonal improvements of OC-DEC-MDP. First, it speeds up 
OC-DECMDP by an order of magnitude by maintaining and manipulating
a value function for each state (as a function of time) rather than a
separate value for each pair of sate and time interval. Furthermore,
it achieves better solution qualities than OC-DEC-MDP because,
as our analytical results show, it does not overestimate the expected
total reward like OC-DEC- MDP. We test both improvements 
independently in a crisis-management domain as well as for other
types of domains. Our experimental results demonstrate a 
significant speedup of VFP over OC-DEC-MDP as well as higher solution
qualities in a variety of situations.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
IntelligenceMulti-agent Systems
General Terms
Algorithms, Theory
1. INTRODUCTION
The development of algorithms for effective coordination of 
multiple agents acting as a team in uncertain and time critical domains
has recently become a very active research field with potential 
applications ranging from coordination of agents during a hostage 
rescue mission [11] to the coordination of Autonomous Mars 
Exploration Rovers [2]. Because of the uncertain and dynamic 
characteristics of such domains, decision-theoretic models have received
a lot of attention in recent years, mainly thanks to their 
expressiveness and the ability to reason about the utility of actions over
time.
Key decision-theoretic models that have become popular in the 
literature include Decentralized Markov Decision Processes 
(DECMDPs) and Decentralized, Partially Observable Markov Decision
Processes (DEC-POMDPs). Unfortunately, solving these models
optimally has been proven to be NEXP-complete [3], hence more
tractable subclasses of these models have been the subject of 
intensive research. In particular, Network Distributed POMDP [13]
which assume that not all the agents interact with each other, 
Transition Independent DEC-MDP [2] which assume that transition 
function is decomposable into local transition functions or DEC-MDP
with Event Driven Interactions [1] which assume that interactions
between agents happen at fixed time points constitute good 
examples of such subclasses. Although globally optimal algorithms for
these subclasses have demonstrated promising results, domains on
which these algorithms run are still small and time horizons are
limited to only a few time ticks.
To remedy that, locally optimal algorithms have been proposed
[12] [4] [5]. In particular, Opportunity Cost DEC-MDP [4] [5],
referred to as OC-DEC-MDP, is particularly notable, as it has been
shown to scale up to domains with hundreds of tasks and double
digit time horizons. Additionally, OC-DEC-MDP is unique in its
ability to address both temporal constraints and uncertain method
execution durations, which is an important factor for real-world 
domains. OC-DEC-MDP is able to scale up to such domains mainly
because instead of searching for the globally optimal solution, it
carries out a series of policy iterations; in each iteration it performs
a value iteration that reuses the data computed during the previous
policy iteration. However, OC-DEC-MDP is still slow, especially
as the time horizon and the number of methods approach large 
values. The reason for high runtimes of OC-DEC-MDP for such 
domains is a consequence of its huge state space, i.e., OC-DEC-MDP
introduces a separate state for each possible pair of method and
method execution interval. Furthermore, OC-DEC-MDP 
overestimates the reward that a method expects to receive for enabling
the execution of future methods. This reward, also referred to as
the opportunity cost, plays a crucial role in agent decision making,
and as we show later, its overestimation leads to highly suboptimal
policies.
In this context, we present VFP (= Value Function P ropagation),
an efficient solution technique for the DEC-MDP model with 
temporal constraints and uncertain method execution durations, that
builds on the success of OC-DEC-MDP. VFP introduces our two
orthogonal ideas: First, similarly to [7] [9] and [10], we maintain
830
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
and manipulate a value function over time for each method rather
than a separate value for each pair of method and time interval.
Such representation allows us to group the time points for which
the value function changes at the same rate (= its slope is 
constant), which results in fast, functional propagation of value 
functions. Second, we prove (both theoretically and empirically) that
OC-DEC- MDP overestimates the opportunity cost, and to remedy
that, we introduce a set of heuristics, that correct the opportunity
cost overestimation problem.
This paper is organized as follows: In section 2 we motivate this
research by introducing a civilian rescue domain where a team of
fire- brigades must coordinate in order to rescue civilians trapped in
a burning building. In section 3 we provide a detailed description of
our DEC-MDP model with Temporal Constraints and in section 4
we discuss how one could solve the problems encoded in our model
using globally optimal and locally optimal solvers. Sections 5 and
6 discuss the two orthogonal improvements to the state-of-the-art
OC-DEC-MDP algorithm that our VFP algorithm implements. 
Finally, in section 7 we demonstrate empirically the impact of our two
orthogonal improvements, i.e., we show that: (i) The new 
heuristics correct the opportunity cost overestimation problem leading to
higher quality policies, and (ii) By allowing for a systematic 
tradeoff of solution quality for time, the VFP algorithm runs much faster
than the OC-DEC-MDP algorithm
2. MOTIVATING EXAMPLE
We are interested in domains where multiple agents must 
coordinate their plans over time, despite uncertainty in plan execution
duration and outcome. One example domain is large-scale disaster,
like a fire in a skyscraper. Because there can be hundreds of 
civilians scattered across numerous floors, multiple rescue teams have
to be dispatched, and radio communication channels can quickly
get saturated and useless. In particular, small teams of fire-brigades
must be sent on separate missions to rescue the civilians trapped in
dozens of different locations.
Picture a small mission plan from Figure (1), where three 
firebrigades have been assigned a task to rescue the civilians trapped
at site B, accessed from site A (e.g. an office accessed from the
floor)1
. General fire fighting procedures involve both: (i) putting
out the flames, and (ii) ventilating the site to let the toxic, high 
temperature gases escape, with the restriction that ventilation should
not be performed too fast in order to prevent the fire from spreading.
The team estimates that the civilians have 20 minutes before the fire
at site B becomes unbearable, and that the fire at site A has to be
put out in order to open the access to site B. As has happened in
the past in large scale disasters, communication often breaks down;
and hence we assume in this domain that there is no 
communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and
FB3). Consequently, FB2 does not know if it is already safe to 
ventilate site A, FB1 does not know if it is already safe to enter site A
and start fighting fire at site B, etc. We assign the reward 50 for
evacuating the civilians from site B, and a smaller reward 20 for
the successful ventilation of site A, since the civilians themselves
might succeed in breaking out from site B.
One can clearly see the dilemma, that FB2 faces: It can only 
estimate the durations of the Fight fire at site A methods to be 
executed by FB1 and FB3, and at the same time FB2 knows that time
is running out for civilians. If FB2 ventilates site A too early, the
fire will spread out of control, whereas if FB2 waits with the 
ventilation method for too long, fire at site B will become unbearable for
the civilians. In general, agents have to perform a sequence of such
1
We explain the EST and LET notation in section 3
Figure 1: Civilian rescue domain and a mission plan. Dotted 
arrows represent implicit precedence constraints within an agent.
difficult decisions; in particular, decision process of FB2 involves
first choosing when to start ventilating site A, and then 
(depending on the time it took to ventilate site A), choosing when to start
evacuating the civilians from site B. Such sequence of decisions
constitutes the policy of an agent, and it must be found fast because
time is running out.
3. MODEL DESCRIPTION
We encode our decision problems in a model which we refer to as
Decentralized MDP with Temporal Constraints 2
. Each instance of
our decision problems can be described as a tuple M, A, C, P, R
where M = {mi}
|M|
i=1 is the set of methods, and A = {Ak}
|A|
k=1
is the set of agents. Agents cannot communicate during mission
execution. Each agent Ak is assigned to a set Mk of methods,
such that
S|A|
k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø. Also, each
method of agent Ak can be executed only once, and agent Ak can
execute only one method at a time. Method execution times are
uncertain and P = {pi}
|M|
i=1 is the set of distributions of method
execution durations. In particular, pi(t) is the probability that the
execution of method mi consumes time t. C is a set of 
temporal constraints in the system. Methods are partially ordered and
each method has fixed time windows inside which it can be 
executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor
constraints and C[ ] is the set of time window constraints. For
c ∈ C≺, c = mi, mj means that method mi precedes method
mj i.e., execution of mj cannot start before mi terminates. In 
particular, for an agent Ak, all its methods form a chain linked by
predecessor constraints. We assume, that the graph G = M, C≺
is acyclic, does not have disconnected nodes (the problem cannot
be decomposed into independent subproblems), and its source and
sink vertices identify the source and sink methods of the system.
For c ∈ C[ ], c = mi, EST, LET means that execution of mi
can only start after the Earliest Starting Time EST and must 
finish before the Latest End Time LET; we allow methods to have
multiple disjoint time window constraints. Although distributions
pi can extend to infinite time horizons, given the time window 
constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is 
considered as the mission deadline. Finally, R = {ri}
|M|
i=1 is the set of
non-negative rewards, i.e., ri is obtained upon successful 
execution of mi.
Since there is no communication allowed, an agent can only 
estimate the probabilities that its methods have already been enabled
2
One could also use the OC-DEC-MDP framework, which models
both time and resource constraints
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831
by other agents. Consequently, if mj ∈ Mk is the next method
to be executed by the agent Ak and the current time is t ∈ [0, Δ],
the agent has to make a decision whether to Execute the method
mj (denoted as E), or to Wait (denoted as W). In case agent Ak
decides to wait, it remains idle for an arbitrary small time , and 
resumes operation at the same place (= about to execute method mj)
at time t + . In case agent Ak decides to Execute the next method,
two outcomes are possible:
Success: The agent Ak receives reward rj and moves on to its
next method (if such method exists) so long as the following 
conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that 
directly enable method mj have already been completed, (ii) 
Execution of method mj started in some time window of method mj, i.e.,
∃ mj ,τ,τ ∈C[ ]
such that t ∈ [τ, τ ], and (iii) Execution of method
mj finished inside the same time window, i.e., agent Ak completed
method mj in time less than or equal to τ − t.
Failure: If any of the above-mentioned conditions does not hold,
agent Ak stops its execution. Other agents may continue their 
execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become
enabled.
The policy πk of an agent Ak is a function πk : Mk × [0, Δ] →
{W, E}, and πk( m, t ) = a means, that if Ak is at method m
at time t, it will choose to perform the action a. A joint policy
π = [πk]
|A|
k=1 is considered to be optimal (denoted as π∗
), if it
maximizes the sum of expected rewards for all the agents.
4. SOLUTION TECHNIQUES
4.1 Optimal Algorithms
Optimal joint policy π∗
is usually found by using the Bellman 
update principle, i.e., in order to determine the optimal policy for
method mj, optimal policies for methods mk ∈ {m| mj, m ∈
C≺} are used. Unfortunately, for our model, the optimal 
policy for method mj also depends on policies for methods mi ∈
{m| m, mj ∈ C≺}. This double dependency results from the
fact, that the expected reward for starting the execution of method
mj at time t also depends on the probability that method mj will be
enabled by time t. Consequently, if time is discretized, one needs to
consider Δ|M|
candidate policies in order to find π∗
. Thus, 
globally optimal algorithms used for solving real-world problems are
unlikely to terminate in reasonable time [11]. The complexity of
our model could be reduced if we considered its more restricted
version; in particular, if each method mj was allowed to be 
enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm
(CSA) [1] could be used. However, CSA complexity is double 
exponential in the size of Ti, and for our domains Tj can store all
values ranging from 0 to Δ.
4.2 Locally Optimal Algorithms
Following the limited applicability of globally optimal algorithms
for DEC-MDPs with Temporal Constraints, locally optimal 
algorithms appear more promising. Specially, the OC-DEC-MDP 
algorithm [4] is particularly significant, as it has shown to easily scale
up to domains with hundreds of methods. The idea of the 
OC-DECMDP algorithm is to start with the earliest starting time policy π0
(according to which an agent will start executing the method m as
soon as m has a non-zero chance of being already enabled), and
then improve it iteratively, until no further improvement is 
possible. At each iteration, the algorithm starts with some policy π,
which uniquely determines the probabilities Pi,[τ,τ ] that method
mi will be performed in the time interval [τ, τ ]. It then performs
two steps:
Step 1: It propagates from sink methods to source methods the
values Vi,[τ,τ ], that represent the expected utility for executing
method mi in the time interval [τ, τ ]. This propagation uses the
probabilities Pi,[τ,τ ] from previous algorithm iteration. We call
this step a value propagation phase.
Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses
the most profitable method execution intervals which are stored in
a new policy π . It then propagates the new probabilities Pi,[τ,τ ]
from source methods to sink methods. We call this step a 
probability propagation phase. If policy π does not improve π, the
algorithm terminates.
There are two shortcomings of the OC-DEC-MDP algorithm that
we address in this paper. First, each of OC-DEC-MDP states is a
pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method
mj can be executed. While such state representation is beneficial,
in that the problem can be solved with a standard value iteration 
algorithm, it blurs the intuitive mapping from time t to the expected
total reward for starting the execution of mj at time t. 
Consequently, if some method mi enables method mj, and the values
Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the 
values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase),
runs in time O(I2
), where I is the number of time intervals 3
. Since
the runtime of the whole algorithm is proportional to the runtime of
this operation, especially for big time horizons Δ, the OC- 
DECMDP algorithm runs slow.
Second, while OC-DEC-MDP emphasizes on precise calculation
of values Vj,[τ,τ ], it fails to address a critical issue that determines
how the values Vj,[τ,τ ] are split given that the method mj has 
multiple enabling methods. As we show later, OC-DEC-MDP splits
Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up
again. As a result, methods that precede the method mj 
overestimate the value for enabling mj which, as we show later, can have
disastrous consequences. In the next two sections, we address both
of these shortcomings.
5. VALUE FUNCTION PROPAGATION (VFP)
The general scheme of the VFP algorithm is identical to the 
OCDEC-MDP algorithm, in that it performs a series of policy 
improvement iterations, each one involving a Value and Probability
Propagation Phase. However, instead of propagating separate 
values, VFP maintains and propagates the whole functions, we 
therefore refer to these phases as the value function propagation phase
and the probability function propagation phase. To this end, for
each method mi ∈ M, we define three new functions:
Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the
expected total reward for starting the execution of method mi at
time t.
Opportunity Cost Function, denoted as Vi(t), that maps time
t ∈ [0, Δ] to the expected total reward for starting the execution
of method mi at time t assuming that mi is enabled.
Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ]
to the probability that method mi will be completed before time
t.
Such functional representation allows us to easily read the current
policy, i.e., if an agent Ak is at method mi at time t, then it will
wait as long as value function vi(t) will be greater in the future.
Formally:
πk( mi, t ) =
j
W if ∃t >t such that vi(t) < vi(t )
E otherwise.
We now develop an analytical technique for performing the value
function and probability function propagation phases.
3
Similarly for the probability propagation phase
832 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
5.1 Value Function Propagation Phase
Suppose, that we are performing a value function propagation phase
during which the value functions are propagated from the sink 
methods to the source methods. At any time during this phase we 
encounter a situation shown in Figure 2, where opportunity cost 
functions [Vjn ]N
n=0 of methods [mjn ]N
n=0 are known, and the 
opportunity cost Vi0 of method mi0 is to be derived. Let pi0 be the
probability distribution function of method mi0 execution 
duration, and ri0 be the immediate reward for starting and 
completing the execution of method mi0 inside a time interval [τ, τ ] such
that mi0 τ, τ ∈ C[ ]. The function Vi0 is then derived from ri0
and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.
Formally:
Vi0 (t) =
8
>><
>>:
R τ −t
0
pi0 (t )(ri0 +
PN
n=0 Vjn,i0 (t + t ))dt
if ∃ mi0
τ,τ ∈C[ ]
such that t ∈ [τ, τ ]
0 otherwise
(1)
Note, that for t ∈ [τ, τ ], if h(t) := ri0 +
PN
n=0 Vjn,i0 (τ −t) then
Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).
Assume for now, that Vjn,i0 represents a full opportunity cost, 
postponing the discussion on different techniques for splitting the 
opportunity cost Vj0 into [Vj0,ik ]K
k=0 until section 6. We now show
how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the
same scheme).
Figure 2: Fragment of an MDP of agent Ak. Probability 
functions propagate forward (left to right) whereas value functions
propagate backward (right to left).
Let V j0,i0 (t) be the opportunity cost of starting the execution of
method mj0 at time t given that method mi0 has been completed.
It is derived by multiplying Vi0 by the probability functions of all
methods other than mi0 that enable mj0 . Formally:
V j0,i0 (t) = Vj0 (t) ·
KY
k=1
Pik (t).
Where similarly to [4] and [5] we ignored the dependency of [Plk ]K
k=1.
Observe that V j0,i0 does not have to be monotonically 
decreasing, i.e., delaying the execution of the method mi0 can sometimes
be profitable. Therefore the opportunity cost Vj0,i0 (t) of enabling
method mi0 at time t must be greater than or equal to V j0,i0 . 
Furthermore, Vj0,i0 should be non-increasing. Formally:
Vj0,i0 = min
f∈F
f (2)
Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.
Knowing the opportunity cost Vi0 , we can then easily derive the
value function vi0 . Let Ak be an agent assigned to the method mi0 .
If Ak is about to start the execution of mi0 it means, that Ak must
have completed its part of the mission plan up to the method mi0 .
Since Ak does not know if other agents have completed methods
[mlk ]k=K
k=1 , in order to derive vi0 , it has to multiply Vi0 by the 
probability functions of all methods of other agents that enable mi0 .
Formally:
vi0 (t) = Vi0 (t) ·
KY
k=1
Plk (t)
Where the dependency of [Plk ]K
k=1 is also ignored.
We have consequently shown a general scheme how to propagate
the value functions: Knowing [vjn ]N
n=0 and [Vjn ]N
n=0 of methods
[mjn ]N
n=0 we can derive vi0 and Vi0 of method mi0 . In general, the
value function propagation scheme starts with sink nodes. It then
visits at each time a method m, such that all the methods that m
enables have already been marked as visited. The value function
propagation phase terminates when all the source methods have
been marked as visited.
5.2 Reading the Policy
In order to determine the policy of agent Ak for the method mj0
we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such
that:
∀t∈[z,z ] πk( mj0 , t ) = W.
One can easily identify the intervals of Zj0 by looking at the time
intervals in which the value function vj0 does not decrease 
monotonically.
5.3 Probability Function Propagation Phase
Assume now, that value functions and opportunity cost values have
all been propagated from sink methods to source nodes and the sets
Zj for all methods mj ∈ M have been identified. Since value
function propagation phase was using probabilities Pi(t) for 
methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm
iteration, we now have to find new values Pi(t), in order to prepare
the algorithm for its next iteration. We now show how in the general
case (Figure 2) propagate the probability functions forward through
one method, i.e., we assume that the probability functions [Pik ]K
k=0
of methods [mik ]K
k=0 are known, and the probability function Pj0
of method mj0 must be derived. Let pj0 be the probability 
distribution function of method mj0 execution duration, and Zj0 be the
set of intervals of inactivity for method mj0 , found during the last
value function propagation phase. If we ignore the dependency of
[Pik ]K
k=0 then the probability Pj0 (t) that the execution of method
mj0 starts before time t is given by:
Pj0 (t) =
(QK
k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ )
QK
k=0 Pik (t) otherwise.
Given Pj0 (t), the probability Pj0 (t) that method mj0 will be 
completed by time t is derived by:
Pj0 (t) =
Z t
0
Z t
0
(
∂Pj0
∂t
)(t ) · pj0 (t − t )dt dt (3)
Which can be written compactly as
∂Pj0
∂t
= pj0 ∗
∂P j0
∂t
.
We have consequently shown how to propagate the probability 
functions [Pik ]K
k=0 of methods [mik ]K
k=0 to obtain the probability 
function Pj0 of method mj0 . The general, the probability function
propagation phase starts with source methods msi for which we
know that Psi = 1 since they are enabled by default. We then
visit at each time a method m such that all the methods that enable
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833
m have already been marked as visited. The probability function
propagation phase terminates when all the sink methods have been
marked as visited.
5.4 The Algorithm
Similarly to the OC-DEC-MDP algorithm, VFP starts the policy
improvement iterations with the earliest starting time policy π0
.
Then at each iteration it: (i) Propagates the value functions [vi]
|M|
i=1
using the old probability functions [Pi]
|M|
i=1 from previous algorithm
iteration and establishes the new sets [Zi]
|M|
i=1 of method inactivity
intervals, and (ii) propagates the new probability functions [Pi ]
|M|
i=1
using the newly established sets [Zi]
|M|
i=1. These new functions
[Pi ]
|M|
i=1 are then used in the next iteration of the algorithm. 
Similarly to OC-DEC-MDP, VFP terminates if a new policy does not
improve the policy from the previous algorithm iteration.
5.5 Implementation of Function Operations
So far, we have derived the functional operations for value function
and probability function propagation without choosing any 
function representation. In general, our functional operations can 
handle continuous time, and one has freedom to choose a desired 
function approximation technique, such as piecewise linear [7] or 
piecewise constant [9] approximation. However, since one of our goals
is to compare VFP with the existing OC-DEC- MDP algorithm, that
works only for discrete time, we also discretize time, and choose to
approximate value functions and probability functions with 
piecewise linear (PWL) functions.
When the VFP algorithm propagates the value functions and 
probability functions, it constantly carries out operations represented by
equations (1) and (3) and we have already shown that these 
operations are convolutions of some functions p(t) and h(t). If time is
discretized, functions p(t) and h(t) are discrete; however, h(t) can
be nicely approximated with a PWL function bh(t), which is exactly
what VFP does. As a result, instead of performing O(Δ2
) 
multiplications to compute f(t), VFP only needs to perform O(k · Δ)
multiplications to compute f(t), where k is the number of linear
segments of bh(t) (note, that since h(t) is monotonic, bh(t) is 
usually close to h(t) with k Δ). Since Pi values are in range
[0, 1] and Vi values are in range [0,
P
mi∈M ri], we suggest to 
approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t)
within error P . We now prove that the overall approximation error
accumulated during the value function propagation phase can be
expressed in terms of P and V :
THEOREM 1. Let C≺ be a set of precedence constraints of a
DEC-MDP with Temporal Constraints, and P and V be the 
probability function and value function approximation errors 
respectively. The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of
value function propagation phase is then bounded by:
|C≺|

V + ((1 + P )|C≺|
− 1)
P
mi∈M ri

.
PROOF. In order to establish the bound for π, we first prove
by induction on the size of C≺, that the overall error of 
probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) −
bP(t)| is bounded by (1 + P )|C≺|
− 1.
Induction base: If n = 1 only two methods are present, and we
will perform the operation identified by Equation (3) only once,
introducing the error π(P ) = P = (1 + P )|C≺|
− 1.
Induction step: Suppose, that π(P ) for |C≺| = n is bounded by
(1 + P )n
− 1, and we want to prove that this statement holds for
|C≺| = n. Let G = M, C≺ be a graph with at most n + 1
edges, and G = M, C≺ be a subgraph of G, such that C≺ =
C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the
induction assumption we have, that C≺ introduces the probability
propagation phase error bounded by (1 + P )n
− 1. We now add
back the link { mi, mj } to C≺, which affects the error of only
one probability function, namely Pj, by a factor of (1 + P ). Since
probability propagation phase error in C≺ was bounded by (1 +
P )n
− 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 +
P )n
− 1)(1 + P ) < (1 + P )n+1
− 1. Thus, if opportunity cost
functions are not overestimated, they are bounded by
P
mi∈M ri
and the error of a single value function propagation operation will
be at most
Z Δ
0
p(t)( V +((1+ P )
|C≺|
−1)
X
mi∈M
ri) dt < V +((1+ P )
|C≺|
−1)
X
mi∈M
ri.
Since the number of value function propagation operations is |C≺|,
the total error π of the value function propagation phase is bounded
by: |C≺|

V + ((1 + P )|C≺|
− 1)
P
mi∈M ri

.
6. SPLITTING THE OPPORTUNITY COST
FUNCTIONS
In section 5 we left out the discussion about how the 
opportunity cost function Vj0 of method mj0 is split into opportunity cost
functions [Vj0,ik ]K
k=0 sent back to methods [mik ]K
k=0 , that 
directly enable method mj0 . So far, we have taken the same 
approach as in [4] and [5] in that the opportunity cost function Vj0,ik
that the method mik sends back to the method mj0 is a 
minimal, non-increasing function that dominates function V j0,ik (t) =
(Vj0 ·
Q
k ∈{0,...,K}
k =k
Pik
)(t). We refer to this approach, as 
heuristic H 1,1 . Before we prove that this heuristic overestimates the
opportunity cost, we discuss three problems that might occur when
splitting the opportunity cost functions: (i) overestimation, (ii) 
underestimation and (iii) starvation. Consider the situation in Figure
Figure 3: Splitting the value function of method mj0 among
methods [mik ]K
k=0.
(3) when value function propagation for methods [mik ]K
k=0 is 
performed. For each k = 0, ..., K, Equation (1) derives the 
opportunity cost function Vik from immediate reward rk and 
opportunity cost function Vj0,ik . If m0 is the only methods that precedes
method mk, then V ik,0 = Vik is propagated to method m0, and
consequently the opportunity cost for completing the method m0 at
time t is equal to
PK
k=0 Vik,0(t). If this cost is overestimated, then
an agent A0 at method m0 will have too much incentive to finish
the execution of m0 at time t. Consequently, although the 
probability P(t) that m0 will be enabled by other agents by time t is low,
agent A0 might still find the expected utility of starting the 
execution of m0 at time t higher than the expected utility of doing it later.
As a result, it will choose at time t to start executing method m0
instead of waiting, which can have disastrous consequences. 
Similarly, if
PK
k=0 Vik,0(t) is underestimated, agent A0 might loose
interest in enabling the future methods [mik ]K
k=0 and just focus on
834 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
maximizing the chance of obtaining its immediate reward r0. Since
this chance is increased when agent A0 waits4
, it will consider at
time t to be more profitable to wait, instead of starting the 
execution of m0, which can have similarly disastrous consequences.
Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the
method mik that underestimates the opportunity cost of enabling
method mj0 , and the similar reasoning applies. We call such 
problem a starvation of method mk. That short discussion shows the
importance of splitting the opportunity cost function Vj0 in such a
way, that overestimation, underestimation, and starvation problem
is avoided. We now prove that:
THEOREM 2. Heuristic H 1,1 can overestimate the 
opportunity cost.
PROOF. We prove the theorem by showing a case where the
overestimation occurs. For the mission plan from Figure (3), let
H 1,1 split Vj0 into [V j0,ik = Vj0 ·
Q
k ∈{0,...,K}
k =k
Pik
]K
k=0 sent to
methods [mik ]K
k=0 respectively. Also, assume that methods [mik ]K
k=0
provide no local reward and have the same time windows, i.e.,
rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the
overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ]
such that the opportunity cost
PK
k=0 Vik (t) for methods [mik ]K
k=0
at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).
From Equation (1) we have:
Vik
(t) =
Z Δ−t
0
pik
(t )Vj0,ik
(t + t )dt
Summing over all methods [mik ]K
k=0 we obtain:
KX
k=0
Vik
(t) =
KX
k=0
Z Δ−t
0
pik
(t )Vj0,ik
(t + t )dt (4)
≥
KX
k=0
Z Δ−t
0
pik
(t )V j0,ik
(t + t )dt
=
KX
k=0
Z Δ−t
0
pik
(t )Vj0 (t + t )
Y
k ∈{0,...,K}
k =k
Pik
(t + t )dt
Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0
and ∀k=0,..,K we have
Q
k ∈{0,...,K}
k =k
Pik
(t) > c. Then:
KX
k=0
Vik
(t0) >
KX
k=0
Z Δ−t0
0
pik
(t )Vj0 (t0 + t ) · c dt
Because Pjk
is non-decreasing. Now, suppose there exists t1 ∈
(t0, Δ], such that
PK
k=0
R t1−t0
0
pik (t )dt >
Vj0
(t0)
c·Vj0
(t1)
. Since 
decreasing the upper limit of the integral over positive function also
decreases the integral, we have:
KX
k=0
Vik
(t0) > c
KX
k=0
Z t1
t0
pik
(t − t0)Vj0 (t )dt
And since Vj0 (t ) is non-increasing we have:
KX
k=0
Vik
(t0) > c · Vj0 (t1)
KX
k=0
Z t1
t0
pik
(t − t0)dt (5)
= c · Vj0 (t1)
KX
k=0
Z t1−t0
0
pik
(t )dt
> c · Vj0 (t1)
Vj(t0)
c · Vj(t1)
= Vj(t0)
4
Assuming LET0 t
Consequently, the opportunity cost
PK
k=0 Vik (t0) of starting the
execution of methods [mik ]K
k=0 at time t ∈ [0, .., Δ] is greater
than the opportunity cost Vj0 (t0) which proves the theorem.Figure
4 shows that the overestimation of opportunity cost is easily 
observable in practice.
To remedy the problem of opportunity cost overestimation, we 
propose three alternative heuristics that split the opportunity cost 
functions:
• Heuristic H 1,0 : Only one method, mik gets the full 
expected reward for enabling method mj0 , i.e., V j0,ik
(t) = 0
for k ∈ {0, ..., K}\{k} and V j0,ik (t) = (Vj0 ·
Q
k ∈{0,...,K}
k =k
Pik
)(t).
• Heuristic H 1/2,1/2 : Each method [mik ]K
k=0 gets the full
opportunity cost for enabling method mj0 divided by the
number K of methods enabling the method mj0 , i.e., V j0,ik (t) =
1
K
(Vj0 ·
Q
k ∈{0,...,K}
k =k
Pik
)(t) for k ∈ {0, ..., K}.
• Heuristic bH 1,1 : This is a normalized version of the H 1,1
heuristic in that each method [mik ]K
k=0 initially gets the full
opportunity cost for enabling the method mj0 . To avoid 
opportunity cost overestimation, we normalize the split 
functions when their sum exceeds the opportunity cost function
to be split. Formally:
V j0,ik (t) =
8
><
>:
V
H 1,1
j0,ik
(t) if
PK
k=0 V
H 1,1
j0,ik
(t) < Vj0 (t)
Vj0 (t)
V
H 1,1
j0,ik
(t)
PK
k=0
V
H 1,1
j0,ik
(t)
otherwise
Where V
H 1,1
j0,ik
(t) = (Vj0 ·
Q
k ∈{0,...,K}
k =k
Pjk
)(t).
For the new heuristics, we now prove, that:
THEOREM 3. Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not
overestimate the opportunity cost.
PROOF. When heuristic H 1,0 is used to split the opportunity
cost function Vj0 , only one method (e.g. mik ) gets the opportunity
cost for enabling method mj0 . Thus:
KX
k =0
Vik
(t) =
Z Δ−t
0
pik
(t )Vj0,ik
(t + t )dt (6)
And since Vj0 is non-increasing
≤
Z Δ−t
0
pik
(t )Vj0 (t + t ) ·
Y
k ∈{0,...,K}
k =k
Pjk
(t + t )dt
≤
Z Δ−t
0
pik
(t )Vj0 (t + t )dt ≤ Vj0 (t)
The last inequality is also a consequence of the fact that Vj0 is
non-increasing.
For heuristic H 1/2,1/2 we similarly have:
KX
k=0
Vik
(t) ≤
KX
k=0
Z Δ−t
0
pik
(t )
1
K
Vj0 (t + t )
Y
k ∈{0,...,K}
k =k
Pjk
(t + t )dt
≤
1
K
KX
k=0
Z Δ−t
0
pik
(t )Vj0 (t + t )dt
≤
1
K
· K · Vj0 (t) = Vj0 (t).
For heuristic bH 1,1 , the opportunity cost function Vj0 is by 
definition split in such manner, that
PK
k=0 Vik (t) ≤ Vj0 (t). 
Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2
and bH 1,1 avoid the overestimation of the opportunity cost.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835
The reason why we have introduced all three new heuristics is the
following: Since H 1,1 overestimates the opportunity cost, one
has to choose which method mik will receive the reward from 
enabling the method mj0 , which is exactly what the heuristic H 1,0
does. However, heuristic H 1,0 leaves K − 1 methods that 
precede the method mj0 without any reward which leads to starvation.
Starvation can be avoided if opportunity cost functions are split 
using heuristic H 1/2,1/2 , that provides reward to all enabling 
methods. However, the sum of split opportunity cost functions for the
H 1/2,1/2 heuristic can be smaller than the non-zero split 
opportunity cost function for the H 1,0 heuristic, which is clearly 
undesirable. Such situation (Figure 4, heuristic H 1,0 ) occurs because
the mean f+g
2
of two functions f, g is not smaller than f nor g
only if f = g. This is why we have proposed the bH 1,1 heuristic,
which by definition avoids the overestimation, underestimation and
starvation problems.
7. EXPERIMENTAL EVALUATION
Since the VFP algorithm that we introduced provides two 
orthogonal improvements over the OC-DEC-MDP algorithm, the 
experimental evaluation we performed consisted of two parts: In part 1,
we tested empirically the quality of solutions that an locally optimal
solver (either OC-DEC-MDP or VFP) finds, given it uses different
opportunity cost function splitting heuristic, and in part 2, we 
compared the runtimes of the VFP and OC-DEC- MDP algorithms for
a variety of mission plan configurations.
Part 1: We first ran the VFP algorithm on a generic mission plan
configuration from Figure 3 where only methods mj0 , mi1 , mi2
and m0 were present. Time windows of all methods were set to
400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1
400
and durations pi1 , pi2 of methods mi1 , mi2 were normal 
distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ =
200, σ = 100). We assumed that only method mj0 provided 
reward, i.e. rj0 = 10 was the reward for finishing the execution of
method mj0 before time t = 400. We show our results in Figure
(4) where the x-axis of each of the graphs represents time whereas
the y-axis represents the opportunity cost. The first graph confirms,
that when the opportunity cost function Vj0 was split into 
opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the 
function Vi1 +Vi2 was not always below the Vj0 function. In particular,
Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%. When 
heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4),
the function Vi1 + Vi2 was always below Vj0 .
We then shifted our attention to the civilian rescue domain 
introduced in Figure 1 for which we sampled all action execution 
durations from the normal distribution N = (μ = 5, σ = 2)). To
obtain the baseline for the heuristic performance, we implemented
a globally optimal solver, that found a true expected total reward
for this domain (Figure (6a)). We then compared this reward with
a expected total reward found by a locally optimal solver guided
by each of the discussed heuristics. Figure (6a), which plots on
the y-axis the expected total reward of a policy complements our
previous results: H 1,1 heuristic overestimated the expected total
reward by 280% whereas the other heuristics were able to guide the
locally optimal solver close to a true expected total reward.
Part 2: We then chose H 1,1 to split the opportunity cost 
functions and conducted a series of experiments aimed at testing the
scalability of VFP for various mission plan configurations, using
the performance of the OC-DEC-MDP algorithm as a benchmark.
We began the VFP scalability tests with a configuration from Figure
(5a) associated with the civilian rescue domain, for which method
execution durations were extended to normal distributions N(μ =
Figure 5: Mission plan configurations: (a) civilian rescue 
domain, (b) chain of n methods, (c) tree of n methods with
branching factor = 3 and (d) square mesh of n methods.
Figure 6: VFP performance in the civilian rescue domain.
30, σ = 5), and the deadline was extended to Δ = 200.
We decided to test the runtime of the VFP algorithm running with
three different levels of accuracy, i.e., different approximation 
parameters P and V were chosen, such that the cumulative error
of the solution found by VFP stayed within 1%, 5% and 10% of
the solution found by the OC- DEC-MDP algorithm. We then run
both algorithms for a total of 100 policy improvement iterations.
Figure (6b) shows the performance of the VFP algorithm in the
civilian rescue domain (y-axis shows the runtime in milliseconds).
As we see, for this small domain, VFP runs 15% faster than 
OCDEC-MDP when computing the policy with an error of less than
1%. For comparison, the globally optimal solved did not terminate
within the first three hours of its runtime which shows the strength
of the opportunistic solvers, like OC-DEC-MDP.
We next decided to test how VFP performs in a more difficult 
domain, i.e., with methods forming a long chain (Figure (5b)). We
tested chains of 10, 20 and 30 methods, increasing at the same
time method time windows to 350, 700 and 1050 to ensure that
later methods can be reached. We show the results in Figure (7a),
where we vary on the x-axis the number of methods and plot on
the y-axis the algorithm runtime (notice the logarithmic scale). As
we observe, scaling up the domain reveals the high performance of
VFP: Within 1% error, it runs up to 6 times faster than 
OC-DECMDP.
We then tested how VFP scales up, given that the methods are 
arranged into a tree (Figure (5c)). In particular, we considered trees
with branching factor of 3, and depth of 2, 3 and 4, increasing at
the same time the time horizon from 200 to 300, and then to 400.
We show the results in Figure (7b). Although the speedups are
smaller than in case of a chain, the VFP algorithm still runs up to 4
times faster than OC-DEC-MDP when computing the policy with
an error of less than 1%.
We finally tested how VFP handles the domains with methods 
arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i =
1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. In particular, we consider
836 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 4: Visualization of heuristics for opportunity costs splitting.
Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations.
meshes of 3×3, 4×4, and 5×5 methods. For such configurations
we have to greatly increase the time horizon since the 
probabilities of enabling the final methods by a particular time decrease
exponentially. We therefore vary the time horizons from 3000 to
4000, and then to 5000. We show the results in Figure (7c) where,
especially for larger meshes, the VFP algorithm runs up to one 
order of magnitude faster than OC-DEC-MDP while finding a policy
that is within less than 1% from the policy found by OC- 
DECMDP.
8. CONCLUSIONS
Decentralized Markov Decision Process (DEC-MDP) has been very
popular for modeling of agent-coordination problems, it is very 
difficult to solve, especially for the real-world domains. In this 
paper, we improved a state-of-the-art heuristic solution method for
DEC-MDPs, called OC-DEC-MDP, that has recently been shown
to scale up to large DEC-MDPs. Our heuristic solution method,
called Value Function Propagation (VFP), provided two 
orthogonal improvements of OC-DEC-MDP: (i) It speeded up 
OC-DECMDP by an order of magnitude by maintaining and manipulating a
value function for each method rather than a separate value for each
pair of method and time interval, and (ii) it achieved better solution
qualities than OC-DEC-MDP because it corrected the 
overestimation of the opportunity cost of OC-DEC-MDP.
In terms of related work, we have extensively discussed the 
OCDEC-MDP algorithm [4]. Furthermore, as discussed in Section 4,
there are globally optimal algorithms for solving DEC-MDPs with
temporal constraints [1] [11]. Unfortunately, they fail to scale up to
large-scale domains at present time. Beyond OC-DEC-MDP, there
are other locally optimal algorithms for DEC-MDPs and 
DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with
uncertain execution times and temporal constraints. Finally, value
function techniques have been studied in context of single agent
MDPs [7] [9]. However, similarly to [6], they fail to address the
lack of global state knowledge, which is a fundamental issue in
decentralized planning.
Acknowledgments
This material is based upon work supported by the DARPA/IPTO
COORDINATORS program and the Air Force Research 
Laboratory under Contract No. FA875005C0030. The authors also want
to thank Sven Koenig and anonymous reviewers for their valuable
comments.
9. REFERENCES
[1] R. Becker, V. Lesser, and S. Zilberstein. Decentralized MDPs with
Event-Driven Interactions. In AAMAS, pages 302-309, 2004.
[2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.
Transition-Independent Decentralized Markov Decision Processes. In
AAMAS, pages 41-48, 2003.
[3] D. S. Bernstein, S. Zilberstein, and N. Immerman. The complexity of
decentralized control of Markov decision processes. In UAI, pages
32-37, 2000.
[4] A. Beynier and A. Mouaddib. A polynomial algorithm for
decentralized Markov decision processes with temporal constraints.
In AAMAS, pages 963-969, 2005.
[5] A. Beynier and A. Mouaddib. An iterative algorithm for solving
constrained decentralized Markov decision processes. In AAAI, pages
1089-1094, 2006.
[6] C. Boutilier. Sequential optimality and coordination in multiagent
systems. In IJCAI, pages 478-485, 1999.
[7] J. Boyan and M. Littman. Exact solutions to time-dependent MDPs.
In NIPS, pages 1026-1032, 2000.
[8] C. Goldman and S. Zilberstein. Optimizing information exchange in
cooperative multi-agent systems, 2003.
[9] L. Li and M. Littman. Lazy approximation for solving continuous
finite-horizon MDPs. In AAAI, pages 1175-1180, 2005.
[10] Y. Liu and S. Koenig. Risk-sensitive planning with one-switch utility
functions: Value iteration. In AAAI, pages 993-999, 2005.
[11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and
M. Boddy. Coordinated plan management using multiagent MDPs. In
AAAI Spring Symposium, 2006.
[12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella. Taming
decentralized POMDPs: Towards efficient policy computation for
multiagent settings. In IJCAI, pages 705-711, 2003.
[13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo. Networked
distributed POMDPs: A synergy of distributed constraint
optimization and POMDPs. In IJCAI, pages 1758-1760, 2005.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837
Agents, Beliefs, and Plausible Behavior in a Temporal
Setting
Nils Bulling and Wojciech Jamroga
Department of Informatics, Clausthal University
of Technology, Germany
{bulling,wjamroga}@in.tu-clausthal.de
ABSTRACT
Logics of knowledge and belief are often too static and 
inflexible to be used on real-world problems. In particular, they
usually offer no concept for expressing that some course of
events is more likely to happen than another. We address
this problem and extend CTLK (computation tree logic
with knowledge) with a notion of plausibility, which allows
for practical and counterfactual reasoning. The new logic
CTLKP (CTLK with plausibility) includes also a 
particular notion of belief. A plausibility update operator is added
to this logic in order to change plausibility assumptions 
dynamically. Furthermore, we examine some important 
properties of these concepts. In particular, we show that, for a
natural class of models, belief is a KD45 modality. We also
show that model checking CTLKP is PTIME-complete
and can be done in time linear with respect to the size of
models and formulae.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
Intelligence-Multiagent Systems; I.2.4 [Artificial Intelligence]:
Knowledge Representation Formalisms and Methods-Modal
logic
General Terms
Theory
1. INTRODUCTION
Notions like time, knowledge, and beliefs are very 
important for analyzing the behavior of agents and multi-agent
systems. In this paper, we extend modal logics of time and
knowledge with a concept of plausible behavior: this notion
is added to the language of CTLK [19], which is a 
straightforward combination of the branching-time temporal logic
CTL [4, 3] and standard epistemic logic [9, 5].
In our approach, plausibility can be seen as a temporal
property of behaviors. That is, some behaviors of the 
system can be assumed plausible and others implausible, with
the underlying idea that the latter should perhaps be 
ignored in practical reasoning about possible future courses
of action. Moreover, behaviors can be formally understood
as temporal paths in the Kripke structure modeling a 
multiagent system. As a consequence, we obtain a language to
reason about what can (or must) plausibly happen. We
propose a particular notion of beliefs (inspired by [20, 7]),
defined in terms of epistemic relations and plausibility. The
main intuition is that beliefs are facts that an agent would
know if he assumed that only plausible things could happen.
We believe that humans use such a concept of plausibility
and practical beliefs quite often in their everyday 
reasoning. Restricting one"s reasoning to plausible possibilities is
essential to make the reasoning feasible, as the space of all
possibilities is exceedingly large in real life. We investigate
some important properties of plausibility, knowledge, and
belief in this new framework. In particular, we show that
knowledge is an S5 modality, and that beliefs satisfy 
axioms K45 in general, and KD45 for the class of plausibly
serial models. Finally, we show that the relationship 
between knowledge and belief for plausibly serial models is
natural and reflects the initial intuition well. We also show
how plausibility assumptions can be specified in the object
language via a plausibility update operator, and we study
properties of such updates. Finally, we show that model
checking of the new logic is no more complex than model
checking CTL and CTLK.
Our ultimate goal is to come up with a logic that 
allows the study of strategies, time, knowledge, and 
plausible/rational behavior under both perfect and imperfect 
information. As combining all these dimensions is highly 
nontrivial (cf. [12, 14]) it seems reasonable to split this task.
While this paper deals with knowledge, plausibility, and 
belief, the companion paper [11] proposes a general framework
for multi-agent systems that regard game-theoretical 
rationality criteria like Nash equilibrium, Pareto optimality, etc.
The latter approach is based on the more powerful logic
ATL [1].
The paper is structured as follows. Firstly, we briefly
present branching-time logic with knowledge, CTLK. In
Section 3 we present our approach to plausibility and 
formally define CTLK with plausibility. We also show how
582
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
temporal formulae can be used to describe plausible paths,
and we compare our logic with existing related work. In 
Section 4, properties of knowledge, belief, and plausibility are
explored. Finally, we present verification complexity results
for CTLKP in Section 5.
2. BRANCHING TIME AND KNOWLEDGE
In this paper we develop a framework for agents" beliefs
about how the world can (or must) evolve. Thus, we need a
notion of time and change, plus a notion of what the agents
are supposed to know in particular situations. CTLK [19]
is a straightforward combination of the computation tree
logic CTL [4, 3] and standard epistemic logic [9, 5].
CTL includes operators for temporal properties of 
systems: i.e., path quantifier E (there is a path), together
with temporal operators: f(in the next state), 2 
(always from now on) and U (until).1
Every occurrence of
a temporal operator is preceded by exactly one path 
quantifier in CTL (this variant of the language is sometimes called
vanilla CTL). Epistemic logic uses operators for 
representing agents" knowledge: Kaϕ is read as agent a knows
that ϕ.
Let Π be a set of atomic propositions with a typical 
element p, and Agt = {1, ..., k} be a set of agents with a typical
element a. The language of CTLK consists of formulae ϕ,
given as follows:
ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | Eγ | Kaϕ
γ ::= fϕ | 2 ϕ | ϕU ϕ.
We will sometimes refer to formulae ϕ as (vanilla) state
formulae and to formulae γ as (vanilla) path formulae.
The semantics of CTLK is based on Kripke models M =
Q, R, ∼1, ..., ∼k, π , which include a nonempty set of states
Q, a state transition relation R ⊆ Q × Q, epistemic 
indistinguishability relations ∼a⊆ Q × Q (one per agent), and a
valuation of propositions π : Π → P(Q). We assume that
relation R is serial and that all ∼a are equivalence relations.
A path λ in M refers to a possible behavior (or 
computation) of system M, and can be represented as an infinite
sequence of states that follow relation R, that is, a sequence
q0q1q2... such that qiRqi+1 for every i = 0, 1, 2, ... We 
denote the ith state in λ by λ[i]. The set of all paths in M
is denoted by ΛM (if the model is clear from context, M
will be omitted). A q-path is a path that starts from q,
i.e., λ[0] = q. A q-subpath is a sequence of states, starting
from q, which is a subpath of some path in the model, i.e.
a sequence q0q1... such that q = q0 and there are q0
, ..., qi
such that q0
...qi
q0q1... ∈ ΛM.2
The semantics of CTLK is
defined as follows:
M, q |= p iff q ∈ π(p);
M, q |= ¬ϕ iff M, q |= ϕ;
M, q |= ϕ ∧ ψ iff M, q |= ϕ and M, q |= ψ;
M, q |= E fϕ iff there is a q-path λ such that M, λ[1] |= ϕ;
M, q |= E2 ϕ iff there is a q-path λ such that M, λ[i] |= ϕ
for every i ≥ 0;
1
Additional operators A (for every path) and ♦ 
(sometime in the future) are defined in the usual way.
2
For CTLK models, λ is a q-subpath iff it is a q-path. It
will not always be so when plausible paths are introduced.
M, q |= EϕU ψ iff there is a q-path λ and i ≥ 0 such that
M, λ[i] |= ψ, and M, λ[j] |= ϕ for every 0 ≤ j < i;
M, q |= Kaϕ iff M, q |= ϕ for every q such that q ∼a q .
3. EXTENDING TIME AND KNOWLEDGE
WITH PLAUSIBILITY AND BELIEFS
In this section we discuss the central concept of this 
paper, i.e. the concept of plausibility. First, we outline the
idea informally. Then, we extend CTLK with the notion
of plausibility by adding plausible path operators Pl a and
physical path operator Ph to the logic. Formula Pl aϕ has
the intended meaning: according to agent a, it is plausible
that ϕ holds; formula Ph ϕ reads as: ϕ holds in all 
physically possible scenarios (i.e., even in implausible ones). The
plausible path operator restricts statements only to those
paths which are defined to be sensible, whereas the 
physical path operator generates statements about all paths that
may theoretically occur. Furthermore, we define beliefs on
top of plausibility and knowledge, as the facts that an agent
would know if he assumed that only plausible things could
happen. Finally, we discuss related work [7, 8, 20, 18, 16],
and compare it with our approach.
3.1 The Concept of Plausibility
It is well known how knowledge (or beliefs) can be 
modeled with Kripke structures. However, it is not so obvious
how we can capture knowledge and beliefs in a sensible way
in one framework. Clearly, there should be a connection
between these two notions. Our approach is to use the 
notion of plausibility for this purpose. Plausibility can serve
as a primitive concept that helps to define the semantics
of beliefs, in a similar way as indistinguishability of states
(represented by relation ∼a) is the semantic concept that
underlies knowledge. In this sense, our work follows [7, 20]:
essentially, beliefs are what an agent would know if he took
only plausible options into account. In our approach, 
however, plausibility is explicitly seen as a temporal property.
That is, we do not consider states (or possible worlds) to be
more plausible than others but rather define some behaviors
to be plausible, and others implausible. Moreover, 
behaviors can be formally understood as temporal paths in the
Kripke structure modeling a multi-agent system.
An actual notion of plausibility (that is, a particular set of
plausible paths) can emerge in many different ways. It may
result from observations and learning; an agent can learn
from its observations and see specific patterns of events as
plausible (a lot of people wear black shoes if they wear a
suit). Knowledge exchange is another possibility (e.g., an
agent a can tell agent b that player c always bluffs when he
is smiling). Game theory, with its rationality criteria 
(undominated strategies, maxmin, Nash equilibrium etc.) is 
another viable source of plausibility assumptions. Last but not
least, folk knowledge can be used to establish 
plausibilityrelated classifications of behavior (players normally want
to win a game, people want to live).
In any case, restricting the reasoning to plausible 
possibilities can be essential if we want to make the reasoning 
feasible, as the space of all possibilities (we call them physical
possibilities in the rest of the paper) is exceedingly large in
real life. Of course, this does not exclude a more extensive
analysis in special cases, e.g. when our plausibility 
assumptions do not seem accurate any more, or when the cost of
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 583
inaccurate assumptions can be too high (as in the case of
high-budget business decisions). But even in these cases, we
usually do not get rid of plausibility assumptions completely
- we only revise them to make them more cautious.3
To formalize this idea, we extend models of CTLK with
sets of plausible paths and add plausibility operators Pl a,
physical paths operator Ph , and belief operators Ba to the
language of CTLK. Now, it is possible to make statements
that refer to plausible paths only, as well as statements that
regard all paths that may occur in the system.
3.2 CTLK with Plausibility
In this section, we extend the logic of CTLK with 
plausibility; we call the resulting logic CTLKP. Formally, the
language of CTLKP is defined as:
ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | Eγ | Pl aϕ | Ph ϕ | Kaϕ | Baϕ
γ ::= fϕ | 2 ϕ | ϕU ϕ.
For instance, we may claim it is plausible to assume that
a shop is closed after the opening hours, though the manager
may be physically able to open it at any time: Pl aA2 (late →
¬open) ∧ Ph E♦ (late ∧ open).
The semantics of CTLKP extends that of CTLK as 
follows. Firstly, we augment the models with sets of plausible
paths. A model with plausibility is given as
M = Q, R, ∼1, ..., ∼k, Υ1, ..., Υk, π ,
where Q, R, ∼1, ..., ∼k, π is a CTLK model, and Υa ⊆ ΛM
is the set of paths in M that are plausible according to agent
a. If we want to make it clear that Υa is taken from model
M, we will write ΥM
a . It seems worth emphasizing that this
notion of plausibility is subjective and holistic. It is 
subjective because Υa represents agent a"s subjective view on what
is plausible - and indeed, different agents may have 
different ideas on plausibility (i.e., Υa may differ from Υb). It is
holistic because Υa represents agent a"s idea of the 
plausible behavior of the whole system (including the behavior of
other agents).
Remark 1. In our models, plausibility is also global, i.e.,
plausibility sets do not depend on the state of the system.
Investigating systems, in which plausibility is relativized with
respect to states (like in [7]), might be an interesting avenue
of future work. However, such an approach - while obviously
more flexible - allows for potentially counterintuitive system
descriptions. For example, it might be the case that path λ
is plausible in q = λ[0], but the set of plausible paths in
q = λ[1] is empty. That is, by following plausible path λ we
are bound to get to an implausible situation. But then, does
it make sense to consider λ as plausible?
Secondly, we use a non-standard satisfaction relation |=P ,
which we call plausible satisfaction. Let M be a CTLKP
3
That is, when planning to open an industrial plant in the
UK, we will probably consider the possibility of our main
contractor taking her life, but we will still not take into 
account the possibilities of: an invasion of UFO, England being
destroyed by a meteorite, Fidel Castro becoming the British
Prime Minister etc. Note that this is fundamentally different
from using a probabilistic model in which all these unlikely
scenarios are assigned very low probabilities: in that case,
they also have a very small influence on our final decision,
but we must process the whole space of physical possibilities
to evaluate the options.
model and P ⊆ ΛM be an arbitrary subset of paths in M
(not necessarily any ΥM
a ). |=P restricts the evaluation of
temporal formulae to the paths given in P only. The 
absolute satisfaction relation |= is defined as |=ΛM .
Let on(P) be the set of all states that lie on at least one
path in P, i.e. on(P) = {q ∈ Q | ∃λ ∈ P∃i (λ[i] = q)}. Now,
the semantics of CTLKP can be given through the 
following clauses:
M, q |=P p iff q ∈ π(p);
M, q |=P ¬ϕ iff M, q |=P ϕ;
M, q |=P ϕ ∧ ψ iff M, q |=P ϕ and M, q |=P ψ;
M, q |=P E fϕ iff there is a q-subpath λ ∈ P such that
M, λ[1] |=P ϕ;
M, q |=P E2 ϕ iff there is a q-subpath λ ∈ P such that
M, λ[i] |=P ϕ for every i ≥ 0;
M, q |=P EϕU ψ iff there is a q-subpath λ ∈ P and i ≥ 0
such that M, λ[i] |=P ψ, and M, λ[j] |=P ϕ for every
0 ≤ j < i;
M, q |=P Pl aϕ iff M, q |=Υa
ϕ;
M, q |=P Ph ϕ iff M, q |= ϕ;
M, q |=P Kaϕ iff M, q |= ϕ for every q such that q ∼a q ;
M, q |=P Baϕ iff for all q ∈ on(Υa) with q ∼a q , we have
that M, q |=Υa
ϕ.
One of the main reasons for using the concept of 
plausibility is that we want to define agents" beliefs out of more
primitive concepts - in our case, these are plausibility and
indistinguishability - in a way analogous to [20, 7]. If an
agent knows that ϕ, he must be sure about it. However,
beliefs of an agent are not necessarily about reliable facts.
Still, they should make sense to the agent; if he believes that
ϕ, then the formula should at least hold in all futures that
he envisages as plausible. Thus, beliefs of an agent may be
seen as things known to him if he disregards all non-plausible
possibilities.
We say that ϕ is M-true (M |= ϕ) if M, q |= ϕ for all
q ∈ QM. ϕ is valid (|= ϕ) if M |= ϕ for all models M. ϕ
is M-strongly true (M |≡ ϕ) if M, q |=P ϕ for all q ∈ QM
and all P ⊆ ΛM. ϕ is strongly valid ( |≡ ϕ) if M |≡ ϕ for all
models M.
Proposition 2. Strong truth and strong validity imply
truth and validity, respectively. The reverse does not hold.
Ultimately, we are going to be interested in normal (not
strong) validity, as parameterizing the satisfaction relation
with a set P is just a technical device for propagating sets
of plausible paths Υa into the semantics of nested formulae.
The importance of strong validity, however, lies in the fact
that |≡ ϕ ↔ ψ makes ϕ and ψ completely interchangeable,
while the same is not true for normal validity.
Proposition 3. Let Φ[ϕ/ψ] denote formula Φ in which
every occurrence of ψ was replaced by ϕ. Also, let |≡ ϕ ↔ ψ.
Then for all M, q, P: M, q |=P Φ iff M, q |=P Φ[ϕ/ψ] (in
particular, M, q |= Φ iff M, q |= Φ[ϕ/ψ]).
Note that |= ϕ ↔ ψ does not even imply that M, q |= Φ iff
M, q |= Φ[ϕ/ψ].
584 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 1: Guessing Robots game
Example 1 (Guessing Robots). Consider a simple
game with two agents a and b, shown in Figure 1. First,
a chooses a real number r ∈ [0, 1] (without revealing the
number to b); then, b chooses a real number r ∈ [0, 1].
The agents win the game (and collect EUR 1, 000, 000) if
both chose 1, otherwise they lose. Formally, we model the
game with a CTLKP model M, in which the set of states
Q includes qs for the initial situation, states qr, r ∈ [0, 1],
for the situations after a has chosen number r, and final
states qw, ql for the winning and the losing situation, 
respectively. The transition relation is as follows: qsRqr and
qrRql for all r ∈ [0, 1]; q1Rqw, qwRqw, and qlRql. Moreover,
π(one) = {q1} and π(win) = {qw}. Player a has perfect 
information in the game (i.e., q ∼a q iff q = q ), but player
b does not distinguish between states qr (i.e., qr ∼b qr for
all r, r ∈ [0, 1]). Obviously, the only sensible thing to do
for both agents is to choose 1 (using game-theoretical 
vocabulary, these strategies are strongly dominant for the 
respective players). Thus, there is only one plausible course of
events if we assume that our players are rational, and hence
Υa = Υb = {qsq1qwqw . . .}.
Note that, in principle, the outcome of the game is 
uncertain: M, qs |= ¬A♦ win∧¬A2 ¬win. However, assuming 
rationality of the players makes it only plausible that the game
must end up with a win: M, qs |= Pla A♦ win ∧ Plb A♦ win,
and the agents believe that this will be the case: M, qs |=
BaA♦ win ∧ BbA♦ win. Note also that, in any of the states
qr, agent b believes that a (being rational) has played 1:
M, qr |= Bbone for all r ∈ [0, 1].
3.3 Defining Plausible Paths with Formulae
So far, we have assumed that sets of plausible paths are
somehow given in models. In this section we present a 
dynamic approach where an actual notion of plausibility can
be specified in the object language. Note that we want to
specify (usually infinite) sets of infinite paths, and we need a
finite representation of these structures. One logical solution
is given by using path formulae γ. These formulae describe
properties of paths; therefore, a specific formula can be used
to characterize a set of paths. For instance, think about a
country in Africa where it has never snowed. Then, 
plausible paths might be defined as ones in which it never snows,
i.e., all paths that satisfy 2 ¬snows. Formally, let γ be a
CTLK path formula. We define |γ|M to be the set of paths
that satisfy γ in model M:
| fϕ|M = {λ | M, λ[1] |= ϕ}
|2 ϕ|M = {λ | ∀i (M, λ[i] |= ϕ)}
|ϕ1U ϕ2|M = {λ | ∃i
`
M, λ[i] |= ϕ2 ∧
∀j(0 ≤ j < i ⇒ M, λ[j] |= ϕ1)
´
}.
Moreover, we define the plausible paths model update as
follows. Let M = Q, R, ∼1, ..., ∼k, Υ1, ..., Υk, π be a
CTLKP model, and let P ⊆ ΛM be a set of paths. Then
Ma,P
= Q, R, ∼1, ..., ∼k, Υ1, ..., Υa−1, P, Υa+1, ..., Υk, π 
denotes model M with a"s set of plausible paths reset to P.
Now we can extend the language of CTLKP with 
formulae (set-pla γ)ϕ with the intuitive reading: suppose that
γ exactly characterizes the set of plausible paths, then ϕ
holds, and formal semantics given below:
M, q |=P (set-pla γ)ϕ iff Ma,|γ|M , q |=P ϕ.
We observe that this update scheme is similar to the one
proposed in [13].
3.4 Comparison to Related Work
Several modal notions of plausibility were already 
discussed in the existing literature [7, 8, 20, 18, 16]. In these
papers, like in ours, plausibility is used as a primitive 
semantic concept that helps to define beliefs on top of agents"
knowledge. A similar idea was introduced by Moses and
Shoham in [18]. Their work preceded both [7, 8] and 
[20]and although Moses and Shoham do not explicitly mention
the term plausibility, it seems appropriate to summarize
their idea first.
Moses and Shoham: Beliefs as Conditional Knowledge
In [18], beliefs are relativized with respect to a formula α
(which can be seen as a plausibility assumption expressed
in the object language). More precisely, worlds that satisfy α
can be considered as plausible. This concept is expressed via
symbols Bα
i ϕ; the index i ∈ {1, 2, 3} is used to distinguish
between three different implementations of beliefs. The first
version is given by Bα
1 ϕ ≡ K(α → ϕ).4
A drawback of
this version is that if α is false, then everything will be
believed with respect to α. The second version overcomes
this problem: Bα
2 ϕ ≡ K(α → ϕ) ∧ (K¬α → Kϕ); now ϕ is
only believed if it is known that ϕ follows from assumption
α, and ϕ must be known if assumption α is known to be false.
Finally, Bα
3 ϕ ≡ K(α → ϕ) ∧ ¬K¬α: if the assumption α is
known to be false, nothing should be believed with respect to
α. The strength of these different notions is given as follows:
Bα
3 ϕ implies Bα
2 ϕ, and Bα
2 ϕ implies Bα
1 ϕ. In this approach,
belief is strongly connected to knowledge in the sense that
belief is knowledge with respect to a given assumption.
Friedman and Halpern: Plausibility Spaces
The work of Friedman and Halpern [7] extends the concepts
of knowledge and belief with an explicit notion of 
plausibility; i.e., some worlds are more plausible for an agent
than others. To implement this idea, Kripke models are
extended with function P which assigns a plausibility space
P(q, a) = (Ω(q,a), (q,a)) to every state, or more generally
every possible world q, and agent a. The plausibility space
4
Unlike in most approaches, K is interpreted over all worlds
and not only over the indistinguishable worlds.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 585
is just a partially ordered subset of states/worlds; that is,
Ω(q, a) ⊆ Q, and (q,a)⊆ Q ×Q is a reflexive and transitive
relation. Let S, T ⊆ Ω(q,a) be finite subsets of states; now,
T is defined to be plausible given S with respect to P(q, a),
denoted by S →P (q,a) T, iff all minimal points/states in
S (with respect to (q,a)) are also in T.5
Friedman and
Halpern"s view to modal plausibility is closely related to
probability and, more generally, plausibility measures. 
Logics of plausibility can be seen as a qualitative description of
agents preferences/knowledge; logics of probability [6, 15],
on the other hand, offer a quantitative description.
The logic from [7] is defined by the following grammar:
ϕ ::= p | ϕ∧ϕ | ¬ϕ | Kaϕ | ϕ →a ϕ, where the semantics of
all operators except →a is given as usual, and formulae ϕ →a
ψ have the meaning that ψ is true in the most plausible
worlds in which ϕ holds. Formally, the semantics for →a
is given as: M, q |= ϕ →a ψ iff Sϕ
P (q,a) →P(q,a)
Sψ
P (q,a),
where Sϕ
(q,a) = {q ∈ Ω(q,a) | M, q |= ϕ} are the states in
Ω(q,a) that satisfy ϕ. The idea of defining beliefs is given
by the assumption that an agent believes in something if he
knows that it is true in the most plausible worlds of Ω(q,a);
formally, this can be stated as Baϕ ≡ Ka( →a ϕ).
Friedman and Halpern have shown that the KD45 
axioms are valid for operator Ba if plausibility spaces satisfy
consistency (for all states q ∈ Q it holds that Ω(q,a) ⊆ { q ∈
Q | q ∼a q }) and normality (for all states q ∈ Q it holds
that Ω(q,a) = ∅).6
A temporal extension of the language
(mentioned briefly in [7], and discussed in more detail in [8])
uses the interpreted systems approach [10, 5]. A system R
is given by runs, where a run r : N → Q is a function from
time moments (modeled by N) to global states, and a time
point (r, i) is given by a time point i ∈ N and a run r. A
global state is a combination of local states, one per agent.
An interpreted system M = (R, π) is given by a system R
and a valuation of propositions π. Epistemic relations are
defined over time points, i.e., (r , m ) ∼a (r, m) iff agent
a"s local states ra(m ) and ra(m) of (r , m ) and (r, m) are
equal. Formulae are interpreted in a straightforward way
with respect to interpreted systems, e.g. M, r, m |= Kaϕ iff
M, r , m |= ϕ for all (r , m ) ∼a (r, m). Now, these are time
points that play the role of possible worlds; consequently,
plausibility spaces P(r,m,a) are assigned to each point (r, m)
and agent a.
Su et al.: KBC Logic
Su et al. [20] have developed a multi-modal, 
computationally grounded logic with modalities K, B, and C (knowledge,
belief, and certainty). The computational model consists of
(global) states q = (qvis
, qinv
, qper
, Qpls
) where the 
environment is divided into a visible (qvis
) and an invisible part
(qinv
), and qper
captures the agent"s perception of the visible
part of the environment. External sources may provide the
agent with information about the invisible part of a state,
which results in a set of states Qpls
that are plausible for
the agent.
Given a global state q, we additionally define V is(q) =
qvis
, Inv(q) = qinv
, Per(q) = qper
, and Pls(q) = Qpls
. The
5
When there are infinite chains . . . q3 q2 a q1, the
definition is much more sophisticated. An interested reader
is referred to [7] for more details.
6
Note that this normality is essentially seriality of states
wrt plausibility spaces.
semantics is given by an extension of interpreted systems [10,
5], here, it is called interpreted KBC systems. KBC 
formulae are defined as ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | Kϕ | Bϕ | Cϕ.
The epistemic relation ∼vis is captured in the following way:
(r, i) ∼vis (r , i ) iff V is(r(i)) = V is(r (i )). The semantic
clauses for belief and certainty are given below.
M, r, i |= Bϕ iff M, r , i |= ϕ for all (r , i ) with V is(r (i )) =
Per(r(i)) and Inv(r (i )) ∈ Pls(r(i))
M, r, i |= Cϕ iff M, r , i |= ϕ for all (r (i )) with V is(r (i )) =
Per(r(i))
Thus, an agent believes ϕ if, and only if, ϕ is true in all
states which look like what he sees now and seem plausible
in the current state. Certainty is stronger: if an agent is
certain about ϕ, the formula must hold in all states with
a visible part equal to the current perception, regardless of
whether the invisible part is plausible or not.
The logic does not include temporal formulae, although
it might be extended with temporal operators, as time is
already present in KBC models.
What Are the Differences to Our Logic?
In our approach, plausibility is explicitly seen as a temporal
property, i.e., it is a property of temporal paths rather than
states. In the object language, this is reflected by the fact
that plausibility assumptions are specified through path 
formulae. In contrast, the approach of [18] and [20] is static:
not only the logics do not include operators for talking about
time and/or change, but these are states that are assumed
plausible or not in their semantics.
The differences to [7, 8] are more subtle. Firstly, the
framework of Friedman and Halpern is static in the sense
that plausibility is taken as a property of (abstract) 
possible worlds. This formulation is flexible enough to allow for
incorporating time; still, in our approach, time is inherent
to plausibility rather than incidental.
Secondly, our framework is more computationally oriented.
The implementation of temporal plausibility in [7, 8] is based
on the interpreted systems approach with time points (r, m)
being subject to plausibility. As runs are included in time
points, they can also be defined plausible or implausible.7
However, it also means that time points serve the role of
possible worlds in the basic formulation, which yields Kripke
structures with uncountable possible world spaces in all but
the most trivial cases.
Thirdly, [7, 8] build on linear time: a run (more precisely,
a time moment (r, m)) is fixed when a formula is interpreted.
In contrast, we use branching time with explicit 
quantification over temporal paths.8
We believe that branching time
is more suitable for non-deterministic domains (cf. e.g. [4]),
of which multi-agent systems are a prime example. Note
that branching time makes our notion of belief different from
Friedman and Halpern"s. Most notably, property Kϕ → Bϕ
is valid in their approach, but not in ours: an agent may
7
Friedman and Halpern even briefly mention how 
plausibility of runs can be embedded in their framework.
8
To be more precise, time in [7] does implicitly branch at
epistemic states. This is because (r, m) ∼a (r , m ) iff a"s
local state corresponding to both time points is the same
(ra(m) = ra(m )). In consequence, the semantics of Kaϕ
can be read as for every run, and every moment on this
run that yields the same local state as now, ϕ holds.
586 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
know that some course of events is in principle possible,
without believing that it can really become the case (see
Section 4.2). As Proposition 13 suggests, such a subtle 
distinction between knowledge and beliefs is possible in our
approach because branching time logics allow for existential
quantification over runs.
Fourthly, while Friedman and Halpern"s models are very
flexible, they also enable system descriptions that may seem
counterintuitive. Suppose that (r, m) is plausible in itself
(formally: (r, m) is minimal wrt (r,m,a)), but (r, m + 1) is
not plausible in (r, m + 1). This means that following the
plausible path makes it implausible (cf. Remark 1), which
is even stranger in the case of linear time. Combining the
argument with computational aspects, we suggest that our
approach can be more natural and straightforward for many
applications.
Last but not least, our logic provides a mechanism for
specifying (and updating) sets of plausible paths in the 
object language. Thus, plausibility sets can be specified in a
succinct way, which is another feature that makes our 
framework computation-friendly. The model checking results from
Section 5 are especially encouraging in this light.
4. PLAUSIBILITY, KNOWLEDGE, AND 
BELIEFS IN CTLKP
In this section we study some relevant properties of 
plausibility, knowledge, and beliefs; in particular, axioms KDT45
are examined. But first, we identify two important 
subclasses of models with plausibility.
A CTLKP model is plausibly serial (or p-serial) for agent
a if every state of the system is part of a plausible path 
according to a, i.e. on(Υa) = Q. As we will see further, a
weaker requirement is sometimes sufficient. We call a model
weakly p-serial if every state has at least one 
indistinguishable counterpart which lies on a plausible path, i.e. for each
q ∈ Q there is a q ∈ Q such that q ∼a q and q ∈ on(Υa).
Obviously, p-seriality implies weak p-seriality. We get the
following characterization of both model classes.
Proposition 4. M is plausibly serial for agent a iff 
formula Pl aE f is valid in M. M is weakly p-serial for agent
a iff ¬KaPl aA f⊥ is valid in M.
4.1 Axiomatic Properties
Theorem 5. Axioms K, D, 4, and 5 for knowledge are
strongly valid, and axiom T is valid. That is, modalities Ka
form system S5 in the sense of normal validity, and KD45
in the sense of strong validity.
We do not include proofs here due to lack of space. The
interested reader is referred to [2], where detailed proofs are
given.
Proposition 6. Axioms K, 4, and 5 for beliefs are
strongly valid. That is, we have:
|≡ (Baϕ ∧ Ba(ϕ → ψ)) → Baψ, |≡ (Baϕ → BaBaϕ), and
|≡ (¬Baϕ → Ba¬Baϕ).
The next proposition concerns the consistency axiom
D: Baϕ → ¬Ba¬ϕ. It is easy to see that the axiom is not
valid in general: as we have no restrictions on plausibility
sets Υa, it may be as well that Υa = ∅. In that case we have
Baϕ ∧ Ba¬ϕ for all formulae ϕ, because the set of states to
be considered becomes empty. However, it turns out that D
is valid for a very natural class of models.
Proposition 7. Axiom D for beliefs is not valid in the
class of all CTLKP models. However, it is strongly valid in
the class of weak p-serial models (and therefore also in the
class of p-serial models).
Moreover, as one may expect, beliefs do not have to be
always true.
Proposition 8. Axiom T for beliefs is not valid; i.e.,
|= (Baϕ → ϕ). The axiom is not even valid in the class of
p-serial models.
Theorem 9. Belief modalities Ba form system K45 in
the class of all models, and KD45 in the class of weakly
plausibly serial models (in the sense of both normal and
strong validity). Axiom T is not even valid for p-serial 
models.
4.2 Plausibility, Knowledge, and Beliefs
First, we investigate the relationship between knowledge
and plausibility/physicality operators. Then, we look at the
interaction between knowledge and beliefs.
Proposition 10. Let ϕ be a CTLKP formula, and M
be a CTLKP model. We have the following strong validities:
(i) |≡ Pl aKaϕ ↔ Kaϕ
(ii) |≡ Ph Kaϕ ↔ KaPh ϕ and |≡ KaPh ϕ ↔ Kaϕ
We now want to examine the relationship between 
knowledge and belief. For instance, if agent a believes in 
something, he knows that he believes it. Or, if he knows a fact,
he also believes that he knows it. On the other hand, for
instance, an agent does not necessarily believe in all the
things he knows. For example, we may know that an 
invasion from another galaxy is in principle possible (KaE♦
invasion), but if we do not take this possibility as plausible
(¬Pl aE♦ invasion), then we reject the corresponding belief
in consequence (¬BaE♦ invasion). Note that this property
reflects the strong connection between belief and plausibility
in our framework.
Proposition 11. The following formulae are strongly
valid:
(i) Baϕ → KaBaϕ, (ii) KaBaϕ → Baϕ,
(iii) Kaϕ → BaKaϕ.
The following formulae are not valid:
(iv) Baϕ → BaKaϕ, (v) Kaϕ → Baϕ
The last invalidity is especially important: it is not the
case that knowing something implies believing in it. This
emphasizes that we study a specific concept of beliefs here.
Note that its specific is not due to the plausibility-based 
definition of beliefs. The reason lies rather in the fact that we
investigate knowledge, beliefs and plausibility in a temporal
framework, as Proposition 12 shows.
Proposition 12. Let ϕ be a CTLKP formula that does
not include any temporal operators. Then Kaϕ → Baϕ is
strongly valid, and in the class of p-serial models we have
even that |≡ Kaϕ ↔ Baϕ.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 587
Moreover, it is important that we use branching time with
explicit quantification over paths; this observation is 
formalized in Proposition 13.
Definition 1. We define the universal sublanguage of
CTLK in a way similar to [21]:
ϕu ::= p | ¬p | ϕu ∧ ϕu | ϕu ∨ ϕu | Aγu | Kaϕu,
γu ::= fϕu | 2 ϕu | ϕuU ϕu.
We call such ϕu universal formulae, and γu universal path
formulae.
Proposition 13. Let ϕu be a universal CTLK formula.
Then |≡ Kaϕu → Baϕu.
The following two theorems characterize the relationship
between knowledge and beliefs: first for the class of p-serial
models, and then, finally, for all models.
Theorem 14. The following formulae are strongly valid
in the class of plausibly serial CTLKP models:
(i) Baϕ ↔ KaPl aϕ, (ii) Kaϕ ↔ BaPh ϕ.
Theorem 15. Formula Baϕ ↔ KaPl a(E f → ϕ) is
strongly valid.
Note that this characterization has a strong commonsense
reading: believing in ϕ is knowing that ϕ plausibly holds in
all plausibly imaginable situations.
4.3 Properties of the Update
The first notable property of plausibility update is that it
influences only formulae in which plausibility plays a role,
i.e. ones in which belief or plausibility modalities occur.
Proposition 16. Let ϕ be a CTLKP formula that does
not include operators Pl a and Ba, and γ be a CTLKP path
formula. Then, we have |≡ ϕ ↔ (set-pla γ)ϕ.
What can be said about the result of an update? At first
sight, formula (set-pla γ)Pl aAγ seems a natural 
characterization; however, it is not valid. This is because, by leaving
the other (implausible) paths out of scope, we may leave out
of |γ| some paths that were needed to satisfy γ (see the 
example in Section 4.2). We propose two alternative ways out:
the first one restricts the language of the update similarly
to [21]; the other refers to physical possibilities, in a way
analogous to [13].
Proposition 17. The CTLKP formula (set-pla γ)Pl aAγ
is not valid. However, we have the following validities:
(i) |≡ (set-pla γu)Pl aAγu, where γu is a universal CTLK
path formula from Definition 1.
(ii) If ϕ, ϕ1, ϕ2 are arbitrary CTLK formulae, then:
|≡ (set-pla
fϕ)Pl aA f(Ph ϕ),
|≡ (set-pla 2 ϕ)Pl aA2 (Ph ϕ), and
|≡ (set-pla ϕ1U ϕ2)Pl aA(Ph ϕ1)U (Ph ϕ2).
5. VERIFICATION OF PLAUSIBILITY,
TIME AND BELIEFS
In this section we report preliminary results on model
checking CTLKP formulae. Clearly, verifying CTLKP
properties directly against models with plausibility does not
make much sense, since these models are inherently infinite;
what we need is a finite representation of plausibility sets.
One such representation has been discussed in Section 3.3:
plausibility sets can be defined by path formulae and the
update operator (set-pla γ).
We follow this idea here, studying the complexity of model
checking CTLKP formulae against CTLK models (which
can be seen as a compact representation of CTLKP 
models in which all the paths are assumed plausible), with the
underlying idea that plausibility sets, when needed, must be
defined explicitly in the object language. Below we sketch
an algorithm that model-checks CTLKP formulae in time
linear wrt the size of the model and the length of the 
formula. This means that we have extended CTLK to a more
expressive language with no computational price to pay.
First of all, we get rid of the belief operators (due to 
Theorem 15), replacing every occurrence of Baϕ with KaPl a(E f
→ ϕ). Now, let −→γ = γ1, ..., γk be a vector of vanilla
path formulae (one per agent), with the initial vector −→γ0 =
, ..., , and −→γ [γ /a] denoting vector −→γ , in which −→γ [a]
is replaced with γ . Additionally, we define −→γ [0] = . We
translate the resulting CTLKP formulae to ones without
plausibility via function tr(ϕ) = tr−→γ0,0(ϕ), defined as 
follows:
tr−→γ ,i(p) = p,
tr−→γ ,i(ϕ1 ∧ ϕ2) = tr−→γ ,i(ϕ1) ∧ tr−→γ ,i(ϕ2),
tr−→γ ,i(¬ϕ) = ¬tr−→γ ,i(ϕ),
tr−→γ ,i(Kaϕ) = Ka tr−→γ ,0(ϕ),
tr−→γ ,i(Pla ϕ) = tr−→γ ,a(ϕ),
tr−→γ ,i((set-pla γ )ϕ) = tr−→γ [γ /a],i(ϕ),
tr−→γ ,i(Ph ϕ) = tr−→γ ,0(ϕ),
tr−→γ ,i( fϕ) = ftr−→γ ,i(ϕ),
tr−→γ ,i(2 ϕ) = 2 tr−→γ ,i(ϕ),
tr−→γ ,i(ϕ1U ϕ2) = tr−→γ ,i(ϕ1)U tr−→γ ,i(ϕ2),
tr−→γ ,i(Eγ ) = E(−→γ [i] ∧ tr−→γ ,i(γ )).
Note that the resulting sentences belong to the logic of
CTLK+, that is CTL+ (where each path quantifier can be
followed by a Boolean combination of vanilla path 
formulae)9
with epistemic modalities. The following proposition
justifies the translation.
Proposition 18. For any CTLKP formula ϕ without
Ba, we have that M, q |=CTLKP ϕ iff M, q |=CTLK+ tr(ϕ).
In general, model checking CTL+ (and also CTLK+)
is ΔP
2 -complete. However, in our case, the Boolean 
combinations of path subformulae are always conjunctions of at
most two non-negated elements, which allows us to propose
the following model checking algorithm. First, subformulae
are evaluated recursively: for every subformula ψ of ϕ, the
set of states in M that satisfy ψ is computed and labeled
with a new proposition pψ. Now, it is enough to define
checking M, q |= ϕ for ϕ in which all (state) subformulae
are propositions, with the following cases:
Case M, q |= E(2 p ∧ γ): If M, q |= p, then return no. 
Otherwise, remove from M all the states that do not satisfy
p (yielding a sparser model M ), and check the CTL
formula Eγ in M , q with any CTL model-checker.
Case M, q |= E( fp ∧ γ): Create M by adding a copy q of
state q, in which only the transitions to states 
satisfying p are kept (i.e., M, q |= r iff M, q |= r; and q Rq
iff qRq and M, q |= p). Then, check Eγ in M , q .
9
For the semantics of CTL+, and discussion of model
checking complexity, cf. [17].
588 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Case M, q |= E(p1U p2 ∧ p3U p4): Note that this is 
equivalent to checking E(p1 ∧ p3)U (p2 ∧ Ep3U p4) ∨ E(p1 ∧
p3)U (p4 ∧ Ep1U p2), which is a CTL formula.
Other cases: The above cases cover all possible formulas
that begin with a path quantifier. For other cases,
standard CTLK model checking can be used.
Theorem 19. Model checking CTLKP against CTLK
models is PTIME-complete, and can be done in time O(ml),
where m is the number of transitions in the model, and l is
the length of the formula to be checked. That is, the 
complexity is no worse than for CTLK itself.
6. CONCLUSIONS
In this paper a notion of plausible behavior is considered,
with the underlying idea that implausible options should be
usually ignored in practical reasoning about possible future
courses of action. We add the new notion of plausibility to
the logic of CTLK [19], and obtain a language which 
enables reasoning about what can (or must) plausibly happen.
As a technical device to define the semantics of the resulting
logic, we use a non-standard satisfaction relation |=P that
allows to propagate the current set of plausible paths into
subformulae. Furthermore, we propose a non-standard 
notion of beliefs, defined in terms of indistinguishability and
plausibility. We also propose how plausibility assumptions
can be specified in the object language via a plausibility 
update operator (in a way similar to [13]).
We use this new framework to investigate some important
properties of plausibility, knowledge, beliefs, and updates.
In particular, we show that knowledge is an S5 modality,
and that beliefs satisfy axioms K45 in general, and KD45
for the class of plausibly serial models. We also prove that
believing in ϕ is knowing that ϕ plausibly holds in all 
plausibly possible situations. That is, the relationship between
knowledge and beliefs is very natural and reflects the 
initial intuition precisely. Moreover, the model checking 
results from Section 5 show that verification for CTLKP is
no more complex than for CTL and CTLK.
We would like to stress that we do not see this contribution
as a mere technical exercise in formal logic. Human agents
use a similar concept of plausibility and practical beliefs
in their everyday reasoning in order to reduce the search
space and make the reasoning feasible. As a consequence, we
suggest that the framework we propose may prove suitable
for modeling, design, and analysis resource-bounded agents
in general.
We would like to thank Juergen Dix for fruitful 
discussions, useful comments and improvements.
7. REFERENCES
[1] R. Alur, T. A. Henzinger, and O. Kupferman.
Alternating-time Temporal Logic. Journal of the
ACM, 49:672-713, 2002.
[2] N. Bulling and W. Jamroga. Agents, beliefs and
plausible behavior in a temporal setting. Technical
Report IfI-06-05, Clausthal Univ. of Technology, 2006.
[3] E. A. Emerson. Temporal and modal logic. In J. van
Leeuwen, editor, Handbook of Theoretical Computer
Science, volume B, pages 995-1072. Elsevier, 1990.
[4] E.A. Emerson and J.Y. Halpern. sometimes and
not never revisited: On branching versus linear time
temporal logic. Journal of the ACM, 33(1):151-178,
1986.
[5] R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi.
Reasoning about Knowledge. MIT Press: Cambridge,
MA, 1995.
[6] R. Fagin and J.Y. Halpern. Reasoning about
knowledge and probability. Journal of ACM,
41(2):340-367, 1994.
[7] N. Friedman and J.Y. Halpern. A knowledge-based
framework for belief change, Part I: Foundations. In
Proceedings of TARK, pages 44-64, 1994.
[8] N. Friedman and J.Y. Halpern. A knowledge-based
framework for belief change, Part II: Revision and
update. In Proceedings of KR"94, 1994.
[9] J.Y. Halpern. Reasoning about knowledge: a survey.
In Handbook of Logic in Artificial Intelligence and
Logic Programming. Vol. 4: Epistemic and Temporal
Reasoning, pages 1-34. Oxford University Press,
Oxford, 1995.
[10] J.Y. Halpern and R. Fagin. Modelling knowledge and
action in distributed systems. Distributed Computing,
3(4):159-177, 1989.
[11] W. Jamroga and N. Bulling. A general framework for
reasoning about rational agents. In Proceedings of
AAMAS"07, 2007. Short paper.
[12] W. Jamroga and W. van der Hoek. Agents that know
how to play. Fundamenta Informaticae,
63(2-3):185-219, 2004.
[13] W. Jamroga, W. van der Hoek, and M. Wooldridge.
Intentions and strategies in game-like scenarios. In
Progress in Artificial Intelligence: Proceedings of
EPIA 2005, volume 3808 of LNAI, pages 512-523.
Springer Verlag, 2005.
[14] W. Jamroga and Thomas ˚Agotnes. Constructive
knowledge: What agents can achieve under incomplete
information. Technical Report IfI-05-10, Clausthal
University of Technology, 2005.
[15] B.P. Kooi. Probabilistic dynamic epistemic logic.
Journal of Logic, Language and Information,
12(4):381-408, 2003.
[16] P. Lamarre and Y. Shoham. Knowledge, certainty,
belief, and conditionalisation (abbreviated version). In
Proceedings of KR"94, pages 415-424, 1994.
[17] F. Laroussinie, N. Markey, and Ph. Schnoebelen.
Model checking CTL+ and FCTL is hard. In
Proceedings of FoSSaCS"01, volume 2030 of LNCS,
pages 318-331. Springer, 2001.
[18] Y. Moses and Y. Shoham. Belief as defeasible
knowledge. Artificial Intelligence, 64(2):299-321, 1993.
[19] W. Penczek and A. Lomuscio. Verifying epistemic
properties of multi-agent systems via bounded model
checking. In Proceedings of AAMAS"03, pages
209-216, New York, NY, USA, 2003. ACM Press.
[20] K. Su, A. Sattar, G. Governatori, and Q. Chen. A
computationally grounded logic of knowledge, belief
and certainty. In Proceedings of AAMAS"05, pages
149-156. ACM Press, 2005.
[21] W. van der Hoek, M. Roberts, and M. Wooldridge.
Social laws in alternating time: Effectiveness,
feasibility and synthesis. Synthese, 2005.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 589
Graphical Models for Online Solutions to Interactive
POMDPs
Prashant Doshi
Dept. of Computer Science
University of Georgia
Athens, GA 30602, USA
pdoshi@cs.uga.edu
Yifeng Zeng
Dept. of Computer Science
Aalborg University
DK-9220 Aalborg, Denmark
yfzeng@cs.aau.edu
Qiongyu Chen
Dept. of Computer Science
National Univ. of Singapore
117543, Singapore
chenqy@comp.nus.edu.sg
ABSTRACT
We develop a new graphical representation for interactive partially
observable Markov decision processes (I-POMDPs) that is 
significantly more transparent and semantically clear than the previous
representation. These graphical models called interactive dynamic
influence diagrams (I-DIDs) seek to explicitly model the structure
that is often present in real-world problems by decomposing the 
situation into chance and decision variables, and the dependencies 
between the variables. I-DIDs generalize DIDs, which may be viewed
as graphical representations of POMDPs, to multiagent settings in
the same way that I-POMDPs generalize POMDPs. I-DIDs may be
used to compute the policy of an agent online as the agent acts and
observes in a setting that is populated by other interacting agents.
Using several examples, we show how I-DIDs may be applied and
demonstrate their usefulness.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems
General Terms
Theory
1. INTRODUCTION
Interactive partially observable Markov decision processes 
(IPOMDPs) [9] provide a framework for sequential decision-making
in partially observable multiagent environments. They generalize
POMDPs [13] to multiagent settings by including the other agents"
computable models in the state space along with the states of the
physical environment. The models encompass all information 
influencing the agents" behaviors, including their preferences, 
capabilities, and beliefs, and are thus analogous to types in Bayesian
games [11]. I-POMDPs adopt a subjective approach to 
understanding strategic behavior, rooted in a decision-theoretic framework that
takes a decision-maker"s perspective in the interaction.
In [15], Polich and Gmytrasiewicz introduced interactive 
dynamic influence diagrams (I-DIDs) as the computational 
representations of I-POMDPs. I-DIDs generalize DIDs [12], which may
be viewed as computational counterparts of POMDPs, to 
multiagents settings in the same way that I-POMDPs generalize POMDPs.
I-DIDs contribute to a growing line of work [19] that includes
multi-agent influence diagrams (MAIDs) [14], and more recently,
networks of influence diagrams (NIDs) [8]. These formalisms seek
to explicitly model the structure that is often present in real-world
problems by decomposing the situation into chance and decision
variables, and the dependencies between the variables. MAIDs
provide an alternative to normal and extensive game forms using
a graphical formalism to represent games of imperfect information
with a decision node for each agent"s actions and chance nodes
capturing the agent"s private information. MAIDs objectively 
analyze the game, efficiently computing the Nash equilibrium profile
by exploiting the independence structure. NIDs extend MAIDs to
include agents" uncertainty over the game being played and over
models of the other agents. Each model is a MAID and the network
of MAIDs is collapsed, bottom up, into a single MAID for 
computing the equilibrium of the game keeping in mind the different 
models of each agent. Graphical formalisms such as MAIDs and NIDs
open up a promising area of research that aims to represent 
multiagent interactions more transparently. However, MAIDs provide an
analysis of the game from an external viewpoint and the 
applicability of both is limited to static single play games. Matters are more
complex when we consider interactions that are extended over time,
where predictions about others" future actions must be made using
models that change as the agents act and observe. I-DIDs address
this gap by allowing the representation of other agents" models as
the values of a special model node. Both, other agents" models and
the original agent"s beliefs over these models are updated over time
using special-purpose implementations.
In this paper, we improve on the previous preliminary 
representation of the I-DID shown in [15] by using the insight that the static
I-ID is a type of NID. Thus, we may utilize NID-specific language
constructs such as multiplexers to represent the model node, and
subsequently the I-ID, more transparently. Furthermore, we clarify
the semantics of the special purpose policy link introduced in the
representation of I-DID by [15], and show that it could be replaced
by traditional dependency links. In the previous representation of
the I-DID, the update of the agent"s belief over the models of others
as the agents act and receive observations was denoted using a 
special link called the model update link that connected the model
nodes over time. We explicate the semantics of this link by 
showing how it can be implemented using the traditional dependency
links between the chance nodes that constitute the model nodes.
The net result is a representation of I-DID that is significantly more
transparent, semantically clear, and capable of being implemented
using the standard algorithms for solving DIDs. We show how 
IDIDs may be used to model an agent"s uncertainty over others"
models, that may themselves be I-DIDs. Solution to the I-DID is
a policy that prescribes what the agent should do over time, given
its beliefs over the physical state and others" models. Analogous to
DIDs, I-DIDs may be used to compute the policy of an agent online
as the agent acts and observes in a setting that is populated by other
interacting agents.
2. BACKGROUND: FINITELY NESTED 
IPOMDPS
Interactive POMDPs generalize POMDPs to multiagent settings
by including other agents" models as part of the state space [9].
Since other agents may also reason about others, the interactive
state space is strategically nested; it contains beliefs about other
agents" models and their beliefs about others. For simplicity of
presentation we consider an agent, i, that is interacting with one
other agent, j.
A finitely nested I-POMDP of agent i with a strategy level l is
defined as the tuple:
I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri
where: • ISi,l denotes a set of interactive states defined as, ISi,l =
S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and
ISi,0 = S, where S is the set of states of the physical 
environment. Θj,l−1 is the set of computable intentional models of agent
j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj,
OCj . Here, j is Bayes rational and OCj is j"s optimality criterion.
SMj is the set of subintentional models of j. Simple examples of
subintentional models include a no-information model [10] and a
fictitious play model [6], both of which are history independent.
We give a recursive bottom-up construction of the interactive state
space below.
ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)}
ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)}
.
.
.
.
.
.
ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)}
Similar formulations of nested spaces have appeared in [1, 3].
• A = Ai × Aj is the set of joint actions of all agents in the
environment; • Ti : S ×A×S → [0, 1], describes the effect of the
joint actions on the physical states of the environment; • Ωi is the
set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives
the likelihood of the observations given the physical state and joint
action; • Ri : ISi × A → R describes agent i"s preferences over
its interactive states. Usually only the physical states will matter.
Agent i"s policy is the mapping, Ω∗
i → Δ(Ai), where Ω∗
i is
the set of all observation histories of agent i. Since belief over the
interactive states forms a sufficient statistic [9], the policy can also
be represented as a mapping from the set of all beliefs of agent i to
a distribution over its actions, Δ(ISi) → Δ(Ai).
2.1 Belief Update
Analogous to POMDPs, an agent within the I-POMDP 
framework updates its belief as it acts and observes. However, there are
two differences that complicate the belief update in multiagent 
settings when compared to single agent ones. First, since the state of
the physical environment depends on the actions of both agents, i"s
prediction of how the physical state changes has to be made based
on its prediction of j"s actions. Second, changes in j"s models have
to be included in i"s belief update. Specifically, if j is intentional
then an update of j"s beliefs due to its action and observation has
to be included. In other words, i has to update its belief based on
its prediction of what j would observe and how j would update
its belief. If j"s model is subintentional, then j"s probable 
observations are appended to the observation history contained in the
model. Formally, we have:
Pr(ist
|at−1
i , bt−1
i,l ) = β ISt−1:mt−1
j =θt
j
bt−1
i,l (ist−1
)
× at−1
j
Pr(at−1
j |θt−1
j,l−1)Oi(st
, at−1
i , at−1
j , ot
i)
×Ti(st−1
, at−1
i , at−1
j , st
) ot
j
Oj(st
, at−1
i , at−1
j , ot
j)
×τ(SEθt
j
(bt−1
j,l−1, at−1
j , ot
j) − bt
j,l−1)
(1)
where β is the normalizing constant, τ is 1 if its argument is 0
otherwise it is 0, Pr(at−1
j |θt−1
j,l−1) is the probability that at−1
j is
Bayes rational for the agent described by model θt−1
j,l−1, and SE(·)
is an abbreviation for the belief update. For a version of the belief
update when j"s model is subintentional, see [9].
If agent j is also modeled as an I-POMDP, then i"s belief update
invokes j"s belief update (via the term SEθt
j
( bt−1
j,l−1 , at−1
j , ot
j)),
which in turn could invoke i"s belief update and so on. This 
recursion in belief nesting bottoms out at the 0th
level. At this level, the
belief update of the agent reduces to a POMDP belief update. 1
For
illustrations of the belief update, additional details on I-POMDPs,
and how they compare with other multiagent frameworks, see [9].
2.2 Value Iteration
Each belief state in a finitely nested I-POMDP has an associated
value reflecting the maximum payoff the agent can expect in this
belief state:
Un
( bi,l, θi ) = max
ai∈Ai is∈ISi,l
ERi(is, ai)bi,l(is)+
γ
oi∈Ωi
Pr(oi|ai, bi,l)Un−1
( SEθi
(bi,l, ai, oi), θi )
(2)
where, ERi(is, ai) = aj
Ri(is, ai, aj)Pr(aj|mj,l−1) (since
is = (s, mj,l−1)). Eq. 2 is a basis for value iteration in I-POMDPs.
Agent i"s optimal action, a∗
i , for the case of finite horizon with
discounting, is an element of the set of optimal actions for the belief
state, OPT(θi), defined as:
OPT( bi,l, θi ) = argmax
ai∈Ai is∈ISi,l
ERi(is, ai)bi,l(is)
+γ
oi∈Ωi
Pr(oi|ai, bi,l)Un
( SEθi
(bi,l, ai, oi), θi )
(3)
3. INTERACTIVEINFLUENCEDIAGRAMS
A naive extension of influence diagrams (IDs) to settings 
populated by multiple agents is possible by treating other agents as 
automatons, represented using chance nodes. However, this approach
assumes that the agents" actions are controlled using a probability
distribution that does not change over time. Interactive influence
diagrams (I-IDs) adopt a more sophisticated approach by 
generalizing IDs to make them applicable to settings shared with other
agents who may act and observe, and update their beliefs.
3.1 Syntax
In addition to the usual chance, decision, and utility nodes, 
IIDs include a new type of node called the model node. We show a
general level l I-ID in Fig. 1(a), where the model node (Mj,l−1) is
denoted using a hexagon. We note that the probability distribution
over the chance node, S, and the model node together represents
agent i"s belief over its interactive states. In addition to the model
1
The 0th
level model is a POMDP: Other agent"s actions are treated
as exogenous events and folded into the T, O, and R functions.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 815
Figure 1: (a) A generic level l I-ID for agent i situated with one other agent j. The hexagon is the model node (Mj,l−1) whose structure we show in
(b). Members of the model node are I-IDs themselves (m1
j,l−1, m2
j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to
the corresponding chance nodes (A1
j , A2
j ). Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to
the node Aj. (c) The transformed I-ID with the model node replaced by the chance nodes and the relationships between them.
node, I-IDs differ from IDs by having a dashed link (called the
policy link in [15]) between the model node and a chance node,
Aj, that represents the distribution over the other agent"s actions
given its model. In the absence of other agents, the model node and
the chance node, Aj, vanish and I-IDs collapse into traditional IDs.
The model node contains the alternative computational models
ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where
Θj,l−1 and SMj were defined previously in Section 2. Thus, a
model in the model node may itself be an I-ID or ID, and the 
recursion terminates when a model is an ID or subintentional. Because
the model node contains the alternative models of the other agent
as its values, its representation is not trivial. In particular, some of
the models within the node are I-IDs that when solved generate the
agent"s optimal policy in their decision nodes. Each decision node
is mapped to the corresponding chance node, say A1
j , in the 
following way: if OPT is the set of optimal actions obtained by solving
the I-ID (or ID), then Pr(aj ∈ A1
j ) = 1
|OP T |
if aj ∈ OPT, 0
otherwise.
Borrowing insights from previous work [8], we observe that the
model node and the dashed policy link that connects it to the
chance node, Aj, could be represented as shown in Fig. 1(b). The
decision node of each level l − 1 I-ID is transformed into a chance
node, as we mentioned previously, so that the actions with the
largest value in the decision node are assigned uniform 
probabilities in the chance node while the rest are assigned zero probability.
The different chance nodes (A1
j , A2
j ), one for each model, and 
additionally, the chance node labeled Mod[Mj] form the parents of the
chance node, Aj. Thus, there are as many action nodes (A1
j , A2
j )
in Mj,l−1 as the number of models in the support of agent i"s 
beliefs. The conditional probability table of the chance node, Aj,
is a multiplexer that assumes the distribution of each of the action
nodes (A1
j , A2
j ) depending on the value of Mod[Mj]. The values of
Mod[Mj] denote the different models of j. In other words, when
Mod[Mj] has the value m1
j,l−1, the chance node Aj assumes the
distribution of the node A1
j , and Aj assumes the distribution of A2
j
when Mod[Mj] has the value m2
j,l−1. The distribution over the
node, Mod[Mj], is the agent i"s belief over the models of j given a
physical state. For more agents, we will have as many model nodes
as there are agents. Notice that Fig. 1(b) clarifies the semantics of
the policy link, and shows how it can be represented using the
traditional dependency links.
In Fig. 1(c), we show the transformed I-ID when the model node
is replaced by the chance nodes and relationships between them. In
contrast to the representation in [15], there are no special-purpose
policy links, rather the I-ID is composed of only those types of
nodes that are found in traditional IDs and dependency 
relationships between the nodes. This allows I-IDs to be represented and
implemented using conventional application tools that target IDs.
Note that we may view the level l I-ID as a NID. Specifically, each
of the level l − 1 models within the model node are blocks in the
NID (see Fig. 2). If the level l = 1, each block is a traditional ID,
otherwise if l > 1, each block within the NID may itself be a NID.
Note that within the I-IDs (or IDs) at each level, there is only a
single decision node. Thus, our NID does not contain any MAIDs.
Figure 2: A level l I-ID represented as a NID. The probabilities 
assigned to the blocks of the NID are i"s beliefs over j"s models 
conditioned on a physical state.
3.2 Solution
The solution of an I-ID proceeds in a bottom-up manner, and is
implemented recursively. We start by solving the level 0 models,
which, if intentional, are traditional IDs. Their solutions provide
probability distributions over the other agents" actions, which are
entered in the corresponding chance nodes found in the model node
of the level 1 I-ID. The mapping from the level 0 models" decision
nodes to the chance nodes is carried out so that actions with the
largest value in the decision node are assigned uniform 
probabilities in the chance node while the rest are assigned zero probability.
Given the distributions over the actions within the different chance
nodes (one for each model of the other agent), the level 1 I-ID is
transformed as shown in Fig. 1(c). During the transformation, the
conditional probability table (CPT) of the node, Aj, is populated
such that the node assumes the distribution of each of the chance
nodes depending on the value of the node, Mod[Mj]. As we 
mentioned previously, the values of the node Mod[Mj] denote the 
different models of the other agent, and its distribution is the agent i"s
belief over the models of j conditioned on the physical state. The
transformed level 1 I-ID is a traditional ID that may be solved 
us816 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
(a) (b)
Figure 3: (a) A generic two time-slice level l I-DID for agent i in a setting with one other agent j. Notice the dotted model update link that denotes
the update of the models of j and the distribution over the models over time. (b) The semantics of the model update link.
ing the standard expected utility maximization method [18]. This
procedure is carried out up to the level l I-ID whose solution gives
the non-empty set of optimal actions that the agent should perform
given its belief. Notice that analogous to IDs, I-IDs are suitable for
online decision-making when the agent"s current belief is known.
4. INTERACTIVE DYNAMIC INFLUENCE
DIAGRAMS
Interactive dynamic influence diagrams (I-DIDs) extend I-IDs
(and NIDs) to allow sequential decision-making over several time
steps. Just as DIDs are structured graphical representations of POMDPs,
I-DIDs are the graphical online analogs for finitely nested I-POMDPs.
I-DIDs may be used to optimize over a finite look-ahead given 
initial beliefs while interacting with other, possibly similar, agents.
4.1 Syntax
We depict a general two time-slice I-DID in Fig. 3(a). In 
addition to the model nodes and the dashed policy link, what 
differentiates an I-DID from a DID is the model update link shown as a
dotted arrow in Fig. 3(a). We explained the semantics of the model
node and the policy link in the previous section; we describe the
model updates next.
The update of the model node over time involves two steps: First,
given the models at time t, we identify the updated set of models
that reside in the model node at time t + 1. Recall from Section 2
that an agent"s intentional model includes its belief. Because the
agents act and receive observations, their models are updated to
reflect their changed beliefs. Since the set of optimal actions for
a model could include all the actions, and the agent may receive
any one of |Ωj| possible observations, the updated set at time step
t + 1 will have at most |Mt
j,l−1||Aj||Ωj| models. Here, |Mt
j,l−1|
is the number of models at time step t, |Aj| and |Ωj| are the largest
spaces of actions and observations respectively, among all the 
models. Second, we compute the new distribution over the updated
models given the original distribution and the probability of the
agent performing the action and receiving the observation that led
to the updated model. These steps are a part of agent i"s belief
update formalized using Eq. 1.
In Fig. 3(b), we show how the dotted model update link is 
implemented in the I-DID. If each of the two level l − 1 models 
ascribed to j at time step t results in one action, and j could make
one of two possible observations, then the model node at time step
t + 1 contains four updated models (mt+1,1
j,l−1 ,mt+1,2
j,l−1 , mt+1,3
j,l−1 , and
mt+1,4
j,l−1 ). These models differ in their initial beliefs, each of which
is the result of j updating its beliefs due to its action and a possible
observation. The decision nodes in each of the I-DIDs or DIDs that
represent the lower level models are mapped to the corresponding
Figure 4: Transformed I-DID with the model nodes and model update
link replaced with the chance nodes and the relationships (in bold).
chance nodes, as mentioned previously. Next, we describe how the
distribution over the updated set of models (the distribution over the
chance node Mod[Mt+1
j ] in Mt+1
j,l−1) is computed. The probability
that j"s updated model is, say mt+1,1
j,l−1 , depends on the probability
of j performing the action and receiving the observation that led to
this model, and the prior distribution over the models at time step
t. Because the chance node At
j assumes the distribution of each
of the action nodes based on the value of Mod[Mt
j ], the 
probability of the action is given by this chance node. In order to obtain
the probability of j"s possible observation, we introduce the chance
node Oj, which depending on the value of Mod[Mt
j ] assumes the
distribution of the observation node in the lower level model 
denoted by Mod[Mt
j ]. Because the probability of j"s observations
depends on the physical state and the joint actions of both agents,
the node Oj is linked with St+1
, At
j, and At
i. 2
Analogous to At
j,
the conditional probability table of Oj is also a multiplexer 
modulated by Mod[Mt
j ]. Finally, the distribution over the prior models
at time t is obtained from the chance node, Mod[Mt
j ] in Mt
j,l−1.
Consequently, the chance nodes, Mod[Mt
j ], At
j, and Oj, form the
parents of Mod[Mt+1
j ] in Mt+1
j,l−1. Notice that the model update
link may be replaced by the dependency links between the chance
nodes that constitute the model nodes in the two time slices. In
Fig. 4 we show the two time-slice I-DID with the model nodes 
replaced by the chance nodes and the relationships between them.
Chance nodes and dependency links that not in bold are standard,
usually found in DIDs.
Expansion of the I-DID over more time steps requires the 
repetition of the two steps of updating the set of models that form the
2
Note that Oj represents j"s observation at time t + 1.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 817
values of the model node and adding the relationships between the
chance nodes, as many times as there are model update links. We
note that the possible set of models of the other agent j grows 
exponentially with the number of time steps. For example, after T steps,
there may be at most |Mt=1
j,l−1|(|Aj||Ωj|)T −1
candidate models 
residing in the model node.
4.2 Solution
Analogous to I-IDs, the solution to a level l I-DID for agent i
expanded over T time steps may be carried out recursively. For the
purpose of illustration, let l=1 and T=2. The solution method uses
the standard look-ahead technique, projecting the agent"s action
and observation sequences forward from the current belief state [17],
and finding the possible beliefs that i could have in the next time
step. Because agent i has a belief over j"s models as well, the 
lookahead includes finding out the possible models that j could have in
the future. Consequently, each of j"s subintentional or level 0 
models (represented using a standard DID) in the first time step must be
solved to obtain its optimal set of actions. These actions are 
combined with the set of possible observations that j could make in that
model, resulting in an updated set of candidate models (that include
the updated beliefs) that could describe the behavior of j. Beliefs
over this updated set of candidate models are calculated using the
standard inference methods using the dependency relationships 
between the model nodes as shown in Fig. 3(b). We note the recursive
nature of this solution: in solving agent i"s level 1 I-DID, j"s level 0
DIDs must be solved. If the nesting of models is deeper, all models
at all levels starting from 0 are solved in a bottom-up manner.
We briefly outline the recursive algorithm for solving agent i"s
Algorithm for solving I-DID
Input : level l ≥ 1 I-ID or level 0 ID, T
Expansion Phase
1. For t from 1 to T − 1 do
2. If l ≥ 1 then
Populate Mt+1
j,l−1
3. For each mt
j in Range(Mt
j,l−1) do
4. Recursively call algorithm with the l − 1 I-ID (or ID)
that represents mt
j and the horizon, T − t + 1
5. Map the decision node of the solved I-ID (or ID),
OPT(mt
j), to a chance node Aj
6. For each aj in OPT(mt
j) do
7. For each oj in Oj (part of mt
j) do
8. Update j"s belief, bt+1
j ← SE(bt
j, aj, oj)
9. mt+1
j ← New I-ID (or ID) with bt+1
j as the
initial belief
10. Range(Mt+1
j,l−1)
∪
← {mt+1
j }
11. Add the model node, Mt+1
j,l−1, and the dependency links
between Mt
j,l−1 and Mt+1
j,l−1 (shown in Fig. 3(b))
12. Add the chance, decision, and utility nodes for t + 1 time
slice and the dependency links between them
13. Establish the CPTs for each chance node and utility node
Look-Ahead Phase
14. Apply the standard look-ahead and backup method to solve
the expanded I-DID
Figure 5: Algorithm for solving a level l ≥ 0 I-DID.
level l I-DID expanded over T time steps with one other agent j in
Fig. 5. We adopt a two-phase approach: Given an I-ID of level l
(described previously in Section 3) with all lower level models also
represented as I-IDs or IDs (if level 0), the first step is to expand
the level l I-ID over T time steps adding the dependency links and
the conditional probability tables for each node. We particularly
focus on establishing and populating the model nodes (lines 3-11).
Note that Range(·) returns the values (lower level models) of the
random variable given as input (model node). In the second phase,
we use a standard look-ahead technique projecting the action and
observation sequences over T time steps in the future, and backing
up the utility values of the reachable beliefs. Similar to I-IDs, the
I-DIDs reduce to DIDs in the absence of other agents.
As we mentioned previously, the 0-th level models are the 
traditional DIDs. Their solutions provide probability distributions over
actions of the agent modeled at that level to I-DIDs at level 1. Given
probability distributions over other agent"s actions the level 1 
IDIDs can themselves be solved as DIDs, and provide probability
distributions to yet higher level models. Assume that the number
of models considered at each level is bound by a number, M. 
Solving an I-DID of level l in then equivalent to solving O(Ml
) DIDs.
5. EXAMPLE APPLICATIONS
To illustrate the usefulness of I-DIDs, we apply them to three
problem domains. We describe, in particular, the formulation of
the I-DID and the optimal prescriptions obtained on solving it.
5.1 Followership-Leadership in the Multiagent
Tiger Problem
We begin our illustrations of using I-IDs and I-DIDs with a slightly
modified version of the multiagent tiger problem discussed in [9].
The problem has two agents, each of which can open the right door
(OR), the left door (OL) or listen (L). In addition to hearing growls
(from the left (GL) or from the right (GR)) when they listen, the
agents also hear creaks (from the left (CL), from the right (CR), or
no creaks (S)), which noisily indicate the other agent"s opening one
of the doors. When any door is opened, the tiger persists in its 
original location with a probability of 95%. Agent i hears growls with
a reliability of 65% and creaks with a reliability of 95%. Agent j,
on the other hand, hears growls with a reliability of 95%. Thus,
the setting is such that agent i hears agent j opening doors more
reliably than the tiger"s growls. This suggests that i could use j"s
actions as an indication of the location of the tiger, as we discuss
below. Each agent"s preferences are as in the single agent game
discussed in [13]. The transition, observation, and reward 
functions are shown in [16].
A good indicator of the usefulness of normative methods for
decision-making like I-DIDs is the emergence of realistic social
behaviors in their prescriptions. In settings of the persistent 
multiagent tiger problem that reflect real world situations, we demonstrate
followership between the agents and, as shown in [15], deception
among agents who believe that they are in a follower-leader type
of relationship. In particular, we analyze the situational and 
epistemological conditions sufficient for their emergence. The 
followership behavior, for example, results from the agent knowing its own
weaknesses, assessing the strengths, preferences, and possible 
behaviors of the other, and realizing that its best for it to follow the
other"s actions in order to maximize its payoffs.
Let us consider a particular setting of the tiger problem in which
agent i believes that j"s preferences are aligned with its own - both
of them just want to get the gold - and j"s hearing is more reliable
in comparison to itself. As an example, suppose that j, on listening
can discern the tiger"s location 95% of the times compared to i"s
65% accuracy. Additionally, agent i does not have any initial 
information about the tiger"s location. In other words, i"s single-level
nested belief, bi,1, assigns 0.5 to each of the two locations of the
tiger. In addition, i considers two models of j, which differ in j"s
flat level 0 initial beliefs. This is represented in the level 1 I-ID
shown in Fig. 6(a). According to one model, j assigns a 
probability of 0.9 that the tiger is behind the left door, while the other
818 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 6: (a) Level 1 I-ID of agent i, (b) two level 0 IDs of agent j
whose decision nodes are mapped to the chance nodes, A1
j , A2
j , in (a).
model assigns 0.1 to that location (see Fig. 6(b)). Agent i is 
undecided on these two models of j. If we vary i"s hearing ability,
and solve the corresponding level 1 I-ID expanded over three time
steps, we obtain the normative behavioral policies shown in Fig 7
that exhibit followership behavior. If i"s probability of correctly
hearing the growls is 0.65, then as shown in the policy in Fig. 7(a),
i begins to conditionally follow j"s actions: i opens the same door
that j opened previously iff i"s own assessment of the tiger"s 
location confirms j"s pick. If i loses the ability to correctly interpret
the growls completely, it blindly follows j and opens the same door
that j opened previously (Fig. 7(b)).
Figure 7: Emergence of (a) conditional followership, and (b) blind
followership in the tiger problem. Behaviors of interest are in bold. * is
a wildcard, and denotes any one of the observations.
We observed that a single level of belief nesting - beliefs about
the other"s models - was sufficient for followership to emerge in the
tiger problem. However, the epistemological requirements for the
emergence of leadership are more complex. For an agent, say j, to
emerge as a leader, followership must first emerge in the other agent
i. As we mentioned previously, if i is certain that its preferences
are identical to those of j, and believes that j has a better sense
of hearing, i will follow j"s actions over time. Agent j emerges
as a leader if it believes that i will follow it, which implies that
j"s belief must be nested two levels deep to enable it to recognize
its leadership role. Realizing that i will follow presents j with an
opportunity to influence i"s actions in the benefit of the collective
good or its self-interest alone. For example, in the tiger problem,
let us consider a setting in which if both i and j open the correct
door, then each gets a payoff of 20 that is double the original. If
j alone selects the correct door, it gets the payoff of 10. On the
other hand, if both agents pick the wrong door, their penalties are
cut in half. In this setting, it is in both j"s best interest as well as the
collective betterment for j to use its expertise in selecting the 
correct door, and thus be a good leader. However, consider a slightly
different problem in which j gains from i"s loss and is penalized
if i gains. Specifically, let i"s payoff be subtracted from j"s, 
indicating that j is antagonistic toward i - if j picks the correct door
and i the wrong one, then i"s loss of 100 becomes j"s gain. Agent
j believes that i incorrectly thinks that j"s preferences are those
that promote the collective good and that it starts off by believing
with 99% confidence where the tiger is. Because i believes that its
preferences are similar to those of j, and that j starts by believing
almost surely that one of the two is the correct location (two level
0 models of j), i will start by following j"s actions. We show i"s
normative policy on solving its singly-nested I-DID over three time
steps in Fig. 8(a). The policy demonstrates that i will blindly 
follow j"s actions. Since the tiger persists in its original location with
a probability of 0.95, i will select the same door again. If j begins
the game with a 99% probability that the tiger is on the right, 
solving j"s I-DID nested two levels deep, results in the policy shown in
Fig. 8(b). Even though j is almost certain that OL is the correct
action, it will start by selecting OR, followed by OL. Agent j"s 
intention is to deceive i who, it believes, will follow j"s actions, so
as to gain $110 in the second time step, which is more than what j
would gain if it were to be honest.
Figure 8: Emergence of deception between agents in the tiger 
problem. Behaviors of interest are in bold. * denotes as before. (a) Agent
i"s policy demonstrating that it will blindly follow j"s actions. (b) Even
though j is almost certain that the tiger is on the right, it will start by
selecting OR, followed by OL, in order to deceive i.
5.2 Altruism and Reciprocity in the Public
Good Problem
The public good (PG) problem [7], consists of a group of M
agents, each of whom must either contribute some resource to a
public pot or keep it for themselves. Since resources contributed to
the public pot are shared among all the agents, they are less 
valuable to the agent when in the public pot. However, if all agents
choose to contribute their resources, then the payoff to each agent
is more than if no one contributes. Since an agent gets its share of
the public pot irrespective of whether it has contributed or not, the
dominating action is for each agent to not contribute, and instead
free ride on others" contributions. However, behaviors of human
players in empirical simulations of the PG problem differ from the
normative predictions. The experiments reveal that many players
initially contribute a large amount to the public pot, and continue
to contribute when the PG problem is played repeatedly, though
in decreasing amounts [4]. Many of these experiments [5] report
that a small core group of players persistently contributes to the
public pot even when all others are defecting. These experiments
also reveal that players who persistently contribute have altruistic
or reciprocal preferences matching expected cooperation of others.
For simplicity, we assume that the game is played between M =
2 agents, i and j. Let each agent be initially endowed with XT
amount of resources. While the classical PG game formulation 
permits each agent to contribute any quantity of resources (≤ XT ) to
the public pot, we simplify the action space by allowing two 
possible actions. Each agent may choose to either contribute (C) a fixed
amount of the resources, or not contribute. The latter action is 
deThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 819
noted as defect (D). We assume that the actions are not observable
to others. The value of resources in the public pot is discounted
by ci for each agent i, where ci is the marginal private return. We
assume that ci < 1 so that the agent does not benefit enough that
it contributes to the public pot for private gain. Simultaneously,
ciM > 1, making collective contribution pareto optimal.
i/j C D
C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P
D XT + ciXT − P, cjXT − cp XT , XT
Table 1: The one-shot PG game with punishment.
In order to encourage contributions, the contributing agents 
punish free riders but incur a small cost for administering the 
punishment. Let P be the punishment meted out to the defecting agent
and cp the non-zero cost of punishing for the contributing agent.
For simplicity, we assume that the cost of punishing is same for
both the agents. The one-shot PG game with punishment is shown
in Table. 1. Let ci = cj, cp > 0, and if P > XT − ciXT , then 
defection is no longer a dominating action. If P < XT − ciXT , then
defection is the dominating action for both. If P = XT − ciXT ,
then the game is not dominance-solvable.
Figure 9: (a) Level 1 I-ID of agent i, (b) level 0 IDs of agent j with
decision nodes mapped to the chance nodes, A1
j and A2
j , in (a).
We formulate a sequential version of the PG problem with 
punishment from the perspective of agent i. Though in the repeated PG
game, the quantity in the public pot is revealed to all the agents after
each round of actions, we assume in our formulation that it is 
hidden from the agents. Each agent may contribute a fixed amount, xc,
or defect. An agent on performing an action receives an observation
of plenty (PY) or meager (MR) symbolizing the state of the 
public pot. Notice that the observations are also indirectly indicative of
agent j"s actions because the state of the public pot is influenced by
them. The amount of resources in agent i"s private pot, is perfectly
observable to i. The payoffs are analogous to Table. 1. 
Borrowing from the empirical investigations of the PG problem [5], we
construct level 0 IDs for j that model altruistic and non-altruistic
types (Fig. 9(b)). Specifically, our altruistic agent has a high 
marginal private return (cj is close to 1) and does not punish others
who defect. Let xc = 1 and the level 0 agent be punished half the
times it defects. With one action remaining, both types of agents
choose to contribute to avoid being punished. With two actions
to go, the altruistic type chooses to contribute, while the other 
defects. This is because cj for the altruistic type is close to 1, thus the
expected punishment, 0.5P > (1 − cj), which the altruistic type
avoids. Because cj for the non-altruistic type is less, it prefers not
to contribute. With three steps to go, the altruistic agent contributes
to avoid punishment (0.5P > 2(1 − cj)), and the non-altruistic
type defects. For greater than three steps, while the altruistic agent
continues to contribute to the public pot depending on how close
its marginal private return is to 1, the non-altruistic type prescribes
defection.
We analyzed the decisions of an altruistic agent i modeled using
a level 1 I-DID expanded over 3 time steps. i ascribes the two level
0 models, mentioned previously, to j (see Fig. 9). If i believes with
a probability 1 that j is altruistic, i chooses to contribute for each of
the three steps. This behavior persists when i is unaware of whether
j is altruistic (Fig. 10(a)), and when i assigns a high probability to
j being the non-altruistic type. However, when i believes with a
probability 1 that j is non-altruistic and will thus surely defect, i
chooses to defect to avoid being punished and because its marginal
private return is less than 1. These results demonstrate that the 
behavior of our altruistic type resembles that found experimentally.
The non-altruistic level 1 agent chooses to defect regardless of how
likely it believes the other agent to be altruistic. We analyzed the
behavior of a reciprocal agent type that matches expected 
cooperation or defection. The reciprocal type"s marginal private return
is similar to that of the non-altruistic type, however, it obtains a
greater payoff when its action is similar to that of the other. We
consider the case when the reciprocal agent i is unsure of whether
j is altruistic and believes that the public pot is likely to be half
full. For this prior belief, i chooses to defect. On receiving an 
observation of plenty, i decides to contribute, while an observation of
meager makes it defect (Fig. 10(b)). This is because an 
observation of plenty signals that the pot is likely to be greater than half
full, which results from j"s action to contribute. Thus, among the
two models ascribed to j, its type is likely to be altruistic making
it likely that j will contribute again in the next time step. Agent i
therefore chooses to contribute to reciprocate j"s action. An 
analogous reasoning leads i to defect when it observes a meager pot.
With one action to go, i believing that j contributes, will choose to
contribute too to avoid punishment regardless of its observations.
Figure 10: (a) An altruistic level 1 agent always contributes. (b) A
reciprocal agent i starts off by defecting followed by choosing to 
contribute or defect based on its observation of plenty (indicating that j is
likely altruistic) or meager (j is non-altruistic).
5.3 Strategies in Two-Player Poker
Poker is a popular zero sum card game that has received much 
attention among the AI research community as a testbed [2]. Poker is
played among M ≥ 2 players in which each player receives a hand
of cards from a deck. While several flavors of Poker with 
varying complexity exist, we consider a simple version in which each
player has three plys during which the player may either exchange
a card (E), keep the existing hand (K), fold (F) and withdraw from
the game, or call (C), requiring all players to show their hands. To
keep matters simple, let M = 2, and each player receive a hand
consisting of a single card drawn from the same suit. Thus, during
a showdown, the player who has the numerically larger card (2 is
the lowest, ace is the highest) wins the pot. During an exchange of
cards, the discarded card is placed either in the L pile, indicating to
the other agent that it was a low numbered card less than 8, or in the
820 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
H pile, indicating that the card had a rank greater than or equal to
8. Notice that, for example, if a lower numbered card is discarded,
the probability of receiving a low card in exchange is now reduced.
We show the level 1 I-ID for the simplified two-player Poker in
Fig. 11. We considered two models (personality types) of agent j.
The conservative type believes that it is likely that its opponent has
a high numbered card in its hand. On the other hand, the 
aggressive agent j believes with a high probability that its opponent has
a lower numbered card. Thus, the two types differ in their beliefs
over their opponent"s hand. In both these level 0 models, the 
opponent is assumed to perform its actions following a fixed, uniform
distribution. With three actions to go, regardless of its hand 
(unless it is an ace), the aggressive agent chooses to exchange its card,
with the intent of improving on its current hand. This is because it
believes the other to have a low card, which improves its chances
of getting a high card during the exchange. The conservative agent
chooses to keep its card, no matter its hand because its chances of
getting a high card are slim as it believes that its opponent has one.
Figure 11: (a) Level 1 I-ID of agent i. The observation reveals 
information about j"s hand of the previous time step, (b) level 0 IDs of agent
j whose decision nodes are mapped to the chance nodes, A1
j , A2
j , in (a).
The policy of a level 1 agent i who believes that each card 
except its own has an equal likelihood of being in j"s hand (neutral
personality type) and j could be either an aggressive or 
conservative type, is shown in Fig. 12. i"s own hand contains the card
numbered 8. The agent starts by keeping its card. On seeing that
j did not exchange a card (N), i believes with probability 1 that j
is conservative and hence will keep its cards. i responds by either
keeping its card or exchanging it because j is equally likely to have
a lower or higher card. If i observes that j discarded its card into
the L or H pile, i believes that j is aggressive. On observing L,
i realizes that j had a low card, and is likely to have a high card
after its exchange. Because the probability of receiving a low card
is high now, i chooses to keep its card. On observing H, 
believing that the probability of receiving a high numbered card is high,
i chooses to exchange its card. In the final step, i chooses to call
regardless of its observation history because its belief that j has a
higher card is not sufficiently high to conclude that its better to fold
and relinquish the payoff. This is partly due to the fact that an 
observation of, say, L resets the agent i"s previous time step beliefs
over j"s hand to the low numbered cards only.
6. DISCUSSION
We showed how DIDs may be extended to I-DIDs that enable
online sequential decision-making in uncertain multiagent settings.
Our graphical representation of I-DIDs improves on the previous
Figure 12: A level 1 agent i"s three step policy in the Poker problem.
i starts by believing that j is equally likely to be aggressive or 
conservative and could have any card in its hand with equal probability.
work significantly by being more transparent, semantically clear,
and capable of being solved using standard algorithms that target
DIDs. I-DIDs extend NIDs to allow sequential decision-making
over multiple time steps in the presence of other interacting agents.
I-DIDs may be seen as concise graphical representations for 
IPOMDPs providing a way to exploit problem structure and carry
out online decision-making as the agent acts and observes given its
prior beliefs. We are currently investigating ways to solve I-DIDs
approximately with provable bounds on the solution quality.
Acknowledgment: We thank Piotr Gmytrasiewicz for some
useful discussions related to this work. The first author would like
to acknowledge the support of a UGARF grant.
7. REFERENCES
[1] R. J. Aumann. Interactive epistemology i: Knowledge. International
Journal of Game Theory, 28:263-300, 1999.
[2] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron. The challenge
of poker. AIJ, 2001.
[3] A. Brandenburger and E. Dekel. Hierarchies of beliefs and common
knowledge. Journal of Economic Theory, 59:189-198, 1993.
[4] C. Camerer. Behavioral Game Theory: Experiments in Strategic
Interaction. Princeton University Press, 2003.
[5] E. Fehr and S. Gachter. Cooperation and punishment in public goods
experiments. American Economic Review, 90(4):980-994, 2000.
[6] D. Fudenberg and D. K. Levine. The Theory of Learning in Games.
MIT Press, 1998.
[7] D. Fudenberg and J. Tirole. Game Theory. MIT Press, 1991.
[8] Y. Gal and A. Pfeffer. A language for modeling agent"s
decision-making processes in games. In AAMAS, 2003.
[9] P. Gmytrasiewicz and P. Doshi. A framework for sequential planning
in multiagent settings. JAIR, 24:49-79, 2005.
[10] P. Gmytrasiewicz and E. Durfee. Rational coordination in
multi-agent environments. JAAMAS, 3(4):319-350, 2000.
[11] J. C. Harsanyi. Games with incomplete information played by
bayesian players. Management Science, 14(3):159-182, 1967.
[12] R. A. Howard and J. E. Matheson. Influence diagrams. In R. A.
Howard and J. E. Matheson, editors, The Principles and Applications
of Decision Analysis. Strategic Decisions Group, Menlo Park, CA
94025, 1984.
[13] L. Kaelbling, M. Littman, and A. Cassandra. Planning and acting in
partially observable stochastic domains. Artificial Intelligence
Journal, 2, 1998.
[14] D. Koller and B. Milch. Multi-agent influence diagrams for
representing and solving games. In IJCAI, pages 1027-1034, 2001.
[15] K. Polich and P. Gmytrasiewicz. Interactive dynamic influence
diagrams. In GTDT Workshop, AAMAS, 2006.
[16] B. Rathnas., P. Doshi, and P. J. Gmytrasiewicz. Exact solutions to
interactive pomdps using behavioral equivalence. In Autonomous
Agents and Multi-Agent Systems Conference (AAMAS), 2006.
[17] S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach
(Second Edition). Prentice Hall, 2003.
[18] R. D. Shachter. Evaluating influence diagrams. Operations Research,
34(6):871-882, 1986.
[19] D. Suryadi and P. Gmytrasiewicz. Learning models of other agents
using influence diagrams. In UM, 1999.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 821
A Formal Model for Situated Semantic Alignment
Manuel Atencia Marco Schorlemmer
IIIA, Artificial Intelligence Research Institute
CSIC, Spanish National Research Council
Bellaterra (Barcelona), Catalonia, Spain
{manu, marco}@iiia.csic.es
ABSTRACT
Ontology matching is currently a key technology to achieve
the semantic alignment of ontological entities used by
knowledge-based applications, and therefore to enable their
interoperability in distributed environments such as 
multiagent systems. Most ontology matching mechanisms, 
however, assume matching prior integration and rely on 
semantics that has been coded a priori in concept hierarchies or 
external sources. In this paper, we present a formal model for
a semantic alignment procedure that incrementally aligns
differing conceptualisations of two or more agents relative
to their respective perception of the environment or domain
they are acting in. It hence makes the situation in which
the alignment occurs explicit in the model. We resort to
Channel Theory to carry out the formalisation.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
Intelligence-coherence and coordination, multiagent systems;
D.2.12 [Software Engineering]: Interoperability-data
mapping; I.2.4 [Artificial Intelligence]: Knowledge 
Representation Formalisms and Methods-semantic networks,
relation systems.
General Terms
Theory
1. INTRODUCTION
An ontology is commonly defined as a specification of the
conceptualisation of a particular domain. It fixes the 
vocabulary used by knowledge engineers to denote concepts and
their relations, and it constrains the interpretation of this
vocabulary to the meaning originally intended by knowledge
engineers. As such, ontologies have been widely adopted as
a key technology that may favour knowledge sharing in 
distributed environments, such as multi-agent systems, 
federated databases, or the Semantic Web. But the proliferation
of many diverse ontologies caused by different 
conceptualisations of even the same domain -and their subsequent
specification using varying terminology- has highlighted
the need of ontology matching techniques that are 
capable of computing semantic relationships between entities of
separately engineered ontologies. [5, 11]
Until recently, most ontology matching mechanisms 
developed so far have taken a classical functional approach
to the semantic heterogeneity problem, in which ontology
matching is seen as a process taking two or more 
ontologies as input and producing a semantic alignment of 
ontological entities as output [3]. Furthermore, matching 
often has been carried out at design-time, before 
integrating knowledge-based systems or making them interoperate.
This might have been successful for clearly delimited and
stable domains and for closed distributed systems, but it is
untenable and even undesirable for the kind of applications
that are currently deployed in open systems. Multi-agent
communication, peer-to-peer information sharing, and 
webservice composition are all of a decentralised, dynamic, and
open-ended nature, and they require ontology matching to
be locally performed during run-time. In addition, in many
situations peer ontologies are not even open for inspection
(e.g., when they are based on commercially confidential 
information).
Certainly, there exist efforts to efficiently match 
ontological entities at run-time, taking only those ontology 
fragment that are necessary for the task at hand [10, 13, 9, 8].
Nevertheless, the techniques used by these systems to 
establish the semantic relationships between ontological entities
-even though applied at run-time- still exploit a priori
defined concept taxonomies as they are represented in the
graph-based structures of the ontologies to be matched, use
previously existing external sources such as thesauri (e.g.,
WordNet) and upper-level ontologies (e.g., CyC or SUMO),
or resort to additional background knowledge repositories or
shared instances.
We claim that semantic alignment of ontological 
terminology is ultimately relative to the particular situation in which
the alignment is carried out, and that this situation should
be made explicit and brought into the alignment 
mechanism. Even two agents with identical conceptualisation 
capabilities, and using exactly the same vocabulary to specify
their respective conceptualisations may fail to interoperate
1278
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
in a concrete situation because of their differing perception
of the domain. Imagine a situation in which two agents
are facing each other in front of a checker board. Agent
A1 may conceptualise a figure on the board as situated on
the left margin of the board, while agent A2 may 
conceptualise the same figure as situated on the right. Although
the conceptualisation of ‘left" and ‘right" is done in exactly
the same manner by both agents, and even if both use the
terms left and right in their communication, they still will
need to align their respective vocabularies if they want to
successfully communicate to each other actions that change
the position of figures on the checker board. Their semantic
alignment, however, will only be valid in the scope of their
interaction within this particular situation or environment.
The same agents situated differently may produce a different
alignment.
This scenario is reminiscent to those in which a group of
distributed agents adapt to form an ontology and a shared
lexicon in an emergent, bottom-up manner, with only local
interactions and no central control authority [12]. This sort
of self-organised emergence of shared meaning is namely 
ultimately grounded on the physical interaction of agents with
the environment. In this paper, however, we address the
case in which agents are already endowed with a top-down
engineered ontology (it can even be the same one), which
they do not adapt or refine, but for which they want to
find the semantic relationships with separate ontologies of
other agents on the grounds of their communication within a
specific situation. In particular, we provide a formal model
that formalises situated semantic alignment as a sequence of
information-channel refinements in the sense of Barwise and
Seligman"s theory of information flow [1]. This theory is 
particularly useful for our endeavour because it models the flow
of information occurring in distributed systems due to the
particular situations -or tokens- that carry information.
Analogously, the semantic alignment that will allow 
information to flow ultimately will be carried by the particular
situation agents are acting in.
We shall therefore consider a scenario with two or more
agents situated in an environment. Each agent will have its
own viewpoint of the environment so that, if the 
environment is in a concrete state, both agents may have different
perceptions of this state. Because of these differences there
may be a mismatch in the meaning of the syntactic 
entities by which agents describe their perceptions (and which
constitute the agents" respective ontologies). We state that
these syntactic entities can be related according to the 
intrinsic semantics provided by the existing relationship 
between the agents" viewpoint of the environment. The 
existence of this relationship is precisely justified by the fact that
the agents are situated and observe the same environment.
In Section 2 we describe our formal model for Situated
Semantic Alignment (SSA). First, in Section 2.1 we associate
a channel to the scenario under consideration and show how
the distributed logic generated by this channel provides the
logical relationships between the agents" viewpoints of the
environment. Second, in Section 2.2 we present a method by
which agents obtain approximations of this distributed logic.
These approximations gradually become more reliable as the
method is applied. In Section 3 we report on an application
of our method. Conclusions and further work are analyzed
in Section 4. Finally, an appendix summarizes the terms and
theorems of Channel theory used along the paper. We do not
assume any knowledge of Channel Theory; we restate basic
definitions and theorems in the appendix, but any detailed
exposition of the theory is outside the scope of this paper.
2. A FORMAL MODEL FOR SSA
2.1 The Logic of SSA
Consider a scenario with two agents A1 and A2 situated
in an environment E (the generalization to any numerable
set of agents is straightforward). We associate a numerable
set S of states to E and, at any given instant, we suppose
E to be in one of these states. We further assume that
each agent is able to observe the environment and has its
own perception of it. This ability is faithfully captured by
a surjective function seei : S → Pi, where i ∈ {1, 2}, and
typically see1 and see2 are different.
According to Channel Theory, information is only viable
where there is a systematic way of classifying some range
of things as being this way or that, in other words, where
there is a classification (see appendix A). So in order to be
within the framework of Channel Theory, we must associate
classifications to the components of our system.
For each i ∈ {1, 2}, we consider a classification Ai that
models Ai"s viewpoint of E. First, tok(Ai) is composed of
Ai"s perceptions of E states, that is, tok(Ai) = Pi. Second,
typ(Ai) contains the syntactic entities by which Ai describes
its perceptions, the ones constituting the ontology of Ai.
Finally, |=Ai synthesizes how Ai relates its perceptions with
these syntactic entities.
Now, with the aim of associating environment E with a
classification E we choose the power classification of S as E,
which is the classification whose set of types is equal to 2S
,
whose tokens are the elements of S, and for which a token
e is of type ε if e ∈ ε. The reason for taking the power
classification is because there are no syntactic entities that
may play the role of types for E since, in general, there is no
global conceptualisation of the environment. However, the
set of types of the power classification includes all possible
token configurations potentially described by types. Thus
tok(E) = S, typ(E) = 2S
and e |=E ε if and only if e ∈ ε.
The notion of channel (see appendix A) is fundamental in
Barwise and Seligman"s theory. The information flow among
the components of a distributed system is modelled in terms
of a channel and the relationships among these components
are expressed via infomorphisms (see appendix A) which
provide a way of moving information between them.
The information flow of the scenario under consideration
is accurately described by channel E = {fi : Ai → E}i∈{1,2}
defined as follows:
• ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈
typ(Ai)
• ˇfi(e) = seei(e) for each e ∈ tok(E)
where i ∈ {1, 2}. Definition of ˇfi seems natural while ˆfi is
defined in such a way that the fundamental property of the
infomorphisms is fulfilled:
ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi)
iff e ∈ ˆfi(α) (by definition of ˆfi)
iff e |=E
ˆfi(α) (by definition of |=E)
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279
Consequently, E is the core of channel E and a state
e ∈ tok(E) connects agents" perceptions ˇf1(e) and ˇf2(e) (see
Figure 1).
typ(E)
typ(A1)
ˆf1
99ttttttttt
typ(A2)
ˆf2
eeJJJJJJJJJ
tok(E)
|=E







ˇf1yyttttttttt
ˇf2 %%JJJJJJJJJ
tok(A1)
|=A1







tok(A2)
|=A2







Figure 1: Channel E
E explains the information flow of our scenario by virtue
of agents A1 and A2 being situated and perceiving the same
environment E. We want to obtain meaningful relations
among agents" syntactic entities, that is, agents" types. We
state that meaningfulness must be in accord with E.
The sum operation (see appendix A) gives us a way of
putting the two agents" classifications of channel E together
into a single classification, namely A1 +A2, and also the two
infomorphisms together into a single infomorphism, f1 +f2 :
A1 + A2 → E.
A1 + A2 assembles agents" classifications in a very coarse
way. tok(A1 + A2) is the cartesian product of tok(A1) and
tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a
token of A1 + A2 is a pair of agents" perceptions with no
restrictions. typ(A1 + A2) is the disjoint union of typ(A1)
and typ(A2), and p1, p2 is of type i, α if pi is of type
α. We attach importance to take the disjoint union because
A1 and A2 could use identical types with the purpose of
describing their respective perceptions of E.
Classification A1 + A2 seems to be the natural place in
which to search for relations among agents" types. Now,
Channel Theory provides a way to make all these relations
explicit in a logical fashion by means of theories and local
logics (see appendix A). The theory generated by the sum
classification, Th(A1 + A2), and hence its logic generated,
Log(A1 + A2), involve all those constraints among agents"
types valid according to A1 +A2. Notice however that these
constraints are obvious. As we stated above, meaningfulness
must be in accord with channel E.
Classifications A1 + A2 and E are connected via the sum
infomorphism, f = f1 + f2, where:
• ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for
each i, α ∈ typ(A1 + A2)
• ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈
tok(E)
Meaningful constraints among agents" types are in accord
with channel E because they are computed making use of f
as we expound below.
As important as the notion of channel is the concept of
distributed logic (see appendix A). Given a channel C and
a logic L on its core, DLogC(L) represents the reasoning
about relations among the components of C justified by L.
If L = Log(C), the distributed logic, we denoted by Log(C),
captures in a logical fashion the information flow inherent
in the channel.
In our case, Log(E) explains the relationship between the
agents" viewpoints of the environment in a logical fashion.
On the one hand, constraints of Th(Log(E)) are defined by:
Γ Log(E) Δ if ˆf[Γ] Log(E)
ˆf[Δ] (1)
where Γ, Δ ⊆ typ(A1 + A2). On the other hand, the set of
normal tokens, NLog(E), is equal to the range of function ˇf:
NLog(E) = ˇf[tok(E)]
= { see1(e), see2(e) | e ∈ tok(E)}
Therefore, a normal token is a pair of agents" perceptions
that are restricted by coming from the same environment
state (unlike A1 + A2 tokens).
All constraints of Th(Log(E)) are satisfied by all normal
tokens (because of being a logic). In this particular case, this
condition is also sufficient (the proof is straightforward); as
alternative to (1) we have:
Γ Log(E) Δ iff for all e ∈ tok(E),
if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ]
then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2)
where Γ, Δ ⊆ typ(A1 + A2).
Log(E) is the logic of SSA. Th(Log(E)) comprises the
most meaningful constraints among agents" types in accord
with channel E. In other words, the logic of SSA contains
and also justifies the most meaningful relations among those
syntactic entities that agents use in order to describe their
own environment perceptions.
Log(E) is complete since Log(E) is complete but it is not
necessarily sound because although Log(E) is sound, ˇf is
not surjective in general (see appendix B). If Log(E) is also
sound then Log(E) = Log(A1 +A2) (see appendix B). That
means there is no significant relation between agents" points
of view of the environment according to E. It is just the fact
that Log(E) is unsound what allows a significant relation
between the agents" viewpoints. This relation is expressed
at the type level in terms of constraints by Th(Log(E)) and
at the token level by NLog(E).
2.2 Approaching the logic of SSA
through communication
We have dubbed Log(E) the logic of SSA. Th(Log(E))
comprehends the most meaningful constraints among agents"
types according to E. The problem is that neither agent
can make use of this theory because they do not know E
completely. In this section, we present a method by which
agents obtain approximations to Th(Log(E)). We also prove
these approximations gradually become more reliable as the
method is applied.
Agents can obtain approximations to Th(Log(E)) through
communication. A1 and A2 communicate by exchanging
information about their perceptions of environment states.
This information is expressed in terms of their own 
classification relations. Specifically, if E is in a concrete state e,
we assume that agents can convey to each other which types
are satisfied by their respective perceptions of e and which
are not. This exchange generates a channel C = {fi : Ai →
1280 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
C}i∈{1,2} and Th(Log(C)) contains the constraints among
agents" types justified by the fact that agents have observed
e. Now, if E turns to another state e and agents proceed
as before, another channel C = {fi : Ai → C }i∈{1,2} gives
account of the new situation considering also the previous
information. Th(Log(C )) comprises the constraints among
agents" types justified by the fact that agents have observed
e and e . The significant point is that C is a refinement of
C (see appendix A). Theorem 2.1 below ensures that the
refined channel involves more reliable information.
The communication supposedly ends when agents have
observed all the environment states. Again this situation can
be modeled by a channel, call it C∗
= {f∗
i : Ai → C∗
}i∈{1,2}.
Theorem 2.2 states that Th(Log(C∗
)) = Th(Log(E)).
Theorem 2.1 and Theorem 2.2 assure that applying the
method agents can obtain approximations to Th(Log(E))
gradually more reliable.
Theorem 2.1. Let C = {fi : Ai → C}i∈{1,2} and C =
{fi : Ai → C }i∈{1,2} be two channels. If C is a refinement
of C then:
1. Th(Log(C )) ⊆ Th(Log(C))
2. NLog(C ) ⊇ NLog(C)
Proof. Since C is a refinement of C then there exists a
refinement infomorphism r from C to C; so fi = r ◦ fi . Let
A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2.
1. Let Γ and Δ be subsets of typ(A) and assume that
Γ Log(C ) Δ, which means ˆf [Γ] C
ˆf [Δ]. We have
to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C
ˆf[Δ].
We proceed by reductio ad absurdum. Suppose c ∈
tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] . Then
c |=C
ˆf(γ) for all γ ∈ Γ and c |=C
ˆf(δ) for all δ ∈ Δ.
Let us choose an arbitrary γ ∈ Γ. We have that
γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}. Thus
ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).
Therefore:
c |=C
ˆf(γ) iff c |=C ˆr( ˆfi (α))
iff ˇr(c) |=C
ˆfi (α)
iff ˇr(c) |=C
ˆf ( i, α )
iff ˇr(c) |=C
ˆf (γ)
Consequently, ˇr(c) |=C
ˆf (γ) for all γ ∈ Γ. Since
ˆf [Γ] C
ˆf [Δ] then there exists δ∗
∈ Δ such that
ˇr(c) |=C
ˆf (δ∗
). A sequence of equivalences similar to
the above one justifies c |=C
ˆf(δ∗
), contradicting that c
is a counterexample to ˆf[Γ], ˆf[Δ] . Hence Γ Log(C) Δ
as we wanted to prove.
2. Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).
Therefore, there exists c token in C such that a1, a2 =
ˇf(c). Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)),
for i ∈ {1, 2}. Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈
NLog(C ). Consequently, NLog(C ) ⊇ NLog(C) which
concludes the proof.
Remark 2.1. Theorem 2.1 asserts that the more refined
channel gives more reliable information. Even though its
theory has less constraints, it has more normal tokens to
which they apply.
In the remainder of the section, we explicitly describe the
process of communication and we conclude with the proof
of Theorem 2.2.
Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S
is infinite numerable, though the finite case can be treated
in a similar form. We also choose an infinite numerable set
of symbols {cn
| n ∈ N}1
.
We omit informorphisms superscripts when no confusion
arises. Types are usually denoted by greek letters and tokens
by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and
f(a) ≡ ˇf(a).
Agents communication starts from the observation of E.
Let us suppose that E is in state e1
∈ S = tok(E). A1"s
perception of e1
is f1(e1
) and A2"s perception of e1
is f2(e1
).
We take for granted that A1 can communicate A2 those
types that are and are not satisfied by f1(e1
) according to
its classification A1. So can A2 do. Since both typ(A1) and
typ(A2) are finite, this process eventually finishes. After
this communication a channel C1
= {f1
i : Ai → C1
}i=1,2
arises (see Figure 2).
C1
A1
f1
1
==||||||||
A2
f1
2
aaCCCCCCCC
Figure 2: The first communication stage
On the one hand, C1
is defined by:
• tok(C1
) = {c1
}
• typ(C1
) = typ(A1 + A2)
• c1
|=C1 i, α if fi(e1
) |=Ai α
(for every i, α ∈ typ(A1 + A2))
On the other hand, f1
i , with i ∈ {1, 2}, is defined by:
• f1
i (α) = i, α
(for every α ∈ typ(Ai))
• f1
i (c1
) = fi(e1
)
Log(C1
) represents the reasoning about the first stage of
communication. It is easy to prove that Th(Log(C1
)) =
Th(C1
). The significant point is that both agents know C1
as the result of the communication. Hence they can compute
separately theory Th(C1
) = typ(C1
), C1 which contains
the constraints among agents" types justified by the fact that
agents have observed e1
.
Now, let us assume that E turns to a new state e2
. Agents
can proceed as before, exchanging this time information
about their perceptions of e2
. Another channel C2
= {f2
i :
Ai → C2
}i∈{1,2} comes up. We define C2
so as to take also
into account the information provided by the previous stage
of communication.
On the one hand, C2
is defined by:
• tok(C2
) = {c1
, c2
}
1
We write these symbols with superindices because we limit
the use of subindices for what concerns to agents. Note this
set is chosen with the same cardinality of S.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281
• typ(C2
) = typ(A1 + A2)
• ck
|=C2 i, α if fi(ek
) |=Ai α
(for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2))
On the other hand, f2
i , with i ∈ {1, 2}, is defined by:
• f2
i (α) = i, α
(for every α ∈ typ(Ai))
• f2
i (ck
) = fi(ek
)
(for every k ∈ {1, 2})
Log(C2
) represents the reasoning about the former and
the later communication stages. Th(Log(C2
)) is equal to
Th(C2
) = typ(C2
), C2 , then it contains the constraints
among agents" types justified by the fact that agents have
observed e1
and e2
. A1 and A2 knows C2
so they can use
these constraints. The key point is that channel C2
is a
refinement of C1
. It is easy to check that f1
defined as
the identity function on types and the inclusion function on
tokens is a refinement infomorphism (see at the bottom of
Figure 3). By Theorem 2.1, C2
constraints are more reliable
than C1
constraints.
In the general situation, once the states e1
, e2
, . . . , en−1
(n ≥ 2) have been observed and a new state en
appears,
channel Cn
= {fn
i : Ai → Cn
}i∈{1,2} informs about agents
communication up to that moment. Cn
definition is 
similar to the previous ones and analogous remarks can be
made (see at the top of Figure 3). Theory Th(Log(Cn
)) =
Th(Cn
) = typ(Cn
), Cn contains the constraints among
agents" types justified by the fact that agents have observed
e1
, e2
, . . . , en
.
Cn
fn−1

A1
fn−1
1
99PPPPPPPPPPPPP
fn
1
UUnnnnnnnnnnnnn
f2
1
%%44444444444444444444444444
f1
1
"",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2
fn
2
ggPPPPPPPPPPPPP
fn−1
2
wwnnnnnnnnnnnnn
f2
2
ÕÕ


























f1
2
ØØ
Cn−1

.
.
.

C2
f1

C1
Figure 3: Agents communication
Remember we have assumed that S is infinite numerable.
It is therefore unpractical to let communication finish when
all environment states have been observed by A1 and A2.
At that point, the family of channels {Cn
}n∈N would inform
of all the communication stages. It is therefore up to the
agents to decide when to stop communicating should a good
enough approximation have been reached for the purposes of
their respective tasks. But the study of possible termination
criteria is outside the scope of this paper and left for future
work. From a theoretical point of view, however, we can
consider the channel C∗
= {f∗
i : Ai → C∗
}i∈{1,2} which
informs of the end of the communication after observing all
environment states.
On the one hand, C∗
is defined by:
• tok(C∗
) = {cn
| n ∈ N}
• typ(C∗
) = typ(A1 + A2)
• cn
|=C∗ i, α if fi(en
) |=Ai α
(for n ∈ N and i, α ∈ typ(A1 + A2))
On the other hand, f∗
i , with i ∈ {1, 2}, is defined by:
• f∗
i (α) = i, α
(for α ∈ typ(Ai))
• f∗
i (cn
) = fi(en
)
(for n ∈ N)
Theorem below constitutes the cornerstone of the model
exposed in this paper. It ensures, together with Theorem
2.1, that at each communication stage agents obtain a theory
that approximates more closely to the theory generated by
the logic of SSA.
Theorem 2.2. The following statements hold:
1. For all n ∈ N, C∗
is a refinement of Cn
.
2. Th(Log(E)) = Th(C∗
) = Th(Log(C∗
)).
Proof.
1. It is easy to prove that for each n ∈ N, gn
defined as the
identity function on types and the inclusion function
on tokens is a refinement infomorphism from C∗
to Cn
.
2. The second equality is straightforward; the first one
follows directly from:
cn
|=C∗ i, α iff ˇfi(en
) |=Ai α
(by definition of |=C∗ )
iff en
|=E
ˆfi(α)
(because fi is infomorphim)
iff en
|=E
ˆf( i, α )
(by definition of ˆf)
E
C∗
gn

A1
fn
1
99OOOOOOOOOOOOO
f∗
1
UUooooooooooooo
f1
cc
A2
f∗
2
ggOOOOOOOOOOOOO
fn
2
wwooooooooooooo
f2
?????????????????
Cn
1282 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
3. AN EXAMPLE
In the previous section we have described in great detail
our formal model for SSA. However, we have not tackled
the practical aspect of the model yet. In this section, we
give a brushstroke of the pragmatic view of our approach.
We study a very simple example and explain how agents
can use those approximations of the logic of SSA they can
obtain through communication.
Let us reflect on a system consisting of robots located in
a two-dimensional grid looking for packages with the aim of
moving them to a certain destination (Figure 4). Robots
can carry only one package at a time and they can not move
through a package.
Figure 4: The scenario
Robots have a partial view of the domain and there exist
two kinds of robots according to the visual field they have.
Some robots are capable of observing the eight adjoining
squares but others just observe the three squares they have
in front (see Figure 5). We call them URDL (shortened
form of Up-Right-Down-Left) and LCR (abbreviation for
Left-Center-Right) robots respectively.
Describing the environment states as well as the robots"
perception functions is rather tedious and even unnecessary.
We assume the reader has all those descriptions in mind.
All robots in the system must be able to solve package
distribution problems cooperatively by communicating their
intentions to each other. In order to communicate, agents
send messages using some ontology. In our scenario, there
coexist two ontologies, the UDRL and LCR ontologies. Both
of them are very simple and are just confined to describe
what robots observe.
Figure 5: Robots field of vision
When a robot carrying a package finds another package
obstructing its way, it can either go around it or, if there is
another robot in its visual field, ask it for assistance. Let
us suppose two URDL robots are in a situation like the one
depicted in Figure 6. Robot1 (the one carrying a package)
decides to ask Robot2 for assistance and sends a request.
This request is written below as a KQML message and it
should be interpreted intuitively as: Robot2, pick up the
package located in my Up square, knowing that you are
located in my Up-Right square.
`
request
:sender Robot1
:receiver Robot2
:language Packages distribution-language
:ontology URDL-ontology
:content (pick up U(Package) because UR(Robot2)
´
Figure 6: Robot assistance
Robot2 understands the content of the request and it can
use a rule represented by the following constraint:
1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package)
2, U(Package)
The above constraint should be interpreted intuitively as:
if Robot2 is situated in Robot1"s Up-Right square, Robot1
is situated in Robot2"s Up-Left square and a package is
located in Robot1"s Up square, then a package is located
in Robot2"s Up square.
Now, problems arise when a LCR robot and a URDL
robot try to interoperate. See Figure 7. Robot1 sends a
request of the form:
`
request
:sender Robot1
:receiver Robot2
:language Packages distribution-language
:ontology LCR-ontology
:content (pick up R(Robot2) because C(Package)
´
Robot2 does not understand the content of the request but
they decide to begin a process of alignment -corresponding
with a channel C1
. Once finished, Robot2 searches in Th(C1
)
for constraints similar to the expected one, that is, those of
the form:
1, R(Robot2) , 2, UL(Robot1) , 1, C(Package)
C1 2, λ(Package)
where λ ∈ {U, R, D, L, UR, DR, DL, UL}. From these, only
the following constraints are plausible according to C1
:
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283
Figure 7: Ontology mismatch
1, R(Robot2) , 2, UL(Robot1) , 1, C(Package)
C1 2, U(Package)
1, R(Robot2) , 2, UL(Robot1) , 1, C(Package)
C1 2, L(Package)
1, R(Robot2) , 2, UL(Robot1) , 1, C(Package)
C1 2, DR(Package)
If subsequently both robots adopting the same roles take
part in a situation like the one depicted in Figure 8, a new
process of alignment -corresponding with a channel C2
- takes
place. C2
also considers the previous information and hence
refines C1
. The only constraint from the above ones that
remains plausible according to C2
is :
1, R(Robot2) , 2, UL(Robot1) , 1, C(Package)
C2 2, U(Package)
Notice that this constraint is an element of the theory of the
distributed logic. Agents communicate in order to cooperate
successfully and success is guaranteed using constrains of the
distributed logic.
Figure 8: Refinement
4. CONCLUSIONS AND FURTHER WORK
In this paper we have exposed a formal model of semantic
alignment as a sequence of information-channel refinements
that are relative to the particular states of the environment
in which two agents communicate and align their respective
conceptualisations of these states. Before us, Kent [6] and
Kalfoglou and Schorlemmer [4, 10] have applied Channel
Theory to formalise semantic alignment using also Barwise
and Seligman"s insight to focus on tokens as the enablers
of information flow. Their approach to semantic alignment,
however, like most ontology matching mechanisms 
developed to date (regardless of whether they follow a functional,
design-time-based approach, or an interaction-based, 
runtime-based approach), still defines semantic alignment in
terms of a priori design decisions such as the concept 
taxonomy of the ontologies or the external sources brought into
the alignment process. Instead the model we have presented
in this paper makes explicit the particular states of the 
environment in which agents are situated and are attempting
to gradually align their ontological entities.
In the future, our effort will focus on the practical side of
the situated semantic alignment problem. We plan to 
further refine the model presented here (e.g., to include 
pragmatic issues such as termination criteria for the alignment
process) and to devise concrete ontology negotiation 
protocols based on this model that agents may be able to enact.
The formal model exposed in this paper will constitute a
solid base of future practical results.
Acknowledgements
This work is supported under the UPIC project, sponsored
by Spain"s Ministry of Education and Science under grant
number TIN2004-07461-C02- 02 and also under the 
OpenKnowledge Specific Targeted Research Project (STREP),
sponsored by the European Commission under contract 
number FP6-027253. Marco Schorlemmer is supported by a
Ram´on y Cajal Research Fellowship from Spain"s Ministry
of Education and Science, partially funded by the European
Social Fund.
5. REFERENCES
[1] J. Barwise and J. Seligman. Information Flow: The
Logic of Distributed Systems. Cambridge University
Press, 1997.
[2] C. Ghidini and F. Giunchiglia. Local models
semantics, or contextual reasoning = locality +
compatibility. Artificial Intelligence, 127(2):221-259,
2001.
[3] F. Giunchiglia and P. Shvaiko. Semantic matching.
The Knowledge Engineering Review, 18(3):265-280,
2004.
[4] Y. Kalfoglou and M. Schorlemmer. IF-Map: An
ontology-mapping method based on information-flow
theory. In Journal on Data Semantics I, LNCS 2800,
2003.
[5] Y. Kalfoglou and M. Schorlemmer. Ontology mapping:
The sate of the art. The Knowledge Engineering
Review, 18(1):1-31, 2003.
[6] R. E. Kent. Semantic integration in the Information
Flow Framework. In Semantic Interoperability and
Integration, Dagstuhl Seminar Proceedings 04391,
2005.
[7] D. Lenat. CyC: A large-scale investment in knowledge
infrastructure. Communications of the ACM, 38(11),
1995.
[8] V. L´opez, M. Sabou, and E. Motta. PowerMap:
Mapping the real Semantic Web on the fly.
Proceedings of the ISWC"06, 2006.
[9] F. McNeill. Dynamic Ontology Refinement. PhD
1284 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
thesis, School of Informatics, The University of
Edinburgh, 2006.
[10] M. Schorlemmer and Y. Kalfoglou. Progressive
ontology alignment for meaning coordination: An
information-theoretic foundation. In 4th Int. Joint
Conf. on Autonomous Agents and Multiagent Systems,
2005.
[11] P. Shvaiko and J. Euzenat. A survey of schema-based
matching approaches. In Journal on Data Semantics
IV, LNCS 3730, 2005.
[12] L. Steels. The Origins of Ontologies and
Communication Conventions in Multi-Agent Systems.
In Journal of Autonomous Agents and Multi-Agent
Systems, 1(2), 169-194, 1998.
[13] J. van Diggelen et al. ANEMONE: An Effective
Minimal Ontology Negotiation Environment In 5th
Int. Joint Conf. on Autonomous Agents and
Multiagent Systems, 2006
APPENDIX
A. CHANNEL THEORY TERMS
Classification: is a tuple A = tok(A), typ(A), |=A where
tok(A) is a set of tokens, typ(A) is a set of types and
|=A is a binary relation between tok(A) and typ(A). If
a |=A α then a is said to be of type α.
Infomorphism: f : A → B from classifications A to B is
a contravariant pair of functions f = ˆf, ˇf , where ˆf :
typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying
the following fundamental property:
ˇf(b) |=A α iff b |=B
ˆf(α)
for each token b ∈ tok(B) and each type α ∈ typ(A).
Channel: consists of two infomorphisms C = {fi : Ai →
C}i∈{1,2} with a common codomain C, called the core
of C. C tokens are called connections and a connection
c is said to connect tokens ˇf1(c) and ˇf2(c).2
Sum: given classifications A and B, the sum of A and B,
denoted by A + B, is the classification with tok(A +
B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈
tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ |
i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and
relation |=A+B defined by:
a, b |=A+B 1, α if a |=A α
a, b |=A+B 2, β if b |=B β
Given infomorphisms f : A → C and g : B → C,
the sum f + g : A + B → C is defined on types by
ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and
on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .
Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of
subsets of Σ. A binary relation between subsets of
Σ is called a consequence relation on Σ. A theory is a
pair T = Σ, where is a consequence relation on
Σ. A sequent Γ, Δ of Σ for which Γ Δ is called a
constraint of the theory T. T is regular if it satisfies:
1. Identity: α α
2. Weakening: if Γ Δ, then Γ, Γ Δ, Δ
2
In fact, this is the definition of a binary channel. A channel
can be defined with an arbitrary index set.
3. Global Cut: if Γ, Π0 Δ, Π1 for each partition
Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅),
then Γ Δ
for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3
Theory generated by a classification: let A be a 
classification. A token a ∈ tok(A) satisfies a sequent Γ, Δ
of typ(A) provided that if a is of every type in Γ then
it is of some type in Δ. The theory generated by A,
denoted by Th(A), is the theory typ(A), A where
Γ A Δ if every token in A satisfies Γ, Δ .
Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL
where:
1. tok(L), typ(L), |=L is a classification denoted by
Cla(L),
2. typ(L), L is a regular theory denoted by Th(L),
3. NL is a subset of tok(L), called the normal tokens
of L, which satisfy all constraints of Th(L).
A local logic L is sound if every token in Cla(L) is
normal, that is, NL = tok(L). L is complete if every
sequent of typ(L) satisfied by every normal token is a
constraint of Th(L).
Local logic generated by a classification: given a 
classification A, the local logic generated by A, written
Log(A), is the local logic on A (i.e., Cla(Log(A)) =
A), with Th(Log(A)) = Th(A) and such that all its
tokens are normal, i.e., NLog(A) = tok(A).
Inverse image: given an infomorphism f : A → B and
a local logic L on B, the inverse image of L under
f, denoted f−1
[L], is the local logic on A such that
Γ f−1[L] Δ if ˆf[Γ] L
ˆf[Δ] and Nf−1[L] = ˇf[NL ] =
{a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.
Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a
channel and L a local logic on its core C, the distributed
logic of C generated by L, written DLogC(L), is the
inverse image of L under the sum f1 + f2.
Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi :
Ai → C }i∈{1,2} be two channels with the same 
component classifications A1 and A2. A refinement 
infomorphism from C to C is an infomorphism r : C → C
such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi
and ˇfi = ˇfi ◦ˇr). Channel C is a refinement of C if there
exists a refinement infomorphism r from C to C.
B. CHANNEL THEORY THEOREMS
Theorem B.1. The logic generated by a classification is
sound and complete. Furthermore, given a classification A
and a logic L on A, L is sound and complete if and only if
L = Log(A).
Theorem B.2. Let L be a logic on a classification B and
f : A → B an infomorphism.
1. If L is complete then f−1
[L] is complete.
2. If L is sound and ˇf is surjective then f−1
[L] is sound.
3
All theories considered in this paper are regular.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285
Combinatorial Resource Scheduling for Multiagent MDPs
Dmitri A. Dolgov, Michael R. James, and Michael E. Samples
AI and Robotics Group
Technical Research, Toyota Technical Center USA
{ddolgov, michael.r.james, michael.samples}@gmail.com
ABSTRACT
Optimal resource scheduling in multiagent systems is a 
computationally challenging task, particularly when the values
of resources are not additive. We consider the combinatorial
problem of scheduling the usage of multiple resources among
agents that operate in stochastic environments, modeled as
Markov decision processes (MDPs). In recent years, 
efficient resource-allocation algorithms have been developed for
agents with resource values induced by MDPs. However, this
prior work has focused on static resource-allocation 
problems where resources are distributed once and then utilized
in infinite-horizon MDPs. We extend those existing models
to the problem of combinatorial resource scheduling, where
agents persist only for finite periods between their 
(predefined) arrival and departure times, requiring resources only
for those time periods. We provide a computationally 
efficient procedure for computing globally optimal resource
assignments to agents over time. We illustrate and 
empirically analyze the method in the context of a stochastic 
jobscheduling domain.
Categories and Subject Descriptors
I.2.8 [Artificial Intelligence]: Problem Solving, Control
Methods, and Search; I.2.11 [Artificial Intelligence]: 
Distributed Artificial Intelligence-Multiagent systems
General Terms
Algorithms, Performance, Design
1. INTRODUCTION
The tasks of optimal resource allocation and scheduling
are ubiquitous in multiagent systems, but solving such 
optimization problems can be computationally difficult, due to
a number of factors. In particular, when the value of a set of
resources to an agent is not additive (as is often the case with
resources that are substitutes or complements), the utility
function might have to be defined on an exponentially large
space of resource bundles, which very quickly becomes 
computationally intractable. Further, even when each agent has
a utility function that is nonzero only on a small subset of
the possible resource bundles, obtaining optimal allocation
is still computationally prohibitive, as the problem becomes
NP-complete [14].
Such computational issues have recently spawned several
threads of work in using compact models of agents" 
preferences. One idea is to use any structure present in utility
functions to represent them compactly, via, for example, 
logical formulas [15, 10, 4, 3]. An alternative is to directly model
the mechanisms that define the agents" utility functions and
perform resource allocation directly with these models [9]. A
way of accomplishing this is to model the processes by which
an agent might utilize the resources and define the utility
function as the payoff of these processes. In particular, if
an agent uses resources to act in a stochastic environment,
its utility function can be naturally modeled with a Markov
decision process, whose action set is parameterized by the
available resources. This representation can then be used to
construct very efficient resource-allocation algorithms that
lead to an exponential speedup over a straightforward 
optimization problem with flat representations of combinatorial
preferences [6, 7, 8].
However, this existing work on resource allocation with
preferences induced by resource-parameterized MDPs makes
an assumption that the resources are only allocated once and
are then utilized by the agents independently within their
infinite-horizon MDPs. This assumption that no reallocation
of resources is possible can be limiting in domains where
agents arrive and depart dynamically.
In this paper, we extend the work on resource allocation
under MDP-induced preferences to discrete-time scheduling
problems, where agents are present in the system for finite
time intervals and can only use resources within these 
intervals. In particular, agents arrive and depart at arbitrary
(predefined) times and within these intervals use resources
to execute tasks in finite-horizon MDPs. We address the
problem of globally optimal resource scheduling, where the
objective is to find an allocation of resources to the agents
across time that maximizes the sum of the expected rewards
that they obtain.
In this context, our main contribution is a 
mixed-integerprogramming formulation of the scheduling problem that
chooses globally optimal resource assignments, starting times,
and execution horizons for all agents (within their 
arrival1220
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
departure intervals). We analyze and empirically compare
two flavors of the scheduling problem: one, where agents
have static resource assignments within their finite-horizon
MDPs, and another, where resources can be dynamically
reallocated between agents at every time step.
In the rest of the paper, we first lay down the necessary
groundwork in Section 2 and then introduce our model and
formal problem statement in Section 3. In Section 4.2, we
describe our main result, the optimization program for 
globally optimal resource scheduling. Following the discussion of
our experimental results on a job-scheduling problem in 
Section 5, we conclude in Section 6 with a discussion of possible
extensions and generalizations of our method.
2. BACKGROUND
Similarly to the model used in previous work on 
resourceallocation with MDP-induced preferences [6, 7], we define
the value of a set of resources to an agent as the value of the
best MDP policy that is realizable, given those resources.
However, since the focus of our work is on scheduling 
problems, and a large part of the optimization problem is to
decide how resources are allocated in time among agents
with finite arrival and departure times, we model the agents"
planning problems as finite-horizon MDPs, in contrast to
previous work that used infinite-horizon discounted MDPs.
In the rest of this section, we first introduce some 
necessary background on finite-horizon MDPs and present a
linear-programming formulation that serves as the basis for
our solution algorithm developed in Section 4. We also 
outline the standard methods for combinatorial resource 
scheduling with flat resource values, which serve as a comparison
benchmark for the new model developed here.
2.1 Markov Decision Processes
A stationary, finite-domain, discrete-time MDP (see, for
example, [13] for a thorough and detailed development) can
be described as S, A, p, r , where: S is a finite set of 
system states; A is a finite set of actions that are available to
the agent; p is a stationary stochastic transition function,
where p(σ|s, a) is the probability of transitioning to state σ
upon executing action a in state s; r is a stationary reward
function, where r(s, a) specifies the reward obtained upon
executing action a in state s.
Given such an MDP, a decision problem under a finite
horizon T is to choose an optimal action at every time step
to maximize the expected value of the total reward accrued
during the agent"s (finite) lifetime. The agent"s optimal 
policy is then a function of current state s and the time until
the horizon. An optimal policy for such a problem is to act
greedily with respect to the optimal value function, defined
recursively by the following system of finite-time Bellman
equations [2]:
v(s, t) = max
a
r(s, a) +
X
σ
p(σ|s, a)v(σ, t + 1),
∀s ∈ S, t ∈ [1, T − 1];
v(s, T) = 0, ∀s ∈ S;
where v(s, t) is the optimal value of being in state s at time
t ∈ [1, T].
This optimal value function can be easily computed using
dynamic programming, leading to the following optimal 
policy π, where π(s, a, t) is the probability of executing action
a in state s at time t:
π(s, a, t) =
(
1, a = argmaxa r(s, a) +
P
σ p(σ|s, a)v(σ, t + 1),
0, otherwise.
The above is the most common way of computing the
optimal value function (and therefore an optimal policy) for
a finite-horizon MDP. However, we can also formulate the
problem as the following linear program (similarly to the
dual LP for infinite-horizon discounted MDPs [13, 6, 7]):
max
X
s
X
a
r(s, a)
X
t
x(s, a, t)
subject to:
X
a
x(σ, a, t + 1) =
X
s,a
p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1];
X
a
x(s, a, 1) = α(s), ∀s ∈ S;
(1)
where α(s) is the initial distribution over the state space, and
x is the (non-stationary) occupation measure (x(s, a, t) ∈
[0, 1] is the total expected number of times action a is 
executed in state s at time t). An optimal (non-stationary)
policy is obtained from the occupation measure as follows:
π(s, a, t) = x(s, a, t)/
X
a
x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2)
Note that the standard unconstrained finite-horizon MDP,
as described above, always has a uniformly-optimal 
solution (optimal for any initial distribution α(s)). Therefore,
an optimal policy can be obtained by using an arbitrary
constant α(s) > 0 (in particular, α(s) = 1 will result in
x(s, a, t) = π(s, a, t)).
However, for MDPs with resource constraints (as defined
below in Section 3), uniformly-optimal policies do not in
general exist. In such cases, α becomes a part of the 
problem input, and a resulting policy is only optimal for that
particular α. This result is well known for infinite-horizon
MDPs with various types of constraints [1, 6], and it also
holds for our finite-horizon model, which can be easily 
established via a line of reasoning completely analogous to the
arguments in [6].
2.2 Combinatorial Resource Scheduling
A straightforward approach to resource scheduling for a
set of agents M, whose values for the resources are induced
by stochastic planning problems (in our case, finite-horizon
MDPs) would be to have each agent enumerate all possible
resource assignments over time and, for each one, compute
its value by solving the corresponding MDP. Then, each
agent would provide valuations for each possible resource
bundle over time to a centralized coordinator, who would
compute the optimal resource assignments across time based
on these valuations.
When resources can be allocated at different times to 
different agents, each agent must submit valuations for 
every combination of possible time horizons. Let each agent
m ∈ M execute its MDP within the arrival-departure time
interval τ ∈ [τa
m, τd
m]. Hence, agent m will execute an MDP
with time horizon no greater than Tm = τd
m−τa
m+1. Let bτ be
the global time horizon for the problem, before which all of
the agents" MDPs must finish. We assume τd
m < bτ, ∀m ∈ M.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221
For the scheduling problem where agents have static 
resource requirements within their finite-horizon MDPs, the
agents provide a valuation for each resource bundle for each
possible time horizon (from [1, Tm]) that they may use. Let
Ω be the set of resources to be allocated among the agents.
An agent will get at most one resource bundle for one of the
time horizons. Let the variable ψ ∈ Ψm enumerate all 
possible pairs of resource bundles and time horizons for agent
m, so there are 2|Ω|
× Tm values for ψ (the space of bundles
is exponential in the number of resource types |Ω|).
The agent m must provide a value vψ
m for each ψ, and
the coordinator will allocate at most one ψ (resource, time
horizon) pair to each agent. This allocation is expressed as
an indicator variable zψ
m ∈ {0, 1} that shows whether ψ is
assigned to agent m. For time τ and resource ω, the function
nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses
resource ω at time τ (we make the assumption that agents
have binary resource requirements). This allocation problem
is NP-complete, even when considering only a single time
step, and its difficulty increases significantly with multiple
time steps because of the increasing number of values of ψ.
The problem of finding an optimal allocation that satisfies
the global constraint that the amount of each resource ω
allocated to all agents does not exceed the available amount
bϕ(ω) can be expressed as the following integer program:
max
X
m∈M
X
ψ∈Ψm
zψ
mvψ
m
subject to:
X
ψ∈Ψm
zψ
m ≤ 1, ∀m ∈ M;
X
m∈M
X
ψ∈Ψm
zψ
mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω;
(3)
The first constraint in equation 3 says that no agent can
receive more than one bundle, and the second constraint
ensures that the total assignment of resource ω does not, at
any time, exceed the resource bound.
For the scheduling problem where the agents are able to
dynamically reallocate resources, each agent must specify
a value for every combination of bundles and time steps
within its time horizon. Let the variable ψ ∈ Ψm in this case
enumerate all possible resource bundles for which at most
one bundle may be assigned to agent m at each time step.
Therefore, in this case there are
P
t∈[1,Tm](2|Ω|
)t
∼ 2|Ω|Tm
possibilities of resource bundles assigned to different time
slots, for the Tm different time horizons.
The same set of equations (3) can be used to solve this
dynamic scheduling problem, but the integer program is 
different because of the difference in how ψ is defined. In this
case, the number of ψ values is exponential in each agent"s
planning horizon Tm, resulting in a much larger program.
This straightforward approach to solving both of these
scheduling problems requires an enumeration and solution
of either 2|Ω|
Tm (static allocation) or
P
t∈[1,Tm] 2|Ω|t

(dynamic reallocation) MDPs for each agent, which very quickly
becomes intractable with the growth of the number of 
resources |Ω| or the time horizon Tm.
3. MODEL AND PROBLEM STATEMENT
We now formally introduce our model of the 
resourcescheduling problem. The problem input consists of the 
following components:
• M, Ω, bϕ, τa
m, τd
m, bτ are as defined above in Section 2.2.
• {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents
m ∈ M. Without loss of generality, we assume that state
and action spaces of all agents are the same, but each has
its own transition function pm, reward function rm, and
initial conditions αm.
• ϕm : A×Ω → {0, 1} is the mapping of actions to resources
for agent m. ϕm(a, ω) indicates whether action a of agent
m needs resource ω. An agent m that receives a set of
resources that does not include resource ω cannot execute
in its MDP policy any action a for which ϕm(a, ω) = 0. We
assume all resource requirements are binary; as discussed
below in Section 6, this assumption is not limiting.
Given the above input, the optimization problem we 
consider is to find the globally optimal-maximizing the sum
of expected rewards-mapping of resources to agents for all
time steps: Δ : τ × M × Ω → {0, 1}. A solution is feasible
if the corresponding assignment of resources to the agents
does not violate the global resource constraint:
X
m
Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4)
We consider two flavors of the resource-scheduling 
problem. The first formulation restricts resource assignments to
the space where the allocation of resources to each agent is
static during the agent"s lifetime. The second formulation 
allows reassignment of resources between agents at every time
step within their lifetimes.
Figure 1 depicts a resource-scheduling problem with three
agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3},
and a global problem horizon of bτ = 11. The agents" arrival
and departure times are shown as gray boxes and are {1, 6},
{3, 7}, and {2, 11}, respectively. A solution to this problem
is shown via horizontal bars within each agents" box, where
the bars correspond to the allocation of the three resource
types. Figure 1a shows a solution to a static scheduling 
problem. According to the shown solution, agent m1 begins the
execution of its MDP at time τ = 1 and has a lock on all
three resources until it finishes execution at time τ = 3. Note
that agent m1 relinquishes its hold on the resources before
its announced departure time of τd
m1
= 6, ostensibly because
other agents can utilize the resources more effectively. Thus,
at time τ = 4, resources ω1 and ω3 are allocated to agent
m2, who then uses them to execute its MDP (using only
actions supported by resources ω1 and ω3) until time τ = 7.
Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].
Figure 1b shows a possible solution to the dynamic version
of the same problem. There, resources can be reallocated
between agents at every time step. For example, agent m1
gives up its use of resource ω2 at time τ = 2, although it
continues the execution of its MDP until time τ = 6. Notice
that an agent is not allowed to stop and restart its MDP, so
agent m1 is only able to continue executing in the interval
τ ∈ [3, 4] if it has actions that do not require any resources
(ϕm(a, ω) = 0).
Clearly, the model and problem statement described above
make a number of assumptions about the problem and the
desired solution properties. We discuss some of those 
assumptions and their implications in Section 6.
1222 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
(a) (b)
Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static
resource assignments (resource assignments are constant within agents" lifetimes; b) dynamic assignment (resource
assignments are allowed to change at every time step).
4. RESOURCE SCHEDULING
Our resource-scheduling algorithm proceeds in two stages.
First, we perform a preprocessing step that augments the
agent MDPs; this process is described in Section 4.1. 
Second, using these augmented MDPs we construct a global
optimization problem, which is described in Section 4.2.
4.1 Augmenting Agents" MDPs
In the model described in the previous section, we assume
that if an agent does not possess the necessary resources to
perform actions in its MDP, its execution is halted and the
agent leaves the system. In other words, the MDPs cannot
be paused and resumed. For example, in the problem
shown in Figure 1a, agent m1 releases all resources after time
τ = 3, at which point the execution of its MDP is halted.
Similarly, agents m2 and m3 only execute their MDPs in the
intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively. Therefore, an
important part of the global decision-making problem is to
decide the window of time during which each of the agents
is active (i.e., executing its MDP).
To accomplish this, we augment each agent"s MDP with
two new states (start and finish states sb
, sf
, 
respectively) and a new start/stop action a∗
, as illustrated in
Figure 2. The idea is that an agent stays in the start state
sb
until it is ready to execute its MDP, at which point it
performs the start/stop action a∗
and transitions into the
state space of the original MDP with the transition 
probability that corresponds to the original initial distribution
α(s). For example, in Figure 1a, for agent m2 this would
happen at time τ = 4. Once the agent gets to the end of its
activity window (time τ = 6 for agent m2 in Figure 1a), it
performs the start/stop action, which takes it into the sink
finish state sf
at time τ = 7.
More precisely, given an MDP S, A, pm, rm, αm , we 
define an augmented MDP S , A , pm, rm, αm as follows:
S = S ∪ sb
∪ sf
; A = A ∪ a∗
;
p (s|sb
, a∗
) = α(s), ∀s ∈ S; p (sb
|sb
, a) = 1.0, ∀a ∈ A;
p (sf
|s, a∗
) = 1.0, ∀s ∈ S;
p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A;
r (sb
, a) = r (sf
, a) = 0, ∀a ∈ A ;
r (s, a) = r(s, a), ∀s ∈ S, a ∈ A;
α (sb
) = 1; α (s) = 0, ∀s ∈ S;
where all non-specified transition probabilities are assumed
to be zero. Further, in order to account for the new starting
state, we begin the MDP one time-step earlier, setting τa
m ←
τa
m − 1. This will not affect the resource allocation due to
the resource constraints only being enforced for the original
MDP states, as will be discussed in the next section. For
example, the augmented MDPs shown in Figure 2b (which
starts in state sb
at time τ = 2) would be constructed from
an MDP with original arrival time τ = 3. Figure 2b also
shows a sample trajectory through the state space: the agent
starts in state sb
, transitions into the state space S of the
original MDP, and finally exists into the sink state sf
.
Note that if we wanted to model a problem where agents
could pause their MDPs at arbitrary time steps (which might
be useful for domains where dynamic reallocation is 
possible), we could easily accomplish this by including an extra
action that transitions from each state to itself with zero
reward.
4.2 MILP for Resource Scheduling
Given a set of augmented MDPs, as defined above, the
goal of this section is to formulate a global optimization 
program that solves the resource-scheduling problem. In this
section and below, all MDPs are assumed to be the 
augmented MDPs as defined in Section 4.1.
Our approach is similar to the idea used in [6]: we 
begin with the linear-program formulation of agents" MDPs
(1) and augment it with constraints that ensure that the
corresponding resource allocation across agents and time is
valid. The resulting optimization problem then 
simultaneously solves the agents" MDPs and resource-scheduling 
problems. In the rest of this section, we incrementally develop a
mixed integer program (MILP) that achieves this.
In the absence of resource constraints, the agents" 
finitehorizon MDPs are completely independent, and the globally
optimal solution can be trivially obtained via the following
LP, which is simply an aggregation of single-agent 
finitehorizon LPs:
max
X
m
X
s
X
a
rm(s, a)
X
t
xm(s, a, t)
subject to:
X
a
xm(σ, a, t + 1) =
X
s,a
pm(σ|s, a)xm(s, a, t),
∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1];
X
a
xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S;
(12)
where xm(s, a, t) is the occupation measure of agent m, and
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223
(a) (b)
Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original
two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗
(note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as
a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.
Objective Function
(sum of expected rewards over all agents)
max
X
m
X
s
X
a
rm(s, a)
X
t
xm(s, a, t) (5)
Meaning Implication Linear Constraints
Tie x to θ. Agent is
only active when 
occupation measure is nonzero
in original MDP states.
θm(τ) = 0 =⇒ xm(s, a, τ −τa
m+1) = 0
∀s /∈ {sb
, sf
}, a ∈ A
X
s/∈{sb,sf }
X
a
xm(s, a, t) ≤ θm(τa
m + t − 1)
∀m ∈ M, ∀t ∈ [1, Tm]
(6)
Agent can only be active
in τ ∈ (τa
m, τd
m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa
m, τd
m) (7)
Cannot use resources
when not active
θm(τ) = 0 =⇒ Δm(τ, ω) = 0
∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8)
Tie x to Δ (nonzero x
forces corresponding Δ
to be nonzero.)
Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒
xm(s, a, τ − τa
m + 1) = 0
∀s /∈ {sb
, sf
}
1/|A|
X
a
ϕm(a, ω)
X
s/∈{sb,sf }
xm(s, a, t) ≤ Δm(t + τa
m − 1, ω)
∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm]
(9)
Resource bounds
X
m
Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10)
Agent cannot change
resources while 
active. Only enabled for
scheduling with static
assignments.
θm(τ) = 1 and θm(τ + 1) = 1 =⇒
Δm(τ, ω) = Δm(τ + 1, ω)
Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤
Δm(τ + 1, ω) + Z(1 − θm(τ))
Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥
Δm(τ + 1, ω) − Z(1 − θm(τ))
∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ]
(11)
Table 1: MILP for globally optimal resource scheduling.
Tm = τd
m − τa
m + 1 is the time horizon for the agent"s MDP.
Using this LP as a basis, we augment it with constraints
that ensure that the resource usage implied by the agents"
occupation measures {xm} does not violate the global 
resource requirements bϕ at any time step τ ∈ [0, bτ]. To 
formulate these resource constraints, we use the following binary
variables:
• Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which
serve as indicator variables that define whether agent m
possesses resource ω at time τ. These are analogous to
the static indicator variables used in the one-shot static
resource-allocation problem in [6].
• θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables
that specify whether agent m is active (i.e., executing
its MDP) at time τ.
The meaning of resource-usage variables Δ is illustrated in
Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to
agent m at time τ. The meaning of the activity 
indicators θ is illustrated in Figure 2b: when agent m is in either
the start state sb
or the finish state sf
, the corresponding
θm = 0, but once the agent becomes active and enters one
of the other states, we set θm = 1 . This meaning of θ can be
enforced with a linear constraint that synchronizes the 
values of the agents" occupation measures xm and the activity
1224 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
indicators θ, as shown in (6) in Table 1.
Another constraint we have to add-because the activity
indicators θ are defined on the global timeline τ-is to 
enforce the fact that the agent is inactive outside of its 
arrivaldeparture window. This is accomplished by constraint (7) in
Table 1.
Furthermore, agents should not be using resources while
they are inactive. This constraint can also be enforced via a
linear inequality on θ and Δ, as shown in (8).
Constraint (6) sets the value of θ to match the policy
defined by the occupation measure xm. In a similar fashion,
we have to make sure that the resource-usage variables Δ are
also synchronized with the occupation measure xm. This is
done via constraint (9) in Table 1, which is nearly identical
to the analogous constraint from [6].
After implementing the above constraint, which enforces
the meaning of Δ, we add a constraint that ensures that the
agents" resource usage never exceeds the amounts of 
available resources. This condition is also trivially expressed as
a linear inequality (10) in Table 1.
Finally, for the problem formulation where resource 
assignments are static during a lifetime of an agent, we add a
constraint that ensures that the resource-usage variables Δ
do not change their value while the agent is active (θ = 1).
This is accomplished via the linear constraint (11), where
Z ≥ 2 is a constant that is used to turn off the constraints
when θm(τ) = 0 or θm(τ + 1) = 0. This constraint is not
used for the dynamic problem formulation, where resources
can be reallocated between agents at every time step.
To summarize, Table 1 together with the 
conservationof-flow constraints from (12) defines the MILP that 
simultaneously computes an optimal resource assignment for all
agents across time as well as optimal finite-horizon MDP
policies that are valid under that resource assignment.
As a rough measure of the complexity of this MILP, let
us consider the number of optimization variables and 
constraints. Let TM =
P
Tm =
P
m(τa
m − τd
m + 1) be the sum
of the lengths of the arrival-departure windows across all
agents. Then, the number of optimization variables is:
TM + bτ|M||Ω| + bτ|M|,
TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are
binary (Δ and θ). However, notice that all but TM|M| of
the θ are set to zero by constraint (7), which also 
immediately forces all but TM|M||Ω| of the Δ to be zero via the
constraints (8). The number of constraints (not including
the degenerate constraints in (7)) in the MILP is:
TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.
Despite the fact that the complexity of the MILP is, in the
worst case, exponential1
in the number of binary variables,
the complexity of this MILP is significantly (exponentially)
lower than that of the MILP with flat utility functions, 
described in Section 2.2. This result echos the efficiency gains
reported in [6] for single-shot resource-allocation problems,
but is much more pronounced, because of the explosion of
the flat utility representation due to the temporal aspect of
the problem (recall the prohibitive complexity of the 
combinatorial optimization in Section 2.2). We empirically analyze
the performance of this method in Section 5.
1
Strictly speaking, solving MILPs to optimality is 
NPcomplete in the number of integer variables.
5. EXPERIMENTAL RESULTS
Although the complexity of solving MILPs is in the worst
case exponential in the number of integer variables, there
are many efficient methods for solving MILPs that allow
our algorithm to scale well for parameters common to 
resource allocation and scheduling problems. In particular,
this section introduces a problem domain-the repairshop
problem-used to empirically evaluate our algorithm"s 
scalability in terms of the number of agents |M|, the number of
shared resources |Ω|, and the varied lengths of global time
bτ during which agents may enter and exit the system.
The repairshop problem is a simple parameterized MDP
adopting the metaphor of a vehicular repair shop. Agents
in the repair shop are mechanics with a number of 
independent tasks that yield reward only when completed. In our
MDP model of this system, actions taken to advance through
the state space are only allowed if the agent holds certain
resources that are publicly available to the shop. These 
resources are in finite supply, and optimal policies for the shop
will determine when each agent may hold the limited 
resources to take actions and earn individual rewards. Each
task to be completed is associated with a single action, 
although the agent is required to repeat the action numerous
times before completing the task and earning a reward.
This model was parameterized in terms of the number
of agents in the system, the number of different types of
resources that could be linked to necessary actions, a global
time during which agents are allowed to arrive and depart,
and a maximum length for the number of time steps an agent
may remain in the system.
All datapoints in our experiments were obtained with 20
evaluations using CPLEX to solve the MILPs on a 
Pentium4 computer with 2Gb of RAM. Trials were conducted on
both the static and the dynamic version of the 
resourcescheduling problem, as defined earlier.
Figure 3 shows the runtime and policy value for 
independent modifications to the parameter set. The top row
shows how the solution time for the MILP scales as we 
increase the number of agents |M|, the global time horizon bτ,
and the number of resources |Ω|. Increasing the number of
agents leads to exponential complexity scaling, which is to
be expected for an NP-complete problem. However, 
increasing the global time limit bτ or the total number of resource
types |Ω|-while holding the number of agents 
constantdoes not lead to decreased performance. This occurs because
the problems get easier as they become under-constrained,
which is also a common phenomenon for NP-complete 
problems. We also observe that the solution to the dynamic 
version of the problem can often be computed much faster than
the static version.
The bottom row of Figure 3 shows the joint policy value
of the policies that correspond to the computed optimal
resource-allocation schedules. We can observe that the 
dynamic version yields higher reward (as expected, since the
reward for the dynamic version is always no less than the
reward of the static version). We should point out that these
graphs should not be viewed as a measure of performance of
two different algorithms (both algorithms produce optimal
solutions but to different problems), but rather as 
observations about how the quality of optimal solutions change as
more flexibility is allowed in the reallocation of resources.
Figure 4 shows runtime and policy value for trials in which
common input variables are scaled together. This allows
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225
2 4 6 8 10
10
−3
10
−2
10
−1
10
0
10
1
10
2
10
3
10
4
Number of Agents |M|
CPUTime,sec
|Ω| = 5, τ = 50
static
dynamic
50 100 150 200
10
−2
10
−1
10
0
10
1
10
2
10
3
Global Time Boundary τ
CPUTime,sec
|M| = 5, |Ω| = 5
static
dynamic
10 20 30 40 50
10
−2
10
−1
10
0
10
1
10
2
Number of Resources |Ω|
CPUTime,sec
|M| = 5, τ = 50
static
dynamic
2 4 6 8 10
200
400
600
800
1000
1200
1400
1600
Number of Agents |M|
Value
|Ω| = 5, τ = 50
static
dynamic
50 100 150 200
400
500
600
700
800
900
1000
1100
1200
1300
1400
Global Time Boundary τ
Value
|M| = 5, |Ω| = 5
static
dynamic
10 20 30 40 50
500
600
700
800
900
1000
1100
1200
1300
1400
Number of Resources |Ω|
Value
|M| = 5, τ = 50
static
dynamic
Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column
2), and numbers of resource types (column 3). Top row shows CPU time, and bottom row shows the joint reward of
agents" MDP policies. Error bars show the 1st and 3rd quartiles (25% and 75%).
2 4 6 8 10
10
−3
10
−2
10
−1
10
0
10
1
10
2
10
3
Number of Agents |M|
CPUTime,sec
τ = 10|M|
static
dynamic
2 4 6 8 10
10
−3
10
−2
10
−1
10
0
10
1
10
2
10
3
10
4
Number of Agents |M|
CPUTime,sec
|Ω| = 2|M|
static
dynamic
2 4 6 8 10
10
−3
10
−2
10
−1
10
0
10
1
10
2
10
3
10
4
Number of Agents |M|
CPUTime,sec
|Ω| = 5|M|
static
dynamic
2 4 6 8 10
200
400
600
800
1000
1200
1400
1600
1800
2000
2200
Number of Agents |M|
Value
τ = 10|M|
static
dynamic
2 4 6 8 10
200
400
600
800
1000
1200
1400
1600
1800
2000
Number of Agents |M|
Value
|Ω| = 2|M|
static
dynamic
2 4 6 8 10
0
500
1000
1500
2000
2500
Number of Agents |M|
Value
|Ω| = 5|M|
static
dynamic
Figure 4: Evaluation of our MILP using correlated input variables. The left column tracks the performance and CPU
time as the number of agents and global-time window increase together (bτ = 10|M|). The middle and the right column
track the performance and CPU time as the number of resources and the number of agents increase together as
|Ω| = 2|M| and |Ω| = 5|M|, respectively. Error bars show the 1st and 3rd quartiles (25% and 75%).
1226 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
us to explore domains where the total number of agents
scales proportionally to the total number of resource types
or the global time horizon, while keeping constant the 
average agent density (per unit of global time) or the average
number of resources per agent (which commonly occurs in
real-life applications).
Overall, we believe that these experimental results 
indicate that our MILP formulation can be used to effectively
solve resource-scheduling problems of nontrivial size.
6. DISCUSSION AND CONCLUSIONS
Throughout the paper, we have made a number of 
assumptions in our model and solution algorithm; we discuss
their implications below.
• Continual execution. We assume that once an agent
stops executing its MDP (transitions into state sf
), it
exits the system and cannot return. It is easy to relax
this assumption for domains where agents" MDPs can be
paused and restarted. All that is required is to include an
additional pause action which transitions from a given
state back to itself, and has zero reward.
• Indifference to start time. We used a reward model
where agents" rewards depend only on the time horizon
of their MDPs and not the global start time. This is a
consequence of our MDP-augmentation procedure from
Section 4.1. It is easy to extend the model so that the
agents incur an explicit penalty for idling by assigning a
non-zero negative reward to the start state sb
.
• Binary resource requirements. For simplicity, we have
assumed that resource costs are binary: ϕm(a, ω) = {0, 1},
but our results generalize in a straightforward manner to
non-binary resource mappings, analogously to the 
procedure used in [5].
• Cooperative agents. The optimization procedure 
discussed in this paper was developed in the context of 
cooperative agents, but it can also be used to design a 
mechanism for scheduling resources among self-interested agents.
This optimization procedure can be embedded in a 
VickreyClarke-Groves auction, completely analogously to the way
it was done in [7]. In fact, all the results of [7] about the
properties of the auction and information privacy directly
carry over to the scheduling domain discussed in this 
paper, requiring only slight modifications to deal with 
finitehorizon MDPs.
• Known, deterministic arrival and departure times.
Finally, we have assumed that agents" arrival and 
departure times (τa
m and τd
m) are deterministic and known a
priori. This assumption is fundamental to our solution
method. While there are many domains where this 
assumption is valid, in many cases agents arrive and 
depart dynamically and their arrival and departure times
can only be predicted probabilistically, leading to online
resource-allocation problems. In particular, in the case of
self-interested agents, this becomes an interesting version
of an online-mechanism-design problem [11, 12].
In summary, we have presented an MILP formulation for
the combinatorial resource-scheduling problem where agents"
values for possible resource assignments are defined by 
finitehorizon MDPs. This result extends previous work ([6, 7])
on static one-shot resource allocation under MDP-induced
preferences to resource-scheduling problems with a temporal
aspect. As such, this work takes a step in the direction of 
designing an online mechanism for agents with combinatorial
resource preferences induced by stochastic planning 
problems. Relaxing the assumption about deterministic arrival
and departure times of the agents is a focus of our future
work.
We would like to thank the anonymous reviewers for their
insightful comments and suggestions.
7. REFERENCES
[1] E. Altman and A. Shwartz. Adaptive control of
constrained Markov chains: Criteria and policies.
Annals of Operations Research, special issue on
Markov Decision Processes, 28:101-134, 1991.
[2] R. Bellman. Dynamic Programming. Princeton
University Press, 1957.
[3] C. Boutilier. Solving concisely expressed combinatorial
auction problems. In Proc. of AAAI-02, pages
359-366, 2002.
[4] C. Boutilier and H. H. Hoos. Bidding languages for
combinatorial auctions. In Proc. of IJCAI-01, pages
1211-1217, 2001.
[5] D. Dolgov. Integrated Resource Allocation and
Planning in Stochastic Multiagent Environments. PhD
thesis, Computer Science Department, University of
Michigan, February 2006.
[6] D. A. Dolgov and E. H. Durfee. Optimal resource
allocation and policy formulation in loosely-coupled
Markov decision processes. In Proc. of ICAPS-04,
pages 315-324, June 2004.
[7] D. A. Dolgov and E. H. Durfee. Computationally
efficient combinatorial auctions for resource allocation
in weakly-coupled MDPs. In Proc. of AAMAS-05,
New York, NY, USA, 2005. ACM Press.
[8] D. A. Dolgov and E. H. Durfee. Resource allocation
among agents with preferences induced by factored
MDPs. In Proc. of AAMAS-06, 2006.
[9] K. Larson and T. Sandholm. Mechanism design and
deliberative agents. In Proc. of AAMAS-05, pages
650-656, New York, NY, USA, 2005. ACM Press.
[10] N. Nisan. Bidding and allocation in combinatorial
auctions. In Electronic Commerce, 2000.
[11] D. C. Parkes and S. Singh. An MDP-based approach
to Online Mechanism Design. In Proc. of the
Seventeenths Annual Conference on Neural
Information Processing Systems (NIPS-03), 2003.
[12] D. C. Parkes, S. Singh, and D. Yanovsky.
Approximately efficient online mechanism design. In
Proc. of the Eighteenths Annual Conference on Neural
Information Processing Systems (NIPS-04), 2004.
[13] M. L. Puterman. Markov Decision Processes. John
Wiley & Sons, New York, 1994.
[14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.
Computationally manageable combinational auctions.
Management Science, 44(8):1131-1147, 1998.
[15] T. Sandholm. An algorithm for optimal winner
determination in combinatorial auctions. In Proc. of
IJCAI-99, pages 542-547, San Francisco, CA, USA,
1999. Morgan Kaufmann Publishers Inc.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227
The LOGIC Negotiation Model
Carles Sierra
Institut d"Investigacio en Intel.ligencia Artificial
Spanish Scientific Research Council, UAB
08193 Bellaterra, Catalonia, Spain
sierra@iiia.csic.es
John Debenham
Faculty of Information Technology
University of Technology, Sydney
NSW, Australia
debenham@it.uts.edu.au
ABSTRACT
Successful negotiators prepare by determining their position
along five dimensions: Legitimacy, Options, Goals, 
Independence, and Commitment, (LOGIC). We introduce a 
negotiation model based on these dimensions and on two primitive
concepts: intimacy (degree of closeness) and balance (degree
of fairness). The intimacy is a pair of matrices that 
evaluate both an agent"s contribution to the relationship and
its opponent"s contribution each from an information view
and from a utilitarian view across the five LOGIC 
dimensions. The balance is the difference between these 
matrices. A relationship strategy maintains a target intimacy for
each relationship that an agent would like the relationship to
move towards in future. The negotiation strategy maintains
a set of Options that are in-line with the current intimacy
level, and then tactics wrap the Options in argumentation
with the aim of attaining a successful deal and 
manipulating the successive negotiation balances towards the target
intimacy.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
Intelligence-Multiagent systems
General Terms
Theory
1. INTRODUCTION
In this paper we propose a new negotiation model to deal
with long term relationships that are founded on successive
negotiation encounters. The model is grounded on results
from business and psychological studies [1, 16, 9], and 
acknowledges that negotiation is an information exchange 
process as well as a utility exchange process [15, 14]. We 
believe that if agents are to succeed in real application domains
they have to reconcile both views: informational and 
gametheoretical. Our aim is to model trading scenarios where
agents represent their human principals, and thus we want
their behaviour to be comprehensible by humans and to 
respect usual human negotiation procedures, whilst being 
consistent with, and somehow extending, game theoretical and
information theoretical results. In this sense, agents are not
just utility maximisers, but aim at building long lasting 
relationships with progressing levels of intimacy that determine
what balance in information and resource sharing is 
acceptable to them. These two concepts, intimacy and balance are
key in the model, and enable us to understand competitive
and co-operative game theory as two particular theories of
agent relationships (i.e. at different intimacy levels). These
two theories are too specific and distinct to describe how
a (business) relationship might grow because interactions
have some aspects of these two extremes on a continuum in
which, for example, agents reveal increasing amounts of 
private information as their intimacy grows. We don"t follow
the "Co-Opetition" aproach [4] where co-operation and 
competition depend on the issue under negotiation, but instead
we belief that the willingness to co-operate/compete affect
all aspects in the negotiation process. Negotiation strategies
can naturally be seen as procedures that select tactics used
to attain a successful deal and to reach a target intimacy
level. It is common in human settings to use tactics that
compensate for unbalances in one dimension of a 
negotiation with unbalances in another dimension. In this sense,
humans aim at a general sense of fairness in an interaction.
In Section 2 we outline the aspects of human negotiation
modelling that we cover in this work. Then, in Section 3
we introduce the negotiation language. Section 4 explains
in outline the architecture and the concepts of intimacy and
balance, and how they influence the negotiation. Section 5
contains a description of the different metrics used in the
agent model including intimacy. Finally, Section 6 outlines
how strategies and tactics use the LOGIC framework, 
intimacy and balance.
2. HUMAN NEGOTIATION
Before a negotiation starts human negotiators prepare the
dialogic exchanges that can be made along the five LOGIC
dimensions [7]:
• Legitimacy. What information is relevant to the 
negotiation process? What are the persuasive arguments
about the fairness of the options?
1030
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
• Options. What are the possible agreements we can
accept?
• Goals. What are the underlying things we need or care
about? What are our goals?
• Independence. What will we do if the negotiation fails?
What alternatives have we got?
• Commitment. What outstanding commitments do we
have?
Negotiation dialogues, in this context, exchange 
dialogical moves, i.e. messages, with the intention of getting 
information about the opponent or giving away information
about us along these five dimensions: request for 
information, propose options, inform about interests, issue promises,
appeal to standards . . . A key part of any negotiation process
is to build a model of our opponent(s) along these 
dimensions. All utterances agents make during a negotiation give
away information about their current LOGIC model, that
is, about their legitimacy, options, goals, independence, and
commitments. Also, several utterances can have a 
utilitarian interpretation in the sense that an agent can associate
a preferential gain to them. For instance, an offer may 
inform our negotiation opponent about our willingness to sign
a contract in the terms expressed in the offer, and at the
same time the opponent can compute what is its associated
expected utilitarian gain. These two views: 
informationbased and utility-based, are central in the model proposed
in this paper.
2.1 Intimacy and Balance in relationships
There is evidence from psychological studies that humans
seek a balance in their negotiation relationships. The 
classical view [1] is that people perceive resource allocations as
being distributively fair (i.e. well balanced) if they are 
proportional to inputs or contributions (i.e. equitable). 
However, more recent studies [16, 17] show that humans follow
a richer set of norms of distributive justice depending on
their intimacy level: equity, equality, and need. Equity 
being the allocation proportional to the effort (e.g. the profit
of a company goes to the stock holders proportional to their
investment), equality being the allocation in equal amounts
(e.g. two friends eat the same amount of a cake cooked by
one of them), and need being the allocation proportional to
the need for the resource (e.g. in case of food scarcity, a
mother gives all food to her baby). For instance, if we are in
a purely economic setting (low intimacy) we might request
equity for the Options dimension but could accept equality
in the Goals dimension.
The perception of a relation being in balance (i.e. fair)
depends strongly on the nature of the social relationships
between individuals (i.e. the intimacy level). In purely 
economical relationships (e.g., business), equity is perceived as
more fair; in relations where joint action or fostering of social
relationships are the goal (e.g. friends), equality is perceived
as more fair; and in situations where personal development
or personal welfare are the goal (e.g. family), allocations are
usually based on need.
We believe that the perception of balance in dialogues (in
negotiation or otherwise) is grounded on social relationships,
and that every dimension of an interaction between humans
can be correlated to the social closeness, or intimacy, 
between the parties involved. According to the previous 
studies, the more intimacy across the five LOGIC dimensions the
more the need norm is used, and the less intimacy the more
the equity norm is used. This might be part of our social
evolution. There is ample evidence that when human 
societies evolved from a hunter-gatherer structure1
to a 
shelterbased one2
the probability of survival increased when food
was scarce.
In this context, we can clearly see that, for instance, 
families exchange not only goods but also information and 
knowledge based on need, and that few families would consider
their relationships as being unbalanced, and thus unfair,
when there is a strong asymmetry in the exchanges (a mother
explaining everything to her children, or buying toys, does
not expect reciprocity). In the case of partners there is some
evidence [3] that the allocations of goods and burdens (i.e.
positive and negative utilities) are perceived as fair, or in
balance, based on equity for burdens and equality for goods.
See Table 1 for some examples of desired balances along the
LOGIC dimensions.
The perceived balance in a negotiation dialogue allows 
negotiators to infer information about their opponent, about
its LOGIC stance, and to compare their relationships with
all negotiators. For instance, if we perceive that every time
we request information it is provided, and that no significant
questions are returned, or no complaints about not 
receiving information are given, then that probably means that
our opponent perceives our social relationship to be very
close. Alternatively, we can detect what issues are causing
a burden to our opponent by observing an imbalance in the
information or utilitarian senses on that issue.
3. COMMUNICATION MODEL
3.1 Ontology
In order to define a language to structure agent dialogues we
need an ontology that includes a (minimum) repertoire of 
elements: a set of concepts (e.g. quantity, quality, material)
organised in a is-a hierarchy (e.g. platypus is a mammal,
Australian-dollar is a currency), and a set of relations over
these concepts (e.g. price(beer,AUD)).3
We model 
ontologies following an algebraic approach [8] as:
An ontology is a tuple O = (C, R, ≤, σ) where:
1. C is a finite set of concept symbols (including basic
data types);
2. R is a finite set of relation symbols;
3. ≤ is a reflexive, transitive and anti-symmetric relation
on C (a partial order)
4. σ : R → C+
is the function assigning to each relation
symbol its arity
1
In its purest form, individuals in these societies collect food
and consume it when and where it is found. This is a pure
equity sharing of the resources, the gain is proportional to
the effort.
2
In these societies there are family units, around a shelter,
that represent the basic food sharing structure. Usually,
food is accumulated at the shelter for future use. Then the
food intake depends more on the need of the members.
3
Usually, a set of axioms defined over the concepts and 
relations is also required. We will omit this here.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1031
Element A new trading partner my butcher my boss my partner my children
Legitimacy equity equity equity equality need
Options equity equity equity mixeda
need
Goals equity need equity need need
Independence equity equity equality need need
Commitment equity equity equity mixed need
a
equity on burden, equality on good
Table 1: Some desired balances (sense of fairness) examples depending on the relationship.
where ≤ is the traditional is-a hierarchy. To simplify 
computations in the computing of probability distributions we
assume that there is a number of disjoint is-a trees covering
different ontological spaces (e.g. a tree for types of fabric,
a tree for shapes of clothing, and so on). R contains 
relations between the concepts in the hierarchy, this is needed
to define ‘objects" (e.g. deals) that are defined as a tuple of
issues.
The semantic distance between concepts within an 
ontology depends on how far away they are in the structure
defined by the ≤ relation. Semantic distance plays a 
fundamental role in strategies for information-based agency. How
signed contracts, Commit(·), about objects in a particular
semantic region, and their execution, Done(·), affect our
decision making process about signing future contracts in
nearby semantic regions is crucial to modelling the common
sense that human beings apply in managing trading 
relationships. A measure [10] bases the semantic similarity 
between two concepts on the path length induced by ≤ (more
distance in the ≤ graph means less semantic similarity), and
the depth of the subsumer concept (common ancestor) in the
shortest path between the two concepts (the deeper in the
hierarchy, the closer the meaning of the concepts). Semantic
similarity is then defined as:
Sim(c, c ) = e−κ1l
·
eκ2h
− e−κ2h
eκ2h + e−κ2h
where l is the length (i.e. number of hops) of the 
shortest path between the concepts, h is the depth of the deepest
concept subsuming both concepts, and κ1 and κ2 are 
parameters scaling the contributions of the shortest path length
and the depth respectively.
3.2 Language
The shape of the language that α uses to represent the 
information received and the content of its dialogues depends on
two fundamental notions. First, when agents interact within
an overarching institution they explicitly or implicitly accept
the norms that will constrain their behaviour, and accept
the established sanctions and penalties whenever norms are
violated. Second, the dialogues in which α engages are built
around two fundamental actions: (i) passing information,
and (ii) exchanging proposals and contracts. A contract
δ = (a, b) between agents α and β is a pair where a and b
represent the actions that agents α and β are responsible
for respectively. Contracts signed by agents and 
information passed by agents, are similar to norms in the sense that
they oblige agents to behave in a particular way, so as to
satisfy the conditions of the contract, or to make the world
consistent with the information passed. Contracts and 
Information can thus be thought of as normative statements
that restrict an agent"s behaviour.
Norms, contracts, and information have an obvious 
temporal dimension. Thus, an agent has to abide by a norm
while it is inside an institution, a contract has a validity
period, and a piece of information is true only during an
interval in time. The set of norms affecting the behaviour of
an agent defines the context that the agent has to take into
account.
α"s communication language has two fundamental 
primitives: Commit(α, β, ϕ) to represent, in ϕ, the world that α
aims at bringing about and that β has the right to verify,
complain about or claim compensation for any deviations
from, and Done(μ) to represent the event that a certain
action μ4
has taken place. In this way, norms, contracts,
and information chunks will be represented as instances of
Commit(·) where α and β can be individual agents or 
institutions. C is:
μ ::= illoc(α, β, ϕ, t) | μ; μ |
Let context In μ End
ϕ ::= term | Done(μ) | Commit(α, β, ϕ) | ϕ ∧ ϕ |
ϕ ∨ ϕ | ¬ϕ | ∀v.ϕv | ∃v.ϕv
context ::= ϕ | id = ϕ | prolog clause | context; context
where ϕv is a formula with free variable v, illoc is any 
appropriate set of illocutionary particles, ‘;" means sequencing,
and context represents either previous agreements, previous
illocutions, the ontological working context, that is a 
projection of the ontological trees that represent the focus of
the conversation, or code that aligns the ontological 
differences between the speakers needed to interpret an action
a. Representing an ontology as a set predicates in Prolog
is simple. The set term contains instances of the ontology
concepts and relations.5
For example, we can represent the following offer: If you
spend a total of more than e100 in my shop during 
October then I will give you a 10% discount on all goods in
November, as:
Offer( α, β,spent(β, α, October, X) ∧ X ≥ e100 →
∀ y. Done(Inform(ξ, α, pay(β, α, y), November)) →
Commit(α, β, discount(y,10%)))
ξ is an institution agent that reports the payment.
4
Without loss of generality we will assume that all actions
are dialogical.
5
We assume the convention that C(c) means that c is an
instance of concept C and r(c1, . . . , cn) implicitly determines
that ci is an instance of the concept in the i-th position of
the relation r.
1032 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 1: The LOGIC agent architecture
4. AGENT ARCHITECTURE
A multiagent system {α, β1, . . . , βn, ξ, θ1, . . . , θt}, contains
an agent α that interacts with other argumentation agents,
βi, information providing agents, θj, and an institutional
agent, ξ, that represents the institution where we assume
the interactions happen [2]. The institutional agent reports
promptly and honestly on what actually occurs after an
agent signs a contract, or makes some other form of 
commitment. In Section 4.1 this enables us to measure the 
difference between an utterance and a subsequent observation.
The communication language C introduced in Section 3.2 
enables us both to structure the dialogues and to structure the
processing of the information gathered by agents. Agents
have a probabilistic first-order internal language L used to
represent a world model, Mt
. A generic information-based
architecture is described in detail in [15].
The LOGIC agent architecture is shown in Figure 1. Agent
α acts in response to a need that is expressed in terms of the
ontology. A need may be exogenous such as a need to trade
profitably and may be triggered by another agent offering to
trade, or endogenous such as α deciding that it owns more
wine than it requires. Needs trigger α"s goal/plan 
proactive reasoning, while other messages are dealt with by α"s
reactive reasoning.6
Each plan prepares for the negotiation
by assembling the contents of a ‘LOGIC briefcase" that the
agent ‘carries" into the negotiation7
. The relationship 
strategy determines which agent to negotiate with for a given
need; it uses risk management analysis to preserve a 
strategic set of trading relationships for each mission-critical need
- this is not detailed here. For each trading relationship
this strategy generates a relationship target that is expressed
in the LOGIC framework as a desired level of intimacy to
be achieved in the long term.
Each negotiation consists of a dialogue, Ψt
, between two
agents with agent α contributing utterance μ and the 
part6
Each of α"s plans and reactions contain constructors for an
initial world model Mt
. Mt
is then maintained from 
percepts received using update functions that transform 
percepts into constraints on Mt
- for details, see [14, 15].
7
Empirical evidence shows that in human negotiation, 
better outcomes are achieved by skewing the opening Options
in favour of the proposer. We are unaware of any 
empirical investigation of this hypothesis for autonomous agents
in real trading scenarios.
ner β contributing μ using the language described in 
Section 3.2. Each dialogue, Ψt
, is evaluated using the LOGIC
framework in terms of the value of Ψt
to both α and β - see
Section 5.2. The negotiation strategy then determines the
current set of Options {δi}, and then the tactics, guided by
the negotiation target, decide which, if any, of these Options
to put forward and wraps them in argumentation dialogue
- see Section 6. We now describe two of the distributions
in Mt
that support offer exchange.
Pt
(acc(α, β, χ, δ)) estimates the probability that α should
accept proposal δ in satisfaction of her need χ, where δ =
(a, b) is a pair of commitments, a for α and b for β. α will
accept δ if: Pt
(acc(α, β, χ, δ)) > c, for level of certainty c.
This estimate is compounded from subjective and objective
views of acceptability. The subjective estimate takes account
of: the extent to which the enactment of δ will satisfy α"s
need χ, how much δ is ‘worth" to α, and the extent to which
α believes that she will be in a position to execute her 
commitment a [14, 15]. Sα(β, a) is a random variable denoting
α"s estimate of β"s subjective valuation of a over some finite,
numerical evaluation space. The objective estimate captures
whether δ is acceptable on the open market, and variable
Uα(b) denotes α"s open-market valuation of the enactment
of commitment b, again taken over some finite numerical 
valuation space. We also consider needs, the variable Tα(β, a)
denotes α"s estimate of the strength of β"s motivating need
for the enactment of commitment a over a valuation space.
Then for δ = (a, b): Pt
(acc(α, β, χ, δ)) =
Pt
„
Tα(β, a)
Tα(α, b)
«h
×
„
Sα(α, b)
Sα(β, a)
«g
×
Uα(b)
Uα(a)
≥ s
!
(1)
where g ∈ [0, 1] is α"s greed, h ∈ [0, 1] is α"s degree of 
altruism, and s ≈ 1 is derived from the stance8
described in
Section 6. The parameters g and h are independent. We can
imagine a relationship that begins with g = 1 and h = 0.
Then as the agents share increasing amounts of their 
information about their open market valuations g gradually
reduces to 0, and then as they share increasing amounts of
information about their needs h increases to 1. The basis
for the acceptance criterion has thus developed from equity
to equality, and then to need.
Pt
(acc(β, α, δ)) estimates the probability that β would
accept δ, by observing β"s responses. For example, if β
sends the message Offer(δ1) then α derives the constraint:
{Pt
(acc(β, α, δ1)) = 1} on the distribution Pt
(β, α, δ), and
if this is a counter offer to a former offer of α"s, δ0, then:
{Pt
(acc(β, α, δ0)) = 0}. In the not-atypical special case of
multi-issue bargaining where the agents" preferences over the
individual issues only are known and are complementary to
each other"s, maximum entropy reasoning can be applied
to estimate the probability that any multi-issue δ will be
acceptable to β by enumerating the possible worlds that
represent β"s limit of acceptability [6].
4.1 Updating the World Model Mt
α"s world model consists of probability distributions that
represent its uncertainty in the world state. α is interested
8
If α chooses to inflate her opening Options then this is
achieved in Section 6 by increasing the value of s. If s 1
then a deal may not be possible. This illustrates the 
wellknown inefficiency of bilateral bargaining established 
analytically by Myerson and Satterthwaite in 1983.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1033
in the degree to which an utterance accurately describes
what will subsequently be observed. All observations about
the world are received as utterances from an all-truthful 
institution agent ξ. For example, if β communicates the goal
I am hungry and the subsequent negotiation terminates
with β purchasing a book from α (by ξ advising α that a
certain amount of money has been credited to α"s account)
then α may conclude that the goal that β chose to satisfy
was something other than hunger. So, α"s world model 
contains probability distributions that represent its uncertain
expectations of what will be observed on the basis of 
utterances received.
We represent the relationship between utterance, ϕ, and
subsequent observation, ϕ , by Pt
(ϕ |ϕ) ∈ Mt
, where ϕ and
ϕ may be ontological categories in the interest of 
computational feasibility. For example, if ϕ is I will deliver a bucket
of fish to you tomorrow then the distribution P(ϕ |ϕ) need
not be over all possible things that β might do, but could
be over ontological categories that summarise β"s possible
actions.
In the absence of in-coming utterances, the conditional
probabilities, Pt
(ϕ |ϕ), should tend to ignorance as 
represented by a decay limit distribution D(ϕ |ϕ). α may have
background knowledge concerning D(ϕ |ϕ) as t → ∞, 
otherwise α may assume that it has maximum entropy whilst
being consistent with the data. In general, given a 
distribution, Pt
(Xi), and a decay limit distribution D(Xi), Pt
(Xi)
decays by:
Pt+1
(Xi) = Δi(D(Xi), Pt
(Xi)) (2)
where Δi is the decay function for the Xi satisfying the
property that limt→∞ Pt
(Xi) = D(Xi). For example, Δi
could be linear: Pt+1
(Xi) = (1 − νi) × D(Xi) + νi × Pt
(Xi),
where νi < 1 is the decay rate for the i"th distribution.
Either the decay function or the decay limit distribution
could also be a function of time: Δt
i and Dt
(Xi).
Suppose that α receives an utterance μ = illoc(α, β, ϕ, t)
from agent β at time t. Suppose that α attaches an 
epistemic belief Rt
(α, β, μ) to μ - this probability takes account
of α"s level of personal caution. We model the update of
Pt
(ϕ |ϕ) in two cases, one for observations given ϕ, second
for observations given φ in the semantic neighbourhood of
ϕ.
4.2 Update of Pt
(ϕ |ϕ) given ϕ
First, if ϕk is observed then α may set Pt+1
(ϕk|ϕ) to some
value d where {ϕ1, ϕ2, . . . , ϕm} is the set of all possible
observations. We estimate the complete posterior 
distribution Pt+1
(ϕ |ϕ) by applying the principle of minimum
relative entropy9
as follows. Let p(μ) be the distribution:
9
Given a probability distribution q, the minimum relative
entropy distribution p = (p1, . . . , pI ) subject to a set of J
linear constraints g = {gj(p) = aj · p − cj = 0}, j = 1, . . . , J
(that must include the constraint
P
i pi − 1 = 0) is: p =
arg minr
P
j rj log
rj
qj
. This may be calculated by 
introducing Lagrange multipliers λ: L(p, λ) =
P
j pj log
pj
qj
+ λ · g.
Minimising L, { ∂L
∂λj
= gj(p) = 0}, j = 1, . . . , J is the set of
given constraints g, and a solution to ∂L
∂pi
= 0, i = 1, . . . , I
leads eventually to p. Entropy-based inference is a form of
Bayesian inference that is convenient when the data is sparse
[5] and encapsulates common-sense reasoning [12].
arg minx
P
j xj log
xj
Pt(ϕ |ϕ)j
that satisfies the constraint p(μ)k
= d. Then let q(μ) be the distribution:
q(μ) = Rt
(α, β, μ) × p(μ) + (1 − Rt
(α, β, μ)) × Pt
(ϕ |ϕ)
and then let:
r(μ) =
(
q(μ) if q(μ) is more interesting than Pt
(ϕ |ϕ)
Pt
(ϕ |ϕ) otherwise
A general measure of whether q(μ) is more interesting than
Pt
(ϕ |ϕ) is: K(q(μ) D(ϕ |ϕ)) > K(Pt
(ϕ |ϕ) D(ϕ |ϕ)), where
K(x y) =
P
j xj ln
xj
yj
is the Kullback-Leibler distance 
between two probability distributions x and y [11].
Finally incorporating Eqn. 2 we obtain the method for
updating a distribution Pt
(ϕ |ϕ) on receipt of a message μ:
Pt+1
(ϕ |ϕ) = Δi(D(ϕ |ϕ), r(μ)) (3)
This procedure deals with integrity decay, and with two
probabilities: first, the probability z in the utterance μ, and
second the belief Rt
(α, β, μ) that α attached to μ.
4.3 Update of Pt
(φ |φ) given ϕ
The sim method: Given as above μ = illoc(α, β, ϕ, t) and
the observation ϕk we define the vector t by
ti = Pt
(φi|φ) + (1− | Sim(ϕk, ϕ) − Sim(φi, φ) |) · Sim(ϕk, φ)
with {φ1, φ2, . . . , φp} the set of all possible observations in
the context of φ and i = 1, . . . , p. t is not a probability
distribution. The multiplying factor Sim(ϕ , φ) limits the
variation of probability to those formulae whose 
ontological context is not too far away from the observation. The
posterior Pt+1
(φ |φ) is obtained with Equation 3 with r(μ)
defined to be the normalisation of t.
The valuation method: For a given φk, wexp
(φk) =Pm
j=1 Pt
(φj|φk) · w(φj) is α"s expectation of the value of
what will be observed given that β has stated that φk will
be observed, for some measure w. Now suppose that, as
before, α observes ϕk after agent β has stated ϕ. α revises
the prior estimate of the expected valuation wexp
(φk) in the
light of the observation ϕk to:
(wrev
(φk) | (ϕk|ϕ)) =
g(wexp
(φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk))
for some function g - the idea being, for example, that if the
execution, ϕk, of the commitment, ϕ, to supply cheese was
devalued then α"s expectation of the value of a commitment,
φ, to supply wine should decrease. We estimate the posterior
by applying the principle of minimum relative entropy as for
Equation 3, where the distribution p(μ) = p(φ |φ) satisfies the
constraint:
p
X
j=1
p(ϕ ,ϕ)j · wi(φj) =
g(wexp
(φk), Sim(φk, ϕ), w(φk), w(ϕ), wi(ϕk))
5. SUMMARY MEASURES
A dialogue, Ψt
, between agents α and β is a sequence of
inter-related utterances in context. A relationship, Ψ∗t
, is a
sequence of dialogues. We first measure the confidence that
an agent has for another by observing, for each utterance,
the difference between what is said (the utterance) and what
1034 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
subsequently occurs (the observation). Second we evaluate
each dialogue as it progresses in terms of the LOGIC 
framework - this evaluation employs the confidence measures.
Finally we define the intimacy of a relationship as an 
aggregation of the value of its component dialogues.
5.1 Confidence
Confidence measures generalise what are commonly called
trust, reliability and reputation measures into a single 
computational framework that spans the LOGIC categories. In
Section 5.2 confidence measures are applied to valuing 
fulfilment of promises in the Legitimacy category - we formerly
called this honour [14], to the execution of commitments
- we formerly called this trust [13], and to valuing 
dialogues in the Goals category - we formerly called this
reliability [14].
Ideal observations. Consider a distribution of 
observations that represent α"s ideal in the sense that it is the
best that α could reasonably expect to observe. This 
distribution will be a function of α"s context with β denoted
by e, and is Pt
I (ϕ |ϕ, e). Here we measure the relative 
entropy between this ideal distribution, Pt
I (ϕ |ϕ, e), and the
distribution of expected observations, Pt
(ϕ |ϕ). That is:
C(α, β, ϕ) = 1 −
X
ϕ
Pt
I (ϕ |ϕ, e) log
Pt
I (ϕ |ϕ, e)
Pt(ϕ |ϕ)
(4)
where the 1 is an arbitrarily chosen constant being the
maximum value that this measure may have. This equation
measures confidence for a single statement ϕ. It makes sense
to aggregate these values over a class of statements, say over
those ϕ that are in the ontological context o, that is ϕ ≤ o:
C(α, β, o) = 1 −
P
ϕ:ϕ≤o Pt
β(ϕ) [1 − C(α, β, ϕ)]
P
ϕ:ϕ≤o Pt
β(ϕ)
where Pt
β(ϕ) is a probability distribution over the space of
statements that the next statement β will make to α is ϕ.
Similarly, for an overall estimate of β"s confidence in α:
C(α, β) = 1 −
X
ϕ
Pt
β(ϕ) [1 − C(α, β, ϕ)]
Preferred observations. The previous measure requires
that an ideal distribution, Pt
I (ϕ |ϕ, e), has to be specified for
each ϕ. Here we measure the extent to which the 
observation ϕ is preferable to the original statement ϕ. Given a
predicate Prefer(c1, c2, e) meaning that α prefers c1 to c2 in
environment e. Then if ϕ ≤ o:
C(α, β, ϕ) =
X
ϕ
Pt
(Prefer(ϕ , ϕ, o))Pt
(ϕ |ϕ)
and:
C(α, β, o) =
P
ϕ:ϕ≤o Pt
β(ϕ)C(α, β, ϕ)
P
ϕ:ϕ≤o Pt
β(ϕ)
Certainty in observation. Here we measure the 
consistency in expected acceptable observations, or the lack of
expected uncertainty in those possible observations that are
better than the original statement. If ϕ ≤ o let: Φ+(ϕ, o, κ) =˘
ϕ | Pt
(Prefer(ϕ , ϕ, o)) > κ
¯
for some constant κ, and:
C(α, β, ϕ) = 1 +
1
B∗
·
X
ϕ ∈Φ+(ϕ,o,κ)
Pt
+(ϕ |ϕ) log Pt
+(ϕ |ϕ)
where Pt
+(ϕ |ϕ) is the normalisation of Pt
(ϕ |ϕ) for ϕ ∈
Φ+(ϕ, o, κ),
B∗
=
(
1 if |Φ+(ϕ, o, κ)| = 1
log |Φ+(ϕ, o, κ)| otherwise
As above we aggregate this measure for observations in a
particular context o, and measure confidence as before.
Computational Note. The various measures given above
involve extensive calculations. For example, Eqn. 4 containsP
ϕ that sums over all possible observations ϕ . We obtain
a more computationally friendly measure by appealing to
the structure of the ontology described in Section 3.2, and
the right-hand side of Eqn. 4 may be approximated to:
1 −
X
ϕ :Sim(ϕ ,ϕ)≥η
Pt
η,I (ϕ |ϕ, e) log
Pt
η,I (ϕ |ϕ, e)
Pt
η(ϕ |ϕ)
where Pt
η,I (ϕ |ϕ, e) is the normalisation of Pt
I (ϕ |ϕ, e) for
Sim(ϕ , ϕ) ≥ η, and similarly for Pt
η(ϕ |ϕ). The extent
of this calculation is controlled by the parameter η. An
even tighter restriction may be obtained with: Sim(ϕ , ϕ) ≥
η and ϕ ≤ ψ for some ψ.
5.2 Valuing negotiation dialogues
Suppose that a negotiation commences at time s, and by
time t a string of utterances, Φt
= μ1, . . . , μn has been
exchanged between agent α and agent β. This 
negotiation dialogue is evaluated by α in the context of α"s world
model at time s, Ms
, and the environment e that includes
utterances that may have been received from other agents
in the system including the information sources {θi}. Let
Ψt
= (Φt
, Ms
, e), then α estimates the value of this dialogue
to itself in the context of Ms
and e as a 2 × 5 array Vα(Ψt
)
where:
Vx(Ψt
) =
„
IL
x (Ψt
) IO
x (Ψt
) IG
x (Ψt
) II
x(Ψt
) IC
x (Ψt
)
UL
x (Ψt
) UO
x (Ψt
) UG
x (Ψt
) UI
x(Ψt
) UC
x (Ψt
)
«
where the I(·) and U(·) functions are information-based and
utility-based measures respectively as we now describe. α
estimates the value of this dialogue to β as Vβ(Ψt
) by 
assuming that β"s reasoning apparatus mirrors its own.
In general terms, the information-based valuations 
measure the reduction in uncertainty, or information gain, that
the dialogue gives to each agent, they are expressed in terms
of decrease in entropy that can always be calculated. The
utility-based valuations measure utility gain are expressed in
terms of some suitable utility evaluation function U(·) that
can be difficult to define. This is one reason why the 
utilitarian approach has no natural extension to the management
of argumentation that is achieved here by our 
informationbased approach. For example, if α receives the utterance
Today is Tuesday then this may be translated into a 
constraint on a single distribution, and the resulting decrease
in entropy is the information gain. Attaching a utilitarian
measure to this utterance may not be so simple.
We use the term 2 × 5 array loosely to describe Vα in
that the elements of the array are lists of measures that will
be determined by the agent"s requirements. Table 2 shows
a sample measure for each of the ten categories, in it the
dialogue commences at time s and terminates at time t.
In that Table, U(·) is a suitable utility evaluation function,
needs(β, χ) means agent β needs the need χ, cho(β, χ, γ)
means agent β satisfies need χ by choosing to negotiate
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1035
with agent γ, N is the set of needs chosen from the 
ontology at some suitable level of abstraction, Tt
is the set
of offers on the table at time t, com(β, γ, b) means agent
β has an outstanding commitment with agent γ to execute
the commitment b where b is defined in the ontology at
some suitable level of abstraction, B is the number of such
commitments, and there are n + 1 agents in the system.
5.3 Intimacy and Balance
The balance in a negotiation dialogue, Ψt
, is defined as:
Bαβ(Ψt
) = Vα(Ψt
) Vβ(Ψt
) for an element-by-element 
difference operator that respects the structure of V (Ψt
).
The intimacy between agents α and β, I∗t
αβ, is the pattern
of the two 2 × 5 arrays V ∗t
α and V ∗t
β that are computed by
an update function as each negotiation round terminates,
I∗t
αβ =
`
V ∗t
α , V ∗t
β
´
. If Ψt
terminates at time t:
V ∗t+1
x = ν × Vx(Ψt
) + (1 − ν) × V ∗t
x (5)
where ν is the learning rate, and x = α, β. Additionally,
V ∗t
x continually decays by: V ∗t+1
x = τ × V ∗t
x + (1 − τ) ×
Dx, where x = α, β; τ is the decay rate, and Dx is a 2 ×
5 array being the decay limit distribution for the value to
agent x of the intimacy of the relationship in the absence
of any interaction. Dx is the reputation of agent x. The
relationship balance between agents α and β is: B∗t
αβ = V ∗t
α
V ∗t
β . In particular, the intimacy determines values for the
parameters g and h in Equation 1. As a simple example, if
both IO
α (Ψ∗t
) and IO
β (Ψ∗t
) increase then g decreases, and as
the remaining eight information-based LOGIC components
increase, h increases.
The notion of balance may be applied to pairs of 
utterances by treating them as degenerate dialogues. In simple
multi-issue bargaining the equitable information revelation
strategy generalises the tit-for-tat strategy in single-issue
bargaining, and extends to a tit-for-tat argumentation 
strategy by applying the same principle across the LOGIC 
framework.
6. STRATEGIES AND TACTICS
Each negotiation has to achieve two goals. First it may
be intended to achieve some contractual outcome. Second
it will aim to contribute to the growth, or decline, of the
relationship intimacy.
We now describe in greater detail the contents of the 
Negotiation box in Figure 1. The negotiation literature 
consistently advises that an agent"s behaviour should not be
predictable even in close, intimate relationships. The 
required variation of behaviour is normally described as 
varying the negotiation stance that informally varies from 
friendly guy to tough guy. The stance is shown in Figure 1,
it injects bounded random noise into the process, where the
bound tightens as intimacy increases. The stance, St
αβ, is a
2 × 5 matrix of randomly chosen multipliers, each ≈ 1, that
perturbs α"s actions. The value in the (x, y) position in the
matrix, where x = I, U and y = L, O, G, I, C, is chosen at
random from [ 1
l(I∗t
αβ
,x,y)
, l(I∗t
αβ, x, y)] where l(I∗t
αβ, x, y) is the
bound, and I∗t
αβ is the intimacy.
The negotiation strategy is concerned with maintaining a
working set of Options. If the set of options is empty then
α will quit the negotiation. α perturbs the acceptance 
machinery (see Section 4) by deriving s from the St
αβ matrix
such as the value at the (I, O) position. In line with the
comment in Footnote 7, in the early stages of the 
negotiation α may decide to inflate her opening Options. This is
achieved by increasing the value of s in Equation 1. The 
following strategy uses the machinery described in Section 4.
Fix h, g, s and c, set the Options to the empty set, let
Dt
s = {δ | Pt
(acc(α, β, χ, δ) > c}, then:
• repeat the following as many times as desired: add
δ = arg maxx{Pt
(acc(β, α, x)) | x ∈ Dt
s} to Options,
remove {y ∈ Dt
s | Sim(y, δ) < k} for some k from Dt
s
By using Pt
(acc(β, α, δ)) this strategy reacts to β"s history
of Propose and Reject utterances.
Negotiation tactics are concerned with selecting some 
Options and wrapping them in argumentation. Prior 
interactions with agent β will have produced an intimacy pattern
expressed in the form of
`
V ∗t
α , V ∗t
β
´
. Suppose that the 
relationship target is (T∗t
α , T∗t
β ). Following from Equation 5, α
will want to achieve a negotiation target, Nβ(Ψt
) such that:
ν · Nβ(Ψt
) + (1 − ν) · V ∗t
β is a bit on the T∗t
β side of V ∗t
β :
Nβ(Ψt
) =
ν − κ
ν
V ∗t
β ⊕
κ
ν
T∗t
β (6)
for small κ ∈ [0, ν] that represents α"s desired rate of 
development for her relationship with β. Nβ(Ψt
) is a 2 × 5
matrix containing variations in the LOGIC dimensions that
α would like to reveal to β during Ψt
(e.g. I"ll pass a bit
more information on options than usual, I"ll be stronger
in concessions on options, etc.). It is reasonable to 
expect β to progress towards her target at the same rate and
Nα(Ψt
) is calculated by replacing β by α in Equation 6.
Nα(Ψt
) is what α hopes to receive from β during Ψt
. This
gives a negotiation balance target of: Nα(Ψt
) Nβ(Ψt
) that
can be used as the foundation for reactive tactics by 
striving to maintain this balance across the LOGIC dimensions.
A cautious tactic could use the balance to bound the 
response μ to each utterance μ from β by the constraint:
Vα(μ ) Vβ(μ) ≈ St
αβ ⊗ (Nα(Ψt
) Nβ(Ψt
)), where ⊗ is
element-by-element matrix multiplication, and St
αβ is the
stance. A less neurotic tactic could attempt to achieve the
target negotiation balance over the anticipated complete 
dialogue. If a balance bound requires negative information
revelation in one LOGIC category then α will contribute
nothing to it, and will leave this to the natural decay to the
reputation D as described above.
7. DISCUSSION
In this paper we have introduced a novel approach to 
negotiation that uses information and game-theoretical 
measures grounded on business and psychological studies. It
introduces the concepts of intimacy and balance as key 
elements in understanding what is a negotiation strategy and
tactic. Negotiation is understood as a dialogue that affect
five basic dimensions: Legitimacy, Options, Goals, 
Independence, and Commitment. Each dialogical move produces a
change in a 2×5 matrix that evaluates the dialogue along five
information-based measures and five utility-based measures.
The current Balance and intimacy levels and the desired, or
target, levels are used by the tactics to determine what to
say next. We are currently exploring the use of this model as
an extension of a currently widespread eProcurement 
software commercialised by iSOCO, a spin-off company of the
laboratory of one of the authors.
1036 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
IL
α(Ψt
) =
X
ϕ∈Ψt
Ct
(α, β, ϕ) − Cs
(α, β, ϕ) UL
α (Ψt
) =
X
ϕ∈Ψt
X
ϕ
Pt
β(ϕ |ϕ) × Uα(ϕ )
IO
α (Ψt
) =
P
δ∈T t Hs
(acc(β, α, δ)) −
P
δ∈T t Ht
(acc(β, α, δ))
|Tt|
UO
α (Ψt
) =
X
δ∈T t
Pt
(acc(β, α, δ)) ×
X
δ
Pt
(δ |δ)Uα(δ )
IG
α (Ψt
) =
P
χ∈N Hs
(needs(β, χ)) − Ht
(needs(β, χ))
|N|
UG
α (Ψt
) =
X
χ∈N
Pt
(needs(β, χ)) × Et
(Uα(needs(β, χ)))
II
α(Ψt
) =
Po
i=1
P
χ∈N Hs
(cho(β, χ, βi)) − Ht
(cho(β, χ, βi))
n × |N|
UI
α(Ψt
) =
oX
i=1
X
χ∈N
Ut
(cho(β, χ, βi)) − Us
(cho(β, χ, βi))
IC
α (Ψt
) =
Po
i=1
P
δ∈B Hs
(com(β, βi, b)) − Ht
(com(β, βi, b))
n × |B|
UC
α (Ψt
) =
oX
i=1
X
δ∈B
Ut
(com(β, βi, b)) − Us
(com(β, βi, b))
Table 2: Sample measures for each category in Vα(Ψt
). (Similarly for Vβ(Ψt
).)
Acknowledgements Carles Sierra is partially supported
by the OpenKnowledge European STREP project and by
the Spanish IEA Project.
8. REFERENCES
[1] Adams, J. S. Inequity in social exchange. In Advances
in experimental social psychology, L. Berkowitz, Ed.,
vol. 2. New York: Academic Press, 1965.
[2] Arcos, J. L., Esteva, M., Noriega, P.,
Rodr´ıguez, J. A., and Sierra, C. Environment
engineering for multiagent systems. Journal on
Engineering Applications of Artificial Intelligence 18
(2005).
[3] Bazerman, M. H., Loewenstein, G. F., and
White, S. B. Reversal of preference in allocation
decisions: judging an alternative versus choosing
among alternatives. Administration Science Quarterly,
37 (1992), 220-240.
[4] Brandenburger, A., and Nalebuff, B.
Co-Opetition : A Revolution Mindset That Combines
Competition and Cooperation. Doubleday, New York,
1996.
[5] Cheeseman, P., and Stutz, J. Bayesian Inference
and Maximum Entropy Methods in Science and
Engineering. American Institute of Physics, Melville,
NY, USA, 2004, ch. On The Relationship between
Bayesian and Maximum Entropy Inference, pp. 
445461.
[6] Debenham, J. Bargaining with information. In
Proceedings Third International Conference on
Autonomous Agents and Multi Agent Systems
AAMAS-2004 (July 2004), N. Jennings, C. Sierra,
L. Sonenberg, and M. Tambe, Eds., ACM Press, New
York, pp. 664 - 671.
[7] Fischer, R., Ury, W., and Patton, B. Getting to
Yes: Negotiating agreements without giving in.
Penguin Books, 1995.
[8] Kalfoglou, Y., and Schorlemmer, M. IF-Map:
An ontology-mapping method based on
information-flow theory. In Journal on Data
Semantics I, S. Spaccapietra, S. March, and
K. Aberer, Eds., vol. 2800 of Lecture Notes in
Computer Science. Springer-Verlag: Heidelberg,
Germany, 2003, pp. 98-127.
[9] Lewicki, R. J., Saunders, D. M., and Minton,
J. W. Essentials of Negotiation. McGraw Hill, 2001.
[10] Li, Y., Bandar, Z. A., and McLean, D. An
approach for measuring semantic similarity between
words using multiple information sources. IEEE
Transactions on Knowledge and Data Engineering 15,
4 (July / August 2003), 871 - 882.
[11] MacKay, D. Information Theory, Inference and
Learning Algorithms. Cambridge University Press,
2003.
[12] Paris, J. Common sense and maximum entropy.
Synthese 117, 1 (1999), 75 - 93.
[13] Sierra, C., and Debenham, J. An
information-based model for trust. In Proceedings
Fourth International Conference on Autonomous
Agents and Multi Agent Systems AAMAS-2005
(Utrecht, The Netherlands, July 2005), F. Dignum,
V. Dignum, S. Koenig, S. Kraus, M. Singh, and
M. Wooldridge, Eds., ACM Press, New York, pp. 497
- 504.
[14] Sierra, C., and Debenham, J. Trust and honour in
information-based agency. In Proceedings Fifth
International Conference on Autonomous Agents and
Multi Agent Systems AAMAS-2006 (Hakodate, Japan,
May 2006), P. Stone and G. Weiss, Eds., ACM Press,
New York, pp. 1225 - 1232.
[15] Sierra, C., and Debenham, J. Information-based
agency. In Proceedings of Twentieth International
Joint Conference on Artificial Intelligence IJCAI-07
(Hyderabad, India, January 2007), pp. 1513-1518.
[16] Sondak, H., Neale, M. A., and Pinkley, R. The
negotiated allocations of benefits and burdens: The
impact of outcome valence, contribution, and
relationship. Organizational Behaviour and Human
Decision Processes, 3 (December 1995), 249-260.
[17] Valley, K. L., Neale, M. A., and Mannix, E. A.
Friends, lovers, colleagues, strangers: The effects of
relationships on the process and outcome of
negotiations. In Research in Negotiation in
Organizations, R. Bies, R. Lewicki, and B. Sheppard,
Eds., vol. 5. JAI Press, 1995, pp. 65-94.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1037
Unifying Distributed Constraint Algorithms in a BDI
Negotiation Framework
Bao Chau Le Dinh and Kiam Tian Seow
School of Computer Engineering
Nanyang Technological University
Republic of Singapore
{ledi0002,asktseow}@ntu.edu.sg
ABSTRACT
This paper presents a novel, unified distributed constraint 
satisfaction framework based on automated negotiation. The 
Distributed Constraint Satisfaction Problem (DCSP) is one that 
entails several agents to search for an agreement, which is a 
consistent combination of actions that satisfies their mutual constraints
in a shared environment. By anchoring the DCSP search on
automated negotiation, we show that several well-known DCSP
algorithms are actually mechanisms that can reach agreements
through a common Belief-Desire-Intention (BDI) protocol, but
using different strategies. A major motivation for this BDI 
framework is that it not only provides a conceptually clearer 
understanding of existing DCSP algorithms from an agent model 
perspective, but also opens up the opportunities to extend and 
develop new strategies for DCSP. To this end, a new strategy called
Unsolicited Mutual Advice (UMA) is proposed. Performance 
evaluation shows that the UMA strategy can outperform some 
existing mechanisms in terms of computational cycles.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Intelligent
Agents, Multiagent Systems
General Terms
Algorithms, Design, Experimentation
1. INTRODUCTION
At the core of many emerging distributed applications is the
distributed constraint satisfaction problem (DCSP) - one which
involves finding a consistent combination of actions (abstracted as
domain values) to satisfy the constraints among multiple agents
in a shared environment. Important application examples include
distributed resource allocation [1] and distributed scheduling [2].
Many important algorithms, such as distributed breakout (DBO)
[3], asynchronous backtracking (ABT) [4], asynchronous partial
overlay (APO) [5] and asynchronous weak-commitment (AWC)
[4], have been developed to address the DCSP and provide the
agent solution basis for its applications. Broadly speaking, these
algorithms are based on two different approaches, either 
extending from classical backtracking algorithms [6] or introducing 
mediation among the agents.
While there has been no lack of efforts in this promising 
research field, especially in dealing with outstanding issues such as
resource restrictions (e.g., limits on time and communication) [7]
and privacy requirements [8], there is unfortunately no 
conceptually clear treatment to prise open the model-theoretic workings of
the various agent algorithms that have been developed. As a 
result, for instance, a deeper intellectual understanding on why one
algorithm is better than the other, beyond computational issues,
is not possible.
In this paper, we present a novel, unified distributed constraint
satisfaction framework based on automated negotiation [9]. 
Negotiation is viewed as a process of several agents searching for a
solution called an agreement. The search can be realized via a
negotiation mechanism (or algorithm) by which the agents follow
a high level protocol prescribing the rules of interactions, using
a set of strategies devised to select their own preferences at each
negotiation step.
Anchoring the DCSP search on automated negotiation, we
show in this paper that several well-known DCSP algorithms
[3] are actually mechanisms that share the same 
Belief-DesireIntention (BDI) interaction protocol to reach agreements, but
use different action or value selection strategies. The proposed
framework provides not only a clearer understanding of existing
DCSP algorithms from a unified BDI agent perspective, but also
opens up the opportunities to extend and develop new strategies
for DCSP. To this end, a new strategy called Unsolicited Mutual
Advice (UMA) is proposed. Our performance evaluation shows
that UMA can outperform ABT and AWC in terms of the average
number of computational cycles for both the sparse and critical
coloring problems [6].
The rest of this paper is organized as follows. In Section 2,
we provide a formal overview of DCSP. Section 3 presents a BDI
negotiation model by which a DCSP agent reasons. Section 4
presents the existing algorithms ABT, AWC and DBO as 
different strategies formalized on a common protocol. A new strategy
called Unsolicited Mutual Advice is proposed in Section 5; our
empirical results and discussion attempt to highlight the merits
of the new strategy over existing ones. Section 6 concludes the
paper and points to some future work.
2. DCSP: PROBLEM FORMALIZATION
The DCSP [4] considers the following environment.
• There are n agents with k variables x0, x1, · · · , xk−1, n ≤
k, which have values in domains D1, D2, · · · , Dk, 
respectively. We define a partial function B over the 
productrange {0, 1, . . . , (n−1)}×{0, 1, . . . , (k −1)} such that, that
variable xj belongs to agent i is denoted by B(i, j)!. The
exclamation mark ‘!" means ‘is defined".
• There are m constraints c0, c1, · · · cm−1 to be conjunctively
satisfied. In a similar fashion as defined for B(i, j), we use
E(l, j)!, (0 ≤ l < m, 0 ≤ j < k), to denote that xj is
relevant to the constraint cl.
The DCSP may be formally stated as follows.
Problem Statement: ∀i, j (0 ≤ i < n)(0 ≤ j < k) where
B(i, j)!, find the assignment xj = dj ∈ Dj such that ∀l (0 ≤ l <
m) where E(l, j)!, cl is satisfied.
A constraint may consist of different variables belonging to
different agents. An agent cannot change or modify the 
assignment values of other agents" variables. Therefore, in 
cooperatively searching for a DCSP solution, the agents would need to
communicate with one another, and adjust and re-adjust their
own variable assignments in the process.
2.1 DCSP Agent Model
In general, all DCSP agents must cooperatively interact, and
essentially perform the assignment and reassignment of domain
values to variables to resolve all constraint violations. If the
agents succeed in their resolution, a solution is found.
In order to engage in cooperative behavior, a DCSP agent needs
five fundamental parameters, namely, (i) a variable [4] or a 
variable set [10], (ii) domains, (iii) priority, (iv) a neighbor list and
(v) a constraint list.
Each variable assumes a range of values called a domain. A
domain value, which usually abstracts an action, is a possible 
option that an agent may take. Each agent has an assigned priority.
These priority values help decide the order in which they revise
or modify their variable assignments. An agent"s priority may be
fixed (static) or changing (dynamic) when searching for a 
solution. If an agent has more than one variable, each variable can
be assigned a different priority, to help determine which variable
assignment the agent should modify first.
An agent which shares the same constraint with another agent
is called the latter"s neighbor. Each agent needs to refer to its list
of neighbors during the search process. This list may also be kept
unchanged or updated accordingly in runtime. Similarly, each
agent maintains a constraint list. The agent needs to ensure that
there is no violation of the constraints in this list. Constraints can
be added or removed from an agent"s constraint list in runtime.
As with an agent, a constraint can also be associated with a
priority value. Constraints with a high priority are said to be
more important than constraints with a lower priority. To 
distinguish it from the priority of an agent, the priority of a constraint
is called its weight.
3. THE BDI NEGOTIATION MODEL
The BDI model originates with the work of M. Bratman [11].
According to [12, Ch.1], the BDI architecture is based on a 
philosophical model of human practical reasoning, and draws out the
process of reasoning by which an agent decides which actions to
perform at consecutive moments when pursuing certain goals.
Grounding the scope to the DCSP framework, the common goal
of all agents is finding a combination of domain values to satisfy a
set of predefined constraints. In automated negotiation [9], such
a solution is called an agreement among the agents. Within this
scope, we found that we were able to unearth the generic behavior
of a DCSP agent and formulate it in a negotiation protocol, 
prescribed using the powerful concepts of BDI. Thus, our proposed
negotiation model can be said to combine the BDI concepts with
automated negotiation in a multiagent framework, allowing us
to conceptually separate DCSP mechanisms into a common BDI
interaction protocol and the adopted strategies.
3.1 The generic protocol
Figure 1 shows the basic reasoning steps in an arbitrary round
of negotiation that constitute the new protocol. The solid line
indicates the common component or transition which always 
exists regardless of the strategy used. The dotted line indicates the
Percept
Belief
Desire
Intention
Mediation
Execution
P
B
D
I
I
I
Info Message
Info Message
Negotiation Message
Negotiation Message
Negotiation Message
Negotiation Message
Negotiation Message
Negotiation Message
Negotiation Message
Figure 1: The BDI interaction protocol
component or transition which may or may not appear depending
on the adopted strategy.
Two types of messages are exchanged through this protocol,
namely, the info message and the negotiation message.
An info message perceived is a message sent by another agent.
The message will contain the current selected values and priorities
of the variables of that sending agent. The main purpose of this
message is to update the agent about the current environment.
Info message is sent out at the end of one negotiation round (also
called a negotiation cycle), and received at the beginning of next
round.
A negotiation message is a message which may be sent within
a round. This message is for mediation purposes. The agent may
put different contents into this type of message as long as it is
agreed among the group. The format of the negotiation message
and when it is to be sent out are subject to the strategy. A
negotiation message can be sent out at the end of one reasoning
step and received at the beginning of the next step.
Mediation is a step of the protocol that depends on whether the
agent"s interaction with others is synchronous or asynchronous.
In synchronous mechanism, mediation is required in every 
negotiation round. In an asynchronous one, mediation is needed only in
a negotiation round when the agent receives a negotiation 
message. A more in-depth view of this mediation step is provided
later in this section.
The BDI protocol prescribes the skeletal structure for DCSP
negotiation. We will show in Section 4 that several well-known
DCSP mechanisms all inherit this generic model.
The details of the six main reasoning steps for the protocol
(see Figure 1) are described as follows for a DCSP agent. For a
conceptually clearer description, we assume that there is only one
variable per agent.
• Percept. In this step, the agent receives info messages
from its neighbors in the environment, and using its Percept
function, returns an image P. This image contains the
current values assigned to the variables of all agents in its
neighbor list. The image P will drive the agent"s actions
in subsequent steps. The agent also updates its constraint
list C using some criteria of the adopted strategy.
• Belief. Using the image P and constraint list C, the agent
will check if there is any violated constraint. If there is
no violation, the agent will believe it is choosing a correct
option and therefore will take no action. The agent will
do nothing if it is in a local stable state - a snapshot of
the variables assignments of the agent and all its neighbors
by which they satisfy their shared constraints. When all
agents are in their local stable states, the whole 
environment is said to be in a global stable state and an 
agreeThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 525
ment is found. In case the agent finds its value in conflict
with some of its neighbors", i.e., the combination of values
assigned to the variables leads to a constraint violation,
the agent will first try to reassign its own variable using a
specific strategy. If it finds a suitable option which meets
some criteria of the adopted strategy, the agent will believe
it should change to the new option. However it does not
always happen that an agent can successfully find such an
option. If no option can be found, the agent will believe it
has no option, and therefore will request its neighbors to
reconsider their variable assignments.
To summarize, there are three types of beliefs that a DCSP
agent can form: (i) it can change its variable assignment to
improve the current situation, (ii) it cannot change its 
variable assignment and some constraints violations cannot be
resolved and (iii) it need not change its variable assignment
as all the constraints are satisfied.
Once the beliefs are formed, the agent will determine its
desires, which are the options that attempt to resolve the
current constraint violations.
• Desire. If the agent takes Belief (i), it will generate a list of
its own suitable domain values as its desire set. If the agent
takes Belief (ii), it cannot ascertain its desire set, but will
generate a sublist of agents from its neighbor list, whom it
will ask to reconsider their variable assignments. How this
sublist is created depends on the strategy devised for the
agent. In this situation, the agent will use a virtual desire
set that it determines based on its adopted strategy. If the
agent takes Belief (iii), it will have no desire to revise its
domain value, and hence no intention.
• Intention. The agent will select a value from its desire
set as its intention. An intention is the best desired 
option that the agent assigns to its variable. The criteria for
selecting a desire as the agent"s intention depend on the
strategy used. Once the intention is formed, the agent may
either proceed to the execution step, or undergo mediation.
Again, the decision to do so is determined by some criteria
of the adopted strategy.
• Mediation. This is an important function of the agent.
Since, if the agent executes its intention without 
performing intention mediation with its neighbors, the constraint
violation between the agents may not be resolved. Take
for example, suppose two agents have variables, x1 and x2,
associated with the same domain {1, 2}, and their shared
constraint is (x1 + x2 = 3). Then if both the variables are
initialized with value 1, they will both concurrently switch
between the values 2 and 1 in the absence of mediation
between them.
There are two types of mediation: local mediation and
group mediation. In the former, the agents exchange their
intentions. When an agent receives another"s intention
which conflicts with its own, the agent must mediate 
between the intentions, by either changing its own intention
or informing the other agent to change its intention. In the
latter, there is an agent which acts as a group mediator.
This mediator will collect the intentions from the group - a
union of the agent and its neighbors - and determine which
intention is to be executed. The result of this mediation is
passed back to the agents in the group. Following 
mediation, the agent may proceed to the next reasoning step to
execute its intention or begin a new negotiation round.
• Execution. This is the last step of a negotiation round.
The agent will execute by updating its variable assignment
if the intention obtained at this step is its own. Following
execution, the agent will inform its neighbors about its new
variable assignment and updated priority. To do so, the
agent will send out an info message.
3.2 The strategy
A strategy plays an important role in the negotiation process.
Within the protocol, it will often determine the efficiency of the
Percept
Belief
Desire
Intention
Mediation
Execution
P
B
D
I
Info Message
Info Message
Negotiation Message
Negotiation Message
Negotiation Message
Figure 2: BDI protocol with Asynchronous 
Backtracking strategy
search process in terms of computational cycles and message 
communication costs.
The design space when devising a strategy is influenced by the
following dimensions: (i) asynchronous or synchronous, (ii) 
dynamic or static priority, (iii) dynamic or static constraint weight,
(iv) number of negotiation messages to be communicated, (v) the
negotiation message format and (vi) the completeness property.
In other words, these dimensions provide technical considerations
for a strategy design.
4. DCSP ALGORITHMS: BDI PROTOCOL
+ STRATEGIES
In this section, we apply the proposed BDI negotiation model
presented in Section 3 to expose the BDI protocol and the 
different strategies used for three well-known algorithms, ABT, AWC
and DBO. All these algorithms assume that there is only one
variable per agent. Under our framework, we call the strategies
applied the ABT, AWC and DBO strategies, respectively.
To describe each strategy formally, the following mathematical
notations are used:
• n is the number of agents, m is the number of constraints;
• xi denotes the variable held by agent i, (0 ≤ i < n);
• Di denotes the domain of variable xi; Fi denotes the 
neighbor list of agent i; Ci denotes its constraint list;
• pi denotes the priority of agent i; and Pi = {(xj = vj, pj =
k) | agent j ∈ Fi, vj ∈ Dj is the current value assigned
to xj and the priority value k is a positive integer } is the
perception of agent i;
• wl denotes the weight of constraint l, (0 ≤ l < m);
• Si(v) is the total weight of the violated constraints in Ci
when its variable has the value v ∈ Di.
4.1 Asynchronous Backtracking
Figure 2 presents the BDI negotiation model incorporating the
Asynchronous Backtracking (ABT) strategy. As mentioned in
Section 3, for an asynchronous mechanism that ABT is, the 
mediation step is needed only in a negotiation round when an agent
receives a negotiation message.
For agent i, beginning initially with (wl = 1, (0 ≤ l < m);
pi = i, (0 ≤ i < n)) and Fi contains all the agents who share the
constraints with agent i, its BDI-driven ABT strategy is described
as follows.
Step 1 - Percept: Update Pi upon receiving the info 
messages from the neighbors (in Fi). Update Ci to be the list of
526 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
constraints which only consists of agents in Fi that have equal or
higher priority than this agent.
Step 2 - Belief: The belief function GB (Pi,Ci) will return a
value bi ∈ {0, 1, 2}, decided as follows:
• bi = 0 when agent i can find an optimal option, i.e., if
(Si(vi) = 0 or vi is in bad values list) and (∃a ∈ Di)(Si(a) =
0) and a is not in a list of domain values called bad values
list. Initially this list is empty and it will be cleared when a
neighbor of higher priority changes its variable assignment.
• bi = 1 when it cannot find an optimal option, i.e., if (∀a ∈
Di)(Si(a) = 0) or a is in bad values list.
• bi = 2 when its current variable assignment is an optimal
option, i.e., if Si(vi) = 0 and vi is not in bad value list.
Step 3 - Desire: The desire function GD (bi) will return a
desire set denoted by DS, decided as follows:
• If bi = 0, then DS = {a | (a = vi), (Si(a) = 0) and a is not
in the bad value list }.
• If bi = 1, then DS = ∅, the agent also finds agent k which
is determined by {k | pk = min(pj) with agent j ∈ Fi and
pk > pi }.
• If bi = 2, then DS = ∅.
Step 4 - Intention: The intention function GI (DS) will
return an intention, decided as follows:
• If DS = ∅, then select an arbitrary value (say, vi) from DS
as the intention.
• If DS = ∅, then assign nil as the intention (to denote its
lack thereof).
Step 5 - Execution:
• If agent i has a domain value as its intention, the agent will
update its variable assignment with this value.
• If bi = 1, agent i will send a negotiation message to agent
k, then remove k from Fi and begin its next negotiation
round. The negotiation message will contain the list of
variable assignments of those agents in its neighbor list Fi
that have a higher priority than agent i in the current image
Pi.
Mediation: When agent i receives a negotiation message, 
several sub-steps are carried out, as follows:
• If the list of agents associated with the negotiation message
contains agents which are not in Fi, it will add these agents
to Fi, and request these agents to add itself to their 
neighbor lists. The request is considered as a type of negotiation
message.
• Agent i will first check if the sender agent is updated with
its current value vi. The agent will add vi to its bad values
list if it is so, or otherwise send its current value to the
sender agent.
Following this step, agent i proceeds to the next negotiation
round.
4.2 Asynchronous Weak Commitment Search
Figure 3 presents the BDI negotiation model incorporating the
Asynchronous Weak Commitment (AWC) strategy. The model is
similar to that of incorporating the ABT strategy (see Figure 2).
This is not surprising; AWC and ABT are found to be 
strategically similar, differing only in the details of some reasoning steps.
The distinguishing point of AWC is that when the agent cannot
find a suitable variable assignment, it will change its priority to
the highest among its group members ({i} ∪ Fi).
For agent i, beginning initially with (wl = 1, (0 ≤ l < m);
pi = i, (0 ≤ i < n)) and Fi contains all the agents who share
the constraints with agent i, its BDI-driven AWC strategy is 
described as follows.
Step 1 - Percept: This step is identical to the Percept step
of ABT.
Step 2 - Belief: The belief function GB (Pi,Ci) will return a
value bi ∈ {0, 1, 2}, decided as follows:
Percept
Belief
Desire
Intention
Mediation
Execution
P
B
D
I
Info Message
Info Message
Negotiation Message
Negotiation Message
Negotiation Message
Figure 3: BDI protocol with Asynchronous 
WeakCommitment strategy
• bi = 0 when the agent can find an optimal option i.e., if
(Si(vi) = 0 or the assignment xi = vi and the current 
variables assignments of the neighbors in Fi who have higher
priority form a nogood [4]) stored in a list called nogood list
and ∃a ∈ Di, Si(a) = 0 (initially the list is empty).
• bi = 1 when the agent cannot find any optimal option i.e.,
if ∀a ∈ Di, Si(a) = 0.
• bi = 2 when the current assignment is an optimal option
i.e., if Si(vi) = 0 and the current state is not a nogood in
nogood list.
Step 3 - Desire: The desire function GD (bi) will return a
desire set DS, decided as follows:
• If bi = 0, then DS = {a | (a = vi), (Si(a) = 0) and the
number of constraint violations with lower priority agents
is minimized }.
• If bi = 1, then DS = {a | a ∈ Di and the number of
violations of all relevant constraints is minimized }.
• If bi = 2, then DS = ∅.
Following, if bi = 1, agent i will find a list Ki of higher priority
neighbors, defined by Ki = {k | agent k ∈ Fi and pk > pi}.
Step 4 - Intention: This step is similar to the Intention step
of ABT. However, for this strategy, the negotiation message will
contain the variable assignments (of the current image Pi) for
all the agents in Ki. This list of assignment is considered as
a nogood. If the same negotiation message had been sent out
before, agent i will have nil intention. Otherwise, the agent will
send the message and save the nogood in the nogood list.
Step 5 - Execution:
• If agent i has a domain value as its intention, the agent will
update its variable assignment with this value.
• If bi = 1, it will send the negotiation message to its 
neighbors in Ki, and set pi = max{pj} + 1, with agent j ∈ Fi.
Mediation: This step is identical to the Mediation step of
ABT, except that agent i will now add the nogood contained in
the negotiation message received to its own nogood list.
4.3 Distributed Breakout
Figure 4 presents the BDI negotiation model incorporating the
Distributed Breakout (DBO) strategy. Essentially, by this 
synchronous strategy, each agent will search iteratively for 
improvement by reducing the total weight of the violated constraints.
The iteration will continue until no agent can improve further,
at which time if some constraints remain violated, the weights of
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 527
Percept
Belief
Desire
Intention
Mediation
Execution
P
B
D
I
I
A
Info Message
Info Message
Negotiation Message
Negotiation Message
Figure 4: BDI protocol with Distributed Breakout
strategy
these constraints will be increased by 1 to help ‘breakout" from a
local minimum.
For agent i, beginning initially with (wl = 1, (0 ≤ l < m),
pi = i, (0 ≤ i < n)) and Fi contains all the agents who share the
constraints with agent i, its BDI-driven DBO strategy is described
as follows.
Step 1 - Percept: Update Pi upon receiving the info 
messages from the neighbors (in Fi). Update Ci to be the list of its
relevant constraints.
Step 2 - Belief: The belief function GB (Pi,Ci) will return a
value bi ∈ {0, 1, 2}, decided as follows:
• bi = 0 when agent i can find an option to reduce the number
violations of the constraints in Ci, i.e., if ∃a ∈ Di, Si(a) <
Si(vi).
• bi = 1 when it cannot find any option to improve situation,
i.e., if ∀a ∈ Di, a = vi, Si(a) ≥ Si(vi).
• bi = 2 when its current assignment is an optimal option,
i.e., if Si(vi) = 0.
Step 3 - Desire: The desire function GD (bi) will return a
desire set DS, decided as follows:
• If bi = 0, then DS = {a | a = vi, Si(a) < Si(vi) and
(Si(vi)−Si(a)) is maximized }. (max{(Si(vi)−Si(a))} will
be referenced by hmax
i in subsequent steps, and it defines
the maximal reduction in constraint violations).
• Otherwise, DS = ∅.
Step 4 - Intention: The intention function GI (DS) will
return an intention, decided as follows:
• If DS = ∅, then select an arbitrary value (say, vi) from DS
as the intention.
• If DS = ∅, then assign nil as the intention.
Following, agent i will send its intention to all its neighbors.
In return, it will receive intentions from these agents before 
proceeding to Mediation step.
Mediation: Agent i receives all the intentions from its 
neighbors. If it finds that the intention received from a neighbor agent
j is associated with hmax
j > hmax
i , the agent will automatically
cancel its current intention.
Step 5 - Execution:
• If agent i did not cancel its intention, it will update its
variable assignment with the intended value.
Percept
Belief
Desire
Intention
Mediation
Execution
P
B
D
I
I
A
Info Message
Info Message
Negotiation Message
Negotiation Message
Negotiation Message
Negotiation Message
Figure 5: BDI protocol with Unsolicited Mutual 
Advice strategy
• If all intentions received and its own one are nil intention,
the agent will increase the weight of each currently violated
constraint by 1.
5. THE UMA STRATEGY
Figure 5 presents the BDI negotiation model incorporating the
Unsolicited Mutual Advice(UMA) strategy.
Unlike when using the strategies of the previous section, a
DCSP agent using UMA will not only send out a negotiation
message when concluding its Intention step, but also when 
concluding its Desire step. The negotiation message that it sends out
to conclude the Desire step constitutes an unsolicited advice for
all its neighbors. In turn, the agent will wait to receive unsolicited
advices from all its neighbors, before proceeding on to determine
its intention.
For agent i, beginning initially with (wl = 1, (0 ≤ l < m),
pi = i, (0 ≤ i < n)) and Fi contains all the agents who share
the constraints with agent i, its BDI-driven UMA strategy is 
described as follows.
Step 1 - Percept: Update Pi upon receiving the info 
messages from the neighbors (in Fi). Update Ci to be the list of
constraints relevant to agent i.
Step 2 - Belief: The belief function GB (Pi,Ci) will return a
value bi ∈ {0, 1, 2}, decided as follows:
• bi = 0 when agent i can find an option to reduce the number
violations of the constraints in Ci, i.e., if ∃a ∈ Di, Si(a) <
Si(vi) and the assignment xi = a and the current variable
assignments of its neighbors do not form a local state stored
in a list called bad states list (initially this list is empty).
• bi = 1 when it cannot find a value a such as a ∈ Di, Si(a) <
Si(vi), and the assignment xi = a and the current variable
assignments of its neighbors do not form a local state stored
in the bad states list.
• bi = 2 when its current assignment is an optimal option,
i.e., if Si(vi) = 0.
Step 3 - Desire: The desire function GD (bi) will return a
desire set DS, decided as follows:
• If bi = 0, then DS = {a | a = vi, Si(a) < Si(vi) and
(Si(vi) − Si(a)) is maximized } and the assignment xi = a
and the current variable assignments of agent i"s neighbors
do not form a state in the bad states list. In this case, DS is
called a set of voluntary desires. max{(Si(vi)−Si(a))} will
be referenced by hmax
i in subsequent steps, and it defines
528 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
the maximal reduction in constraint violations. It is also
referred to as an improvement).
• If bi = 1, then DS = {a | a = vi, Si(a) is minimized } and
the assignment xi = a and the current variable assignments
of agent i"s neighbors do not form a state in the bad states
list. In this case, DS is called a set of reluctant desires
• If bi = 2, then DS = ∅.
Following, if bi = 0, agent i will send a negotiation message
containing hmax
i to all its neighbors. This message is called a
voluntary advice. If bi = 1, agent i will send a negotiation message
called change advice to the neighbors in Fi who share the violated
constraints with agent i.
Agent i receives advices from all its neighbors and stores them
in a list called A, before proceeding to the next step.
Step 4 - Intention: The intention function GI (DS, A) will
return an intention, decided as follows:
• If there is a voluntary advice from an agent j which is
associated with hmax
j > hmax
i , assign nil as the intention.
• If DS = ∅, DS is a set of voluntary desires and hmax
i is
the biggest improvement among those associated with the
voluntary advices received, select an arbitrary value (say,
vi) from DS as the intention. This intention is called a
voluntary intention.
• If DS = ∅, DS is a set of reluctant desires and agent i 
receives some change advices, select an arbitrary value (say,
vi) from DS as the intention. This intention is called 
reluctant intention.
• If DS = ∅, then assign nil as the intention.
Following, if the improvement hmax
i is the biggest improvement
and equal to some improvements associated with the received
voluntary advices, agent i will send its computed intention to all
its neighbors. If agent i has a reluctant intention, it will also
send this intention to all its neighbors. In both cases, agent i
will attach the number of received change advices in the current
negotiation round with its intention. In return, agent i will receive
the intentions from its neighbors before proceeding to Mediation
step.
Mediation: If agent i does not send out its intention before
this step, i.e., the agent has either a nil intention or a voluntary
intention with biggest improvement, it will proceed to next step.
Otherwise, agent i will select the best intention among all the
intentions received, including its own (if any). The criteria to
select the best intention are listed, applied in descending order of
importance as follows.
• A voluntary intention is preferred over a reluctant intention.
• A voluntary intention (if any) with biggest improvement is
selected.
• If there is no voluntary intention, the reluctant intention
with the lowest number of constraint violations is selected.
• The intention from an agent who has received a higher 
number of change advices in the current negotiation round is
selected.
• Intention from an agent with highest priority is selected.
If the selected intention is not agent i"s intention, it will cancel
its intention.
Step 5 - Execution: If agent i does not cancel its intention,
it will update its variable assignment with the intended value.
Termination Condition: Since each agent does not have
full information about the global state, it may not know when it
has reached a solution, i.e., when all the agents are in a global
stable state. Hence an observer is needed that will keep track
of the negotiation messages communicated in the environment.
Following a certain period of time when there is no more message
communication (and this happens when all the agents have no
more intention to update their variable assignments), the observer
will inform the agents in the environment that a solution has been
found.
1
2
3
4
5
6 7
8
9
10
Figure 6: Example problem
5.1 An Example
To illustrate how UMA works, consider a 2-color graph problem
[6] as shown in Figure 6. In this example, each agent has a color
variable representing a node. There are 10 color variables sharing
the same domain {Black, White}.
The following records the outcome of each step in every 
negotiation round executed.
Round 1:
Step 1 - Percept: Each agent obtains the current color 
assignments of those nodes (agents) adjacent to it, i.e., its 
neighbors".
Step 2 - Belief: Agents which have positive improvements are
agent 1 (this agent believes it should change its color to
White), agent 2 (this believes should change its color to
White), agent 7 (this agent believes it should change its
color to Black) and agent 10 (this agent believes it should
change its value to Black). In this negotiation round, the
improvements achieved by these agents are 1. Agents which
do not have any improvements are agents 4, 5 and 8. Agents
3, 6 and 9 need not change as all their relevant constraints
are satisfied.
Step 3 - Desire: Agents 1, 2, 7 and 10 have the voluntary desire
(White color for agents 1, 2 and Black color for agents 7,
10). These agents will send the voluntary advices to all
their neighbors. Meanwhile, agents 4, 5 and 8 have the
reluctant desires (White color for agent 4 and Black color
for agents 5, 8). Agent 4 will send a change advice to
agent 2 as agent 2 is sharing the violated constraint with
it. Similarly, agents 5 and 8 will send change advices to
agents 7 and 10 respectively. Agents 3, 6 and 9 do not have
any desire to update their color assignments.
Step 4 - Intention: Agents 2, 7 and 10 receive the change 
advices from agents 4, 5 and 8, respectively. They form their
voluntary intentions. Agents 4, 5 and 8 receive the 
voluntary advices from agents 2, 7 and 10, hence they will not
have any intention. Agents 3, 6 and 9 do not have any
intention. Following, the intention from the agents will be
sent to all their neighbors.
Mediation: Agent 1 finds that the intention from agent 2 is
better than its intention. This is because, although both
agents have voluntary intentions with improvement of 1,
agent 2 has received one change advice from agent 4 while
agent 1 has not received any. Hence agent 1 cancels its
intention. Agent 2 will keep its intention.
Agents 7 and 10 keep their intentions since none of their
neighbors has an intention.
The rest of the agents do nothing in this step as they do
not have any intention.
Step 5 - Execution: Agent 2 changes its color to White. Agents
7 and 10 change their colors to Black.
The new state after round 1 is shown in Figure 7.
Round 2:
Step 1 - Percept: The agents obtain the current color 
assignments of their neighbors.
Step 2 - Belief: Agent 3 is the only agent who has a positive
improvement which is 1. It believes it should change its
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 529
1
2
3
4
5
6 7
8
9
10
Figure 7: The graph after round 1
color to Black. Agent 2 does not have any positive 
improvement. The rest of the agents need not make any change as
all their relevant constraints are satisfied. They will have
no desire, and hence no intention.
Step 3 - Desire: Agent 3 desires to change its color to Black
voluntarily, hence it sends out a voluntary advice to its
neighbor, i.e., agent 2. Agent 2 does not have any value for
its reluctant desire set as the only option, Black color, will
bring agent 2 and its neighbors to the previous state which
is known to be a bad state. Since agent 2 is sharing the
constraint violation with agent 3, it sends a change advice
to agent 3.
Step 4 - Intention: Agent 3 will have a voluntary intention
while agent 2 will not have any intention as it receives the
voluntary advice from agent 3.
Mediation: Agent 3 will keep its intention as its only neighbor,
agent 2, does not have any intention.
Step 5 - Execution: Agent 3 changes its color to Black.
The new state after round 2 is shown in Figure 8.
Round 3: In this round, every agent finds that it has no
desire and hence no intention to revise its variable assignment.
Following, with no more negotiation message communication in
the environment, the observer will inform all the agents that a
solution has been found.
2
3
4
5
6 7
8
91
10
Figure 8: The solution obtained
5.2 Performance Evaluation
To facilitate credible comparisons with existing strategies, we
measured the execution time in terms of computational cycles
as defined in [4], and built a simulator that could reproduce the
published results for ABT and AWC. The definition of a 
computational cycle is as follows.
• In one cycle, each agent receives all the incoming messages,
performs local computation and sends out a reply.
• A message which is sent at time t will be received at time
t + 1. The network delay is neglected.
• Each agent has it own clock. The initial clock"s value is
0. Agents attach their clock value as a time-stamp in the
outgoing message and use the time-stamp in the incoming
message to update their own clock"s value.
Four benchmark problems [6] were considered, namely, n-queens
and node coloring for sparse, dense and critical graphs. For each
problem, a finite number of test cases were generated for 
various problem sizes n. The maximum execution time was set to
0
200
400
600
800
1000
10 50 100
Number of queens
Cycles
Asynchronous
Backtracking
Asynchronous Weak
Commitment
Unsolicited Mutual
Advice
Figure 9: Relationship between execution time and
problem size
10000 cycles for node coloring for critical graphs and 1000 cycles
for other problems. The simulator program was terminated after
this period and the algorithm was considered to fail a test case if
it did not find a solution by then. In such a case, the execution
time for the test was counted as 1000 cycles.
5.2.1 Evaluation with n-queens problem
The n-queens problem is a traditional problem of constraint
satisfaction. 10 test cases were generated for each problem size
n ∈ {10, 50 and 100}.
Figure 9 shows the execution time for different problem sizes
when ABT, AWC and UMA were run.
5.2.2 Evaluation with graph coloring problem
The graph coloring problem can be characterized by three 
parameters: (i) the number of colors k, the number of nodes/agents
n and the number of links m. Based on the ratio m/n, the
problem can be classified into three types [3]: (i) sparse (with
m/n = 2), (ii) critical (with m/n = 2.7 or 4.7) and (iii) dense
(with m/n = (n − 1)/4). For this problem, we did not include
ABT in our empirical results as its failure rate was found to be
very high. This poor performance of ABT was expected since
the graph coloring problem is more difficult than the n-queens
problem, on which ABT already did not perform well (see Figure
9).
The sparse and dense (coloring) problem types are relatively
easy while the critical type is difficult to solve. In the 
experiments, we fix k = 3. 10 test cases were created using the method
described in [13] for each value of n ∈ {60, 90, 120}, for each 
problem type.
The simulation results for each type of problem are shown in
Figures 10 - 12.
0
40
80
120
160
200
60 90 120 150
Number of Nodes
Cycles
Asynchronous
Weak
Commitment
Unsolicited
Mutual Advice
Figure 10: Comparison between AWC and UMA
(sparse graph coloring)
5.3 Discussion
5.3.1 Comparison with ABT and AWC
530 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
0
1000
2000
3000
4000
5000
6000
60 90 120
Number of Nodes
Cycles
Asynchronous
Weak
Commitment
Unsolicited
Mutual Advice
Figure 11: Comparison between AWC and UMA
(critical graph coloring)
0
10
20
30
40
50
60 90 120
Number of Nodes
Cycles
Asynchronous
Weak
Commitment
Unsolicited
Mutual Advice
Figure 12: Comparison between AWC and UMA
(dense graph coloring)
Figure 10 shows that the average performance of UMA is slightly
better than AWC for the sparse problem. UMA outperforms
AWC in solving the critical problem as shown in Figure 11. It
was observed that the latter strategy failed in some test cases.
However, as seen in Figure 12, both the strategies are very 
efficient when solving the dense problem, with AWC showing slightly
better performance.
The performance of UMA, in the worst (time complexity) case,
is similar to that of all evaluated strategies. The worst case 
occurs when all the possible global states of the search are reached.
Since only a few agents have the right to change their variable 
assignments in a negotiation round, the number of redundant 
computational cycles and info messages is reduced. As we observe
from the backtracking in ABT and AWC, the difference in the
ordering of incoming messages can result in a different number of
computational cycles to be executed by the agents.
5.3.2 Comparison with DBO
The computational performance of UMA is arguably better
than DBO for the following reasons:
• UMA can guarantee that there will be a variable 
reassignment following every negotiation round whereas DBO 
cannot.
• UMA introduces one more communication round trip (that
of sending a message and awaiting a reply) than DBO,
which occurs due to the need to communicate unsolicited
advices. Although this increases the communication cost
per negotiation round, we observed from our simulations
that the overall communication cost incurred by UMA is
lower due to the significantly lower number of negotiation
rounds.
• Using UMA, in the worst case, an agent will only take 2 or 3
communication round trips per negotiation round, following
which the agent or its neighbor will do a variable 
assignment update. Using DBO, this number of round trips is
uncertain as each agent might have to increase the weights
of the violated constraints until an agent has a positive 
improvement; this could result in a infinite loop [3].
6. CONCLUSION
Applying automated negotiation to DCSP, this paper has 
proposed a protocol that prescribes the generic reasoning of a DCSP
agent in a BDI architecture. Our work shows that several 
wellknown DCSP algorithms, namely ABT, AWC and DBO, can be
described as mechanisms sharing the same proposed protocol, and
only differ in the strategies employed for the reasoning steps per
negotiation round as governed by the protocol. Importantly, this
means that it might furnish a unified framework for DCSP that
not only provides a clearer BDI agent-theoretic view of existing
DCSP approaches, but also opens up the opportunities to enhance
or develop new strategies. Towards the latter, we have proposed
and formulated a new strategy - the UMA strategy. Empirical
results and our discussion suggest that UMA is superior to ABT,
AWC and DBO in some specific aspects.
It was observed from our simulations that UMA possesses the
completeness property. Future work will attempt to formally 
establish this property, as well as formalize other existing DSCP
algorithms as BDI negotiation mechanisms, including the recent
endeavor that employs a group mediator [5]. The idea of DCSP
agents using different strategies in the same environment will also
be investigated.
7. REFERENCES
[1] P. J. Modi, H. Jung, M. Tambe, W.-M. Shen, and
S. Kulkarni, Dynamic distributed resource allocation: A
distributed constraint satisfaction approach, in Lecture
Notes in Computer Science, 2001, p. 264.
[2] H. Schlenker and U. Geske, Simulating large railway
networks using distributed constraint satisfaction, in 2nd
IEEE International Conference on Industrial Informatics
(INDIN-04), 2004, pp. 441- 446.
[3] M. Yokoo, Distributed Constraint Satisfaction :
Foundations of Cooperation in Multi-Agent Systems.
Springer Verlag, 2000, springer Series on Agent Technology.
[4] M. Yokoo, E. H. Durfee, T. Ishida, and K. Kuwabara, The
distributed constraint satisfaction problem : Formalization
and algorithms, IEEE Transactions on Knowledge and
Data Engineering, vol. 10, no. 5, pp. 673-685,
September/October 1998.
[5] R. Mailler and V. Lesser, Using cooperative mediation to
solve distributed constraint satisfaction problems, in
Proceedings of the Third International Joint Conference on
Autonomous Agents and Multiagent Systems
(AAMAS-04), 2004, pp. 446-453.
[6] E. Tsang, Foundation of Constraint Satisfaction.
Academic Press, 1993.
[7] R. Mailler, R. Vincent, V. Lesser, T. Middlekoop, and
J. Shen, Soft Real-Time, Cooperative Negotiation for
Distributed Resource Allocation, AAAI Fall Symposium
on Negotiation Methods for Autonomous Cooperative
Systems, November 2001.
[8] M. Yokoo, K. Suzuki, and K. Hirayama, Secure
distributed constraint satisfaction: Reaching agreement
without revealing private information, Artificial
Intelligence, vol. 161, no. 1-2, pp. 229-246, 2005.
[9] J. S. Rosenschein and G. Zlotkin, Rules of Encounter.
The MIT Press, 1994.
[10] M. Yokoo and K. Hirayama, Distributed constraint
satisfaction algorithm for complex local problems, in
Proceedings of the Third International Conference on
Multiagent Systems (ICMAS-98), 1998, pp. 372-379.
[11] M. E. Bratman, Intentions, Plans and Practical Reason.
Harvard University Press, Cambridge, M.A, 1987.
[12] G. Weiss, Ed., Multiagent System : A Modern Approach to
Distributed Artificial Intelligence. The MIT Press,
London, U.K, 1999.
[13] S. Minton, M. D. Johnson, A. B. Philips, and P. Laird,
Minimizing conflicts: A heuristic repair method for
constraint satisfaction and scheduling problems, Artificial
Intelligence, vol. e58, no. 1-3, pp. 161-205, 1992.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 531
Bidding Algorithms for a Distributed Combinatorial Auction
Benito Mendoza
∗
and Jos´e M. Vidal
Computer Science and Engineering
University of South Carolina
Columbia, SC 29208
mendoza2@engr.sc.edu, vidal@sc.edu
ABSTRACT
Distributed allocation and multiagent coordination 
problems can be solved through combinatorial auctions. 
However, most of the existing winner determination algorithms
for combinatorial auctions are centralized. The PAUSE 
auction is one of a few efforts to release the auctioneer from
having to do all the work (it might even be possible to get
rid of the auctioneer). It is an increasing price 
combinatorial auction that naturally distributes the problem of 
winner determination amongst the bidders in such a way that
they have an incentive to perform the calculation. It can
be used when we wish to distribute the computational load
among the bidders or when the bidders do not wish to reveal
their true valuations unless necessary. PAUSE establishes
the rules the bidders must obey. However, it does not tell
us how the bidders should calculate their bids. We have
developed a couple of bidding algorithms for the bidders in
a PAUSE auction. Our algorithms always return the set of
bids that maximizes the bidder"s utility. Since the problem
is NP-Hard, run time remains exponential on the number
of items, but it is remarkably better than an exhaustive
search. In this paper we present our bidding algorithms,
discuss their virtues and drawbacks, and compare the 
solutions obtained by them to the revenue-maximizing solution
found by a centralized winner determination algorithm.
Categories and Subject Descriptors
I.2.11 [Computing Methodologies]: Distributed 
Artificial Intelligence-Intelligent Agents, Multiagent Systems.
General Terms
Algorithms, Performance.
1. INTRODUCTION
Both the research and practice of combinatorial auctions
have grown rapidly in the past ten years. In a 
combinatorial auction bidders can place bids on combinations of
items, called packages or bidsets, rather than just 
individual items. Once the bidders place their bids, it is necessary
to find the allocation of items to bidders that maximizes
the auctioneer"s revenue. This problem, known as the 
winner determination problem, is a combinatorial optimization
problem and is NP-Hard [10]. Nevertheless, several 
algorithms that have a satisfactory performance for problem
sizes and structures occurring in practice have been 
developed. The practical applications of combinatorial auctions
include: allocation of airport takeoff and landing time slots,
procurement of freight transportation services, procurement
of public transport services, and industrial procurement [2].
Because of their wide applicability, one cannot hope for a
general-purpose winner determination algorithm that can
efficiently solve every instance of the problem. Thus, 
several approaches and algorithms have been proposed to 
address the winner determination problem. However, most of
the existing winner determination algorithms for 
combinatorial auctions are centralized, meaning that they require
all agents to send their bids to a centralized auctioneer who
then determines the winners. Examples of these algorithms
are CASS [3], Bidtree [11] and CABOB [12]. We believe that
distributed solutions to the winner determination problem
should be studied as they offer a better fit for some 
applications as when, for example, agents do not want to reveal
their valuations to the auctioneer.
The PAUSE (Progressive Adaptive User Selection 
Environment) auction [4, 5] is one of a few efforts to distribute
the problem of winner determination amongst the bidders.
PAUSE establishes the rules the participants have to adhere
to so that the work is distributed amongst them. However,
it is not concerned with how the bidders determine what
they should bid.
In this paper we present two algorithms, pausebid and
cachedpausebid, which enable agents in a PAUSE 
auction to find the bidset that maximizes their utility. Our
algorithms implement a myopic utility maximizing strategy
and are guaranteed to find the bidset that maximizes the
agent"s utility given the outstanding best bids at a given
time. pausebid performs a branch and bound search 
completely from scratch every time that it is called. 
cachedpausebid is a caching-based algorithm which explores fewer
nodes, since it caches some solutions.
694
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
2. THE PAUSE AUCTION
A PAUSE auction for m items has m stages. Stage 1 
consists of having simultaneous ascending price open-cry 
auctions and during this stage the bidders can only place bids on
individual items. At the end of this state we will know what
the highest bid for each individual item is and who placed
that bid. Each successive stage k = 2, 3, . . . , m consists of
an ascending price auction where the bidders must submit
bidsets that cover all items but each one of the bids must be
for k items or less. The bidders are allowed to use bids that
other agents have placed in previous rounds when building
their bidsets, thus allowing them to find better solutions.
Also, any new bidset has to have a sum of bid prices which
is bigger than that of the currently winning bidset. At the
end of each stage k all agents know the best bid for every
subset of size k or less. Also, at any point in time after stage
1 has ended there is a standing bidset whose value increases
monotonically as new bidsets are submitted. Since in the
final round all agents consider all possible bidsets, we know
that the final winning bidset will be one such that no agent
can propose a better bidset. Note, however, that this 
bidset is not guaranteed to be the one that maximizes revenue
since we are using an ascending price auction so the 
winning bid for each set will be only slightly bigger than the
second highest bid for the particular set of items. That is,
the final prices will not be the same as the prices in a 
traditional combinatorial auction where all the bidders bid their
true valuation. However, there remains the open question
of whether the final distribution of items to bidders found
in a PAUSE auction is the same as the revenue maximizing
solution. Our test results provide an answer to this question.
The PAUSE auction makes the job of the auctioneer very
easy. All it has to do is to make sure that each new 
bidset has a revenue bigger than the current winning bidset, as
well as make sure that every bid in an agent"s bidset that
is not his does indeed correspond to some other agents" 
previous bid. The computational problem shifts from one of
winner determination to one of bid generation. Each agent
must search over the space of all bidsets which contain at
least one of its bids. The search is made easier by the fact
that the agent needs to consider only the current best bids
and only wants bidsets where its own utility is higher than
in the current winning bidset. Each agent also has a clear
incentive for performing this computation, namely, its 
utility only increases with each bidset it proposes (of course, it
might decrease with the bidsets that others propose). 
Finally, the PAUSE auction has been shown to be envy-free in
that at the conclusion of the auction no bidder would prefer
to exchange his allocation with that of any other bidder [2].
We can even envision completely eliminating the 
auctioneer and, instead, have every agent perform the task of the
auctioneer. That is, all bids are broadcast and when an
agent receives a bid from another agent it updates the set
of best bids and determines if the new bid is indeed better
than the current winning bid. The agents would have an 
incentive to perform their computation as it will increase their
expected utility. Also, any lies about other agents" bids are
easily found out by keeping track of the bids sent out by 
every agent (the set of best bids). Namely, the only one that
can increase an agent"s bid value is the agent itself. 
Anyone claiming a higher value for some other agent is lying.
The only thing missing is an algorithm that calculates the
utility-maximizing bidset for each agent.
3. PROBLEM FORMULATION
A bid b is composed of three elements bitems
(the set of
items the bid is over), bagent
(the agent that placed the bid),
and bvalue
(the value or price of the bid). The agents 
maintain a set B of the current best bids, one for each set of items
of size ≤ k, where k is the current stage. At any point in the
auction, after the first round, there will also be a set W ⊆ B
of currently winning bids. This is the set of bids that covers
all the items and currently maximizes the revenue, where
the revenue of W is given by
r(W) =
b∈W
bvalue
. (1)
Agent i"s value function is given by vi(S) ∈ where S is a
set of items. Given an agent"s value function and the current
winning bidset W we can calculate the agent"s utility from
W as
ui(W) =
b∈W | bagent=i
vi(bitems
) − bvalue
. (2)
That is, the agent"s utility for a bidset W is the value it
receives for the items it wins in W minus the price it must
pay for those items. If the agent is not winning any items
then its utility is zero.
The goal of the bidding agents in the PAUSE auction is to
maximize their utility, subject to the constraint that their
next set of bids must have a total revenue that is at least
bigger than the current revenue, where is the smallest
increment allowed in the auction. Formally, given that W is
the current winning bidset, agent i must find a g∗
i such that
r(g∗
i ) ≥ r(W) + and
g∗
i = arg max
g⊆2B
ui(g), (3)
where each g is a set of bids that covers all items and
∀b∈g (b ∈ B) or (bagent
= i and bvalue
> B(bitems
) and
size(bitems
) ≤ k), and where B(items) is the value of the
bid in B for the set items (if there is no bid for those items
it returns zero). That is, each bid b in g must satisfy at least
one of the two following conditions. 1) b is already in B, 2)
b is a bid of size ≤ k in which the agent i bids higher than
the price for the same items in B.
4. BIDDING ALGORITHMS
According to the PAUSE auction, during the first stage we
have only several English auctions, with the bidders 
submitting bids on individual items. In this case, an agent"s 
dominant strategy is to bid higher than the current winning bid
until it reaches its valuation for that particular item. Our
algorithms focus on the subsequent stages: k > 1. When
k > 1, agents have to find g∗
i . This can be done by 
performing a complete search on B. However, this approach is
computationally expensive since it produces a large search
tree. Our algorithms represent alternative approaches to
overcome this expensive search.
4.1 The PAUSEBID Algorithm
In the pausebid algorithm (shown in Figure 1) we 
implement some heuristics to prune the search tree. Given
that bidders want to maximize their utility and that at any
given point there are likely only a few bids within B which
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 695
pausebid(i, k)
1 my-bids ← ∅
2 their-bids ← ∅
3 for b ∈ B
4 do if bagent
= i or vi(bitems
) > bvalue
5 then my-bids ← my-bids
+new Bid(bitems
, i, vi(bitems
))
6 else their-bids ← their-bids +b
7 for S ∈ subsets of k or fewer items such that
vi(S) > 0 and ¬∃b∈Bbitems
= S
8 do my-bids ← my-bids +new Bid(S, i, vi(S))
9 bids ← my-bids + their-bids
10 g∗
← ∅ £ Global variable
11 u∗
← ui(W)£ Global variable
12 pbsearch(bids, ∅)
13 surplus ← b∈g∗ | bagent=i bvalue
− B(bitems
)
14 if surplus = 0
15 then return g∗
16 my-payment ← vi(g∗
) − u∗
17 for b ∈ g∗
| bagent
= i
18 do if my-payment ≤ 0
19 then bvalue
← B(bitems
)
20 else bvalue
← B(bitems
)
+ my-payment ·bvalue
−B(bitems
)
surplus
21 return g∗
Figure 1: The pausebid algorithm which implements
a branch and bound search. i is the agent and k is
the current stage of the auction, for k ≥ 2.
the agent can dominate, we start by defining my-bids to be
the list of bids for which the agent"s valuation is higher than
the current best bid, as given in B. We set the value of
these bids to be the agent"s true valuation (but we won"t
necessarily be bidding true valuation, as we explain later).
Similarly, we set their-bids to be the rest of the bids from B.
Finally, the agent"s search list is simply the concatenation
of my-bids and their-bids. Note that the agent"s own bids
are placed first on the search list as this will enable us to do
more pruning (pausebid lines 3 to 9). The agent can now
perform a branch and bound search on the branch-on-bids
tree produced by these bids. This branch and bound search
is implemented by pbsearch (Figure 2). Our algorithm not
only implements the standard bound but it also implements
other pruning techniques in order to further reduce the size
of the search tree.
The bound we use is the maximum utility that the agent
can expect to receive from a given set of bids. We call it u∗
.
Initially, u∗
is set to ui(W) (pausebid line 11) since that
is the utility the agent currently receives and any solution
he proposes should give him more utility. If pbsearch ever
comes across a partial solution where the maximum utility
the agent can expect to receive is less than u∗
then that
subtree is pruned (pbsearch line 21). Note that we can
determine the maximum utility only after the algorithm has
searched over all of the agent"s own bids (which are first on
the list) because after that we know that the solution will
not include any more bids where the agent is the winner
thus the agent"s utility will no longer increase. For example,
pbsearch(bids, g)
1 if bids = ∅ then return
2 b ← first(bids)
3 bids ← bids −b
4 g ← g + b
5 ¯Ig ← items not in g
6 if g does not contain a bid from i
7 then return
8 if g includes all items
9 then min-payment ← max(0, r(W) + − (r(g) − ri(g)),
b∈g | bagent=i B(bitems
))
10 max-utility ← vi(g) − min-payment
11 if r(g) > r(W) and max-utility ≥ u∗
12 then g∗
← g
13 u∗
← max-utility
14 pbsearch(bids, g − b) £ b is Out
15 else max-revenue ← r(g) + max(h(¯Ig), hi(¯Ig))
16 if max-revenue ≤ r(W)
17 then pbsearch(bids, g − b) £ b is Out
18 elseif bagent
= i
19 then min-payment ← (r(W) + )
−(r(g) − ri(g)) − h(¯Ig)
20 max-utility ← vi(g) − min-payment
21 if max-utility > u∗
22 then pbsearch({x ∈ bids |
xitems
∩ bitems
= ∅}, g) £ b is In
23 pbsearch(bids, g − b) £ b is Out
24 else
25 pbsearch({x ∈ bids |
xitems
∩ bitems
= ∅}, g) £ b is In
26 pbsearch(bids, g − b) £ b is Out
27 return
Figure 2: The pbsearch recursive procedure where
bids is the set of available bids and g is the current
partial solution.
if an agent has only one bid in my-bids then the maximum
utility he can expect is equal to his value for the items in
that bid minus the minimum possible payment we can make
for those items and still come up with a set of bids that has
revenue greater than r(W). The calculation of the minimum
payment is shown in line 19 for the partial solution case and
line 9 for the case where we have a complete solution in
pbsearch. Note that in order to calculate the min-payment
for the partial solution case we need an upper bound on the
payments that we must make for each item. This upper
bound is provided by
h(S) =
s∈S
max
b∈B | s∈bitems
bvalue
size(bitems)
. (4)
This function produces a bound identical to the one used by
the Bidtree algorithm-it merely assigns to each individual
item in S a value equal to the maximum bid in B divided
by the number of items in that bid.
To prune the branches that cannot lead to a solution with
revenue greater than the current W, the algorithm considers
both the values of the bids in B and the valuations of the
696 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
agent. Similarly to (4) we define
hi(S, k) =
s∈S
max
S | size(S )≤k and s∈S and vi(S )>0
vi(S )
size(S )
(5)
which assigns to each individual item s in S the maximum
value produced by the valuation of S divided by the size
of S , where S is a set for which the agent has a valuation
greater than zero, contains s, and its size is less or equal
than k. The algorithm uses the heuristics h and hi (lines 15
and 19 of pbsearch), to prune the just mentioned branches
in the same way an A∗
algorithm uses its heuristic. A final
pruning technique implemented by the algorithm is ignoring
any branches where the agent has no bids in the current
answer g and no more of the agent"s bids are in the list
(pbsearch lines 6 and 7).
The resulting g∗
found by pbsearch is thus the set of bids
that has revenue bigger than r(W) and maximizes agent i"s
utility. However, agent i"s bids in g∗
are still set to his own
valuation and not to the lowest possible price. Lines 17 to 20
in pausebid are responsible for setting the agent"s payments
so that it can achieve its maximum utility u∗
. If the agent
has only one bid in g∗
then it is simply a matter of reducing
the payment of that bid by u∗
from the current maximum of
the agent"s true valuation. However, if the agent has more
than one bid then we face the problem of how to distribute
the agent"s payments among these bids. There are many
ways of distributing the payments and there does not appear
to be a dominant strategy for performing this distribution.
We have chosen to distribute the payments in proportion to
the agent"s true valuation for each set of items.
pausebid assumes that the set of best bids B and the 
current best winning bidset W remains constant during its 
execution, and it returns the agent"s myopic utility-maximizing
bidset (if there is one) using a branch and bound search.
However it repeats the whole search at every stage. We
can minimize this problem by caching the result of previous
searches.
4.2 The CACHEDPAUSEBID Algorithm
The cachedpausebid algorithm (shown in Figure 3) is
our second approach to solve the bidding problem in the
PAUSE auction. It is based in a cache table called C-Table
where we store some solutions to avoid doing a complete
search every time. The problem is the same; the agent i has
to find g∗
i . We note that g∗
i is a bidset that contains at least
one bid of the agent i. Let S be a set of items for which the
agent i has a valuation such that vi(S) ≥ B(S) > 0, let gS
i
be a bidset over S such that r(gS
i ) ≥ r(W) + and
gS
i = arg max
g⊆2B
ui(g), (6)
where each g is a set of bids that covers all items and
∀b∈g (b ∈ B) or (bagent
= i and bvalue
> B(bitems
)) and
(∃b∈gbitems
= S and bagent
= i). That is, gS
i is i"s best 
bidset for all items which includes a bid from i for all S items.
In the PAUSE auction we cannot bid for sets of items with
size greater than k. So, if we have for each set of items S for
which vi(S) > 0 and size(S) ≤ k its corresponding gS
i then
g∗
i is the gS
i that maximizes the agent"s utility. That is
g∗
i = arg max
{S | vi(S)>0∧size(S)≤k}
ui(gS
i ). (7)
Each agent i implements a hash table C-Table such that
C-Table[S] = gS
for all S which vi(S) ≥ B(S) > 0. We can
cachedpausebid(i, k, k-changed)
1 for each S in C-Table
2 do if vi(S) < B(S)
3 then remove S from C-Table
4 else if k-changed and size(S) = k
5 then B ← B + new Bid(i, S, vi(S))
6 g∗
← ∅
7 u∗
← ui(W)
8 for each S with size(S) ≤ k in C-Table
9 do ¯S ← Items − S
10 gS
← C-Table[S] £ Global variable
11 min-payment ← max(r(W) + , b∈gS B(bitems
))
12 uS
← r(gS
) − min-payment £ Global variable
13 if (k-changed and size(S) = k)
or (∃b∈B bitems
⊆ ¯S and bagent
= i)
14 then B ← {b ∈ B |bitems
⊆ ¯S}
15 bids ← B
+{b ∈ B|bitems
⊆ ¯S and b /∈ B }
16 for b ∈ bids
17 do if vi(bitems
) > bvalue
18 then bagent
← i
19 bvalue
← vi(bitems
)
20 if k-changed and size(S) = k
21 then n ← size(bids)
22 uS
← 0
23 else n ← size(B )
24 g ← ∅ + new Bid(S, i, vi(S))
25 cpbsearch(bids, g, n)
26 C-Table[S] ← gS
27 if uS
> u∗
and r(gS
) ≥ r(W) +
28 then surplus ←
b∈gS | bagent=i bvalue
− B(bitems
)
29 if surplus > 0
30 then my-payment ← vi(gS
) − ui(gS
)
31 for b ∈ gS
| bagent
= i
32 do if my-payment ≤ 0
33 then bvalue
← B(bitems
)
34 else bvalue
← B(bitems
)+
my-payment ·bvalue
−B(bitems
)
surplus
35 u∗
← ui(gS
)
36 g∗
← gS
37 else if uS
≤ 0 and vi(S) < B(S)
38 then remove S from C-Table
39 return g∗
Figure 3: The cachedpausebid algorithm that 
implements a caching based search to find a bidset that
maximizes the utility for the agent i. k is the 
current stage of the auction (for k ≥ 2), and k-changed is
a boolean that is true right after the auction moved
to the next stage.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 697
cpbsearch(bids, g, n)
1 if bids = ∅ or n ≤ 0 then return
2 b ← first(bids)
3 bids ← bids −b
4 g ← g + b
5 ¯Ig ← items not in g
6 if g includes all items
7 then min-payment ← max(0, r(W) + − (r(g) − ri(g)),
b∈g | bagent=i B(bitems
))
8 max-utility ← vi(g) − min-payment
9 if r(g) > r(W) and max-utility ≥ uS
10 then gS
← g
11 uS
← max-utility
12 cpbsearch(bids, g − b, n − 1) £ b is Out
13 else max-revenue ← r(g) + max(h(¯Ig), hi(¯Ig))
14 if max-revenue ≤ r(W)
15 then cpbsearch(bids, g − b, n − 1) £ b is Out
16 elseif bagent
= i
17 then min-payment ← (r(W) + )
−(r(g) − ri(g)) − h(¯Ig)
18 max-utility ← vi(g) − min-payment
19 if max-utility > uS
20 then cpbsearch({x ∈ bids |
xitems
∩ bitems
= ∅}, g, n + 1) £ b is In
21 cpbsearch(bids, g − b, n − 1) £ b is Out
22 else
23 cpbsearch({x ∈ bids |
xitems
∩ bitems
= ∅}, g, n + 1) £ b is In
24 cpbsearch(bids, g − b, n − 1) £ b is Out
25 return
Figure 4: The cpbsearch recursive procedure where
bids is the set of available bids, g is the current 
partial solution and n is a value that indicates how deep
in the list bids the algorithm has to search.
then find g∗
by searching for the gS
, stored in C-Table[S],
that maximizes the agent"s utility, considering only the set
of items S with size(S) ≤ k. The problem remains in 
maintaining the C-Table updated and avoiding to search every
gS
every time. cachedpausebid deals with this and other
details.
Let B be the set of bids that contains the new best bids,
that is, B contains the bids recently added to B and the bids
that have changed price (always higher), bidder, or both and
were already in B. Let ¯S = Items − S be the complement
of S (the set of items not included in S). cachedpausebid
takes three parameters: i the agent, k the current stage of
the auction, and k-changed a boolean that is true right after
the auction moved to the next stage. Initially C-Table has
one row or entry for each set S for which vi(S) > 0. We
start by eliminating the entries corresponding to each set S
for which vi(S) < B(S) from C-Table (line 3). Then, in the
case that k-changed is true, for each set S with size(S) = k,
we add to B a bid for that set with value equal to vi(S)
and bidder agent i (line 5); this a bid that the agent is now
allowed to consider. We then search for g∗
amongst the gS
stored in C-Table, for this we only need to consider the sets
with size(S) ≤ k (line 8). But how do we know that the gS
in C-Table[S] is still the best solution for S? There are only
two cases when we are not sure about that and we need
to do a search to update C-Table[S]. These cases are: i)
When k-changed is true and size(S) ≤ k, since there was
no gS
stored in C-Table for this S. ii) When there exists at
least one bid in B for the set of items ¯S or a subset of it
submitted by an agent different than i, since it is probable
that this new bid can produce a solution better than the one
stored in C-Table[S].
We handle the two cases mentioned above in lines 13 to 26
of cachedpausebid. In both of these cases, since gS
must
contain a bid for S we need to find a bidset that cover the
missing items, that is ¯S. Thus, our search space consists
of all the bids on B for the set of items ¯S or for a subset
of it. We build the list bids that contains only those bids.
However, we put the bids from B at the beginning of bids
(line 14) since they are the ones that have changed. Then,
we replace the bids in bids that have a price lower than the
valuation the agent i has for those same items with a bid
from agent i for those items and value equal to the agent"s
valuation (lines 16-19).
The recursive procedure cpbsearch, called in line 25 of
cachedpausebid and shown in Figure 4, is the one that
finds the new gS
. cpbsearch is a slightly modified version
of our branch and bound search implemented in pbsearch.
The first modification is that it has a third parameter n that
indicates how deep on the list bids we want to search, since
it stops searching when n less or equal to zero and not only
when the list bids is empty (line 1). Each time that there is
a recursive call of cpbsearch n is decreased by one when a
bid from bids is discarded or out (lines 12, 15, 21, and 24)
and n remains the same otherwise (lines 20 and 23). We set
the value of n before calling cpbsearch, to be the size of the
list bids (cachedpausebid line 21) in case i), since we want
cpbsearch to search over all bids; and we set n to be the
number of bids from B included in bids (cachedpausebid
line 23) in case ii), since we know that only the those first n
bids in bids changed and can affect our current gS
.
Another difference with pbsearch is that the bound in
cpbsearch is uS
which we set to be 0 (cachedpausebid line
22) when in case i) and r(gS
)−min-payment (cachedpausebid
line 12) when in case ii). We call cpbsearch with g already
containing a bid for S. After cpbsearch is executed we
are sure that we have the right gS
, so we store it in the
corresponding C-Table[S] (cachedpausebid line 26).
When we reach line 27 in cachedpausebid, we are sure
that we have the right gS
. However, agent i"s bids in gS
are
still set to his own valuation and not to the lowest possible
price. If uS
is greater than the current u∗
, lines 31 to 34
in cachedpausebid are responsible for setting the agent"s
payments so that it can achieve its maximum utility uS
.
As in pausebid, we have chosen to distribute the payments
in proportion to the agent"s true valuation for each set of
items. In the case that uS
less than or equal to zero and
the valuation that the agent i has for the set of items S is
lower than the current value of the bid in B for the same
set of items, we remove the corresponding C-Table[S] since
we know that is not worthwhile to keep it in the cache table
(cachedpausebid line 38).
The cachedpausebid function is called when k > 1 and
returns the agent"s myopic utility-maximizing bidset, if there
is one. It assumes that W and B remains constant during
its execution.
698 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
generatevalues(i, items)
1 for x ∈ items
2 do vi(x) = expd(.01)
3 for n ← 1 . . . (num-bids − items)
4 do s1, s2 ←Two random sets of items with values.
5 vi(s1 ∪ s2) = vi(s1) + vi(s2) + expd(.01)
Figure 5: Algorithm for the generation of random
value functions. expd(x) returns a random number
taken from an exponential distribution with mean
1/x.
0
20
40
60
80
100
2 3 4 5 6 7 8 9 10
Number of Items
CachedPauseBid
3 3 3 3 3 3
3 3
3
3
PauseBid
+ + + + +
+ + +
+
+
Figure 6: Average percentage of convergence
(y-axis), which is the percentage of times that our
algorithms converge to the revenue-maximizing 
solution, as function of the number of items in the
auction.
5. TEST AND COMPARISON
We have implemented both algorithms and performed a
series of experiments in order to determine how their 
solution compares to the revenue-maximizing solution and how
their times compare with each other. In order to do our
tests we had to generate value functions for the agents1
.
The algorithm we used is shown in Figure 5. The type of
valuations it generates correspond to domains where a set
of agents must perform a set of tasks but there are cost 
savings for particular agents if they can bundle together certain
subsets of tasks. For example, imagine a set of robots which
must pick up and deliver items to different locations. Since
each robot is at a different location and has different 
abilities, each one will have different preferences over how to
bundle. Their costs for the item bundles are subadditive,
which means that their preferences are superadditive. The
first experiment we performed simply ensured the proper
1
Note that we could not use CATS [6] because it generates
sets of bids for an indeterminate number of agents. It is as
if you were told the set of bids placed in a combinatorial
auction but not who placed each bid or even how many
people placed bids, and then asked to determine the value
function of every participant in the auction.
0
20
40
60
80
100
2 3 4 5 6 7 8 9 10
Number of Items
CachedPauseBid
3
3
3 3
3 3 3 3 3
3
PauseBid
+ +
+ +
+
+ + +
+
+
Figure 7: Average percentage of revenue from our
algorithms relative to maximum revenue (y-axis) as
function of the number of items in the auction.
functioning of our algorithms. We then compared the 
solutions found by both of them to the revenue-maximizing
solution as found by CASS when given a set of bids that
corresponds to the agents" true valuation. That is, for each
agent i and each set of items S for which vi(S) > 0 we 
generated a bid. This set of bids was fed to CASS which 
implements a centralized winner determination algorithm to find
the solution which maximizes revenue. Note, however, that
the revenue from the PAUSE auction on all the auctions is
always smaller than the revenue of the revenue-maximizing
solution when the agents bid their true valuations. Since
PAUSE uses English auctions the final prices (roughly) 
represent the second-highest valuation, plus , for that set of
items.
We fixed the number of agents to be 5 and we 
experimented with different number of items, namely from 2 to
10. We ran both algorithms 100 times for each 
combination. When we compared the solutions of our algorithms
to the revenue-maximizing solution, we realized that they
do not always find the same distribution of items as the
revenue-maximizing solution (as shown in Figure 6). The
cases where our algorithms failed to arrive at the 
distribution of the revenue-maximizing solution are those where
there was a large gap between the first and second 
valuation for a set (or sets) of items. If the revenue-maximizing
solution contains the bid (or bids) using these higher 
valuation then it is impossible for the PAUSE auction to find this
solution because that bid (those bids) is never placed. For
example, if agent i has vi(1) = 1000 and the second highest
valuation for (1) is only 10 then i only needs to place a bid
of 11 in order to win that item. If the revenue-maximizing
solution requires that 1 be sold for 1000 then that solution
will never be found because that bid will never be placed.
We also found that average percentage of times that our 
algorithms converges to the revenue-maximizing solution 
decreases as the number of items increases. For 2 items is
almost 100% but decreases a little bit less than 1 percent as
the items increase, so that this average percentage of 
convergence is around 90% for 10 items. In a few instances our
algorithms find different solutions this is due to the different
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 699
1
10
100
1000
10000
2 3 4 5 6 7 8 9 10
Number of Items
CachedPauseBid
3
3
3
3
3
3
3
3
3
PauseBid
+
+
+
+
+
+
+
+
+
+
Figure 8: Average number of expanded nodes
(y-axis) as function of items in the auction.
ordering of the bids in the bids list which makes them search
in different order.
We know that the revenue generated by the PAUSE 
auction is generally lower than the revenue of the 
revenuemaximizing solution, but how much lower? To answer this
question we calculated percentage representing the 
proportion of the revenue given by our algorithms relative to the
revenue given by CASS. We found that the percentage of
revenue of our algorithms increases in average 2.7% as the
number of items increases, as shown in Figure 7. However,
we found that cachedpausebid generates a higher revenue
than pausebid (4.3% higher in average) except for auctions
with 2 items where both have about the same percentage.
Again, this difference is produced by the order of the search.
In the case of 2 items both algorithms produce in average
a revenue proportion of 67.4%, while in the other extreme
(10 items), cachedpausebid produced in average a revenue
proportion of 91.5% while pausebid produced in average a
revenue proportion of 87.7%.
The scalability of our algorithms can be determined by
counting the number of nodes expanded in the search tree.
For this we count the number of times that pbsearch gets
invoked for each time that pausebid is called and the 
number of times that fastpausebidsearch gets invoked for each
time that cachedpausebid, respectively for each of our 
algorithms. As expected since this is an NP-Hard problem,
the number of expanded nodes does grow exponentially with
the number of items (as shown in Figure 8). However, we
found that cachedpausebid outperforms pausebid, since
it expands in average less than half the number of nodes.
For example, the average number of nodes expanded when
2 items is zero for cachedpausebid while for pausebid is
2; and in the other extreme (10 items) cachedpausebid 
expands in average only 633 nodes while pausebid expands in
average 1672 nodes, a difference of more than 1000 nodes.
Although the number of nodes expanded by our algorithms
increases as function of the number of items, the actual 
number of nodes is a much smaller than the worst-case scenario
of nn
where n is the number of items. For example, for 10
items we expand slightly more than 103
nodes for the case of
pausebid and less than that for the case of 
cachedpause0.1
1
10
100
1000
2 3 4 5 6 7 8 9 10
Number of Items
CachedPauseBid
3
3
3
3
3
3
3
3
3
3
PauseBid
+
+
+
+
+
+
+
+
+
+
Figure 9: Average time in seconds that takes to 
finish an auction (y-axis) as function of the number of
items in the auction.
bid which are much smaller numbers than 1010
. Notice also
that our value generation algorithm (Figure 5) generates a
number of bids that is exponential on the number of items,
as might be expected in many situations. As such, these
results do not support the conclusion that time grows 
exponentially with the number of items when the number of
bids is independent of the number of items. We expect that
both algorithms will grow exponentially as a function the
number of bids, but stay roughly constant as the number of
items grows.
We wanted to make sure that less expanded nodes does
indeed correspond to faster execution, especially since our
algorithms execute different operations. We thus ran the
same experiment with all the agents in the same machine,
an Intel Centrino 2.0 GHz laptop PC with 1 GB of RAM and
a 7200 RMP 60 GB hard drive, and calculated the average
time that takes to finish an auction for each algorithm. As
shown in Figure 9, cachedpausebid is faster than 
pausebid, the difference in execution speed is even more clear as
the number of items increases.
6. RELATED WORK
A lot of research has been done on various aspects of 
combinatorial auctions. We recommend [2] for a good review.
However, the study of distributed winner determination 
algorithms for combinatorial auctions is still relatively new.
One approach is given by the algorithms for distributing
the winner determination problem in combinatorial auctions
presented in [7], but these algorithms assume the 
computational entities are the items being sold and thus end up
with a different type of distribution. The VSA algorithm
[3] is another way of performing distributed winner 
determination in combinatorial auction but it assumes the bids
themselves perform the computation. This algorithm also
fails to converge to a solution for most cases. In [9] the 
authors present a distributed mechanism for calculating VCG
payments in a mechanism design problem. Their 
mechanism roughly amounts to having each agent calculate the
payments for two other agents and give these to a secure
700 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
central server which then checks to make sure results from
all pairs agree, otherwise a re-calculation is ordered. This
general idea, which they call the redundancy principle, could
also be applied to our problem but it requires the existence
of a secure center agent that everyone trusts. Another 
interesting approach is given in [8] where the bidding agents
prioritize their bids, thus reducing the set of bids that the
centralized winner determination algorithm must consider,
making that problem easier. Finally, in the computation
procuring clock auction [1] the agents are given an 
everincreasing percentage of the surplus achieved by their 
proposed solution over the current best. As such, it assumes
the agents are impartial computational entities, not the set
of possible buyers as assumed by the PAUSE auction.
7. CONCLUSIONS
We believe that distributed solutions to the winner 
determination problem should be studied as they offer a better fit
for some applications as when, for example, agents do not
want to reveal their valuations to the auctioneer or when
we wish to distribute the computational load among the
bidders. The PAUSE auction is one of a few approaches
to decentralize the winner determination problem in 
combinatorial auctions. With this auction, we can even envision
completely eliminating the auctioneer and, instead, have 
every agent performe the task of the auctioneer. However,
while PAUSE establishes the rules the bidders must obey, it
does not tell us how the bidders should calculate their bids.
We have presented two algorithms, pausebid and 
cachedpausebid, that bidder agents can use to engage in a PAUSE
auction. Both algorithms implement a myopic utility 
maximizing strategy that is guaranteed to find the bidset that
maximizes the agent"s utility given the set of outstanding
best bids at any given time, without considering possible
future bids. Both algorithms find, most of the time, the
same distribution of items as the revenue-maximizing 
solution. The cases where our algorithms failed to arrive at that
distribution are those where there was a large gap between
the first and second valuation for a set (or sets) of items.
As it is an NP-Hard problem, the running time of our 
algorithms remains exponential but it is significantly better than
a full search. pausebid performs a branch and bound search
completely from scratch each time it is invoked. 
cachedpausebid caches partial solutions and performs a branch
and bound search only on the few portions affected by the
changes on the bids between consecutive times. 
cachedpausebid has a better performance since it explores fewer
nodes (less than half) and it is faster. As expected the
revenue generated by a PAUSE auction is lower than the
revenue of a revenue-maximizing solution found by a 
centralized winner determination algorithm, however we found
that cachedpausebid generates in average 4.7% higher 
revenue than pausebid. We also found that the revenue 
generated by our algorithms increases as function of the number
of items in the auction.
Our algorithms have shown that it is feasible to implement
the complex coordination constraints supported by 
combinatorial auctions without having to resort to a centralized
winner determination algorithm. Moreover, because of the
design of the PAUSE auction, the agents in the auction also
have an incentive to perform the required computation. Our
bidding algorithms can be used by any multiagent system
that would use combinatorial auctions for coordination but
would rather not implement a centralized auctioneer.
8. REFERENCES
[1] P. J. Brewer. Decentralized computation procurement
and computational robustness in a smart market.
Economic Theory, 13(1):41-92, January 1999.
[2] P. Cramton, Y. Shoham, and R. Steinberg, editors.
Combinatorial Auctions. MIT Press, 2006.
[3] Y. Fujishima, K. Leyton-Brown, and Y. Shoham.
Taming the computational complexity of
combinatorial auctions: Optimal and approximate
approaches. In Proceedings of the Sixteenth
International Joint Conference on Artificial
Intelligence, pages 548-553. Morgan Kaufmann
Publishers Inc., 1999.
[4] F. Kelly and R. Stenberg. A combinatorial auction
with multiple winners for universal service.
Management Science, 46(4):586-596, 2000.
[5] A. Land, S. Powell, and R. Steinberg. PAUSE: A
computationally tractable combinatorial auction. In
Cramton et al. [2], chapter 6, pages 139-157.
[6] K. Leyton-Brown, M. Pearson, and Y. Shoham.
Towards a universal test suite for combinatorial
auction algorithms. In Proceedings of the 2nd ACM
conference on Electronic commerce, pages 66-76.
ACM Press, 2000. http://cats.stanford.edu.
[7] M. V. Narumanchi and J. M. Vidal. Algorithms for
distributed winner determination in combinatorial
auctions. In LNAI volume of AMEC/TADA. Springer,
2006.
[8] S. Park and M. H. Rothkopf. Auctions with
endogenously determined allowable combinations.
Technical report, Rutgets Center for Operations
Research, January 2001. RRR 3-2001.
[9] D. C. Parkes and J. Shneidman. Distributed
implementations of vickrey-clarke-groves auctions. In
Proceedings of the Third International Joint
Conference on Autonomous Agents and MultiAgent
Systems, pages 261-268. ACM, 2004.
[10] M. H. Rothkopf, A. Pekec, and R. M. Harstad.
Computationally manageable combinational auctions.
Management Science, 44(8):1131-1147, 1998.
[11] T. Sandholm. An algorithm for winner determination
in combinatorial auctions. Artificial Intelligence,
135(1-2):1-54, February 2002.
[12] T. Sandholm, S. Suri, A. Gilpin, and D. Levine.
CABOB: a fast optimal algorithm for winner
determination in combinatorial auctions. Management
Science, 51(3):374-391, 2005.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 701
A Complete Distributed Constraint Optimization Method
For Non-Traditional Pseudotree Arrangements∗
James Atlas
Computer and Information Sciences
University of Delaware
Newark, DE 19716
atlas@cis.udel.edu
Keith Decker
Computer and Information Sciences
University of Delaware
Newark, DE 19716
decker@cis.udel.edu
ABSTRACT
Distributed Constraint Optimization (DCOP) is a general 
framework that can model complex problems in multi-agent systems.
Several current algorithms that solve general DCOP instances, 
including ADOPT and DPOP, arrange agents into a traditional 
pseudotree structure. We introduce an extension to the DPOP algorithm
that handles an extended set of pseudotree arrangements. Our 
algorithm correctly solves DCOP instances for pseudotrees that 
include edges between nodes in separate branches. The algorithm
also solves instances with traditional pseudotree arrangements 
using the same procedure as DPOP.
We compare our algorithm with DPOP using several metrics 
including the induced width of the pseudotrees, the maximum 
dimensionality of messages and computation, and the maximum 
sequential path cost through the algorithm. We prove that for some 
problem instances it is not possible to generate a traditional pseudotree
using edge-traversal heuristics that will outperform a cross-edged
pseudotree. We use multiple heuristics to generate pseudotrees and
choose the best pseudotree in linear space-time complexity. For
some problem instances we observe significant improvements in
message and computation sizes compared to DPOP.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
Intelligence-Multiagent Systems
General Terms
Algorithms
1. INTRODUCTION
Many historical problems in the AI community can be 
transformed into Constraint Satisfaction Problems (CSP). With the 
advent of distributed AI, multi-agent systems became a popular way
to model the complex interactions and coordination required to
solve distributed problems. CSPs were originally extended to
distributed agent environments in [9]. Early domains for 
distributed constraint satisfaction problems (DisCSP) included job
shop scheduling [1] and resource allocation [2]. Many domains
for agent systems, especially teamwork coordination, distributed
scheduling, and sensor networks, involve overly constrained 
problems that are difficult or impossible to satisfy for every constraint.
Recent approaches to solving problems in these domains rely
on optimization techniques that map constraints into multi-valued
utility functions. Instead of finding an assignment that satisfies all
constraints, these approaches find an assignment that produces a
high level of global utility. This extension to the original DisCSP
approach has become popular in multi-agent systems, and has been
labeled the Distributed Constraint Optimization Problem (DCOP)
[1].
Current algorithms that solve complete DCOPs use two main
approaches: search and dynamic programming. Search based 
algorithms that originated from DisCSP typically use some form of
backtracking [10] or bounds propagation, as in ADOPT [3]. 
Dynamic programming based algorithms include DPOP and its 
extensions [5, 6, 7]. To date, both categories of algorithms arrange
agents into a traditional pseudotree to solve the problem.
It has been shown in [6] that any constraint graph can be mapped
into a traditional pseudotree. However, it was also shown that 
finding the optimal pseudotree was NP-Hard. We began to 
investigate the performance of traditional pseudotrees generated by 
current edge-traversal heuristics. We found that these heuristics 
often produced little parallelism as the pseudotrees tended to have
high depth and low branching factors. We suspected that there
could be other ways to arrange the pseudotrees that would 
provide increased parallelism and smaller message sizes. After 
exploring these other arrangements we found that cross-edged 
pseudotrees provide shorter depths and higher branching factors than
the traditional pseudotrees. Our hypothesis was that these 
crossedged pseudotrees would outperform traditional pseudotrees for
some problem types.
In this paper we introduce an extension to the DPOP algorithm
that handles an extended set of pseudotree arrangements which
include cross-edged pseudotrees. We begin with a definition of
741
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
DCOP, traditional pseudotrees, and cross-edged pseudotrees. We
then provide a summary of the original DPOP algorithm and 
introduce our DCPOP algorithm. We discuss the complexity of our
algorithm as well as the impact of pseudotree generation 
heuristics. We then show that our Distributed Cross-edged Pseudotree
Optimization Procedure (DCPOP) performs significantly better in
practice than the original DPOP algorithm for some problem 
instances. We conclude with a selection of ideas for future work and
extensions for DCPOP.
2. PROBLEM DEFINITION
DCOP has been formalized in slightly different ways in recent
literature, so we will adopt the definition as presented in [6]. A
Distributed Constraint Optimization Problem with n nodes and m
constraints consists of the tuple < X, D, U > where:
• X = {x1,..,xn} is a set of variables, each one assigned to a
unique agent
• D = {d1,..,dn} is a set of finite domains for each variable
• U = {u1,..,um} is a set of utility functions such that each
function involves a subset of variables in X and defines a
utility for each combination of values among these variables
An optimal solution to a DCOP instance consists of an assignment
of values in D to X such that the sum of utilities in U is maximal.
Problem domains that require minimum cost instead of maximum
utility can map costs into negative utilities. The utility functions
represent soft constraints but can also represent hard constraints
by using arbitrarily large negative values. For this paper we only
consider binary utility functions involving two variables. Higher
order utility functions can be modeled with minor changes to the
algorithm, but they also substantially increase the complexity.
2.1 Traditional Pseudotrees
Pseudotrees are a common structure used in search procedures
to allow parallel processing of independent branches. As defined in
[6], a pseudotree is an arrangement of a graph G into a rooted tree
T such that vertices in G that share an edge are in the same branch
in T. A back-edge is an edge between a node X and any node which
lies on the path from X to the root (excluding X"s parent). Figure 1
shows a pseudotree with four nodes, three edges (A-B, B-C, 
BD), and one back-edge (A-C). Also defined in [6] are four types of
relationships between nodes exist in a pseudotree:
• P(X) - the parent of a node X: the single node higher in the
pseudotree that is connected to X directly through a tree edge
• C(X) - the children of a node X: the set of nodes lower in
the pseudotree that are connected to X directly through tree
edges
• PP(X) - the pseudo-parents of a node X: the set of nodes
higher in the pseudotree that are connected to X directly
through back-edges (In Figure 1, A = PP(C))
• PC(X) - the pseudo-children of a node X: the set of nodes
lower in the pseudotree that are connected to X directly
through back-edges (In Figure 1, C = PC(A))
Figure 1: A traditional pseudotree. Solid line edges 
represent parent-child relationships and the dashed line represents
a pseudo-parent-pseudo-child relationship.
Figure 2: A cross-edged pseudotree. Solid line edges represent
parent-child relationships, the dashed line represents a 
pseudoparent-pseudo-child relationship, and the dotted line 
represents a branch-parent-branch-child relationship. The bolded
node, B, is the merge point for node E.
2.2 Cross-edged Pseudotrees
We define a cross-edge as an edge from node X to a node Y that is
above X but not in the path from X to the root. A cross-edged 
pseudotree is a traditional pseudotree with the addition of cross-edges.
Figure 2 shows a cross-edged pseudotree with a cross-edge (D-E).
In a cross-edged pseudotree we designate certain edges as primary.
The set of primary edges defines a spanning tree of the nodes. The
parent, child, pseudo-parent, and pseudo-child relationships from
the traditional pseudotree are now defined in the context of this 
primary edge spanning tree. This definition also yields two additional
types of relationships that may exist between nodes:
• BP(X) - the branch-parents of a node X: the set of nodes
higher in the pseudotree that are connected to X but are not
in the primary path from X to the root (In Figure 2, D =
BP(E))
• BC(X) - the branch-children of a node X: the set of nodes
lower in the pseudotree that are connected to X but are not in
any primary path from X to any leaf node (In Figure 2, E =
BC(D))
2.3 Pseudotree Generation
742 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Current algorithms usually have a pre-execution phase to 
generate a traditional pseudotree from a general DCOP instance. Our
DCPOP algorithm generates a cross-edged pseudotree in the same
fashion. First, the DCOP instance < X, D, U > translates directly
into a graph with X as the set of vertices and an edge for each pair
of variables represented in U. Next, various heuristics are used to
arrange this graph into a pseudotree. One common heuristic is to
perform a guided depth-first search (DFS) as the resulting traversal
is a pseudotree, and a DFS can easily be performed in a distributed
fashion. We define an edge-traversal based method as any method
that produces a pseudotree in which all parent/child pairs share an
edge in the original graph. This includes DFS, breadth-first search,
and best-first search based traversals. Our heuristics that generate
cross-edged pseudotrees use a distributed best-first search traversal.
3. DPOP ALGORITHM
The original DPOP algorithm operates in three main phases. The
first phase generates a traditional pseudotree from the DCOP 
instance using a distributed algorithm. The second phase joins utility
hypercubes from children and the local node and propagates them
towards the root. The third phase chooses an assignment for each
domain in a top down fashion beginning with the agent at the root
node.
The complexity of DPOP depends on the size of the largest 
computation and utility message during phase two. It has been shown
that this size directly corresponds to the induced width of the 
pseudotree generated in phase one [6]. DPOP uses polynomial time
heuristics to generate the pseudotree since finding the minimum
induced width pseudotree is NP-hard. Several distributed 
edgetraversal heuristics have been developed to find low width 
pseudotrees [8]. At the end of the first phase, each agent knows its
parent, children, pseudo-parents, and pseudo-children.
3.1 Utility Propagation
Agents located at leaf nodes in the pseudotree begin the process
by calculating a local utility hypercube. This hypercube at node
X contains summed utilities for each combination of values in the
domains for P(X) and PP(X). This hypercube has dimensional size
equal to the number of pseudo-parents plus one. A message 
containing this hypercube is sent to P(X). Agents located at non-leaf
nodes wait for all messages from children to arrive. Once the agent
at node Y has all utility messages, it calculates its local utility 
hypercube which includes domains for P(Y), PP(Y), and Y. The local
utility hypercube is then joined with all of the hypercubes from
the child messages. At this point all utilities involving node Y are
known, and the domain for Y may be safely eliminated from the
joined hypercube. This elimination process chooses the best utility
over the domain of Y for each combination of the remaining 
domains. A message containing this hypercube is now sent to P(Y).
The dimensional size of this hypercube depends on the number of
overlapping domains in received messages and the local utility 
hypercube. This dynamic programming based propagation phase 
continues until the agent at the root node of the pseudotree has received
all messages from its children.
3.2 Value Propagation
Value propagation begins when the agent at the root node Z has
received all messages from its children. Since Z has no parents
or pseudo-parents, it simply combines the utility hypercubes 
received from its children. The combined hypercube contains only
values for the domain for Z. At this point the agent at node Z 
simply chooses the assignment for its domain that has the best utility.
A value propagation message with this assignment is sent to each
node in C(Z). Each other node then receives a value propagation
message from its parent and chooses the assignment for its domain
that has the best utility given the assignments received in the 
message. The node adds its domain assignment to the assignments it
received and passes the set of assignments to its children. The 
algorithm is complete when all nodes have chosen an assignment for
their domain.
4. DCPOP ALGORITHM
Our extension to the original DPOP algorithm, shown in 
Algorithm 1, shares the same three phases. The first phase generates the
cross-edged pseudotree for the DCOP instance. The second phase
merges branches and propagates the utility hypercubes. The third
phase chooses assignments for domains at branch merge points and
in a top down fashion, beginning with the agent at the root node.
For the first phase we generate a pseudotree using several 
distributed heuristics and select the one with lowest overall 
complexity. The complexity of the computation and utility message size
in DCPOP does not directly correspond to the induced width of
the cross-edged pseudotree. Instead, we use a polynomial time
method for calculating the maximum computation and utility 
message size for a given cross-edged pseudotree. A description of
this method and the pseudotree selection process appears in 
Section 5. At the end of the first phase, each agent knows its 
parent, children, pseudo-parents, pseudo-children, branch-parents, and
branch-children.
4.1 Merging Branches and Utility 
Propagation
In the original DPOP algorithm a node X only had utility 
functions involving its parent and its pseudo-parents. In DCPOP, a node
X is allowed to have a utility function involving a branch-parent.
The concept of a branch can be seen in Figure 2 with node E 
representing our node X. The two distinct paths from node E to node
B are called branches of E. The single node where all branches of
E meet is node B, which is called the merge point of E.
Agents with nodes that have branch-parents begin by sending
a utility propagation message to each branch-parent. This 
message includes a two dimensional utility hypercube with domains for
the node X and the branch-parent BP(X). It also includes a branch
information structure which contains the origination node of the
branch, X, the total number of branches originating from X, and the
number of branches originating from X that are merged into a 
single representation by this branch information structure (this 
number starts at 1). Intuitively when the number of merged branches
equals the total number of originating branches, the algorithm has
reached the merge point for X. In Figure 2, node E sends a utility
propagation message to its branch-parent, node D. This message
has dimensions for the domains of E and D, and includes branch
information with an origin of E, 2 total branches, and 1 merged
branch.
As in the original DPOP utility propagation phase, an agent at
leaf node X sends a utility propagation message to its parent. In
DCPOP this message contains dimensions for the domains of P(X)
and PP(X). If node X also has branch-parents, then the utility 
propagation message also contains a dimension for the domain of X,
and will include a branch information structure. In Figure 2, node
E sends a utility propagation message to its parent, node C. This
message has dimensions for the domains of E and C, and includes
branch information with an origin of E, 2 total branches, and 1
merged branch.
When a node Y receives utility propagation messages from all of
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 743
its children and branch-children, it merges any branches with the
same origination node X. The merged branch information structure
accumulates the number of merged branches for X. If the 
cumulative total number of merged branches equals the total number of
branches, then Y is the merge point for X. This means that the
utility hypercubes present at Y contain all information about the
valuations for utility functions involving node X. In addition to the
typical elimination of the domain of Y from the utility hypercubes,
we can now safely eliminate the domain of X from the utility 
hypercubes. To illustrate this process, we will examine what happens
in the second phase for node B in Figure 2.
In the second phase Node B receives two utility propagation
messages. The first comes from node C and includes dimensions
for domains E, B, and A. It also has a branch information structure
with origin of E, 2 total branches, and 1 merged branch. The second
comes from node D and includes dimensions for domains E and B.
It also has a branch information structure with origin of E, 2 total
branches, and 1 merged branch. Node B then merges the branch
information structures from both messages because they have the
same origination, node E. Since the number of merged branches
originating from E is now 2 and the total branches originating from
E is 2, node B now eliminates the dimensions for domain E. Node
B also eliminates the dimension for its own domain, leaving only
information about domain A. Node B then sends a utility 
propagation message to node A, containing only one dimension for the
domain of A.
Although not possible in DPOP, this method of utility 
propagation and dimension elimination may produce hypercubes at node Y
that do not share any domains. In DCPOP we do not join domain
independent hypercubes, but instead may send multiple hypercubes
in the utility propagation message sent to the parent of Y. This lazy
approach to joins helps to reduce message sizes.
4.2 Value Propagation
As in DPOP, value propagation begins when the agent at the root
node Z has received all messages from its children. At this point
the agent at node Z chooses the assignment for its domain that has
the best utility. If Z is the merge point for the branches of some
node X, Z will also choose the assignment for the domain of X.
Thus any node that is a merge point will choose assignments for
a domain other than its own. These assignments are then passed
down the primary edge hierarchy. If node X in the hierarchy has
branch-parents, then the value assignment message from P(X) will
contain an assignment for the domain of X. Every node in the 
hierarchy adds any assignments it has chosen to the ones it received
and passes the set of assignments to its children. The algorithm is
complete when all nodes have chosen or received an assignment for
their domain.
4.3 Proof of Correctness
We will prove the correctness of DCPOP by first noting that
DCPOP fully extends DPOP and then examining the two cases for
value assignment in DCPOP. Given a traditional pseudotree as 
input, the DCPOP algorithm execution is identical to DPOP. Using a
traditional pseudotree arrangement no nodes have branch-parents
or branch-children since all edges are either back-edges or tree
edges. Thus the DCPOP algorithm using a traditional pseudotree
sends only utility propagation messages that contain domains 
belonging to the parent or pseudo-parents of a node. Since no node
has any branch-parents, no branches exist, and thus no node serves
as a merge point for any other node. Thus all value propagation
assignments are chosen at the node of the assignment domain.
For DCPOP execution with cross-edged pseudotrees, some
nodes serve as merge points. We note that any node X that is not a
merge point assigns its value exactly as in DPOP. The local utility
hypercube at X contains domains for X, P(X), PP(X), and BC(X).
As in DPOP the value assignment message received at X includes
the values assigned to P(X) and PP(X). Also, since X is not a merge
point, all assignments to BC(X) must have been calculated at merge
points higher in the tree and are in the value assignment message
from P(X). Thus after eliminating domains for which assignments
are known, only the domain of X is left. The agent at node X can
now correctly choose the assignment with maximum utility for its
own domain.
If node X is a merge point for some branch-child Y, we know
that X must be a node along the path from Y to the root, and from
P(Y) and all BP(Y) to the root. From the algorithm, we know that
Y necessarily has all information from C(Y), PC(Y), and BC(Y)
since it waits for their messages. Node X has information about all
nodes below it in the tree, which would include Y, P(Y), BP(Y),
and those PP(Y) that are below X in the tree. For any PP(Y) above
X in the tree, X receives the assignment for the domain of PP(Y)
in the value assignment message from P(X). Thus X has utility 
information about all of the utility functions of which Y is a part.
By eliminating domains included in the value assignment message,
node X is left with a local utility hypercube with domains for X and
Y. The agent at node X can now correctly choose the assignments
with maximum utility for the domains of X and Y.
4.4 Complexity Analysis
The first phase of DCPOP sends one message to each P(X),
PP(X), and BP(X). The second phase sends one value assignment
message to each C(X). Thus, DCPOP produces a linear number of
messages with respect to the number of edges (utility functions) in
the cross-edged pseudotree and the original DCOP instance. The
actual complexity of DCPOP depends on two additional 
measurements: message size and computation size.
Message size and computation size in DCPOP depend on the
number of overlapping branches as well as the number of 
overlapping back-edges. It was shown in [6] that the number of 
overlapping back-edges is equal to the induced width of the pseudotree. In
a poorly constructed cross-edged pseudotree, the number of 
overlapping branches at node X can be as large as the total number
of descendants of X. Thus, the total message size in DCPOP in a
poorly constructed instance can be space-exponential in the total
number of nodes in the graph. However, in practice a well 
constructed cross-edged pseudotree can achieve much better results.
Later we address the issue of choosing well constructed 
crossedged pseudotrees from a set.
We introduce an additional measurement of the maximum 
sequential path cost through the algorithm. This measurement 
directly relates to the maximum amount of parallelism achievable by
the algorithm. To take this measurement we first store the total
computation size for each node during phase two and three. This
computation size represents the number of individual accesses to a
value in a hypercube at each node. For example, a join between two
domains of size 4 costs 4 ∗ 4 = 16. Two directed acyclic graphs
(DAG) can then be drawn; one with the utility propagation 
messages as edges and the phase two costs at nodes, and the other with
value assignment messages and the phase three costs at nodes. The
maximum sequential path cost is equal to the sum of the longest
path on each DAG from the root to any leaf node.
5. HEURISTICS
In our assessment of complexity in DCPOP we focused on the
worst case possibly produced by the algorithm. We acknowledge
744 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Algorithm 1 DCPOP Algorithm
1: DCPOP(X; D; U)
Each agent Xi executes:
Phase 1: pseudotree creation
2: elect leader from all Xj ∈ X
3: elected leader initiates pseudotree creation
4: afterwards, Xi knows P(Xi), PP(Xi), BP(Xi), C(Xi), BC(Xi)
and PC(Xi)
Phase 2: UTIL message propagation
5: if |BP(Xi)| > 0 then
6: BRANCHXi ← |BP(Xi)| + 1
7: for all Xk ∈BP(Xi) do
8: UTILXi (Xk) ←Compute utils(Xi, Xk)
9: Send message(Xk,UTILXi (Xk),BRANCHXi )
10: if |C(Xi)| = 0(i.e. Xi is a leaf node) then
11: UTILXi (P(Xi)) ← Compute utils(P(Xi),PP(Xi))
for all PP(Xi)
12: Send message(P(Xi),
UTILXi (P(Xi)),BRANCHXi )
13: Send message(PP(Xi), empty UTIL,
empty BRANCH) to all PP(Xi)
14: activate UTIL Message handler()
Phase 3: VALUE message propagation
15: activate VALUE Message handler()
END ALGORITHM
UTIL Message handler(Xk,UTILXk (Xi),
BRANCHXk )
16: store UTILXk (Xi),BRANCHXk (Xi)
17: if UTIL messages from all children and branch children arrived
then
18: for all Bj ∈BRANCH(Xi) do
19: if Bj is merged then
20: join all hypercubes where Bj ∈UTIL(Xi)
21: eliminate Bj from the joined hypercube
22: if P(Xi) == null (that means Xi is the root) then
23: v ∗ i ← Choose optimal(null)
24: Send VALUE(Xi, v ∗ i) to all C(Xi)
25: else
26: UTILXi (P(Xi)) ← Compute utils(P(Xi),
PP(Xi))
27: Send message(P(Xi),UTILXi (P(Xi)),
BRANCHXi (P(Xi)))
VALUE Message handler(VALUEXi ,P(Xi))
28: add all Xk ← v ∗ k ∈VALUEXi ,P(Xi) to agent view
29: Xi ← v ∗ i =Choose optimal(agent view)
30: Send VALUEXl , Xi to all Xl ∈C(Xi)
that in real world problems the generation of the pseudotree has
a significant impact on the actual performance. The problem of
finding the best pseudotree for a given DCOP instance is NP-Hard.
Thus a heuristic is used for generation, and the performance of the
algorithm depends on the pseudotree found by the heuristic. Some
previous research focused on finding heuristics to generate good
pseudotrees [8]. While we have developed some heuristics that
generate good cross-edged pseudotrees for use with DCPOP, our
focus has been to use multiple heuristics and then select the best
pseudotree from the generated pseudotrees.
We consider only heuristics that run in polynomial time with 
respect to the number of nodes in the original DCOP instance. The
actual DCPOP algorithm has worst case exponential complexity,
but we can calculate the maximum message size, computation size,
and sequential path cost for a given cross-edged pseudotree in 
linear space-time complexity. To do this, we simply run the algorithm
without attempting to calculate any of the local utility hypercubes
or optimal value assignments. Instead, messages include 
dimensional and branch information but no utility hypercubes.
After each heuristic completes its generation of a pseudotree, we
execute the measurement procedure and propagate the 
measurement information up to the chosen root in that pseudotree. The
root then broadcasts the total complexity for that heuristic to all
nodes. After all heuristics have had a chance to complete, every
node knows which heuristic produced the best pseudotree. Each
node then proceeds to begin the DCPOP algorithm using its 
knowledge of the pseudotree generated by the best heuristic.
The heuristics used to generate traditional pseudotrees perform
a distributed DFS traversal. The general distributed algorithm uses
a token passing mechanism and a linear number of messages. 
Improved DFS based heuristics use a special procedure to choose the
root node, and also provide an ordering function over the neighbors
of a node to determine the order of path recursion. The DFS based
heuristics used in our experiments come from the work done in [4,
8].
5.1 The best-first cross-edged pseudotree
heuristic
The heuristics used to generate cross-edged pseudotrees 
perform a best-first traversal. A general distributed best-first 
algorithm for node expansion is presented in Algorithm 2. An 
evaluation function at each node provides the values that are used to
determine the next best node to expand. Note that in this 
algorithm each node only exchanges its best value with its neighbors.
In our experiments we used several evaluation functions that took
as arguments an ordered list of ancestors and a node, which 
contains a list of neighbors (with each neighbor"s placement depth in
the tree if it was placed). From these we can calculate 
branchparents, branch-children, and unknown relationships for a potential
node placement. The best overall function calculated the value as
ancestors−(branchparents+branchchildren) with the 
number of unknown relationships being a tiebreak. After completion
each node has knowledge of its parent and ancestors, so it can 
easily determine which connected nodes are pseudo-parents, 
branchparents, pseudo-children, and branch-children.
The complexity of the best-first traversal depends on the 
complexity of the evaluation function. Assuming a complexity of O(V )
for the evaluation function, which is the case for our best 
overall function, the best-first traversal is O(V · E) which is at worst
O(n3
). For each v ∈ V we perform a place operation, and find the
next node to place using the getBestNeighbor operation. The place
operation is at most O(V ) because of the sent messages. 
Finding the next node uses recursion and traverses only already placed
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 745
Algorithm 2 Distributed Best-First Search Algorithm
root ← electedleader
next(root, ∅)
place(node, parent)
node.parent ← parent
node.ancestors ← parent.ancestors ∪ parent
send placement message (node, node.ancestors) to all 
neighbors of node
next(current, previous)
if current is not placed then
place(current, previous)
next(current, ∅)
else
best ← getBestNeighbor(current, previous)
if best = ∅ then
if previous = ∅ then
terminate, all nodes are placed
next(previous, ∅)
else
next(best, current)
getBestNeighbor(current, previous)
best ← ∅; score ← 0
for all n ∈ current.neighbors do
if n! = previous then
if n is placed then
nscore ← getBestNeighbor(n, current)
else
nscore ← evaluate(current, n)
if nscore > score then
score ← nscore
best ← n
return best, score
nodes, so it has O(V ) recursions. Each recursion performs a 
recursive getBestNeighbor operation that traverses all placed nodes
and their neighbors. This operation is O(V · E), but results can
be cached using only O(V ) space at each node. Thus we have
O(V ·(V +V +V ·E)) = O(V 2
·E). If we are smart about evaluating
local changes when each node receives placement messages from
its neighbors and cache the results the getBestNeighbor operation
is only O(E). This increases the complexity of the place operation,
but for all placements the total complexity is only O(V · E). Thus
we have an overall complexity of O(V ·E+V ·(V +E)) = O(V ·E).
6. COMPARISON OF COMPLEXITY IN
DPOP AND DCPOP
We have already shown that given the same input, DCPOP 
performs the same as DPOP. We also have shown that we can 
accurately predict performance of a given pseudotree in linear 
spacetime complexity. If we use a constant number of heuristics to 
generate the set of pseudotrees, we can choose the best pseudotree in
linear space-time complexity. We will now show that there exists
a DCOP instance for which a cross-edged pseudotree outperforms
all possible traditional pseudotrees (based on edge-traversal 
heuristics).
In Figure 3(a) we have a DCOP instance with six nodes. This
is a bipartite graph with each partition fully connected to the other
(a) (b) (c)
Figure 3: (a) The DCOP instance (b) A traditional pseudotree
arrangement for the DCOP instance (c) A cross-edged 
pseudotree arrangement for the DCOP instance
partition. In Figure 3(b) we see a traditional pseudotree 
arrangement for this DCOP instance. It is easy to see that any 
edgetraversal based heuristic cannot expand two nodes from the same
partition in succession. We also see that no node can have more
than one child because any such arrangement would be an invalid
pseudotree. Thus any traditional pseudotree arrangement for this
DCOP instance must take the form of Figure 3(b). We can see that
the back-edges F-B and F-A overlap node C. Node C also has a
parent E, and a back-edge with D. Using the original DPOP 
algorithm (or DCPOP since they are identical in this case), we find that
the computation at node C involves five domains: A, B, C, D, and
E.
In contrast, the cross-edged pseudotree arrangement in 
Figure 3(c) requires only a maximum of four domains in any 
computation during DCPOP. Since node A is the merge point for branches
from both B and C, we can see that each of the nodes D, E, and F
have two overlapping branches. In addition each of these nodes has
node A as its parent. Using the DCPOP algorithm we find that the
computation at node D (or E or F) involves four domains: A, B, C,
and D (or E or F).
Since no better traditional pseudotree arrangement can be 
created using an edge-traversal heuristic, we have shown that DCPOP
can outperform DPOP even if we use the optimal pseudotree found
through edge-traversal. We acknowledge that pseudotree 
arrangements that allow parent-child relationships without an actual 
constraint can solve the problem in Figure 3(a) with maximum 
computation size of four domains. However, current heuristics used
with DPOP do not produce such pseudotrees, and such a heuristic
would be difficult to distribute since each node would require 
information about nodes with which it has no constraint. Also, while we
do not prove it here, cross-edged pseudotrees can produce smaller
message sizes than such pseudotrees even if the computation size
is similar. In practice, since finding the best pseudotree 
arrangement is NP-Hard, we find that heuristics that produce cross-edged
pseudotrees often produce significantly smaller computation and
message sizes.
7. EXPERIMENTAL RESULTS
746 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Existing performance metrics for DCOP algorithms include the
total number of messages, synchronous clock cycles, and message
size. We have already shown that the total number of messages is
linear with respect to the number of constraints in the DCOP 
instance. We also introduced the maximum sequential path cost (PC)
as a measurement of the maximum amount of parallelism 
achievable by the algorithm. The maximum sequential path cost is equal
to the sum of the computations performed on the longest path from
the root to any leaf node. We also include as metrics the 
maximum computation size in number of dimensions (CD) and 
maximum message size in number of dimensions (MD). To analyze the
relative complexity of a given DCOP instance, we find the 
minimum induced width (IW) of any traditional pseudotree produced
by a heuristic for the original DPOP.
7.1 Generic DCOP instances
For our initial tests we randomly generated two sets of problems
with 3000 cases in each. Each problem was generated by 
assigning a random number (picked from a range) of constraints to each
variable. The generator then created binary constraints until each
variable reached its maximum number of constraints. The first set
uses 20 variables, and the best DPOP IW ranges from 1 to 16 with
an average of 8.5. The second set uses 100 variables, and the best
DPOP IW ranged from 2 to 68 with an average of 39.3. Since most
of the problems in the second set were too complex to actually 
compute the solution, we took measurements of the metrics using the
techniques described earlier in Section 5 without actually solving
the problem. Results are shown for the first set in Table 1 and for
the second set in Table 2.
For the two problem sets we split the cases into low density and
high density categories. Low density cases consist of those 
problems that have a best DPOP IW less than or equal to half of the
total number of nodes (e.g. IW ≤ 10 for the 20 node problems
and IW ≤ 50 for the 100 node problems). High density problems
consist of the remainder of the problem sets.
In both Table 1 and Table 2 we have listed performance 
metrics for the original DPOP algorithm, the DCPOP algorithm using
only cross-edged pseudotrees (DCPOP-CE), and the DCPOP 
algorithm using traditional and cross-edged pseudotrees (DCPOP-All).
The pseudotrees used for DPOP were generated using 5 
heuristics: DFS, DFS MCN, DFS CLIQUE MCN, DFS MCN DSTB,
and DFS MCN BEC. These are all versions of the guided DFS
traversal discussed in Section 5. The cross-edged pseudotrees used
for DCPOP-CE were generated using 5 heuristics: MCN, LCN,
MCN A-B, LCN A-B, and LCSG A-B. These are all versions of
the best-first traversal discussed in Section 5.
For both DPOP and DCPOP-CE we chose the best pseudotree
produced by their respective 5 heuristics for each problem in the
set. For DCPOP-All we chose the best pseudotree produced by all
10 heuristics for each problem in the set. For the CD and MD 
metrics the value shown is the average number of dimensions. For the
PC metric the value shown is the natural logarithm of the 
maximum sequential path cost (since the actual value grows 
exponentially with the complexity of the problem).
The final row in both tables is a measurement of improvement
of DCPOP-All over DPOP. For the CD and MD metrics the value
shown is a reduction in number of dimensions. For the PC metric
the value shown is a percentage reduction in the maximum 
sequential path cost (% = DP OP −DCP OP
DCP OP
∗ 100). Notice that 
DCPOPAll outperforms DPOP on all metrics. This logically follows from
our earlier assertion that given the same input, DCPOP performs
exactly the same as DPOP. Thus given the choice between the 
pseudotrees produced by all 10 heuristics, DCPOP-All will always 
outLow Density High Density
Algorithm CD MD PC CD MD PC
DPOP 7.81 6.81 3.78 13.34 12.34 5.34
DCPOP-CE 7.94 6.73 3.74 12.83 11.43 5.07
DCPOP-All 7.62 6.49 3.66 12.72 11.36 5.05
Improvement 0.18 0.32 13% 0.62 0.98 36%
Table 1: 20 node problems
Low Density High Density
Algorithm CD MD PC CD MD PC
DPOP 33.35 32.35 14.55 58.51 57.50 19.90
DCPOP-CE 33.49 29.17 15.22 57.11 50.03 20.01
DCPOP-All 32.35 29.57 14.10 56.33 51.17 18.84
Improvement 1.00 2.78 104% 2.18 6.33 256%
Table 2: 100 node problems
Figure 4: Computation Dimension Size
Figure 5: Message Dimension Size
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 747
Figure 6: Path Cost
DCPOP Improvement
Ag Mtg Vars Const IW CD MD PC
10 4 12 13.5 2.25 -0.01 -0.01 5.6%
30 14 44 57.6 3.63 0.09 0.09 10.9%
50 24 76 101.3 4.17 0.08 0.09 10.7%
100 49 156 212.9 5.04 0.16 0.20 30.0%
150 74 236 321.8 5.32 0.21 0.23 35.8%
200 99 316 434.2 5.66 0.18 0.22 29.5%
Table 3: Meeting Scheduling Problems
perform DPOP. Another trend we notice is that the improvement is
greater for high density problems than low density problems. We
show this trend in greater detail in Figures 4, 5, and 6. Notice
how the improvement increases as the complexity of the problem
increases.
7.2 Meeting Scheduling Problem
In addition to our initial generic DCOP tests, we ran a series
of tests on the Meeting Scheduling Problem (MSP) as described
in [6]. The problem setup includes a number of people that are
grouped into departments. Each person must attend a specified
number of meetings. Meetings can be held within departments or
among departments, and can be assigned to one of eight time slots.
The MSP maps to a DCOP instance where each variable represents
the time slot that a specific person will attend a specific meeting.
All variables that belong to the same person have mutual exclusion
constraints placed so that the person cannot attend more than one
meeting during the same time slot. All variables that belong to the
same meeting have equality constraints so that all of the 
participants choose the same time slot. Unary constraints are placed on
each variable to account for a person"s valuation of each meeting
and time slot.
For our tests we generated 100 sample problems for each 
combination of agents and meetings. Results are shown in Table 3. The
values in the first five columns represent (in left to right order), the
total number of agents, the total number of meetings, the total 
number of variables, the average total number of constraints, and the
average minimum IW produced by a traditional pseudotree. The
last three columns show the same metrics we used for the generic
DCOP instances, except this time we only show the improvements
of DCPOP-All over DPOP. Performance is better on average for
all MSP instances, but again we see larger improvements for more
complex problem instances.
8. CONCLUSIONS AND FUTURE WORK
We presented a complete, distributed algorithm that solves 
general DCOP instances using cross-edged pseudotree arrangements.
Our algorithm extends the DPOP algorithm by adding additional
utility propagation messages, and introducing the concept of branch
merging during the utility propagation phase. Our algorithm also
allows value assignments to occur at higher level merge points
for lower level nodes. We have shown that DCPOP fully extends
DPOP by performing the same operations given the same input.
We have also shown through some examples and experimental data
that DCPOP can achieve greater performance for some problem 
instances by extending the allowable input set to include cross-edged
pseudotrees.
We placed particular emphasis on the role that edge-traversal
heuristics play in the generation of pseudotrees. We have shown
that the performance penalty is minimal to generate multiple
heuristics, and that we can choose the best generated pseudotree
in linear space-time complexity. Given the importance of a good
pseudotree for performance, future work will include new 
heuristics to find better pseudotrees. Future work will also include 
adapting existing DPOP extensions [5, 7] that support different problem
domains for use with DCPOP.
9. REFERENCES
[1] J. Liu and K. P. Sycara. Exploiting problem structure for
distributed constraint optimization. In V. Lesser, editor,
Proceedings of the First International Conference on
Multi-Agent Systems, pages 246-254, San Francisco, CA,
1995. MIT Press.
[2] P. J. Modi, H. Jung, M. Tambe, W.-M. Shen, and S. Kulkarni.
A dynamic distributed constraint satisfaction approach to
resource allocation. Lecture Notes in Computer Science,
2239:685-700, 2001.
[3] P. J. Modi, W. Shen, M. Tambe, and M. Yokoo. An
asynchronous complete method for distributed constraint
optimization. In AAMAS 03, 2003.
[4] A. Petcu. Frodo: A framework for open/distributed
constraint optimization. Technical Report No. 2006/001
2006/001, Swiss Federal Institute of Technology (EPFL),
Lausanne (Switzerland), 2006. http://liawww.epfl.ch/frodo/.
[5] A. Petcu and B. Faltings. A-dpop: Approximations in
distributed optimization. In poster in CP 2005, pages
802-806, Sitges, Spain, October 2005.
[6] A. Petcu and B. Faltings. Dpop: A scalable method for
multiagent constraint optimization. In IJCAI 05, pages
266-271, Edinburgh, Scotland, Aug 2005.
[7] A. Petcu, B. Faltings, and D. Parkes. M-dpop: Faithful
distributed implementation of efficient social choice
problems. In AAMAS 06, pages 1397-1404, Hakodate,
Japan, May 2006.
[8] G. Ushakov. Solving meeting scheduling problems using
distributed pseudotree-optimization procedure. Master"s
thesis, ´Ecole Polytechnique F´ed´erale de Lausanne, 2005.
[9] M. Yokoo, E. H. Durfee, T. Ishida, and K. Kuwabara.
Distributed constraint satisfaction for formalizing distributed
problem solving. In International Conference on Distributed
Computing Systems, pages 614-621, 1992.
[10] M. Yokoo, E. H. Durfee, T. Ishida, and K. Kuwabara. The
distributed constraint satisfaction problem: Formalization
and algorithms. Knowledge and Data Engineering,
10(5):673-685, 1998.
748 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Normative System Games
Thomas
◦
Agotnes
Dept of Computer Engineering
Bergen University College
PB. 2030, N-5020 Bergen
Norway
tag@hib.no
Wiebe van der Hoek
Dept of Computer Science
University of Liverpool
Liverpool L69 7ZF
UK
wiebe@csc.liv.ac.uk
Michael Wooldridge
Dept of Computer Science
University of Liverpool
Liverpool L69 7ZF
UK
mjw@csc.liv.ac.uk
ABSTRACT
We develop a model of normative systems in which agents are 
assumed to have multiple goals of increasing priority, and 
investigate the computational complexity and game theoretic properties of
this model. In the underlying model of normative systems, we use
Kripke structures to represent the possible transitions of a 
multiagent system. A normative system is then simply a subset of the
Kripke structure, which contains the arcs that are forbidden by the
normative system. We specify an agent"s goals as a hierarchy of
formulae of Computation Tree Logic (CTL), a widely used logic
for representing the properties of Kripke structures: the intuition is
that goals further up the hierarchy are preferred by the agent over
those that appear further down the hierarchy. Using this scheme,
we define a model of ordinal utility, which in turn allows us to
interpret our Kripke-based normative systems as games, in which
agents must determine whether to comply with the normative 
system or not. We then characterise the computational complexity of
a number of decision problems associated with these Kripke-based
normative system games; for example, we show that the 
complexity of checking whether there exists a normative system which has
the property of being a Nash implementation is NP-complete.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems;
I.2.4 [Knowledge representation formalisms and methods]
General Terms
Theory
1. INTRODUCTION
Normative systems, or social laws, have proved to be an attractive
approach to coordination in multi-agent systems [13, 14, 10, 15, 1].
Although the various approaches to normative systems proposed in
the literature differ on technical details, they all share the same 
basic intuition that a normative system is a set of constraints on the
behaviour of agents in the system; by imposing these constraints,
it is hoped that some desirable objective will emerge. The idea of
using social laws to coordinate multi-agent systems was proposed
by Shoham and Tennenholtz [13, 14]; their approach was extended
by van der Hoek et al. to include the idea of specifying a desirable
global objective for a social law as a logical formula, with the idea
being that the normative system would be regarded as successful
if, after implementing it (i.e., after eliminating all forbidden 
actions), the objective formula was guaranteed to be satisfied in the
system [15]. However, this model did not take into account the
preferences of individual agents, and hence neglected to account
for possible strategic behaviour by agents when deciding whether
to comply with the normative system or not. This model of 
normative systems was further extended by attributing to each agent
a single goal in [16]. However, this model was still too 
impoverished to capture the kinds of decision making that take place when
an agent decides whether or not to comply with a social law. In
reality, strategic considerations come into play: an agent takes into
account not just whether the normative system would be beneficial
for itself, but also whether other agents will rationally choose to
participate.
In this paper, we develop a model of normative systems in which
agents are assumed to have multiple goals, of increasing priority.
We specify an agent"s goals as a hierarchy of formulae of 
Computation Tree Logic (CTL), a widely used logic for representing the
properties of Kripke structures [8]: the intuition is that goals further
up the hierarchy are preferred by the agent over those that appear
further down the hierarchy. Using this scheme, we define a model
of ordinal utility, which in turn allows us to interpret our 
Kripkebased normative systems as games, in which agents must determine
whether to comply with the normative system or not. We thus 
provide a very natural bridge between logical structures and languages
and the techniques and concepts of game theory, which have proved
to be very powerful for analysing social contract-style scenarios
such as normative systems [3, 4]. We then characterise the 
computational complexity of a number of decision problems associated
with these Kripke-based normative system games; for example, we
show that the complexity of checking whether there exists a 
normative system which has the property of being a Nash implementation
is NP-complete.
2. KRIPKE STRUCTURES AND CTL
We use Kripke structures as our basic semantic model for 
multiagent systems [8]. A Kripke structure is essentially a directed
graph, with the vertex set S corresponding to possible states of the
system being modelled, and the relation R ⊆ S × S capturing the
881
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
possible transitions of the system; intuitively, these transitions are
caused by agents in the system performing actions, although we do
not include such actions in our semantic model (see, e.g., [13, 2,
15] for related models which include actions as first class citizens).
We let S0
denote the set of possible initial states of the system.
Our model is intended to correspond to the well-known interleaved
concurrency model from the reactive systems literature: thus an
arc corresponds to the execution of an atomic action by one of the
processes in the system, which we call agents.
It is important to note that, in contrast to such models as [2, 15],
we are therefore here not modelling synchronous action. This 
assumption is not in fact essential for our analysis, but it greatly 
simplifies the presentation. However, we find it convenient to include
within our model the agents that cause transitions. We therefore
assume a set A of agents, and we label each transition in R with
the agent that causes the transition via a function α : R → A. 
Finally, we use a vocabulary Φ = {p, q, . . .} of Boolean variables
to express the properties of individual states S: we use a function
V : S → 2Φ
to label each state with the Boolean variables true (or
satisfied) in that state.
Collecting these components together, an agent-labelled Kripke
structure (over Φ) is a 6-tuple:
K = S, S0
, R, A, α, V , where:
• S is a finite, non-empty set of states,
• S0
⊆ S (S0
= ∅) is the set of initial states;
• R ⊆ S × S is a total binary relation on S, which we refer to
as the transition relation1
;
• A = {1, . . . , n} is a set of agents;
• α : R → A labels each transition in R with an agent; and
• V : S → 2Φ
labels each state with the set of propositional
variables true in that state.
In the interests of brevity, we shall hereafter refer to an 
agentlabelled Kripke structure simply as a Kripke structure. A path
over a transition relation R is an infinite sequence of states π =
s0, s1, . . . which must satisfy the property that ∀u ∈ N: (su , su+1) ∈
R. If u ∈ N, then we denote by π[u] the component indexed by
u in π (thus π[0] denotes the first element, π[1] the second, and so
on). A path π such that π[0] = s is an s-path. Let ΠR(s) denote
the set of s-paths over R; since it will usually be clear from 
context, we often omit reference to R, and simply write Π(s). We will
sometimes refer to and think of an s-path as a possible 
computation, or system evolution, from s.
EXAMPLE 1. Our running example is of a system with a single
non-sharable resource, which is desired by two agents. Consider
the Kripke structure depicted in Figure 1. We have two states, s and
t, and two corresponding Boolean variables p1 and p2, which are
1
In the branching time temporal logic literature, a relation R ⊆
S × S is said to be total iff ∀s ∃s : (s, s ) ∈ R. Note that
the term total relation is sometimes used to refer to relations
R ⊆ S × S such that for every pair of elements s, s ∈ S we
have either (s, s ) ∈ R or (s , s) ∈ R; we are not using the term
in this way here. It is also worth noting that for some domains,
other constraints may be more appropriate than simple totality. For
example, one might consider the agent totality requirement, that in
every state, every agent has at least one possible transition 
available: ∀s∀i ∈ A∃s : (s, s ) ∈ R and α(s, s ) = i.
2p
t
p
2
2
1
s
1
1
Figure 1: The resource control running example.
mutually exclusive. Think of pi as meaning agent i has currently
control over the resource. Each agent has two possible actions,
when in possession of the resource: either give it away, or keep it.
Obviously there are infinitely many different s-paths and t-paths.
Let us say that our set of initial states S0
equals {s, t}, i.e., we
don"t make any assumptions about who initially has control over
the resource.
2.1 CTL
We now define Computation Tree Logic (CTL), a branching time
temporal logic intended for representing the properties of Kripke
structures [8]. Note that since CTL is well known and widely 
documented in the literature, our presentation, though complete, will be
somewhat terse. We will use CTL to express agents" goals.
The syntax of CTL is defined by the following grammar:
ϕ ::= | p | ¬ϕ | ϕ ∨ ϕ | E fϕ | E(ϕ U ϕ) | A fϕ | A(ϕ U ϕ)
where p ∈ Φ. We denote the set of CTL formula over Φ by LΦ;
since Φ is understood, we usually omit reference to it.
The semantics of CTL are given with respect to the satisfaction
relation |=, which holds between pairs of the form K, s, (where
K is a Kripke structure and s is a state in K), and formulae of the
language. The satisfaction relation is defined as follows:
K, s |= ;
K, s |= p iff p ∈ V (s) (where p ∈ Φ);
K, s |= ¬ϕ iff not K, s |= ϕ;
K, s |= ϕ ∨ ψ iff K, s |= ϕ or K, s |= ψ;
K, s |= A fϕ iff ∀π ∈ Π(s) : K, π[1] |= ϕ;
K, s |= E fϕ iff ∃π ∈ Π(s) : K, π[1] |= ϕ;
K, s |= A(ϕ U ψ) iff ∀π ∈ Π(s), ∃u ∈ N, s.t. K, π[u] |= ψ
and ∀v, (0 ≤ v < u) : K, π[v] |= ϕ
K, s |= E(ϕ U ψ) iff ∃π ∈ Π(s), ∃u ∈ N, s.t. K, π[u] |= ψ
and ∀v, (0 ≤ v < u) : K, π[v] |= ϕ
The remaining classical logic connectives (∧, →, ↔) are
assumed to be defined as abbreviations in terms of ¬, ∨, in the
conventional manner. The remaining CTL temporal operators are
defined:
A♦ϕ ≡ A( U ϕ) E♦ϕ ≡ E( U ϕ)
A ϕ ≡ ¬E♦¬ϕ E ϕ ≡ ¬A♦¬ϕ
We say ϕ is satisfiable if K, s |= ϕ for some Kripke structure K
and state s in K; ϕ is valid if K, s |= ϕ for all Kripke structures
K and states s in K. The problem of checking whether K, s |= ϕ
for given K, s, ϕ (model checking) can be done in deterministic
polynomial time, while checking whether a given ϕ is satisfiable or
whether ϕ is valid is EXPTIME-complete [8]. We write K |= ϕ if
K, s0 |= ϕ for all s0 ∈ S0
, and |= ϕ if K |= ϕ for all K.
882 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
3. NORMATIVE SYSTEMS
For our purposes, a normative system is simply a set of constraints
on the behaviour of agents in a system [1]. More precisely, a 
normative system defines, for every possible system transition, whether
or not that transition is considered to be legal or not. Different
normative systems may differ on whether or not a transition is
legal. Formally, a normative system η (w.r.t. a Kripke structure
K = S, S0
, R, A, α, V ) is simply a subset of R, such that R \ η
is a total relation. The requirement that R\η is total is a 
reasonableness constraint: it prevents normative systems which lead to states
with no successor. Let N (R) = {η : (η ⊆ R) & (R \ η is total)}
be the set of normative systems over R. The intended 
interpretation of a normative system η is that (s, s ) ∈ η means transition
(s, s ) is forbidden in the context of η; hence R \ η denotes the
legal transitions of η. Since it is assumed η is reasonable, we are
guaranteed that a legal outward transition exists for every state. We
denote the empty normative system by η∅, so η∅ = ∅. Note that
the empty normative system η∅ is reasonable with respect to any
transition relation R.
The effect of implementing a normative system on a Kripke 
structure is to eliminate from it all transitions that are forbidden 
according to this normative system (see [15, 1]). If K is a Kripke 
structure, and η is a normative system over K, then K † η denotes the
Kripke structure obtained from K by deleting transitions forbidden
in η. Formally, if K = S, S0
, R, A, α, V , and η ∈ N (R), then
let K†η = K be the Kripke structure K = S , S0
, R , A , α , V
where:
• S = S , S0
= S0
, A = A , and V = V ;
• R = R \ η; and
• α is the restriction of α to R :
α (s, s ) =
j
α(s, s ) if (s, s ) ∈ R
undefined otherwise.
Notice that for all K, we have K † η∅ = K.
EXAMPLE 1. (continued) When thinking in terms of fairness, it
seems natural to consider normative systems η that contain (s, s)
or (t, t). A normative system with (s, t) would not be fair, in the
sense that A♦A ¬p1 ∨ A♦A ¬p2 holds: in all paths, from
some moment on, one agent will have control forever. Let us, for
later reference, fix η1 = {(s, s)}, η2 = {(t, t)}, and η3 = {(s, s),
(t, t)}.
Later, we will address the issue of whether or not agents should
rationally choose to comply with a particular normative system. In
this context, it is useful to define operators on normative systems
which correspond to groups of agents defecting from the 
normative system. Formally, let K = S, S0
,R, A,α, V be a Kripke
structure, let C ⊆ A be a set of agents over K, and let η be a
normative system over K. Then:
• η C denotes the normative system that is the same as η
except that it only contains the arcs of η that correspond to
the actions of agents in C. We call η C the restriction of η
to C, and it is defined as:
η C = {(s, s ) : (s, s ) ∈ η & α(s, s ) ∈ C}.
Thus K † (η C) is the Kripke structure that results if only
the agents in C choose to comply with the normative system.
• η C denotes the normative system that is the same as η 
except that it only contains the arcs of η that do not correspond
to actions of agents in C. We call η C the exclusion of C
from η, and it is defined as:
η C = {(s, s ) : (s, s ) ∈ η & α(s, s ) ∈ C}.
Thus K † (η C) is the Kripke structure that results if only
the agents in C choose not to comply with the normative
system (i.e., the only ones who comply are those in A \ C).
Note that we have η C = η (A\C) and η C = η (A\C).
EXAMPLE 1. (Continued) We have η1 {1} = η1 = {(s, s)},
while η1 {1} = η∅ = η1 {2}. Similarly, we have η3 {1} =
{(s, s)} and η3 {1} = {(t, t)}.
4. GOALS AND UTILITIES
Next, we want to be able to capture the goals that agents have, as
these will drive an agent"s strategic considerations - particularly, as
we will see, considerations about whether or not to comply with a
normative system. We will model an agent"s goals as a prioritised
list of CTL formulae, representing increasingly desired properties
that the agent wishes to hold. The intended interpretation of such a
goal hierarchy γi for agent i ∈ A is that the further up the 
hierarchy a goal is, the more it is desired by i. Note that we assume
that if an agent can achieve a goal at a particular level in its goal
hierarchy, then it is unconcerned about goals lower down the 
hierarchy. Formally, a goal hierarchy, γ, (over a Kripke structure K)
is a finite, non-empty sequence of CTL formulae
γ = (ϕ0, ϕ1, . . . , ϕk )
in which, by convention, ϕ0 = . We use a natural number 
indexing notation to extract the elements of a goal hierarchy, so if
γ = (ϕ0, ϕ1, . . . , ϕk ) then γ[0] = ϕ0, γ[1] = ϕ1, and so on. We
denote the largest index of any element in γ by |γ|.
A particular Kripke structure K is said to satisfy a goal at 
index x in goal hierarchy γ if K |= γ[x], i.e., if γ[x] is satisfied in all
initial states S0
of K. An obvious potential property of goal 
hierarchies is monotonicity: where goals at higher levels in the hierarchy
logically imply those at lower levels in the hierarchy. Formally, a
goal hierarchy γ is monotonic if for all x ∈ {1, . . . , |γ|} ⊆ N, we
have |= γ[x] → γ[x − 1]. The simplest type of monotonic goal
hierarchy is where γ[x + 1] = γ[x] ∧ ψx+1 for some ψx+1, so at
each successive level of the hierarchy, we add new constraints to
the goal of the previous level. Although this is a natural property
of many goal hierarchies, it is not a property we demand of all goal
hierarchies.
EXAMPLE 1. (continued) Suppose the agents have similar, but
opposing goals: each agent i wants to keep the source as often and
long as possible for himself. Define each agent"s goal hierarchy as:
γi = ( ϕi
0 = , ϕi
1 = E♦pi ,
ϕi
2 = E E♦pi , ϕi
3 = E♦E pi ,
ϕi
4 = A E♦pi , ϕi
5 = E♦A pi
ϕi
6 = A A♦pi , ϕi
7 = A (A♦pi ∧ E pi ),
ϕi
8 = A pi )
The most desired goal of agent i is to, in every computation, 
always have the resource, pi (this is expressed in ϕi
8). Thanks to our
reasonableness constraint, this goal implies ϕi
7 which says that, no
matter how the computation paths evolve, it will always be that all
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 883
continuations will hit a point in which pi , and, moreover, there is a
continuation in which pi always holds. Goal ϕi
6 is a fairness 
constraint implied by it. Note that A♦pi says that every computation
eventually reaches a pi state. This may mean that after pi has 
happened, it will never happen again. ϕi
6 circumvents this: it says that,
no matter where you are, there should be a future pi state. The goal
ϕi
5 is like the strong goal ϕi
8 but it accepts that this is only achieved
in some computation, eventually. ϕi
4 requires that in every path,
there is always a continuation that eventually gives pi . Goal ϕi
3
says that pi should be true on some branch, from some moment on.
It implies ϕi
2 which expresses that there is a computation such that
everywhere during it, it is possible to choose a continuation that
eventually satisfies pi . This implies ϕi
1, which says that pi should
at least not be impossible. If we even drop that demand, we have
the trivial goal ϕi
0.
We remark that it may seem more natural to express a fairness
constraint ϕi
6 as A ♦pi . However, this is not a proper CTL 
formula. It is in fact a formula in CTL
∗
[9], and in this logic, the two
expressions would be equivalent. However, our basic complexity
results in the next sections would not hold for the richer language
CTL
∗2
, and the price to pay for this is that we have to formulate
our desired goals in a somewhat more cumbersome manner than
we might ideally like. Of course, our basic framework does not
demand that goals are expressed in CTL; they could equally well
be expressed in CTL
∗
or indeed ATL [2] (as in [15]). We 
comment on the implications of alternative goal representations at the
conclusion of the next section.
A multi-agent system collects together a Kripke structure 
(representing the basic properties of a system under consideration: its
state space, and the possible state transitions that may occur in it),
together with a goal hierarchy, one for each agent, representing the
aspirations of the agents in the system. Formally, a multi-agent
system, M , is an (n + 1)-tuple:
M = K, γ1, . . . , γn
where K is a Kripke structure, and for each agent i in K, γi is a
goal hierarchy over K.
4.1 The Utility of Normative Systems
We can now define the utility of a Kripke structure for an agent.
The idea is that the utility of a Kripke structure is the highest index
of any goal that is guaranteed for that agent in the Kripke structure.
We make this precise in the function ui (·):
ui (K) = max{j : 0 ≤ j ≤ |γi | & K |= γi [j ]}
Note that using these definitions of goals and utility, it never
makes sense to have a goal ϕ at index n if there is a logically
weaker goal ψ at index n + k in the hierarchy: by definition of
utility, it could never be n for any structure K.
EXAMPLE 1. (continued) Let M = K, γ1, γ2 be the 
multiagent system of Figure 1, with γ1 and γ2 as defined earlier in this
example. Recall that we have defined S0
as {s, t}. Then, u1(K) =
u2(K) = 4: goal ϕ4 is true in S0
, but ϕ5 is not. To see that
ϕ2
4 = A E♦p2 is true in s for instance: note that on ever path it
is always the case that there is a transition to t, in which p2 is true.
Notice that since for any goal hierarchy γi we have γ[0] = ,
then for all Kripke structures, ui (K) is well defined, with ui (K) ≥
2
CTL
∗
model checking is PSPACE-complete, and hence much
worse (under standard complexity theoretic assumptions) than
model checking CTL [8].
η δ1(K, η) δ2(K, η)
η∅ 0 0
η1 0 3
η2 3 0
η3 2 2
C D
C (2, 2) (0, 3)
D (3, 0) (0, 0)
Figure 2: Benefits of implementing a normative system η (left)
and pay-offs for the game ΣM .
0. Note that this is an ordinal utility measure: it tells us, for any
given agent, the relative utility of different Kripke structures, but
utility values are not on some standard system-wide scale. The fact
that ui (K1) > ui (K2) certainly means that i strictly prefers K1
over K2, but the fact that ui (K) > uj (K) does not mean that i
values K more highly than j . Thus, it does not make sense to 
compare utility values between agents, and so for example, some system
wide measures of utility, (notably those measures that aggregate 
individual utilities, such as social welfare), do not make sense when
applied in this setting. However, as we shall see shortly, other 
measures - such as Pareto efficiency - can be usefully applied.
There are other representations for goals, which would allow us
to define cardinal utilities. The simplest would be to specify goals γ
for an agent as a finite, non-empty, one-to-one relation: γ ⊆ L×R.
We assume that the x values in pairs (ϕ, x) ∈ γ are specified so
that x for agent i means the same as x for agent j , and so we have
cardinal utility. We then define the utility for i of a Kripke structure
K asui (K) = max{x : (ϕ, x) ∈ γi & K |= ϕ}. The results of
this paper in fact hold irrespective of which of these representations
we actually choose; we fix upon the goal hierarchy approach in the
interests of simplicity.
Our next step is to show how, in much the same way, we can lift
the utility function from Kripke structures to normative systems.
Suppose we are given a multi-agent system M = K, γ1, . . . , γn
and an associated normative system η over K. Let for agent i,
δi (K, K ) be the difference in his utility when moving from K to
K : δi (K, K ) = ui (K )− ui (K). Then the utility of η to agent i
wrt K is δi (K, K † η). We will sometimes abuse notation and just
write δi (K, η) for this, and refer to it as the benefit for agent i of
implementing η in K. Note that this benefit can be negative.
Summarising, the utility of a normative system to an agent is the
difference between the utility of the Kripke structure in which the
normative system was implemented and the original Kripke 
structure. If this value is greater than 0, then the agent would be better
off if the normative system were imposed, while if it is less than
0 then the agent would be worse off if η were imposed than in the
original system. We say η is individually rational for i wrt K if
δi (K, η) > 0, and individually rational simpliciter if η is 
individually rational for every agent.
A social system now is a pair
Σ = M , η
where M is a multi-agent system, and η is a normative system over
M .
EXAMPLE 1. The table at the left hand in Figure 2 displays the
utilities δi (K, η) of implementing η in the Kripke structure of our
running example, for the normative systems η = η∅, η1, η2 and η3,
introduced before. Recall that u1(K) = u2(K) = 4.
4.2 Universal and Existential Goals
884 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Keeping in mind that a norm η restricts the possible transitions
of the model under consideration, we make the following 
observation, borrowing from [15]. Some classes of goals are monotonic
or anti-monotonic with respect to adding additional constraints to
a system. Let us therefore define two fragments of the language
of CTL: the universal language Lu
with typical element μ, and the
existential fragment Le
with typical element ε.
μ ::= | p | ¬p | μ ∨ μ | A fμ | A μ | A(μ U μ)
ε ::= | p | ¬p | ε ∨ ε | E fε | E♦ε | E(ε U ε)
Let us say, for two Kripke structures K1 = S, S0
, R1, A, α, V
and K2 = S, S0
, R2, A, α, V that K1 is a subsystem of K2 and
K2 is a supersystem of K1, written K1 K2 iff R1 ⊆ R2. Note
that typically K † η K. Then we have (cf. [15]).
THEOREM 1. Suppose K1 K2, and s ∈ S. Then
∀ε ∈ Le
: K1, s |= ε ⇒ K2, s |= ε
∀μ ∈ Lu
: K2, s |= μ ⇒ K1, s |= μ
This has the following effect on imposing a new norm:
COROLLARY 1. Let K be a structure, and η a normative 
system. Let γi denote a goal hierarchy for agent i.
1. Suppose agent i"s utility ui (K) is n, and γi [n] ∈ Lu
, (i.e.,
γi [n] is a universal formula). Then, for any normative system
η, δi (K, η) ≥ 0.
2. Suppose agent i"s utility ui (K † η) is n, and γi [n] is an
existential formula ε. Then, δi (K † η, K) ≥ 0.
Corollary 1"s first item says that an agent whose current 
maximal goal in a system is a universal formula, need never fear the
imposition of a new norm η. The reason is that his current goal will
at least remain true (in fact a goal higher up in the hierarchy may
become true). It follows from this that an agent with only universal
goals can only gain from the imposition of normative systems η.
The opposite is true for existential goals, according to the second
item of the corollary: it can never be bad for an agent to undo a
norm η. Hence, an agent with only existential goals might well fear
any norm η.
However, these observations implicitly assume that all agents in
the system will comply with the norm. Whether they will in fact do
so, of course, is a strategic decision: it partly depends on what the
agent thinks that other agents will do. This motivates us to consider
normative system games.
5. NORMATIVE SYSTEM GAMES
We now have a principled way of talking about the utility of 
normative systems for agents, and so we can start to apply the technical
apparatus of game theory to analyse them.
Suppose we have a multi-agent system M = K, γ1, . . . , γn
and a normative system η over K. It is proposed to the agents
in M that η should be imposed on K, (typically to achieve some
coordination objective). Our agent - let"s say agent i - is then faced
with a choice: should it comply with the strictures of the normative
system, or not? Note that this reasoning takes place before the agent
is in the system - it is a design time consideration.
We can understand the reasoning here as a game, as follows. A
game in strategic normal form (cf. [11, p.11]) is a structure:
G = AG, S1, . . . , Sn , U1, . . . , Un where:
• AG = {1, . . . , n} is a set of agents - the players of the game;
• Si is the set of strategies for each agent i ∈ AG (a strategy
for an agent i is nothing else than a choice between 
alternative actions); and
• Ui : (S1 × · · · × Sn ) → R is the utility function for agent
i ∈ AG, which assigns a utility to every combination of
strategy choices for the agents.
Now, suppose we are given a social system Σ = M , η where
M = K, γ1, . . . , γn . Then we can associate a game - the 
normative system game - GΣ with Σ, as follows. The agents AG in GΣ
are as in Σ. Each agent i has just two strategies available to it:
• C - comply (cooperate) with the normative system; and
• D - do not comply with (defect from) the normative system.
If S is a tuple of strategies, one for each agent, and x ∈ {C, D},
then we denote by AGx
S the subset of agents that play strategy x in
S. Hence, for a social system Σ = M , η , the normative system
η AGC
S only implements the restrictions for those agents that
choose to cooperate in GΣ. Note that this is the same as η AGD
S :
the normative system that excludes all the restrictions of agents that
play D in GΣ. We then define the utility functions Ui for each
i ∈ AG as:
Ui (S) = δi (K, η AGC
S ).
So, for example, if SD is a collection of strategies in which every
agent defects (i.e., does not comply with the norm), then
Ui (SD ) = δi (K, (η AGD
SD
)) = ui (K † η∅) − ui (K) = 0.
In the same way, if SC is a collection of strategies in which every
agent cooperates (i.e., complies with the norm), then
Ui (SC ) = δi (K, (η AGD
SC
)) = ui (K † (η ∅)) = ui (K † η).
We can now start to investigate some properties of normative
system games.
EXAMPLE 1. (continued) For our example system, we have 
displayed the different U values for our multi agent system with the
norm η3, i.e., {(s, s), (t, t)} as the second table of Figure 2. For
instance, the pair (0, 3) in the matrix under the entry S = C, D
is obtained as follows. U1( C, D ) = δ1(K, η3 AGC
C,D ) =
u1(K † η3 AGC
C,D ) − u1(K). The first term of this is the
utility of 1 in the system K where we implement η3 for the 
cooperating agent, i.e., 1, only. This means that the transitions are
R \ {(s, s)}. In this system, still ϕ1
4 = A E♦p1 is the highest
goal for agent 1. This is the same utility for 1 as in K, and hence,
δ1(K, η3 AGC
C,D ) = 0. Agent 2 of course benefits if agent 1
complies with η3 while 2 does not. His utility would be 3, since
η3 AGC
C,D is in fact η1.
5.1 Individually Rational Normative Systems
A normative system is individually rational if every agent would
fare better if the normative system were imposed than otherwise.
This is a necessary, although not sufficient condition on a norm to
expect that everybody respects it. Note that η3 of our example is
individually rational for both 1 and 2, although this is not a stable
situation: given that the other plays C, i is better of by playing
D. We can easily characterise individually rationality with respect
to the corresponding game in strategic form, as follows. Let Σ =
M , η be a social system. Then the following are equivalent:
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 885
f(xk)
...
s0
s1
s2
s3
s4
s(2k−1)
s2k
t(x1)
f(x1)
t(x2)
f(x2)
t(xk)
Figure 3: The Kripke structure produced in the reduction of
Theorem 2; all transitions are associated with agent 1, the only
initial state is s0.
1. η is individually rational in M ;
2. ∀i ∈ AG, Ui (SC ) > Ui (SD ) in the game GΣ.
The decision problem associated with individually rational 
normative systems is as follows:
INDIVIDUALLY RATIONAL NORMATIVE SYSTEM (IRNS):
Given: Multi-agent system M .
Question: Does there exist an individually rational 
normative system for M ?
THEOREM 2. IRNS is NP-complete, even in one-agent systems.
PROOF. For membership of NP, guess a normative system η,
and verify that it is individually rational. Since η ⊆ R, we will be
able to guess it in nondeterministic polynomial time. To verify that
it is individually rational, we check that for all i, we have ui (K †
η) > ui (K); computing K † η is just set subtraction, so can be
done in polynomial time, while determining the value of ui (K) for
any K can be done with a polynomial number of model checking
calls, each of which requires only time polynomial in the K and γ.
Hence verifying that ui (K † η) > ui (K) requires only polynomial
time.
For NP-hardness, we reduce SAT [12, p.77]. Given a SAT instance
ϕ over Boolean variables x1, . . . , xk , we produce an instance of
IRNS as follows. First, we define a single agent A = {1}. For each
Boolean variable xi in the SAT instance, we create two Boolean
variables t(xi ) and f (xi ) in the IRNS instance. We then create a
Kripke structure Kϕ with 2k + 1 states, as shown in Figure 3: arcs
in this graph correspond to transitions in Kϕ. Let ϕ∗
be the result
of systematically substituting for every Boolean variable xi in ϕ
the CTL expression (E ft(xi )). Next, consider the following 
formulae:
k^
i=1
E f(t(xi ) ∨ f (xi )) (1)
k^
i=1
¬((E ft(xi )) ∧ (E ff (xi ))) (2)
We then define the goal hierarchy for all agent 1 as follows:
γ1[0] =
γ1[1] = (1) ∧ (2) ∧ ϕ∗
We claim there is an individually rational normative system for the
instance so constructed iff ϕ is satisfiable. First, notice that any
individually rational normative system must force γ1[1] to be true,
since in the original system, we do not have γ1[1].
For the ⇒ direction, if there is an individually rational normative
system η, then we construct a satisfying assignment for ϕ by 
considering the arcs that are forbidden by η: formula (1) ensures that
we must forbid an arc to either a t(xi ) or a f (xi ) state for all 
variables xi , but (2) ensures that we cannot forbid arcs to both. So, if
we forbid an arc to a t(xi ) state then in the corresponding valuation
for ϕ we make xi false, while if we forbid an arc to a f (xi ) state
then we make xi true. The fact that ϕ∗
is part of the goal ensures
that the normative system is indeed a valuation for ϕ.
For ⇐, note that for any satisfying valuation for ϕ we can 
construct an individually rational normative system η, as follows: if
the valuation makes xi true, we forbid the arc to the f (xi ) state,
while if the valuation makes xi false, we forbid the arc to the t(xi )
state. The resulting normative system ensures γ1[1], and is thus
individually rational.
Notice that the Kripke structure constructed in the reduction 
contains just a single agent, and so the Theorem is proven.
5.2 Pareto Efficient Normative Systems
Pareto efficiency is a basic measure of how good a particular
outcome is for a group of agents [11, p.7]. Intuitively, an outcome
is Pareto efficient if there is no other outcome that makes every
agent better off. In our framework, suppose we are given a social
system Σ = M , η , and asked whether η is Pareto efficient. This
amounts to asking whether or not there is some other normative
system η such that every agent would be better off under η than
with η. If η makes every agent better off than η, then we say η
Pareto dominates η. The decision problem is as follows:
PARETO EFFICIENT NORMATIVE SYSTEM (PENS):
Given: Multi-agent system M and normative system η
over M .
Question: Is η Pareto efficient for M ?
THEOREM 3. PENS is co-NP-complete, even for one-agent 
systems.
PROOF. Let M and η be as in the Theorem. We show that the
complement problem to PENS, which we refer to as PARETO 
DOMINATED, is NP-complete. In this problem, we are given M and η,
and we are asked whether η is Pareto dominated, i.e., whether or not
there exists some η over M such that η makes every agent better
off than η. For membership of NP, simply guess a normative system
η , and verify that for all i ∈ A, we have ui (K † η ) > ui (K † η)
- verifying requires a polynomial number of model checking 
problems, each of which takes polynomial time. Since η ⊆ R, the
normative system can be guessed in non-deterministic polynomial
time. For NP-hardness, we reduce IRNS, which we know to be 
NPcomplete from Theorem 2. Given an instance M of IRNS, we let M
in the instance of PARETO DOMINATED be as in the IRNS instance,
and define the normative system for PARETO DOMINATED to be η∅,
the empty normative system. Now, it is straightforward that there
exists a normative system η which Pareto dominates η∅ in M iff
there exist an individually rational normative system in M . Since
the complement problem is NP-complete, it follows that PENS is
co-NP-complete.
886 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
η0 η1 η2 η3 η4 η5 η6 η7 η8
u1(K † η) 4 4 7 6 5 0 0 8 0
u2(K † η) 4 7 4 6 0 5 8 0 0
Table 1: Utilities for all possible norms in our example
How about Pareto efficient norms for our toy example? Settling
this question amounts to finding the dominant normative systems
among η0 = η∅, η1, η2, η3 defined before, and η4 = {(s, t)}, η5 =
{(t, s)}, η6 = {(s, s), (t, s)}, η7 = {(t, t), (s, t)} and η8 =
{(s, t), (t, s)}. The utilities for each system are given in Table 1.
From this, we infer that the Pareto efficient norms are η1, η2, η3, η6
and η7. Note that η8 prohibits the resource to be passed from one
agent to another, and this is not good for any agent (since we have
chosen S0
= {s, t}, no agent can be sure to ever get the resource,
i.e., goal ϕi
1 is not true in K † η8).
5.3 Nash Implementation Normative Systems
The most famous solution concept in game theory is of course
Nash equilibrium [11, p.14]. A collection of strategies, one for each
agent, is said to form a Nash equilibrium if no agent can benefit by
doing anything other than playing its strategy, under the 
assumption that the other agents play theirs. Nash equilibria are important
because they provide stable solutions to the problem of what 
strategy an agent should play. Note that in our toy example, although
η3 is individually rational for each agent, it is not a Nash 
equilibrium, since given this norm, it would be beneficial for agent 1 to
deviate (and likewise for 2). In our framework, we say a social
system Σ = M , η (where η = η∅) is a Nash implementation if
SC (i.e., everyone complying with the normative system) forms a
Nash equilibrium in the game GΣ. The intuition is that if Σ is a
Nash implementation, then complying with the normative system
is a reasonable solution for all concerned: there can be no 
benefit to deviating from it, indeed, there is a positive incentive for all
to comply. If Σ is not a Nash implementation, then the normative
system is unlikely to succeed, since compliance is not rational for
some agents. (Our choice of terminology is deliberately chosen to
reflect the way the term Nash implementation is used in 
implementation theory, or mechanism design [11, p.185], where a game
designer seeks to achieve some outcomes by designing the rules of
the game such that these outcomes are equilibria.)
NASH IMPLEMENTATION (NI) :
Given: Multi-agent system M .
Question: Does there exist a non-empty normative 
system η over M such that M , η forms a Nash 
implementation?
Verifying that a particular social system forms a Nash 
implementation can be done in polynomial time - it amounts to checking:
∀i ∈ A : ui (K † η) ≥ ui (K † (η {i})).
This, clearly requires only a polynomial number of model checking
calls, each of which requires only polynomial time.
THEOREM 4. The NI problem is NP-complete, even for 
twoagent systems.
PROOF. For membership of NP, simply guess a normative 
system η and check that it forms a Nash implementation; since η ⊆ R,
guessing can be done in non-deterministic polynomial time, and as
s(2k+1)
1
1
1
1
1
1
11
1 1
11
2
2
2
2
2
2
2
2
2
2
2
t(x1)
f(x1)
t(x2)
f(x2)
t(xk)
f(xk)
2
2
t(x1)
f(x1)
t(x2)
f(x2)
t(xk)
f(xk)
......
s0
Figure 4: Reduction for Theorem 4.
we argued above, verifying that it forms a Nash implementation
can be done in polynomial time.
For NP-hardness, we reduce SAT. Suppose we are given a SAT 
instance ϕ over Boolean variables x1, . . . , xk . Then we construct an
instance of NI as follows. We create two agents, A = {1, 2}. For
each Boolean variable xi we create two Boolean variables, t(xi )
and f (xi ), and we then define a Kripke structure as shown in 
Figure 4, with s0 being the only initial state; the arc labelling in 
Figure 4 gives the α function, and each state is labelled with the 
propositions that are true in that state. For each Boolean variable xi , we
define the formulae xi and x⊥
i as follows:
xi = E f(t(xi ) ∧ E f((E f(t(xi ))) ∧ A f(¬f (xi ))))
x⊥
i = E f(f (xi ) ∧ E f((E f(f (xi ))) ∧ A f(¬t(xi ))))
Let ϕ∗
be the formula obtained from ϕ by systematically 
substituting xi for xi . Each agent has three goals: γi [0] = for both
i ∈ {1, 2}, while
γ1[1] =
k^
i=1
((E f(t(xi ))) ∧ (E f(f (xi ))))
γ2[1] = E fE f
k^
i=1
((E f(t(xi ))) ∧ (E f(f (xi ))))
and finally, for both agents, γi [2] being the conjunction of the 
following formulae:
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 887
k^
i=1
(xi ∨ x⊥
i ) (3)
k^
i=1
¬(xi ∧ x⊥
i ) (4)
k^
i=1
¬(E f(t(xi )) ∧ E f(f (xi ))) (5)
ϕ∗
(6)
We denote the multi-agent system so constructed by Mϕ. Now,
we prove that the SAT instance ϕ is satisfiable iff Mϕ has a Nash
implementation normative system:
For the ⇒ direction, suppose ϕ is satisfiable, and let X be a
satisfying valuation, i.e., a set of Boolean variables making ϕ true.
We can extract from X a Nash implementation normative system η
as follows: if xi ∈ X , then η includes the arc from s0 to the state
in which f (xi ) is true, and also includes the arc from s(2k + 1)
to the state in which f (xi ) is true; if xi ∈ X , then η includes the
arc from s0 to the state in which t(xi ) is true, and also includes
the arc from s(2k + 1) to the state in which t(xi ) is true. No
other arcs, apart from those so defined, as included in η. Notice
that η is individually rational for both agents: if they both comply
with the normative system, then they will have their γi [2] goals
achieved, which they do not in the basic system. To see that η
forms a Nash implementation, observe that if either agent defects
from η, then neither will have their γi [2] goals achieved: agent 1
strictly prefers (C, C) over (D, C), and agent 2 strictly prefers
(C, C) over (C, D).
For the ⇐ direction, suppose there exists a Nash implementation
normative system η, in which case η = ∅. Then ϕ is satisfiable;
for suppose not. Then the goals γi [2] are not achievable by any
normative system, (by construction). Now, since η must forbid at
least one transition, then at least one agent would fail to have its
γi [1] goal achieved if it complied, so at least one would do better
by defecting, i.e., not complying with η. But this contradicts the
assumption that η is a Nash implementation, i.e., that (C, C) forms
a Nash equilibrium.
This result is perhaps of some technical interest beyond the specific
concerns of the present paper, since it is related to two problems
that are of wider interest: the complexity of mechanism design [5],
and the complexity of computing Nash equilibria [6, 7]
5.4 Richer Goal Languages
It is interesting to consider what happens to the complexity of
the problems we consider above if we allow richer languages for
goals: in particular, CTL
∗
[9]. The main difference is that 
determining ui (K) in a given multi-agent system M when such a goal 
language is used involves solving a PSPACE-complete problem (since
model checking for CTL
∗
is PSPACE-complete [8]). In fact, it seems
that for each of the three problems we consider above, the 
corresponding problem under the assumption of a CTL
∗
representation
for goals is also PSPACE-complete. It cannot be any easier, since 
determining the utility of a particular Kripke structure involves 
solving a PSPACE-complete problem. To see membership in PSPACE
we can exploit the fact that PSPACE = NPSPACE [12, p.150], and so
we can guess the desired normative system, applying a PSPACE
verification procedure to check that it has the desired properties.
6. CONCLUSIONS
Social norms are supposed to restrict our behaviour. Of course,
such a restriction does not have to be bad: the fact that an agent"s
behaviour is restricted may seem a limitation, but there may be 
benefits if he can assume that others will also constrain their behaviour.
The question then, for an agent is, how to be sure that others will
comply with a norm. And, for a system designer, how to be sure
that the system will behave socially, that is, according to its norm.
Game theory is a very natural tool to analyse and answer these
questions, which involve strategic considerations, and we have 
proposed a way to translate key questions concerning logic-based 
normative systems to game theoretical questions. We have proposed
a logical framework to reason about such scenarios, and we have
given some computational costs for settling some of the main 
questions about them. Of course, our approach is in many senses open
for extension or enrichment. An obvious issue is to consider is the
complexity of the questions we give for more practical 
representations of models (cf. [1]), and to consider other classes of allowable
goals.
7. REFERENCES
[1] T. Agotnes, W. van der Hoek, J. A. Rodriguez-Aguilar,
C. Sierra, and M. Wooldridge. On the logic of normative
systems. In Proc. IJCAI-07, Hyderabad, India, 2007.
[2] R. Alur, T. A. Henzinger, and O. Kupferman.
Alternating-time temporal logic. Jnl. of the ACM,
49(5):672-713, 2002.
[3] K. Binmore. Game Theory and the Social Contract Volume
1: Playing Fair. The MIT Press: Cambridge, MA, 1994.
[4] K. Binmore. Game Theory and the Social Contract Volume
2: Just Playing. The MIT Press: Cambridge, MA, 1998.
[5] V. Conitzer and T. Sandholm. Complexity of mechanism
design. In Proc. UAI, Edmonton, Canada, 2002.
[6] V. Conitzer and T. Sandholm. Complexity results about nash
equilibria. In Proc. IJCAI-03, pp. 765-771, Acapulco,
Mexico, 2003.
[7] C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou. The
complexity of computing a Nash equilibrium. In Proc.
STOC, Seattle, WA, 2006.
[8] E. A. Emerson. Temporal and modal logic. In Handbook of
Theor. Comp. Sci. Vol. B, pages 996-1072. Elsevier, 1990.
[9] E. A. Emerson and J. Y. Halpern. ‘Sometimes" and ‘not
never" revisited: on branching time versus linear time
temporal logic. Jnl. of the ACM, 33(1):151-178, 1986.
[10] D. Fitoussi and M. Tennenholtz. Choosing social laws for
multi-agent systems: Minimality and simplicity. Artificial
Intelligence, 119(1-2):61-101, 2000.
[11] M. J. Osborne and A. Rubinstein. A Course in Game Theory.
The MIT Press: Cambridge, MA, 1994.
[12] C. H. Papadimitriou. Computational Complexity.
Addison-Wesley: Reading, MA, 1994.
[13] Y. Shoham and M. Tennenholtz. On the synthesis of useful
social laws for artificial agent societies. In Proc. AAAI, San
Diego, CA, 1992.
[14] Y. Shoham and M. Tennenholtz. On social laws for artificial
agent societies: Off-line design. In Computational Theories
of Interaction and Agency, pages 597-618. The MIT Press:
Cambridge, MA, 1996.
[15] W. van der Hoek, M. Roberts, and M. Wooldridge. Social
laws in alternating time: Effectiveness, feasibility, and
synthesis. Synthese, 2007.
[16] M. Wooldridge and W. van der Hoek. On obligations and
normative ability. Jnl. of Appl. Logic, 3:396-420, 2005.
888 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Implementing Commitment-Based Interactions∗
Michael Winikoff
School of Computer Science and IT
RMIT University
Melbourne, Australia
michael.winikoff@rmit.edu.au
ABSTRACT
Although agent interaction plays a vital role in MAS, and 
messagecentric approaches to agent interaction have their drawbacks, present
agent-oriented programming languages do not provide support for
implementing agent interaction that is flexible and robust. Instead,
messages are provided as a primitive building block. In this 
paper we consider one approach for modelling agent interactions: the
commitment machines framework. This framework supports 
modelling interactions at a higher level (using social commitments), 
resulting in more flexible interactions. We investigate how 
commitmentbased interactions can be implemented in conventional agent-oriented
programming languages. The contributions of this paper are: a
mapping from a commitment machine to a collection of BDI-style
plans; extensions to the semantics of BDI programming languages;
and an examination of two issues that arise when distributing 
commitment machines (turn management and race conditions) and 
solutions to these problems.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
IntelligenceMultiagent systems; I.2.5 [Artificial Intelligence]: Programming
Languages and Software
General Terms
Design
1. INTRODUCTION
Agents are social, and agent interaction plays a vital role in 
multiagent systems. Consequently, design and implementation of agent
interaction is an important research topic.
The standard approach for designing agent interactions is 
messagecentric: interactions are defined by interaction protocols that give
the permissible sequences of messages, specified using notations
such as finite state machines, Petri nets, or Agent UML.
It has been argued that this message-centric approach to 
interaction design is not a good match for intelligent agents. Intelligent
agents should exhibit the ability to persist in achieving their goals
in the face of failure (robustness) by trying different approaches
(flexibility). On the other hand, when following an interaction 
protocol, an agent has limited flexibility and robustness: the ability to
persistently try alternative means to achieving the interaction"s aim
is limited to those options that the protocol"s designer provided, and
in practice, message-centric design processes do not tend to lead to
protocols that are flexible or robust.
Recognising these limitations of the traditional approach to 
designing agent interactions, a number of approaches have been 
proposed in recent years that move away from message-centric 
interaction protocols, and instead consider designing agent interactions
using higher-level concepts such as social commitments [8, 10,
18] or interaction goals [2]. There has also been work on richer
forms of interaction in specific settings, such as teams of 
cooperative agents [5, 11].
However, although there has been work on designing flexible and
robust agent interactions, there has been virtually no work on 
providing programming language support for implementing such 
interactions. Current Agent Oriented Programming Languages 
(AOPLs) do not provide support for implementing flexible and robust
agent interactions using higher-level concepts than messages. 
Indeed, modern AOPLs [1], with virtually no exceptions, provide
only simple message sending as the basis for implementing agent
interaction.
This paper presents what, to the best of our knowledge, is the
second AOPL to support high-level, flexible, and robust agent 
interaction implementation. The first such language, STAPLE, was
proposed a few years ago [9], but is not described in detail, and is
arguably impractical for use by non-specialists, due to its logical
basis and heavy reliance on temporal and modal logic.
This paper presents a scheme for extending BDI-like AOPLs
to support direct implementation of agent interactions that are 
designed using Yolum & Singh"s commitment machine (CM) 
framework [19]. In the remainder of this paper we briefly review 
commitment machines and present a simple abstraction of BDI AOPLs
which lies in the common subset of languages such as Jason, 3APL,
and CAN. We then present a scheme for translating commitment
machines to this language, and indicate how the language needs
to be extended to support this. We then extend our scheme to 
address a range of issues concerned with distribution, including turn
tracking [7], and race conditions.
2. BACKGROUND
2.1 Commitment Machines
The aim of the commitment machine framework is to allow for
the definition of interactions that are more flexible than traditional
message-centric approaches. A Commitment Machine (CM) [19]
specifies an interaction between entities (e.g. agents, services, 
processes) in terms of actions that change the interaction state. This
interact state consists of fluents (predicates that change value over
time), but also social commitments, both base-level and conditional.
A base-level social commitment is an undertaking by debtor A to
creditor B to bring about condition p, denoted C(A, B, p). This is
sometimes abbreviated to C(p), where it is not important to specify
the identities of the entities in question. For example, a 
commitment by customer C to merchant M to make the fluent paid true
would be written as C(C, M, paid).
A conditional social commitment is an undertaking by debtor A
to creditor B that should condition q become true, A will then 
commit to bringing about condition p. This is denoted by CC(A, B, q, p),
and, where the identity of the entities involved is unimportant (or
obvious), is abbreviated to CC(q p) where the arrow is a 
reminder of the causal link between q becoming true and the creation
of a commitment to make p true. For example, a commitment to
make the fluent paid true once goods have been received would be
written CC(goods paid).
The semantics of commitments (both base-level and conditional)
is defined with rules that specify how commitments change over
time. For example, the commitment C(p) (or CC(q p)) is 
discharged when p becomes true; and the commitment CC(q p) is
replaced by C(p) when q becomes true. In this paper we use the
more symmetric semantics proposed by [15] and subsequently 
reformalised by [14]. In brief, these semantics deal with a number of
more complex cases, such as where commitments are created when
conditions already hold: if p holds when CC(p q) is meant to
be created, then C(q) is created instead of CC(p q).
An interaction is defined by specifying the entities involved, the
possible contents of the interaction state (both fluents and 
commitments), and (most importantly) the actions that each entity can 
perform along with the preconditions and effects of each action, 
specified as add and delete lists.
A commitment machine (CM) defines a range of possible 
interactions that each start in some state1
, and perform actions until
reaching a final state. A final state is one that has no base-level
commitments. One way of visualising the interactions that are 
possible with a given commitment machine is to generate the finite
state machine corresponding to the CM. For example, figure 1 gives
the FSM2
corresponding to the NetBill [18] commitment machine:
a simple CM where a customer (C) and merchant (M) attempt to
trade using the following actions3
:
1
Unlike standard interaction protocols, or finite state machines,
there is no designated initial state for the interaction.
2
The finite state machine is software-generated: the nodes and 
connections were computed by an implementation of the axioms 
(available from http://www.winikoff.net/CM) and were then laid out by
graphviz (http://www.graphviz.org/).
3
We use the notation A(X) : P ⇒ E to indicate that action A is
performed by entity X, has precondition P (with : P omitted if
empty) and effect E.
• sendRequest(C) ⇒ request
• sendQuote(M) ⇒ offer
where offer ≡ promiseGoods ∧ promiseReceipt and
promiseGoods ≡ CC(M, C, accept, goods) and
promiseReceipt ≡ CC(M, C, pay, receipt)
• sendAccept(C) ⇒ accept
where accept ≡ CC(C, M, goods, pay)
• sendGoods(M) ⇒ promiseReceipt ∧ goods
where promiseReceipt ≡ CC(M, C, pay, receipt)
• sendEPO(C) : goods ⇒ pay
• sendReceipt(M) : pay ⇒ receipt.
The commitment accept is the customer"s promise to pay once
goods have been sent, promiseGoods is the merchant"s promise
to send the goods once the customer accepts, and promiseReceipt
is the merchant"s promise to send a receipt once payment has been
made.
As seen in figure 1, commitment machines can support a range
of interaction sequences.
2.2 An Abstract Agent ProgrammingLanguage
Agent programming languages in the BDI tradition (e.g. dMARS,
JAM, PRS, UM-PRS, JACK, AgentSpeak(L), Jason, 3APL, CAN,
Jadex) define agent behaviour in terms of event-triggered plans,
where each plan specifies what it is triggered by, under what 
situations it can be considered to be applicable (defined using a so-called
context condition), and a plan body: a sequence of steps that can
include posting events which in turn triggers further plans. Given
a collection of plans and an event e that has been posted the agent
first collects all plans types that are triggered by that event (the 
relevant plans), then evaluates the context conditions of these plans to
obtain a set of applicable plan instances. One of these is chosen
and is executed.
We now briefly define the formal syntax and semantics of a 
Simple Abstract (BDI) Agent Programming Language (SAAPL). This
language is intended to be an abstraction that is in the common
subset of such languages as Jason [1, Chapter 1], 3APL [1, 
Chapter 2], and CAN [16]. Thus, it is intentionally incomplete in some
areas, for instance it doesn"t commit to a particular mechanism for
dealing with plan failure, since different mechanisms are used by
different AOPLs.
An agent program (denoted by Π) consists of a collection of plan
clauses of the form e : C ← P where e is an event, C is a context
condition (a logical formula over the agent"s beliefs), and P is the
plan body. The plan body is built up from the following constructs.
We have the empty step which always succeeds and does nothing,
operations to add (+b) and delete (−b) beliefs, sending a message
m to agent N (↑N
m), and posting an event4
(e). These can be
sequenced (P; P).
C ::= b | C ∧ C | C ∨ C | ¬C | ∃x.C
P ::= | +b | −b | e | ↑N
m | P; P
Formal semantics for this language is given in figure 2. This 
semantics is based on the semantics for AgentSpeak given by [12],
which in turn is based on the semantics for CAN [16]. The 
semantics is in the style of Plotkin"s Structural Operational Semantics,
and assumes that operations exist that check whether a condition
4
We use ↓N
m as short hand for the event corresponding to 
receiving message m from agent N.
874 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 1: Finite State Machine for NetBill (shaded = final states)
follows from a belief set, that add a belief to a belief set, and that
delete a belief from a belief set. In the case of beliefs being a set of
ground atoms these operations are respectively consequence 
checking (B |= C), and set addition (B ∪ {b}) and deletion (B \ {b}).
More sophisticated belief management methods may be used, but
are not considered here.
We define a basic configuration S = Q, N, B, P where Q is a
(global) message queue (modelled as a sequence5
where messages
are added at one end and removed from the other end), N is the
name of the agent, B is the beliefs of the agent and P is the plan
body being executed (i.e. the intention). We also define an agent
configuration, where instead of a single plan body P there is a set
of plan instances, Γ. Finally, a complete MAS is a pair Q, As of a
global message queue Q and a set of agent configurations (without
the queue, Q). The global message queue is a sequence of triplets
of the form sender:recipient:message.
A transition S0 −→ S1 specifies that executing S0 a single step
yields S1. We annotate the arrow with an indication of whether
the configuration in question is basic, an agent configuration, or a
MAS configuration. The transition relation is defined using rules
of the form S −→ S or of the form
S −→ Sr
S −→ Sr ; the latter are 
conditional with the top (numerator) being the premise and the bottom
(denominator) being the conclusion.
Note that there is non-determinism in SAAPL, e.g. the choice
of plan to execute from a set of applicable plans. This is resolved
by using selection functions: SO selects one of the applicable plan
instances to handle a given event, SI selects which of the plan 
instances that can be executed should be executed next, and SA 
selects which agent should execute (a step) next.
3. IMPLEMENTING COMMITMENT-BASED
INTERACTIONS
In this section we present a mapping from a commitment 
machine to a collection of SAAPL programs (one for each role). We
begin by considering the simple case of two interacting agents, and
5
The + operator is used to denote sequence concatenation.
assume that the agents take turns to act. In section 4 we relax these
assumptions.
Each action A(X) : P ⇒ E is mapped to a number of plans:
there is a plan (for agent X) with context condition P that 
performs the action (i.e. applies the effects E to the agent"s beliefs)
and sends a message to the other agent, and a plan (for the other
agent) that updates its state when a message is received from X.
For example, given the action sendAccept(C) ⇒ accept we have
the following plans, where each plan is preceded by M: or C:
to indicate which agent that plan belongs to. Note that where the
identify of the sender (respectively recipient) is obvious, i.e. the
other agent, we abbreviate ↑N
m to ↑m (resp. ↓N
m to ↓m). Turn
taking is captured through the event ı (short for interact): the
agent that is active has an ı event that is being handled. Handling
the event involves sending a message to the other agent, and then
doing nothing until a response is received.
C: ı : true ← +accept; ↑sendAccept.
M: ↓sendAccept : true ← +accept; ı.
If the action has a non-trivial precondition then there are two plans
in the recipient: one to perform the action (if possible), and another
to report an error if the action"s precondition doesn"t hold (we 
return to this in section 4). For example, the action sendReceipt(M) :
pay ⇒ receipt generates the following plans:
M: ı : pay ← +receipt; ↑sendReceipt.
C: ↓sendReceipt : pay ← +receipt; ı.
C: ↓sendReceipt : ¬pay ← . . . report error . . . .
In addition to these plans, we also need plans to start and finish
the interaction. An interaction can be completed whenever there
are no base-level commitments, so both agents have the following
plans:
ı : ¬∃p.C(p) ← ↑done.
↓done : ¬∃p.C(p) ← .
↓done : ∃p.C(p) ← . . . report error . . . .
An interaction is started by setting up an agent"s initial beliefs, and
then having it begin to interact. Exactly how to do this depends
on the agent platform: e.g. the agent platform in question may
offer a simple way to load beliefs from a file. A generic approach
that is a little cumbersome, but is portable, is to send each of the
agents involved in the interaction a sequence of init messages, each
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 875
Q, N, B, +b
Basic
−→ Q, N, B ∪ {b},
Q, N, B, −b
Basic
−→ Q, N, B \ {b},
Δ = {Piθ|(ti : ci ← Pi) ∈ Π ∧ tiθ = e ∧ B |= ciθ}
Q, N, B, e
Basic
−→ Q, N, B, SO(Δ)
Q, N, B, P1
Basic
−→ Q , N, B , P
Q, N, B, P1; P2
Basic
−→ Q , N, B , P ; P2
Q, N, B, ; P
Basic
−→ Q, N, B, P
Q, N, B, ↑NB m
Basic
−→ Q + N:NB:m, N, B,
Q = NA:N:m + Q
Q, N, B, Γ
Agent
−→ Q , N, B, Γ ∪ {↓NA m}
P = SI(Γ) Q, N, B, P
Basic
−→ Q , N, B , P
Q, N, B, Γ
Agent
−→ Q , N, B , (Γ \ {P}) ∪ {P }
P = SI(Γ) P =
Q, N, B, Γ
Agent
−→ Q, N, B, (Γ \ {P})
N, B, Γ = SA(As) Q, N, B, Γ
Agent
−→ Q , N, B , Γ
Q, As
MAS
−→ Q , (As ∪ { N, B , Γ }) \ { N, B, Γ }
Figure 2: Operational Semantics for SAAPL
containing a belief to be added; and then send one of the agents a
start message which begins the interaction. Both agents thus have
the following two plans:
↓init(B) : true ← +B.
↓start : true ← ı.
Figure 3 gives the SAAPL programs for both merchant and 
customer that implement the NetBill protocol. For conciseness the
error reporting plans are omitted.
We now turn to refining the context conditions. There are three
refinements that we consider. Firstly, we need to prevent 
performing actions that have no effect on the interaction state. Secondly,
an agent may want to specify that certain actions that it is able to
perform should not be performed unless additional conditions hold.
For example, the customer may not want to agree to the merchant"s
offer unless the goods have a certain price or property. Thirdly, the
context conditions of the plans that terminate the interaction need to
be refined in order to avoid terminating the interaction prematurely.
For each plan of the form ı : P ← +E; ↑m we replace the 
context condition P with the enhanced condition P ∧ P ∧ ¬E where
P is any additional conditions that the agent wishes to impose,
and ¬E is the negation of the effects of the action. For 
example, the customer"s payment plan becomes (assuming no additional
conditions, i.e. no P ): ı : goods ∧ ¬pay ← +pay; ↑sendEPO.
For each plan of the form ↓m : P ← +E; ı we could add ¬E to
the precondition, but this is redundant, since it is already checked
by the performer of the action, and if the action has no effect then
Customer"s plans:
ı : true ← +request; ↑sendRequest.
ı : true ← +accept; ↑sendAccept.
ı : goods ← +pay; ↑sendEPO.
↓sendQuote : true ← +promiseGoods;
+promiseReceipt; ı.
↓sendGoods : true ← +promiseReceipt; +goods; ı.
↓sendReceipt : pay ← +receipt; ı.
Merchant"s plans:
ı : true ← +promiseGoods;
+promiseReceipt; ↑sendQuote.
ı : true ← +promiseReceipt; +goods; ↑sendGoods.
ı : pay ← +receipt; ↑sendReceipt.
↓sendRequest : true ← +request; ı.
↓sendAccept : true ← +accept; ı.
↓sendEPO : goods ← +pay; ı.
Shared plans (i.e. plans of both agents):
ı : ¬∃p.C(p) ← ↑done.
↓done : ¬∃p.C(p) ← .
↓init(B) : true ← +B.
↓start : true ← ı.
Where
accept ≡ CC(goods pay)
promiseGoods ≡ CC(accept goods)
promiseReceipt ≡ CC(pay receipt)
offer ≡ promiseGoods ∧ promiseReceipt
Figure 3: SAAPL Implementation of NetBill
the sender won"t perform it and send the message (see also the 
discussion in section 4).
When specifying additional conditions (P ), some care needs to
be taken to avoid situations where progress cannot be made because
the only action(s) possible are prevented by additional conditions.
One way of indicating preference between actions (in many agent
platforms) is to reorder the agent"s plans. This is clearly safe, since
actions are not prevented, just considered in a different order.
The third refinement of context conditions concerns the plans
that terminate the interaction. In the Commitment Machine 
framework any state that has no base-level commitment is final, in that
the interaction may end there (or it may continue). However, only
some of these final states are desirable final states. Which final
states are considered to be desirable depends on the domain and
the desired interaction outcome. In the NetBill example, the 
desirable final state is one where the goods have been sent and paid
for, and a receipt issued (i.e. goods ∧ pay ∧ receipt). In order to
prevent an agent from terminating the interaction too early we add
this as a precondition to the termination plan:
ı : goods ∧ pay ∧ receipt ∧ ¬∃p.C(p) ← ↑done.
Figure 4 shows the plans that are changed from figure 3.
In order to support the realisation of CMs, we need to change
SAAPL in a number of ways. These changes, which are discussed
below, can be applied to existing BDI languages to make them
commitment machine supportive. We present the three changes,
explain what they involve, and for each change explain how the
change was implemented using the 3APL agent oriented 
programming language. The three changes are:
1. extending the beliefs of the agent so that they can contain
commitments;
876 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Customer"s plans:
ı : ¬request ← +request; ↑sendRequest.
ı : ¬accept ← +accept; ↑sendAccept.
ı : goods ∧ ¬pay ← +pay; ↑sendEPO.
Merchant"s plans:
ı : ¬offer ← +promiseGoods; +promiseReceipt;
↑sendQuote.
ı : ¬(promiseReceipt ∧ goods) ←
+promiseReceipt; +goods; ↑sendGoods.
ı : pay ∧ ¬receipt ← +receipt; ↑sendReceipt.
Where
accept ≡ CC(goods pay)
promiseGoods ≡ CC(accept goods)
promiseReceipt ≡ CC(pay receipt)
offer ≡ promiseGoods ∧ promiseReceipt
Figure 4: SAAPL Implementation of NetBill with refined 
context conditions (changed plans only)
2. changing the definition of |= to encompass implied 
commitments; and
3. whenever a belief is added, updating existing commitments,
according to the rules of commitment dynamics.
Extending the notion of beliefs to encompass commitments in
fact requires no change in agent platforms that are prolog-like and
support terms as beliefs (e.g. Jason, 3APL, CAN). However, other
agent platforms do require an extension. For example, JACK, which
is an extension of Java, would require changes to support 
commitments that can be nested. In the case of 3APL no change is needed
to support this.
Whenever a context condition contains commitments, 
determining whether the context condition is implied by the agent"s beliefs
(B |= C) needs to take into account the notion of implied 
commitments [15]. In brief, a commitment can be considered to follow
from a belief set B if the commitment is in the belief set (C ∈ B),
but also under other conditions. For example, a commitment to pay
C(pay) can be considered to be implied by a belief set containing
pay because the commitment may have held and been discharged
when pay was made true. Similar rules apply for conditional 
commitments. These rules, which were introduced in [15] were 
subsequently re-formalised in a simpler form by [14] resulting in the
four inference rules in the bottom part of figure 5.
The change that needs to be made to SAAPL to support 
commitment machine implementations is to extend the definition of |= to
include these four rules. For 3APL this was realised by having each
agent include the following Prolog clauses:
holds(X) :- clause(X,true).
holds(c(P)) :- holds(P).
holds(c(P)) :- clause(cc(Q,P),true), holds(Q).
holds(cc(_,Q)) :- holds(Q).
holds(cc(_,Q)) :- holds(c(Q)).
The first clause simply says that anything holds if it is in agent"s
beliefs (clause(X,true) is true if X is a fact). The 
remaining four clauses correspond respectively to the inference rules C1,
C2, CC1 and CC2. To use these rules we then modify context
conditions in our program so that instead of writing, for 
example, cc(m,c, pay, receipt) we write holds(cc(m,c,
pay, receipt)).
B = norm(B ∪ {b})
Q, N, B, +b −→ Q, N, B ,
function norm(B)
B ← B
for each b ∈ B do
if b = C(p) ∧ B |= p then B ← B \ {b}
elseif b = CC(p q) then
if B |= q then B ← B \ {b}
elseif B |= p then B ← (B \ {b}) ∪ {C(q)}
elseif B |= C(q) then B ← B \ {b}
endif
endif
endfor
return B
end function
B |= P
B |= C(P)
C1
CC(Q P) ∈ B B |= Q
B |= P
C2
B |= CC(P Q)
B |= Q
CC1
B |= C(Q)
B |= CC(P Q)
CC2
Figure 5: New Operational Semantics
The final change is to update commitments when a belief is
added. Formally, this is done by modifying the semantic rule for
belief addition so that it applies an algorithm to update 
commitments. The modified rule and algorithm (which mirrors the 
definition of norm in [14]) can be found in the top part of figure 5.
For 3APL this final change was achieved by manually inserting
update() after updating beliefs, and defining the following rules
for update():
update() <- c(P) AND holds(P)
| {Deletec(P) ; update()},
update() <- cc(P,Q) AND holds(Q)
| {Deletecc(P,Q) ; update()},
update() <- cc(P,Q) AND holds(P)
| {Deletecc(P,Q) ; Addc(Q) ; update()},
update() <- cc(P,Q) AND holds(c(Q))
| {Deletecc(P,Q) ; update()},
update() <- true | Skip
where Deletec and Deletecc delete respectively a base-level
and conditional commitment, and Addc adds a base-level 
commitment.
One aspect that doesn"t require a change is linking commitments
and actions. This is because commitments don"t trigger actions 
directly: they may trigger actions indirectly, but in general their effect
is to prevent completion of an interaction while there are 
outstanding (base level) commitments.
Figure 6 shows the message sequences from a number of runs of
a 3APL implementation of the NetBill commitment machine6
. In
order to illustrate the different possible interactions the code was
modified so that each agent selected randomly from the actions
that it could perform, and a number of runs were made with the
customer as the initiator, and then with the merchant as the 
initiator. There are other possible sequences of messages, not shown,
6
Source code is available from http://www.winikoff.net/CM
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 877
Figure 6: Sample runs from 3APL implementation (alternating turns)
including the obvious one: request, quote, accept, goods, payment,
receipt, and then done.
One minor difference between the 3APL implementation and
SAAPL concerns the semantics of messages. In the semantics of
SAAPL (and of most AOPLs), receiving a message is treated as an
event. However, in 3APL, receiving a message is modelled as the
addition to the agent"s beliefs of a fact indicating that the message
was received [6]. Thus in the 3APL implementation we have PG
rules that are triggered by these beliefs, rather than by any event.
One issue with this approach is that the belief remains there, so we
need to ensure that the belief in question is either deleted once 
handled, or that we modify preconditions of plans to avoid handling it
more than once. In our implementation we delete these received
beliefs when they are handled, to avoid duplicate handling of 
messages.
4. BEYOND TWO PARTICIPANTS
Generalising to more than two interaction participants requires
revisiting how turn management is done, since it is no longer 
possible to assume alternating turns [7].
In fact, perhaps surprisingly, even in the two participant setting,
an alternating turn setup is an unreasonable assumption! For 
example, consider the path (in figure 1) from state 1 to 15 (sendGoods)
then to state 12 (sendAccept). The result, in an alternating turn
setup, is a dead-end: there is only a single possible action in state
12, namely sendEPO, but this action is done by the customer, and
it is the merchant"s turn to act! Figure 7 shows the FSM for NetBill
with alternating initiative.
A solution to this problem that works in this example, but doesn"t
generalise7
, is to weaken the alternating turn taking regime by 
allowing an agent to act twice in a row if its second action is driven
by a commitment.
A general solution is to track whose turn it is to act. This can be
done by working out which agents have actions that are able to be
performed in the current state. If there is only a single active agent,
then it is clearly that agent"s turn to act. However, if more than
one agent is active then somehow the agents need to work out who
should act next. Working this out by negotiation is not a particularly
good solution for two reasons. Firstly, this negotiation has to be
done at every step of the interaction where more than one agent is
active (in the NetBill, this applies to seven out of sixteen states), so
it is highly desirable to have a light-weight mechanism for doing
this. Secondly, it is not clear how the negotiation can avoid an
infinite regress situation (you go first, no, you go first, . ..)
without imposing some arbitrary rule. It is also possible to resolve
who should act by imposing an arbitrary rule, for example, that the
customer always acts in preference to the merchant, or that each
agent has a numerical priority (perhaps determined by the order in
which they joined the interaction?) that determines who acts.
An alternative solution, which exploits the symmetrical 
properties of commitment machines, is to not try and manage turn taking.
7
Consider actions A1(C) ⇒ p, A2(C) ⇒ q, and A3(M) : p ∧
q ⇒ r.
Figure 7: NetBill with alternating initiative
Instead of tracking and controlling whose turn it is, we simply allow
the agents to act freely, and rely on the properties of the interaction
space to ensure that things work out, a notion that we shall make
precise, and prove, in the remainder of this section.
The issue with having multiple agents be active simultaneously
is that instead of all agents agreeing on the current interaction state,
agents can be in different states. This can be visualised as each
agent having its own copy of the FSM that it navigates through
where it is possible for agents to follow different paths through the
FSM. The two specific issues that need to be addressed are:
1. Can agents end up in different final states?
2. Can an agent be in a position where an error occurs because
it cannot perform an action corresponding to a received 
message?
We will show that, because actions commute under certain 
assumptions, agents cannot end up in different final states, and 
furthermore, that errors cannot occur (again, under certain 
assumptions).
By actions commute we mean that the state resulting from 
performing a sequence of actions A1 . . . An is the same, regardless of
the order in which the actions are performed. This means that even
if agents take different paths through the FSM, they still end up in
the same resulting state, because once all messages have been 
processed, all agents will have performed the same set of actions. This
addresses the issue of ending up in different final states. We return
to the possibility of errors occurring shortly.
Definition 1 (Monotonicity) An action is monotonic if it does not
delete8
any fluents or commitments. A Commitment Machine is
8
That is directly deletes, it is fine to discharge commitments by
adding fluents/commitments.
878 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
monotonic if all of its actions are monotonic. (Adapted from [14,
Definition 6])
Theorem 1 If A1 and A2 are monotonic actions, then performing
A1 followed by A2 has the same effect on the agent"s beliefs as
performing A2 followed by A1. (Adapted from [14, Theorem 2]).
This assumes that both actions can be performed. However, it is
possible for the performance of A1 to disable A2 from being done.
For example, if A1 has the effect +p, and A2 has precondition
¬p, then although both actions may be enabled in the initial state,
they cannot be performed in either order. We can prevent this by
ensuring that actions" preconditions do not contain negation (or 
implication), since a monotonic action cannot result in a precondition
that is negation-free becoming false. Note that this restriction only
applies to the original action precondition, P, not to any additional
preconditions imposed by the agent (P ). This is because only P
is used to determine whether another agent is able to perform the
action.
Thus monotonic CMs with preconditions that do not contain
negations have actions that commute. However, in fact, the 
restriction to monotonic CMs is unnecessarily strong: all that is needed
is that whenever there is a choice of agent that can act, then the
possible actions are monotonic. If there is only a single agent that
can act, then no restriction is needed on the actions: they may or
may not be monotonic.
Definition 2 (Locally Monotonic) A commitment machine is 
locally monotonic if for any state S either (a) only a single agent
has actions that can be performed; or (b) all actions that can be
performed in S are monotonic.
Theorem 2 In a locally monotonic CM, once all messages have
been processed, all agents will be in the same state. Furthermore,
no errors can occur.
Proof: Once all messages have been processed we have that all
agents will have performed the same action set, perhaps in a 
different order. The essence of the proof is to argue that as long as
agents haven"t yet converged to the same state, all actions must
be monotonic, and hence that these actions commute, and cannot
disable any other actions.
Consider the first point of divergence, where an agent performs
action A and at the same time another agent (call it XB) performs
action B. Clearly, this state has actions of more than one agent 
enabled, so, since the CM is locally monotonic, the relevant actions
must be monotonic. Therefore, after doing A, the action B must
still be enabled, and so the message to do B can be processed by
updating the recipient agent"s beliefs with the effects of B. 
Furthermore, because monotonic actions commute, the result of doing
A before B is the same as doing B before A:
S
A
−−−−−→ SA
?
?
yB B
?
?
y
SB −−−−−→
A
SAB
However, what happens if the next action after A is not B, but
C? Because B is enabled, and C is not done by agent XB (see
below), we must have that C is also monotonic, and hence (a) the
result of doing A and B and C is the same regardless of the order
in which the three actions are done; and (b) C doesn"t disable B,
so B can still be done after C.
S
A
−−−−−→ SA
C
−−−−−→ SAC
?
?
yB B
?
?
y B
?
?
y
SB −−−−−→
A
SAB −−−−−→
C
SABC
The reason why C cannot be done by XB is that messages are
processed in the order of their arrival9
. From the perspective of
XB the action B was done before C, and therefore from any other
agent"s perspective the message saying that B was done must be
received (and processed) before a message saying that C is done.
This argument can be extended to show that once agents start
taking different paths through the FSM all actions taken until the
point where they converge on a single state must be monotonic,
and hence it is always possible to converge (because actions aren"t
disabled), so the interaction is error free; and the resulting state
once convergence occurs is the same (because monotonic actions
commute).
This theorem gives a strong theoretical guarantee that not 
doing turn management will not lead to disaster. This is analogous
to proving that disabling all traffic lights would not lead to any 
accidents, and is only possible because the refined CM axioms are
symmetrical.
Based on this theorem the generic transformation from CM to
code should allow agents to act freely, which is achieved by simply
changing ı : P ∧ P ∧ ¬E ← +E; ↑A to
ı : P ∧ P ∧ ¬E ← +E; ↑A; ı
For example, instead of ı : ¬request ← +request; ↑sendRequest
we have ı : ¬request ← +request; ↑sendRequest; ı.
One consequence of the theorem is that it is not necessary to
ensure that agents process messages before continuing to 
interact. However, in order to avoid unnecessary parallelism, which can
make debugging harder, it may still be desirable to process 
messages before performing actions.
Figure 8 shows a number of runs from the 3APL implementation
that has been modified to allow free, non-alternating, interaction.
5. DISCUSSION
We have presented a scheme for mapping commitment machines
to BDI platforms (using SAAPL as an exemplar), identified three
changes that needed to be made to SAAPL to support CM-based 
interaction, and shown that turn management can be avoided in 
CMbased interaction, provided the CM is locally monotonic. The three
changes to SAAPL, and the translation scheme from commitment
machine to BDI plans are both applicable to any BDI language.
As we have mentioned in section 1, there has been some work
on designing flexible and robust agent interaction, but virtually no
work on implementing flexible and robust interactions.
We have already discussed STAPLE [9, 10]. Another piece of
work that is relevant is the work by Cheong and Winikoff on their
Hermes methodology [2]. Although the main focus of their work is
a pragmatic design methodology, they also provide guidelines for
implementing Hermes designs using BDI platforms (specifically
Jadex) [3]. However, since Hermes does not yield a design that is
formal, it is only possible to generate skeleton code that then needs
to be completed. Also, they do not address the turn taking issue:
how to decide which agent acts when more than one agent is able
to act.
9
We also assume that the communication medium does not deliver
messages out of order, which is the case for (e.g.) TCP.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 879
Figure 8: Sample runs from 3APL implementation (non-alternating turns)
The work of Kremer and Flores (e.g. [8]) also uses 
commitments, and deals with implementation. However, they provide 
infrastructure support (CASA) rather than a programming language,
and do not appear to provide assistance to a programmer seeking to
implement agents.
Although we have implemented the NetBill interaction using
3APL, the changes to the semantics were done by modifying our
NetBill 3APL program, rather than by modifying the 3APL 
implementation itself. Clearly, it would be desirable to modify the
semantics of 3APL (or of another language) directly, by changing
the implementation. Also, although we have not done so, it should
be clear that the translation from a CM to its implementation could
easily be automated.
Another area for further work is to look at how the assumptions
required to ensure that actions commute can be relaxed.
Finally, there is a need to perform empirical evaluation. There
has already been some work on comparing Hermes with a 
conventional message-centric approach to designing interaction, and
this has shown that using Hermes results in designs that are 
significantly more flexible and robust [4]. It would be interesting to
compare commitment machines with Hermes, but, since 
commitment machines are a framework, not a design methodology, we
need to compare Hermes with a methodology for designing 
interactions that results in commitment machines [13, 17].
6. REFERENCES
[1] R. H. Bordini, M. Dastani, J. Dix, and A. E. F. Seghrouchni,
editors. Multi-Agent Programming: Languages, Platforms
and Applications. Springer, 2005.
[2] C. Cheong and M. Winikoff. Hermes: Designing
goal-oriented agent interactions. In Proceedings of the 6th
International Workshop on Agent-Oriented Software
Engineering (AOSE-2005), July 2005.
[3] C. Cheong and M. Winikoff. Hermes: Implementing
goal-oriented agent interactions. In Proceedings of the Third
international Workshop on Programming Multi-Agent
Systems (ProMAS), July 2005.
[4] C. Cheong and M. Winikoff. Hermes versus prometheus: A
comparative evaluation of two agent interaction design
approaches. Submitted for publication, 2007.
[5] P. R. Cohen and H. J. Levesque. Teamwork. Nous,
25(4):487-512, 1991.
[6] M. Dastani, J. van der Ham, and F. Dignum. Communication
for goal directed agents. In Proceedings of the Agent
Communication Languages and Conversation Policies
Workshop, 2002.
[7] F. P. Dignum and G. A. Vreeswijk. Towards a testbed for
multi-party dialogues. In Advances in Agent Communication,
pages 212-230. Springer, LNCS 2922, 2004.
[8] R. Kremer and R. Flores. Using a performative subsumption
lattice to support commitment-based conversations. In
F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. P. Singh, and
M. Wooldridge, editors, Autonomous Agents and Multi-Agent
Systems (AAMAS), pages 114-121. ACM Press, 2005.
[9] S. Kumar and P. R. Cohen. STAPLE: An agent programming
language based on the joint intention theory. In Proceedings
of the Third International Joint Conference on Autonomous
Agents & Multi-Agent Systems (AAMAS 2004), pages
1390-1391. ACM Press, July 2004.
[10] S. Kumar, M. J. Huber, and P. R. Cohen. Representing and
executing protocols as joint actions. In Proceedings of the
First International Joint Conference on Autonomous Agents
and Multi-Agent Systems, pages 543 - 550, Bologna, Italy,
15 - 19 July 2002. ACM Press.
[11] M. Tambe and W. Zhang. Towards flexible teamwork in
persistent teams: Extended report. Journal of Autonomous
Agents and Multi-agent Systems, 2000. Special issue on
Best of ICMAS 98.
[12] M. Winikoff. An AgentSpeak meta-interpreter and its
applications. In Third International Workshop on
Programming Multi-Agent Systems (ProMAS), pages
123-138. Springer, LNCS 3862 (post-proceedings, 2006),
2005.
[13] M. Winikoff. Designing commitment-based agent
interactions. In Proceedings of the 2006 IEEE/WIC/ACM
International Conference on Intelligent Agent Technology
(IAT-06), 2006.
[14] M. Winikoff. Implementing flexible and robust agent
interactions using distributed commitment machines.
Multiagent and Grid Systems, 2(4), 2006.
[15] M. Winikoff, W. Liu, and J. Harland. Enhancing
commitment machines. In J. Leite, A. Omicini, P. Torroni,
and P. Yolum, editors, Declarative Agent Languages and
Technologies II, number 3476 in Lecture Notes in Artificial
Intelligence (LNAI), pages 198-220. Springer, 2004.
[16] M. Winikoff, L. Padgham, J. Harland, and J. Thangarajah.
Declarative & procedural goals in intelligent agent systems.
In Proceedings of the Eighth International Conference on
Principles of Knowledge Representation and Reasoning
(KR2002), Toulouse, France, 2002.
[17] P. Yolum. Towards design tools for protocol development. In
F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. P. Singh, and
M. Wooldridge, editors, Autonomous Agents and Multi-Agent
Systems (AAMAS), pages 99-105. ACM Press, 2005.
[18] P. Yolum and M. P. Singh. Flexible protocol specification and
execution: Applying event calculus planning using
commitments. In Proceedings of the 1st Joint Conference on
Autonomous Agents and MultiAgent Systems (AAMAS),
pages 527-534, 2002.
[19] P. Yolum and M. P. Singh. Reasoning about commitments in
the event calculus: An approach for specifying and executing
protocols. Annals of Mathematics and Artificial Intelligence
(AMAI), 2004.
880 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Learning and Joint Deliberation through Argumentation in
Multi-Agent Systems
Santi Ontañón
CCL, Cognitive Computing Lab
Georgia Institute of Technology
Atlanta, GA 303322/0280
santi@cc.gatech.edu
Enric Plaza
IIIA, Artificial Intelligence Research Institute
CSIC, Spanish Council for Scientific Research
Campus UAB, 08193 Bellaterra, Catalonia
(Spain)
enric@iiia.csic.es
ABSTRACT
In this paper we will present an argumentation framework for 
learning agents (AMAL) designed for two purposes: (1) for joint 
deliberation, and (2) for learning from communication. The AMAL 
framework is completely based on learning from examples: the argument
preference relation, the argument generation policy, and the 
counterargument generation policy are case-based techniques. For join
deliberation, learning agents share their experience by forming a
committee to decide upon some joint decision. We experimentally
show that the argumentation among committees of agents improves
both the individual and joint performance. For learning from 
communication, an agent engages into arguing with other agents in 
order to contrast its individual hypotheses and receive 
counterexamples; the argumentation process improves their learning scope and
individual performance.
Categories and Subject Descriptors
I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial 
Intelligence]: Distributed Artificial Intelligence-Multiagent systems,
Intelligent Agents
1. INTRODUCTION
Argumentation frameworks for multi-agent systems can be used
for different purposes like joint deliberation, persuasion, 
negotiation, and conflict resolution. In this paper we will present an 
argumentation framework for learning agents, and show that it can be
used for two purposes: (1) joint deliberation, and (2) learning from
communication.
Argumentation-based joint deliberation involves discussion over
the outcome of a particular situation or the appropriate course of 
action for a particular situation. Learning agents are capable of 
learning from experience, in the sense that past examples (situations and
their outcomes) are used to predict the outcome for the situation
at hand. However, since individual agents experience may be 
limited, individual knowledge and prediction accuracy is also limited.
Thus, learning agents that are capable of arguing their individual
predictions with other agents may reach better prediction accuracy
after such an argumentation process.
Most existing argumentation frameworks for multi-agent 
systems are based on deductive logic or some other deductive logic
formalism specifically designed to support argumentation, such as
default logic [3]). Usually, an argument is seen as a logical 
statement, while a counterargument is an argument offered in opposition
to another argument [4, 13]; agents use a preference relation to 
resolve conflicting arguments. However, logic-based argumentation
frameworks assume agents with preloaded knowledge and 
preference relation. In this paper, we focus on an Argumentation-based
Multi-Agent Learning (AMAL) framework where both knowledge
and preference relation are learned from experience. Thus, we 
consider a scenario with agents that (1) work in the same domain using
a shared ontology, (2) are capable of learning from examples, and
(3) communicate using an argumentative framework.
Having learning capabilities allows agents effectively use a 
specific form of counterargument, namely the use of 
counterexamples. Counterexamples offer the possibility of agents learning 
during the argumentation process. Moreover, learning agents allow
techniques that use learnt experience to generate adequate 
arguments and counterarguments. Specifically, we will need to address
two issues: (1) how to define a technique to generate arguments
and counterarguments from examples, and (2)how to define a 
preference relation over two conflicting arguments that have been 
induced from examples.
This paper presents a case-based approach to address both 
issues. The agents use case-based reasoning (CBR) [1] to learn from
past cases (where a case is a situation and its outcome) in order
to predict the outcome of a new situation. We propose an 
argumentation protocol inside the AMAL framework at supports agents
in reaching a joint prediction over a specific situation or problem
- moreover, the reasoning needed to support the argumentation
process will also be based on cases. In particular, we present two
case-based measures, one for generating the arguments and 
counterarguments adequate to a particular situation and another for 
determining preference relation among arguments. Finally, we 
evaluate (1) if argumentation between learning agents can produce a
joint prediction that improves over individual learning performance
and (2) if learning from the counterexamples conveyed during the
argumentation process increases the individual performance with
precisely those cases being used while arguing among them.
The paper is structured as follows. Section 2 discusses the 
relation among argumentation, collaboration and learning. Then 
Section 3 introduces our multi-agent CBR (MAC) framework and the
notion of justified prediction. After that, Section 4 formally 
defines our argumentation framework. Sections 5 and 6 present our
case-based preference relation and argument generation policies 
respectively. Later, Section 7 presents the argumentation protocol in
our AMAL framework. After that, Section 8 presents an 
exemplification of the argumentation framework. Finally, Section 9 presents
an empirical evaluation of our two main hypotheses. The paper
closes with related work and conclusions sections.
2. ARGUMENTATION,COLLABORATION
AND LEARNING
Both learning and collaboration are ways in which an agent can
improve individual performance. In fact, there is a clear parallelism
between learning and collaboration in multi-agent systems, since
both are ways in which agents can deal with their shortcomings.
Let us show which are the main motivations that an agent can have
to learn or to collaborate.
• Motivations to learn:
- Increase quality of prediction,
- Increase efficiency,
- Increase the range of solvable problems.
• Motivations to collaborate:
- Increase quality of prediction,
- Increase efficiency,
- Increase the range of solvable problems,
- Increase the range of accessible resources.
Looking at the above lists of motivation, we can easily see that
learning and collaboration are very related in multi-agent systems.
In fact, with the exception of the last item in the motivations to
collaborate list, they are two extremes of a continuum of strategies
to improve performance. An agent may choose to increase 
performance by learning, by collaborating, or by finding an intermediate
point that combines learning and collaboration in order to improve
performance.
In this paper we will propose AMAL, an argumentation 
framework for learning agents, and will also also show how AMAL can be
used both for learning from communication and for solving 
problems in a collaborative way:
• Agents can solve problems in a collaborative way via 
engaging an argumentation process about the prediction for the
situation at hand. Using this collaboration, the prediction
can be done in a more informed way, since the information
known by several agents has been taken into account.
• Agents can also learn from communication with other agents
by engaging an argumentation process. Agents that engage
in such argumentation processes can learn from the 
arguments and counterexamples received from other agents, and
use this information for predicting the outcomes of future 
situations.
In the rest of this paper we will propose an argumentation 
framework and show how it can be used both for learning and for solving
problems in a collaborative way.
3. MULTI-AGENT CBR SYSTEMS
A Multi-Agent Case Based Reasoning System (MAC) M =
{(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A =
{Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A
possesses an individual case base Ci. Each individual agent Ai
in a MAC is completely autonomous and each agent Ai has 
access only to its individual and private case base Ci. A case base
Ci = {c1, ..., cm} is a collection of cases. Agents in a MAC
system are able to individually solve problems, but they can also
collaborate with other agents to solve problems.
In this framework, we will restrict ourselves to analytical tasks,
i.e. tasks like classification, where the solution of a problem is
achieved by selecting a solution class from an enumerated set of
solution classes. In the following we will note the set of all the 
solution classes by S = {S1, ..., SK }. Therefore, a case c = P, S is
a tuple containing a case description P and a solution class S ∈ S.
In the following, we will use the terms problem and case 
description indistinctly. Moreover, we will use the dot notation to refer to
elements inside a tuple; e.g., to refer to the solution class of a case
c, we will write c.S.
Therefore, we say a group of agents perform joint deliberation,
when they collaborate to find a joint solution by means of an 
argumentation process. However, in order to do so, an agent has to
be able to justify its prediction to the other agents (i.e. generate an
argument for its predicted solution that can be examined and 
critiqued by the other agents). The next section addresses this issue.
3.1 Justified Predictions
Both expert systems and CBR systems may have an explanation
component [14] in charge of justifying why the system has 
provided a specific answer to the user. The line of reasoning of the
system can then be examined by a human expert, thus increasing
the reliability of the system.
Most of the existing work on explanation generation focuses on
generating explanations to be provided to the user. However, in our
approach we use explanations (or justifications) as a tool for 
improving communication and coordination among agents. We are
interested in justifications since they can be used as arguments.
For that purpose, we will benefit from the ability of some machine
learning methods to provide justifications.
A justification built by a CBR method after determining that the
solution of a particular problem P was Sk is a description that 
contains the relevant information from the problem P that the CBR
method has considered to predict Sk as the solution of P. In 
particular, CBR methods work by retrieving similar cases to the problem
at hand, and then reusing their solutions for the current problem,
expecting that since the problem and the cases are similar, the 
solutions will also be similar. Thus, if a CBR method has retrieved a set
of cases C1, ..., Cn to solve a particular problem P the justification
built will contain the relevant information from the problem P that
made the CBR system retrieve that particular set of cases, i.e. it
will contain the relevant information that P and C1, ..., Cn have in
common.
For example, Figure 1 shows a justification build by a CBR 
system for a toy problem (in the following sections we will show 
justifications for real problems). In the figure, a problem has two 
attributes (Traffic_light, and Cars_passing), the retrieval mechanism
of the CBR system notices that by considering only the attribute
Traffic_light, it can retrieve two cases that predict the same 
solution: wait. Thus, since only this attribute has been used, it is the
only one appearing in the justification. The values of the rest of 
attributes are irrelevant, since whatever their value the solution class
would have been the same.
976 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Problem
Traffic_light: red
Cars_passing: no
Case 1
Traffic_light: red
Cars_passing: no
Solution: wait
Case 3
Traffic_light: red
Cars_passing: yes
Solution: wait
Case 4
Traffic_light: green
Cars_passing: yes
Solution: wait
Case 2
Traffic_light: green
Cars_passing: no
Solution: cross
Retrieved
cases
Solution: wait
Justification
Traffic_light: red
Figure 1: An example of justification generation in a CBR system. Notice that, since the only relevant feature to decide is Traffic_light
(the only one used to retrieve cases), it is the only one appearing in the justification.
In general, the meaning of a justification is that all (or most of)
the cases in the case base of an agent that satisfy the justification
(i.e. all the cases that are subsumed by the justification) belong to
the predicted solution class. In the rest of the paper, we will use
to denote the subsumption relation. In our work, we use LID [2], a
CBR method capable of building symbolic justifications such as the
one exemplified in Figure 1. When an agent provides a justification
for a prediction, the agent generates a justified prediction:
DEFINITION 3.1. A Justified Prediction is a tuple J = A, P,
S, D where agent A considers S the correct solution for problem
P, and that prediction is justified a symbolic description D such
that J.D J.P.
Justifications can have many uses for CBR systems [8, 9]. In this
paper, we are going to use justifications as arguments, in order to
allow learning agents to engage in argumentation processes.
4. ARGUMENTS AND
COUNTERARGUMENTS
For our purposes an argument α generated by an agent A is 
composed of a statement S and some evidence D supporting S as 
correct. In the remainder of this section we will see how this 
general definition of argument can be instantiated in specific kind of
arguments that the agents can generate. In the context of MAC
systems, agents argue about predictions for new problems and can
provide two kinds of information: a) specific cases P, S , and b)
justified predictions: A, P, S, D . Using this information, we can
define three types of arguments: justified predictions, 
counterarguments, and counterexamples.
A justified prediction α is generated by an agent Ai to argue that
Ai believes that the correct solution for a given problem P is α.S,
and the evidence provided is the justification α.D. In the 
example depicted in Figure 1, an agent Ai may generate the argument
α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai
believes that the correct solution for P is Wait because the attribute
Traffic_light equals red.
A counterargument β is an argument offered in opposition to
another argument α. In our framework, a counterargument 
consists of a justified prediction Aj, P, S , D generated by an agent
Aj with the intention to rebut an argument α generated by another
agent Ai, that endorses a solution class S different from that of
α.S for the problem at hand and justifies this with a justification
D . In the example in Figure 1, if an agent generates the argument
α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that
the correct solution is Wait might answer with the counterargument
β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , 
meaning that, although there are no cars passing, the traffic light is red,
and the street cannot be crossed.
A counterexample c is a case that contradicts an argument α.
Thus a counterexample is also a counterargument, one that states
that a specific argument α is not always true, and the evidence 
provided is the case c. Specifically, for a case c to be a 
counterexample of an argument α, the following conditions have to be met:
α.D c and α.S = c.S, i.e. the case must satisfy the justification
α.D and the solution of c must be different than the predicted by
α.
By exchanging arguments and counterarguments (including 
counterexamples), agents can argue about the correct solution of a given
problem, i.e. they can engage a joint deliberation process. 
However, in order to do so, they need a specific interaction protocol, a
preference relation between contradicting arguments, and a 
decision policy to generate counterarguments (including 
counterexamples). In the following sections we will present these elements.
5. PREFERENCE RELATION
A specific argument provided by an agent might not be consistent
with the information known to other agents (or even to some of the
information known by the agent that has generated the justification
due to noise in training data). For that reason, we are going to
define a preference relation over contradicting justified predictions
based on cases. Basically, we will define a confidence measure for
each justified prediction (that takes into account the cases owned by
each agent), and the justified prediction with the highest confidence
will be the preferred one.
The idea behind case-based confidence is to count how many of
the cases in an individual case base endorse a justified prediction,
and how many of them are counterexamples of it. The more the
endorsing cases, the higher the confidence; and the more the 
counterexamples, the lower the confidence. Specifically, to assess the
confidence of a justified prediction α, an agent obtains the set of
cases in its individual case base that are subsumed by α.D. With
them, an agent Ai obtains the Y (aye) and N (nay) values:
• Y Ai
α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number
of cases in the agent"s case base subsumed by the justification
α.D that belong to the solution class α.S,
• NAi
α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number
of cases in the agent"s case base subsumed by justification
α.D that do not belong to that solution class.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977
+ +
+
+
+
+


-
- +
Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.
An agent estimates the confidence of an argument as:
CAi (α) =
Y Ai
α
1 + Y Ai
α + NAi
α
i.e. the confidence on a justified prediction is the number of 
endorsing cases divided by the number of endorsing cases plus 
counterexamples. Notice that we add 1 to the denominator, this is to avoid
giving excessively high confidences to justified predictions whose
confidence has been computed using a small number of cases. 
Notice that this correction follows the same idea than the Laplace 
correction to estimate probabilities. Figure 2 illustrates the individual
evaluation of the confidence of an argument, in particular, three 
endorsing cases and one counterexample are found in the case base
of agents Ai, giving an estimated confidence of 0.6
Moreover, we can also define the joint confidence of an argument
α as the confidence computed using the cases present in the case
bases of all the agents in the group:
C(α) = i Y Ai
α
1 + i Y Ai
α + NAi
α
Notice that, to collaboratively compute the joint confidence, the
agents only have to make public the aye and nay values locally
computed for a given argument.
In our framework, agents use this joint confidence as the 
preference relation: a justified prediction α is preferred over another one
β if C(α) ≥ C(β).
6. GENERATION OF ARGUMENTS
In our framework, arguments are generated by the agents from
cases, using learning methods. Any learning method able to 
provide a justified prediction can be used to generate arguments. For
instance, decision trees and LID [2] are suitable learning methods.
Specifically, in the experiments reported in this paper agents use
LID. Thus, when an agent wants to generate an argument 
endorsing that a specific solution class is the correct solution for a problem
P, it generates a justified prediction as explained in Section 3.1.
For instance, Figure 3 shows a real justification generated by
LID after solving a problem P in the domain of marine sponges
identification. In particular, Figure 3 shows how when an agent
receives a new problem to solve (in this case, a new sponge to
determine its order), the agent uses LID to generate an argument
(consisting on a justified prediction) using the cases in the case
base of the agent. The justification shown in Figure 3 can be 
interpreted saying that the predicted solution is hadromerida 
because the smooth form of the megascleres of the spiculate 
skeleton of the sponge is of type tylostyle, the spikulate skeleton of the
sponge has no uniform length, and there is no gemmules in the 
external features of the sponge. Thus, the argument generated will
be α = A1, P, hadromerida, D1 .
6.1 Generation of Counterarguments
As previously stated, agents may try to rebut arguments by 
generating counterargument or by finding counterexamples. Let us 
explain how they can be generated.
An agent Ai wants to generate a counterargument β to rebut an
argument α when α is in contradiction with the local case base of
Ai. Moreover, while generating such counterargument β, Ai 
expects that β is preferred over α. For that purpose, we will present
a specific policy to generate counterarguments based on the 
specificity criterion [10].
The specificity criterion is widely used in deductive frameworks
for argumentation, and states that between two conflicting 
arguments, the most specific should be preferred since it is, in 
principle, more informed. Thus, counterarguments generated based on
the specificity criterion are expected to be preferable (since they are
more informed) to the arguments they try to rebut. However, there
is no guarantee that such counterarguments will always win, since,
as we have stated in Section 5, agents in our framework use a 
preference relation based on joint confidence. Moreover, one may think
that it would be better that the agents generate counterarguments
based on the joint confidence preference relation; however it is not
obvious how to generate counterarguments based on joint 
confidence in an efficient way, since collaboration is required in order to
evaluate joint confidence. Thus, the agent generating the 
counterargument should constantly communicate with the other agents at
each step of the induction algorithm used to generate 
counterarguments (presently one of our future research lines).
Thus, in our framework, when an agent wants to generate a 
counterargument β to an argument α, β has to be more specific than α
(i.e. α.D < β.D).
The generation of counterarguments using the specificity 
criterion imposes some restrictions over the learning method, although
LID or ID3 can be easily adapted for this task. For instance, LID is
an algorithm that generates a description starting from scratch and
heuristically adding features to that term. Thus, at every step, the
description is made more specific than in the previous step, and the
number of cases that are subsumed by that description is reduced.
When the description covers only (or almost only) cases of a 
single solution class LID terminates and predicts that solution class.
To generate a counterargument to an argument α LID just has to
use as starting point the description α.D instead of starting from
scratch. In this way, the justification provided by LID will always
be subsumed by α.D, and thus the resulting counterargument will
be more specific than α. However, notice that LID may sometimes
not be able to generate counterarguments, since LID may not be
able to specialize the description α.D any further, or because the
agent Ai has no case inCi that is subsumed by α.D. Figure 4 shows
how an agent A2 that disagreed with the argument shown in 
Figure 3, generates a counterargument using LID. Moreover, Figure 4
shows the generation of a counterargument β1
2 for the argument α0
1
(in Figure 3) that is a specialization of α0
1.
978 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Solution: hadromerida
Justification: D1
Sponge
Spikulate
skeleton
External
features
External features
Gemmules: no
Spikulate Skeleton
Megascleres
Uniform length: no
Megascleres
Smooth form: tylostyle
Case Base
of A1
LID
New
sponge
P
Figure 3: Example of a real justification generated by LID in the marine sponges data set.
Specifically, in our experiments, when an agent Ai wants to rebut
an argument α, uses the following policy:
1. Agent Ai uses LID to try to find a counterargument β more
specific than α; if found, β is sent to the other agent as a
counterargument of α.
2. If not found, then Ai searches for a counterexample c ∈ Ci
of α. If a case c is found, then c is sent to the other agent as
a counterexample of α.
3. If no counterexamples are found, then Ai cannot rebut the
argument α.
7. ARGUMENTATION-BASED
MULTI-AGENT LEARNING
The interaction protocol of AMAL allows a group of agents A1,
..., An to deliberate about the correct solution of a problem P by
means of an argumentation process. If the argumentation process
arrives to a consensual solution, the joint deliberation ends; 
otherwise a weighted vote is used to determine the joint solution. 
Moreover, AMAL also allows the agents to learn from the 
counterexamples received from other agents.
The AMAL protocol consists on a series of rounds. In the initial
round, each agent states which is its individual prediction for P.
Then, at each round an agent can try to rebut the prediction made
by any of the other agents. The protocol uses a token passing 
mechanism so that agents (one at a time) can send counterarguments or
counterexamples if they disagree with the prediction made by any
other agent. Specifically, each agent is allowed to send one 
counterargument or counterexample each time he gets the token (notice
that this restriction is just to simplify the protocol, and that it does
not restrict the number of counterargument an agent can sent, since
they can be delayed for subsequent rounds). When an agent 
receives a counterargument or counterexample, it informs the other
agents if it accepts the counterargument (and changes its 
prediction) or not. Moreover, agents have also the opportunity to answer
to counterarguments when they receive the token, by trying to 
generate a counterargument to the counterargument.
When all the agents have had the token once, the token returns
to the first agent, and so on. If at any time in the protocol, all the
agents agree or during the last n rounds no agent has generated
any counterargument, the protocol ends. Moreover, if at the end of
the argumentation the agents have not reached an agreement, then
a voting mechanism that uses the confidence of each prediction as
weights is used to decide the final solution (Thus, AMAL follows
the same mechanism as human committees, first each individual
member of a committee exposes his arguments and discuses those
of the other members (joint deliberation), and if no consensus is
reached, then a voting mechanism is required).
At each iteration, agents can use the following performatives:
• assert(α): the justified prediction held during the next round
will be α. An agent can only hold a single prediction at each
round, thus is multiple asserts are send, only the last one is
considered as the currently held prediction.
• rebut(β, α): the agent has found a counterargument β to the
prediction α.
We will define Ht = αt
1, ..., αt
n as the predictions that each
of the n agents hold at a round t. Moreover, we will also define
contradict(αt
i) = {α ∈ Ht|α.S = αt
i.S} as the set of 
contradicting arguments for an agent Ai in a round t, i.e. the set of
arguments at round t that support a different solution class than αt
i.
The protocol is initiated because one of the agents receives a
problem P to be solved. After that, the agent informs all the other
agents about the problem P to solve, and the protocol starts:
1. At round t = 0, each one of the agents individually solves P,
and builds a justified prediction using its own CBR method.
Then, each agent Ai sends the performative assert(α0
i ) to
the other agents. Thus, the agents know H0 = α0
i , ..., α0
n .
Once all the predictions have been sent the token is given to
the first agent A1.
2. At each round t (other than 0), the agents check whether their
arguments in Ht agree. If they do, the protocol moves to step
5. Moreover, if during the last n rounds no agent has sent any
counterexample or counterargument, the protocol also moves
to step 5. Otherwise, the agent Ai owner of the token tries
to generate a counterargument for each of the opposing 
arguments in contradict(αt
i) ⊆ Ht (see Section 6.1). Then, the
counterargument βt
i against the prediction αt
j with the 
lowest confidence C(αt
j) is selected (since αt
j is the prediction
more likely to be successfully rebutted).
• If βt
i is a counterargument, then, Ai locally compares
αt
i with βt
i by assessing their confidence against its 
individual case base Ci (see Section 5) (notice that Ai is
comparing its previous argument with the 
counterargument that Ai itself has just generated and that is about
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979
Sponge
Spikulate
skeleton
External
features
External features
Gemmules: no
Growing:
Spikulate Skeleton
Megascleres
Uniform length: no
Megascleres
Smooth form: tylostyle
Growing
Grow: massive
Case Base
of A2
LID
Solution: astrophorida
Justification: D2
Figure 4: Generation of a counterargument using LID in the sponges data set.
to send to Aj). If CAi (βt
i ) > CAi (αt
i), then Ai 
considers that βt
i is stronger than its previous argument,
changes its argument to βt
i by sending assert(βt
i ) to
the rest of the agents (the intuition behind this is that
since a counterargument is also an argument, Ai checks
if the newly counterargument is a better argument than
the one he was previously holding) and rebut(βt
i ,
αt
j) to Aj. Otherwise (i.e. CAi (βt
i ) ≤ CAi (αt
i)), Ai
will send only rebut(βt
i , αt
j) to Aj. In any of the two
situations the protocol moves to step 3.
• If βt
i is a counterexample c, then Ai sends rebut(c, αt
j)
to Aj. The protocol moves to step 4.
• If Ai cannot generate any counterargument or 
counterexample, the token is sent to the next agent, a new
round t + 1 starts, and the protocol moves to state 2.
3. The agent Aj that has received the counterargument βt
i , 
locally compares it against its own argument, αt
j, by locally
assessing their confidence. If CAj (βt
i ) > CAj (αt
j), then
Aj will accept the counterargument as stronger than its own
argument, and it will send assert(βt
i ) to the other agents.
Otherwise (i.e. CAj (βt
i ) ≤ CAj (αt
j)), Aj will not accept
the counterargument, and will inform the other agents 
accordingly. Any of the two situations start a new round t + 1,
Ai sends the token to the next agent, and the protocol moves
back to state 2.
4. The agent Aj that has received the counterexample c retains
it into its case base and generates a new argument αt+1
j that
takes into account c, and informs the rest of the agents by
sending assert(αt+1
j ) to all of them. Then, Ai sends the
token to the next agent, a new round t + 1 starts, and the
protocol moves back to step 2.
5. The protocol ends yielding a joint prediction, as follows: if
the arguments in Ht agree then their prediction is the joint
prediction, otherwise a voting mechanism is used to decide
the joint prediction. The voting mechanism uses the joint
confidence measure as the voting weights, as follows:
S = arg max
Sk∈S
αi∈Ht|αi.S=Sk
C(αi)
Moreover, in order to avoid infinite iterations, if an agent sends
twice the same argument or counterargument to the same agent, the
message is not considered.
8. EXEMPLIFICATION
Let us consider a system composed of three agents A1, A2 and
A3. One of the agents, A1 receives a problem P to solve, and 
decides to use AMAL to solve it. For that reason, invites A2 and A3 to
take part in the argumentation process. They accept the invitation,
and the argumentation protocol starts.
Initially, each agent generates its individual prediction for P, and
broadcasts it to the other agents. Thus, all of them can compute
H0 = α0
1, α0
2, α0
3 . In particular, in this example:
• α0
1 = A1, P, hadromerida, D1
• α0
2 = A2, P, astrophorida, D2
• α0
3 = A3, P, axinellida, D3
A1 starts owning the token and tries to generate 
counterarguments for α0
2 and α0
3, but does not succeed, however it has one
counterexample c13 for α0
3. Thus, A1 sends the the message rebut(
c13, α0
3) to A3. A3 incorporates c13 into its case base and tries to
solve the problem P again, now taking c13 into consideration. A3
comes up with the justified prediction α1
3 = A3, P, hadromerida,
D4 , and broadcasts it to the rest of the agents with the message
assert(α1
3). Thus, all of them know the new H1 = α0
1, α0
2, α1
3 .
Round 1 starts and A2 gets the token. A2 tries to generate 
counterarguments for α0
1 and α1
3 and only succeeds to generate a 
counterargument β1
2 = A2, P, astrophorida, D5 against α1
3. The
counterargument is sent to A3 with the message rebut(β1
2 , α1
3).
Agent A3 receives the counterargument and assesses its local 
confidence. The result is that the individual confidence of the 
counterargument β1
2 is lower than the local confidence of α1
3. Therefore, A3
does not accept the counterargument, and thus H2 = α0
1, α0
2, α1
3 .
Round 2 starts and A3 gets the token. A3 generates a 
counterargument β2
3 = A3, P, hadromerida, D6 for α0
2 and sends it to
A2 with the message rebut(β2
3 , α0
2). Agent A2 receives the 
counterargument and assesses its local confidence. The result is that the
local confidence of the counterargument β2
3 is higher than the local
confidence of α0
2. Therefore, A2 accepts the counterargument and
informs the rest of the agents with the message assert(β2
3 ). After
that, H3 = α0
1, β2
3 , α1
3 .
At Round 3, since all the agents agree (all the justified 
predictions in H3 predict hadromerida as the solution class) The 
protocol ends, and A1 (the agent that received the problem) considers
hadromerida as the joint solution for the problem P.
9. EXPERIMENTAL EVALUATION
980 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
SPONGE
75
77
79
81
83
85
87
89
91
2 3 4 5
AMAL
Voting
Individual
SOYBEAN
55
60
65
70
75
80
85
90
2 3 4 5
AMAL
Voting
Individual
Figure 5: Individual and joint accuracy for 2 to 5 agents.
In this section we empirically evaluate the AMAL argumentation
framework. We have made experiments in two different data sets:
soybean (from the UCI machine learning repository) and sponge (a
relational data set). The soybean data set has 307 examples and 19
solution classes, while the sponge data set has 280 examples and 3
solution classes. In an experimental run, the data set is divided in 2
sets: the training set and the test set. The training set examples are
distributed among 5 different agents without replication, i.e. there
is no example shared by two agents. In the testing stage, problems
in the test set arrive randomly to one of the agents, and their goal is
to predict the correct solution.
The experiments are designed to test two hypotheses: (H1) that
argumentation is a useful framework for joint deliberation and can
improve over other typical methods such as voting; and (H2) that
learning from communication improves the individual performance
of a learning agent participating in an argumentation process. 
Moreover, we also expect that the improvement achieved from 
argumentation will increase as the number of agents participating in the 
argumentation increases (since more information will be taken into
account).
Concerning H1 (argumentation is a useful framework for joint
deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents
respectively (in all experiments each agent has a 20% of the training
data, since the training is always distributed among 5 agents).
Figure 5 shows the result of those experiments in the sponge and
soybean data sets. Classification accuracy is plotted in the 
vertical axis, and in the horizontal axis the number of agents that took
part in the argumentation processes is shown. For each number of
agents, three bars are shown: individual, Voting, and AMAL. The
individual bar shows the average accuracy of individual agents 
predictions; the voting bar shows the average accuracy of the joint
prediction achieved by voting but without any argumentation; and
finally the AMAL bar shows the average accuracy of the joint 
prediction using argumentation. The results shown are the average of
5 10-fold cross validation runs.
Figure 5 shows that collaboration (voting and AMAL) 
outperforms individual problem solving. Moreover, as we expected, the
accuracy improves as more agents collaborate, since more 
information is taken into account. We can also see that AMAL always
outperforms standard voting, proving that joint decisions are based
on better information as provided by the argumentation process.
For instance, the joint accuracy for 2 agents in the sponge data
set is of 87.57% for AMAL and 86.57% for voting (while individual
accuracy is just 80.07%). Moreover, the improvement achieved by
AMAL over Voting is even larger in the soybean data set. The 
reason is that the soybean data set is more difficult (in the sense that
agents need more data to produce good predictions). These 
experimental results show that AMAL effectively exploits the opportunity
for improvement: the accuracy is higher only because more agents
have changed their opinion during argumentation (otherwise they
would achieve the same result as Voting).
Concerning H2 (learning from communication in argumentation
processes improves individual prediction ), we ran the following
experiment: initially, we distributed a 25% of the training set among
the five agents; after that, the rest of the cases in the training set is
sent to the agents one by one; when an agent receives a new 
training case, it has several options: the agent can discard it, the agent
can retain it, or the agent can use it for engaging an argumentation
process. Figure 6 shows the result of that experiment for the two
data sets. Figure 6 contains three plots, where NL (not learning)
shows accuracy of an agent with no learning at all; L (learning),
shows the evolution of the individual classification accuracy when
agents learn by retaining the training cases they individually 
receive (notice that when all the training cases have been retained at
100%, the accuracy should be equal to that of Figure 5 for 
individual agents); and finally LFC (learning from communication) shows
the evolution of the individual classification accuracy of learning
agents that also learn by retaining those counterexamples received
during argumentation (i.e. they learn both from training examples
and counterexamples).
Figure 6 shows that if an agent Ai learns also from 
communication, Ai can significantly improve its individual performance with
just a small number of additional cases (those selected as relevant
counterexamples for Ai during argumentation). For instance, in
the soybean data set, individual agents have achieved an accuracy
of 70.62% when they also learn from communication versus an 
accuracy of 59.93% when they only learn from their individual 
experience. The number of cases learnt from communication depends
on the properties of the data set: in the sponges data set, agents
have retained only very few additional cases, and significantly 
improved individual accuracy; namely they retain 59.96 cases in 
average (compared to the 50.4 cases retained if they do not learn from
communication). In the soybean data set more counterexamples are
learnt to significantly improve individual accuracy, namely they 
retain 87.16 cases in average (compared to 55.27 cases retained if
they do not learn from communication). Finally, the fact that both
data sets show a significant improvement points out the adaptive
nature of the argumentation-based approach to learning from 
communication: the useful cases are selected as counterexamples (and
no more than those needed), and they have the intended effect.
10. RELATED WORK
Concerning CBR in a multi-agent setting, the first research was
on negotiated case retrieval [11] among groups of agents. Our
work on multi-agent case-based learning started in 1999 [6]; later
Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR
approach (CCBR) for planning. Finally, another interesting 
approach is multi-case-base reasoning (MCBR) [5], that deals with
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981
SPONGE
60
65
70
75
80
85
25% 40% 55% 70% 85% 100%
LFC
L
NL
SOYBEAN
20
30
40
50
60
70
80
90
25% 40% 55% 70% 85% 100%
LFC
L
NL
Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents.
distributed systems where there are several case bases available for
the same task and addresses the problems of cross-case base 
adaptation. The main difference is that our MAC approach is a way to
distribute the Reuse process of CBR (using a voting system) while
Retrieve is performed individually by each agent; the other 
multiagent CBR approaches, however, focus on distributing the Retrieve
process.
Research on MAS argumentation focus on several issues like a)
logics, protocols and languages that support argumentation, b) 
argument selection and c) argument interpretation. Approaches for
logic and languages that support argumentation include defeasible
logic [4] and BDI models [13]. Although argument selection is a
key aspect of automated argumentation (see [12] and [13]), most 
research has been focused on preference relations among arguments.
In our framework we have addressed both argument selection and
preference relations using a case-based approach.
11. CONCLUSIONS AND FUTURE WORK
In this paper we have presented an argumentation-based 
framework for multi-agent learning. Specifically, we have presented
AMAL, a framework that allows a group of learning agents to 
argue about the solution of a given problem and we have shown how
the learning capabilities can be used to generate arguments and
counterarguments. The experimental evaluation shows that the 
increased amount of information provided to the agents by the 
argumentation process increases their predictive accuracy, and specially
when an adequate number of agents take part in the argumentation.
The main contributions of this work are: a) an argumentation
framework for learning agents; b) a case-based preference relation
over arguments, based on computing an overall confidence 
estimation of arguments; c) a case-based policy to generate 
counterarguments and select counterexamples; and d) an argumentation-based
approach for learning from communication.
Finally, in the experiments presented here a learning agent would
retain all counterexamples submitted by the other agent; however,
this is a very simple case retention policy, and we will like to 
experiment with more informed policies - with the goal that individual
learning agents could significantly improve using only a small set
of cases proposed by other agents. Finally, our approach is focused
on lazy learning, and future works aims at incorporating eager 
inductive learning inside the argumentative framework for learning
from communication.
12. REFERENCES
[1] Agnar Aamodt and Enric Plaza. Case-based reasoning:
Foundational issues, methodological variations, and system
approaches. Artificial Intelligence Communications,
7(1):39-59, 1994.
[2] E. Armengol and E. Plaza. Lazy induction of descriptions for
relational case-based learning. In ECML"2001, pages 13-24,
2001.
[3] Gerhard Brewka. Dynamic argument systems: A formal
model of argumentation processes based on situation
calculus. Journal of Logic and Computation, 11(2):257-282,
2001.
[4] Carlos I. Chesñevar and Guillermo R. Simari. Formalizing
Defeasible Argumentation using Labelled Deductive
Systems. Journal of Computer Science & Technology,
1(4):18-33, 2000.
[5] D. Leake and R. Sooriamurthi. Automatically selecting
strategies for multi-case-base reasoning. In S. Craw and
A. Preece, editors, ECCBR"2002, pages 204-219, Berlin,
2002. Springer Verlag.
[6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.
Knowledge and experience reuse through communications
among competent (peer) agents. International Journal of
Software Engineering and Knowledge Engineering,
9(3):319-341, 1999.
[7] Lorraine McGinty and Barry Smyth. Collaborative
case-based reasoning: Applications in personalized route
planning. In I. Watson and Q. Yang, editors, ICCBR, number
2080 in LNAI, pages 362-376. Springer-Verlag, 2001.
[8] Santi Ontañón and Enric Plaza. Justification-based
multiagent learning. In ICML"2003, pages 576-583. Morgan
Kaufmann, 2003.
[9] Enric Plaza, Eva Armengol, and Santiago Ontañón. The
explanatory power of symbolic similarity in case-based
reasoning. Artificial Intelligence Review, 24(2):145-161,
2005.
[10] David Poole. On the comparison of theories: Preferring the
most specific explanation. In IJCAI-85, pages 144-147,
1985.
[11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.
Retrieval and reasoning in distributed case bases. Technical
report, UMass Computer Science Department, 1995.
[12] K. Sycara S. Kraus and A. Evenchik. Reaching agreements
through argumentation: a logical model and implementation.
Artificial Intelligence Journal, 104:1-69, 1998.
[13] N. R. Jennings S. Parsons, C. Sierra. Agents that reason and
negotiate by arguing. Journal of Logic and Computation,
8:261-292, 1998.
[14] Bruce A. Wooley. Explanation component of software
systems. ACM CrossRoads, 5.1, 1998.
982 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
A Multi-Agent System for Building Dynamic Ontologies
Kévin Ottens
∗
IRIT, Université Paul Sabatier
118 Route de Narbonne
F-31062 TOULOUSE
ottens@irit.fr
Marie-Pierre Gleizes
IRIT, Université Paul Sabatier
118 Route de Narbonne
F-31062 TOULOUSE
gleizes@irit.fr
Pierre Glize
IRIT, Université Paul Sabatier
118 Route de Narbonne
F-31062 TOULOUSE
glize@irit.fr
ABSTRACT
Ontologies building from text is still a time-consuming task which
justifies the growth of Ontology Learning. Our system named 
Dynamo is designed along this domain but following an original 
approach based on an adaptive multi-agent architecture. In this paper
we present a distributed hierarchical clustering algorithm, core of
our approach. It is evaluated and compared to a more conventional
centralized algorithm. We also present how it has been improved
using a multi-criteria approach. With those results in mind, we
discuss the limits of our system and add as perspectives the 
modifications required to reach a complete ontology building solution.
Categories and Subject Descriptors
I.2.11 [Artificial Intelligence]: Distributed Artificial 
IntelligenceMultiagent Systems
General Terms
Algorithms, Experimentation
1. INTRODUCTION
Nowadays, it is well established that ontologies are needed for
semantic web, knowledge management, B2B... For knowledge
management, ontologies are used to annotate documents and to 
enhance the information retrieval. But building an ontology manually
is a slow, tedious, costly, complex and time consuming process.
Currently, a real challenge lies in building them automatically or
semi-automatically and keeping them up to date. It would mean
creating dynamic ontologies [10] and it justifies the emergence of
ontology learning techniques [14] [13].
Our research focuses on Dynamo (an acronym of DYNAMic 
Ontologies), a tool based on an adaptive multi-agent system to 
construct and maintain an ontology from a domain specific set of texts.
Our aim is not to build an exhaustive, general hierarchical ontology
but a domain specific one. We propose a semi-automated tool since
an external resource is required: the "ontologist". An ontologist is
a kind of cognitive engineer, or analyst, who is using information
from texts and expert interviews to design ontologies.
In the multi-agent field, ontologies generally enable agents to 
understand each other [12]. They"re sometimes used to ease the 
ontology building process, in particular for collaborative contexts [3],
but they rarely represent the ontology itself [16]. Most works 
interested in the construction of ontologies [7] propose the refinement of
ontologies. This process consists in using an existing ontology and
building a new one from it. This approach is different from our 
approach because Dynamo starts from scratch. Researchers, working
on the construction of ontologies from texts, claim that the work to
be automated requires external resources such as a dictionary [14],
or web access [5]. In our work, we propose an interaction between
the ontologist and the system, our external resource lies both in the
texts and the ontologist.
This paper first presents, in section 2, the big picture of the 
Dynamo system. In particular the motives that led to its creation and
its general architecture. Then, in section 3 we discuss the 
distributed clustering algorithm used in Dynamo and compare it to
a more classic centralized approach. Section 4 is dedicated to some
enhancement of the agents behavior that got designed by taking
into account criteria ignored by clustering. And finally, in section
5, we discuss the limitations of our approach and explain how it
will be addressed in further work.
2. DYNAMO OVERVIEW
2.1 Ontology as a Multi-Agent System
Dynamo aims at reducing the need for manual actions in 
processing the text analysis results and at suggesting a concept 
network kick-off in order to build ontologies more efficiently. The
chosen approach is completely original to our knowledge and uses
an adaptive multi-agent system. This choice comes from the 
qualities offered by multi-agent system: they can ease the interactive
design of a system [8] (in our case, a conceptual network), they
allow its incremental building by progressively taking into account
new data (coming from text analysis and user interaction), and last
but not least they can be easily distributed across a computer 
network.
Dynamo takes a syntactical and terminological analysis of texts
as input. It uses several criteria based on statistics computed from
the linguistic contexts of terms to create and position the concepts.
As output, Dynamo provides to the analyst a hierarchical 
organization of concepts (the multi-agent system itself) that can be 
validated, refined of modified, until he/she obtains a satisfying state of
1286
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
the semantic network.
An ontology can be seen as a stable map constituted of 
conceptual entities, represented here by agents, linked by labelled 
relations. Thus, our approach considers an ontology as a type of
equilibrium between its concept-agents where their forces are 
defined by their potential relationships. The ontology modification
is a perturbation of the previous equilibrium by the appearance or
disappearance of agents or relationships. In this way, a dynamic
ontology is a self-organizing process occurring when new texts are
included into the corpus, or when the ontologist interacts with it.
To support the needed flexibility of such a system we use a 
selforganizing multi-agent system based on a cooperative approach [9].
We followed the ADELFE method [4] proposed to drive the design
of this kind of multi-agent system. It justifies how we designed
some of the rules used by our agents in order to maximize the 
cooperation degree within Dynamo"s multi-agent system.
2.2 Proposed Architecture
In this section, we present our system architecture. It addresses
the needs of Knowledge Engineering in the context of dynamic 
ontology management and maintenance when the ontology is linked
to a document collection.
The Dynamo system consists of three parts (cf. figure 1):
• a term network, obtained thanks to a term extraction tool
used to preprocess the textual corpus,
• a multi-agent system which uses the term network to make a
hierarchical clustering in order to obtain a taxonomy of 
concepts,
• an interface allowing the ontologist to visualize and control
the clustering process.
??
Ontologist
Interface
System
Concept Agent Term
Term network
Terms
Extraction
Tool
Figure 1: System architecture
The term extractor we use is Syntex, a software that has 
efficiently been used for ontology building tasks [11]. We mainly 
selected it because of its robustness and the great amount of 
information extracted. In particular, it creates a "Head-Expansion" network
which has already proven to be interesting for a clustering system
[1]. In such a network, each term is linked to its head term1
and
1
i.e. the maximum sub-phrase located as head of the term
its expansion term2
, and also to all the terms for which it is a head
or an expansion term. For example, "knowledge engineering from
text" has "knowledge engineering" as head term and "text" as 
expansion term. Moreover, "knowledge engineering" is composed of
"knowledge" as head term and "engineering" as expansion term.
With Dynamo, the term network obtained as the output of the 
extractor is stored in a database. For each term pair, we assume that it
is possible to compute a similarity value in order to make a 
clustering [6] [1]. Because of the nature of the data, we are only focusing
on similarity computation between objects described thanks to 
binary variables, that means that each item is described by the 
presence or absence of a characteristic set [15]. In the case of terms
we are generally dealing with their usage contexts. With Syntex,
those contexts are identified by terms and characterized by some
syntactic relations.
The Dynamo multi-agent system implements the distributed 
clustering algorithm described in detail in section 3 and the rules 
described in section 4. It is designed to be both the system 
producing the resulting structure and the structure itself. It means that
each agent represent a class in the taxonomy. Then, the system
output is the organization obtained from the interaction between
agents, while taking into account feedback coming from the 
ontologist when he/she modifies the taxonomy given his needs or 
expertise.
3. DISTRIBUTED CLUSTERING
This section presents the distributed clustering algorithm used in
Dynamo. For the sake of understanding, and because of its 
evaluation in section 3.1, we recall the basic centralized algorithm used
for a hierarchical ascending clustering in a non metric space, when
a symmetrical similarity measure is available [15] (which is the
case of the measures used in our system).
Algorithm 1: Centralized hierarchical ascending clustering 
algorithm
Data: List L of items to organize as a hierarchy
Result: Root R of the hierarchy
while length(L) > 1 do
max ← 0;
A ← nil;
B ← nil;
for i ← 1 to length(L) do
I ← L[i];
for j ← i + 1 to length(L) do
J ← L[j];
sim ← similarity(I, J);
if sim > max then
max ← sim;
A ← I;
B ← J;
end
end
end
remove(A, L);
remove(B, L);
append((A, B), L);
end
R ← L[1];
In algorithm 1, for each clustering step, the pair of the most 
similar elements is determined. Those two elements are grouped in a
cluster, and the resulting class is appended to the list of remaining
elements. This algorithm stops when the list has only one element
left.
2
i.e. the maximum sub-phrase located as tail of the term
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1287
The hierarchy resulting from algorithm 1 is always a binary tree
because of the way grouping is done. Moreover grouping the most
similar elements is equivalent to moving them away from the least
similar ones. Our distributed algorithm is designed relying on those
two facts. It is executed concurrently in each of the agents of the
system.
Note that, in the following of this paper, we used for both 
algorithms an Anderberg similarity (with α = 0.75) and an average
link clustering strategy [15]. Those choices have an impact on the
resulting tree, but they impact neither the global execution of the
algorithm nor its complexity.
We now present the distributed algorithm used in our system. It
is bootstrapped in the following way:
• a TOP agent having no parent is created, it will be the root of
the resulting taxonomy,
• an agent is created for each term to be positioned in the 
taxonomy, they all have TOP as parent.
Once this basic structure is set, the algorithm runs until it reaches
equilibrium and then provides the resulting taxonomy.
Ak−1 Ak AnA2A1
P
...... ......
A1
Figure 2: Distributed classification: Step 1
The process first step (figure 2) is triggered when an agent (here
Ak) has more than one brother (since we want to obtain a binary
tree). Then it sends a message to its parent P indicating its most
dissimilar brother (here A1). Then P receives the same kind of
message from each of its children. In the following, this kind of
message will be called a "vote".
Ak−1 Ak AnA2A1
P
P"
...... ......
P"
P"
Figure 3: Distributed clustering: Step 2
Next, when P has got messages from all its children, it starts the
second step (figure 3). Thanks to the received messages indicating
the preferences of its children, P can determine three sub-groups
among its children:
• the child which got the most "votes" by its brothers, that is
the child being the most dissimilar from the greatest number
of its brothers. In case of a draw, one of the winners is chosen
randomly (here A1),
• the children that allowed the "election" of the first group, that
is the agents which chose their brother of the first group as
being the most dissimilar one (here Ak to An),
• the remaining children (here A2 to Ak−1).
Then P creates a new agent P (having P as parent) and asks
agents from the second group (here agents Ak to An) to make it
their new parent.
Ak−1 Ak AnA2A1
P
P"
...... ......
Figure 4: Distributed clustering: Step 3
Finally, step 3 (figure 4) is trivial. The children rejected by P
(here agent A2 to An) take its message into account and choose P
as their new parent. The hierarchy just created a new intermediate
level.
Note that this algorithm generally converges, since the number of
brothers of an agent drops. When an agent has only one remaining
brother, its activity stops (although it keeps processing messages
coming from its children). However in a few cases we can reach
a "circular conflict" in the voting procedure when for example A
votes against B, B against C and C against A. With the current
system no decision can be taken. The current procedure should be
improved to address this, probably using a ranked voting method.
3.1 Quantitative Evaluation
Now, we evaluate the properties of our distributed algorithm. It
requires to begin with a quantitative evaluation, based on its 
complexity, while comparing it with the algorithm 1 from the previous
section.
Its theoretical complexity is calculated for the worst case, by
considering the similarity computation operation as elementary. For
the distributed algorithm, the worst case means that for each run,
only a two-item group can be created. Under those conditions, for a
given dataset of n items, we can determine the amount of similarity
computations.
For algorithm 1, we note l = length(L), then the most enclosed
"for" loop is run l − i times. And its body has the only similarity
computation, so its cost is l−i. The second "for" loop is ran l times
for i ranging from 1 to l. Then its cost is
Pl
i=1(l − i) which can
be simplified in l×(l−1)
2
. Finally for each run of the "while" loop,
l is decreased from n to 1 which gives us t1(n) as the amount of
similarity computations for algorithm 1:
t1(n) =
nX
l=1
l × (l − 1)
2
(1)
For the distributed algorithm, at a given step, each one of the l
agents evaluates the similarity with its l −1 brothers. So each steps
has a l × (l − 1) cost. Then, groups are created and another vote
occurs with l decreased by one (since we assume worst case, only
groups of size 2 or l −1 are built). Since l is equal to n on first run,
we obtain tdist(n) as the amount of similarity computations for the
distributed algorithm:
tdist(n) =
nX
l=1
l × (l − 1) (2)
Both algorithms then have an O(n3
) complexity. But in the
worst case, the distributed algorithm does twice the number of 
el1288 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
ementary operations done by the centralized algorithm. This gap
comes from the local decision making in each agent. Because of
this, the similarity computations are done twice for each agent pair.
We could conceive that an agent sends its computation result to its
peer. But, it would simply move the problem by generating more
communication in the system.
0
20000
40000
60000
80000
100000
120000
140000
160000
180000
10 20 30 40 50 60 70 80 90 100
Amountofcomparisons
Amount of input terms
1. Distributed algorithm (on average, with min and max)
2. Logarithmic polynomial
3. Centralized algorithm
Figure 5: Experimental results
In a second step, the average complexity of the algorithm has
been determined by experiments. The multi-agent system has been
executed with randomly generated input data sets ranging from ten
to one hundred terms. The given value is the average of 
comparisons made for one hundred of runs without any user interaction.
It results in the plots of figure 5. The algorithm is then more 
efficient on average than the centralized algorithm, and its average
complexity is below the worst case. It can be explained by the low
probability that a data set forces the system to create only minimal
groups (two items) or maximal (n − 1 elements) for each step of
reasoning. Curve number 2 represents the logarithmic polynomial
minimizing the error with curve number 1. The highest degree term
of this polynomial is in n2
log(n), then our distributed algorithm
has a O(n2
log(n)) complexity on average. Finally, let"s note the
reduced variation of the average performances with the maximum
and the minimum. In the worst case for 100 terms, the variation is
of 1,960.75 for an average of 40,550.10 (around 5%) which shows
the good stability of the system.
3.2 Qualitative Evaluation
Although the quantitative results are interesting, the real 
advantage of this approach comes from more qualitative characteristics
that we will present in this section. All are advantages obtained
thanks to the use of an adaptive multi-agent system.
The main advantage to the use of a multi-agent system for a 
clustering task is to introduce dynamic in such a system. The ontologist
can make modifications and the hierarchy adapts depending on the
request. It is particularly interesting in a knowledge engineering
context. Indeed, the hierarchy created by the system is meant to be
modified by the ontologist since it is the result of a statistic 
computation. During the necessary look at the texts to examine the
usage contexts of terms [2], the ontologist will be able to interpret
the real content and to revise the system proposal. It is extremely
difficult to realize this with a centralized "black-box" approach. In
most cases, one has to find which reasoning step generated the error
and to manually modify the resulting class. Unfortunately, in this
case, all the reasoning steps that occurred after the creation of the
modified class are lost and must be recalculated by taking the 
modification into account. That is why a system like ASIUM [6] tries to
soften the problem with a system-user collaboration by showing to
the ontologist the created classes after each step of reasoning. But,
the ontologist can make a mistake, and become aware of it too late.
Figure 6: Concept agent tree after autonomous stabilization of
the system
In order to illustrate our claims, we present an example thanks to
a few screenshots from the working prototype tested on a medical
related corpus. By using test data and letting the system work by
itself, we obtain the hierarchy from figure 6 after stabilization. It is
clear that the concept described by the term "lésion" (lesion) is 
misplaced. It happens that the similarity computations place it closer to
"femme" (woman) and "chirurgien" (surgeon) than to "infection",
"gastro-entérite" (gastro-enteritis) and "hépatite" (hepatitis). This
wrong position for "lesion" is explained by the fact that without
ontologist input the reasoning is only done on statistics criteria.
Figure 7: Concept agent tree after ontologist modification
Then, the ontologist replaces the concept in the right branch, by
affecting "ConceptAgent:8" as its new parent. The name 
"ConceptAgent:X" is automatically given to a concept agent that is not
described by a term. The system reacts by itself and refines the
clustering hierarchy to obtain a binary tree by creating 
"ConceptAgent:11". The new stable state if the one of figure 7.
This system-user coupling is necessary to build an ontology, but
no particular adjustment to the distributed algorithm principle is
needed since each agent does an autonomous local processing and
communicates with its neighborhood by messages.
Moreover, this algorithm can de facto be distributed on a 
computer network. The communication between agents is then done by
sending messages and each one keeps its decision autonomy. Then,
a system modification to make it run networked would not require
to adjust the algorithm. On the contrary, it would only require to 
rework the communication layer and the agent creation process since
in our current implementation those are not networked.
4. MULTI-CRITERIA HIERARCHY
In the previous sections, we assumed that similarity can be 
computed for any term pair. But, as soon as one uses real data this
property is not verified anymore. Some terms do not have any 
similarity value with any extracted term. Moreover for leaf nodes it is
sometimes interesting to use other means to position them in the
hierarchy. For this low level structuring, ontologists generally base
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1289
their choices on simple heuristics. Using this observation, we built
a new set of rules, which are not based on similarity to support low
level structuring.
4.1 Adding Head Coverage Rules
In this case, agents can act with a very local point of view simply
by looking at the parent/child relation. Each agent can try to 
determine if its parent is adequate. It is possible to guess this because
each concept agent is described by a set of terms and thanks to the
"Head-Expansion" term network.
In the following TX will be the set of terms describing concept
agent X and head(TX ) the set of all the terms that are head of at
least one element of TX . Thanks to those two notations we can
describe the parent adequacy function a(P, C) between a parent P
and a child C:
a(P, C) =
|TP ∩ head(TC )|
|TP ∪ head(TC )|
(3)
Then, the best parent for C is the P agent that maximizes a(P, C).
An agent unsatisfied by its parent can then try to find a better one
by evaluating adequacy with candidates. We designed a 
complementary algorithm to drive this search:
When an agent C is unsatisfied by its parent P, it evaluates
a(Bi, C) with all its brothers (noted Bi) the one maximizing a(Bi, C)
is then chosen as the new parent.
Figure 8: Concept agent tree after autonomous stabilization of
the system without head coverage rule
We now illustrate this rule behavior with an example. Figure 8
shows the state of the system after stabilization on test data. We
can notice that "hépatite viral" (viral hepatitis) is still linked to the
taxonomy root. It is caused by the fact that there is no similarity
value between the "viral hepatitis" term and any of the term of the
other concept agents.
Figure 9: Concept agent tree after activation of the head 
coverage rule
After activating the head coverage rule and letting the system
stabilize again we obtain figure 9. We can see that "viral hepatitis"
slipped through the branch leading to "hepatitis" and chose it as its
new parent. It is a sensible default choice since "viral hepatitis" is
a more specific term than "hepatitis".
This rule tends to push agents described by a set of term to 
become leafs of the concept tree. It addresses our concern to improve
the low level structuring of our taxonomy. But obviously our agents
lack a way to backtrack in case of modifications in the taxonomy
which would make them be located in the wrong branch. That is
one of the point where our system still has to be improved by adding
another set of rules.
4.2 On Using Several Criteria
In the previous sections and examples, we only used one 
algorithm at a time. The distributed clustering algorithm tends to 
introduce new layers in the taxonomy, while the head coverage 
algorithm tends to push some of the agents toward the leafs of the
taxonomy. It obviously raises the question on how to deal with
multiple criteria in our taxonomy building, and how agents 
determine their priorities at a given time.
The solution we chose came from the search for minimizing non
cooperation within the system in accordance with the ADELFE
method. Each agent computes three non cooperation degrees and
chooses its current priority depending on which degree is the 
highest. For a given agent A having a parent P, a set of brothers Bi
and which received a set of messages Mk having the priority pk
the three non cooperation degrees are:
• μH (A) = 1 − a(P, A), is the "head coverage" non 
cooperation degree, determined by the head coverage of the parent,
• μB(A) = max(1 − similarity(A, Bi)), is the 
"brotherhood" non cooperation degree, determined by the worst brother
of A regarding similarities,
• μM (A) = max(pk), is the "message" non cooperation 
degree, determined by the most urgent message received.
Then, the non cooperation degree μ(A) of agent A is:
μ(A) = max(μH (A), μB(A), μM (A)) (4)
Then, we have three cases determining which kind of action A will
choose:
• if μ(A) = μH (A) then A will use the head coverage 
algorithm we detailed in the previous subsection
• if μ(A) = μB(A) then A will use the distributed clustering
algorithm (see section 3)
• if μ(A) = μM (A) then A will process Mk immediately in
order to help its sender
Those three cases summarize the current activities of our agents:
they have to find the best parent for them (μ(A) = μH (A)), 
improve the structuring through clustering (μ(A) = μB(A)) and 
process other agent messages (μ(A) = μM (A)) in order to help them
fulfill their own goals.
4.3 Experimental Complexity Revisited
We evaluated the experimental complexity of the whole 
multiagent system when all the rules are activated. In this case, the 
metric used is the number of messages exchanged in the system. Once
again the system has been executed with input data sets ranging
from ten to one hundred terms. The given value is the average of
message amount sent in the system as a whole for one hundred runs
without user interaction. It results in the plots of figure 10.
Curve number 1 represents the average of the value obtained.
Curve number 2 represents the average of the value obtained when
only the distributed clustering algorithm is activated, not the full
rule set. Curve number 3 represents the polynomial minimizing the
error with curve number 1. The highest degree term of this 
polynomial is in n3
, then our multi-agent system has a O(n3
) complexity
1290 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
0
5000
10000
15000
20000
25000
10 20 30 40 50 60 70 80 90 100
Amountofmessages
Amount of input terms
1. Dynamo, all rules (on average, with min and max)
2. Distributed clustering only (on average)
2. Cubic polynomial
Figure 10: Experimental results
on average. Moreover, let"s note the very small variation of the 
average performances with the maximum and the minimum. In the
worst case for 100 terms, the variation is of 126.73 for an average
of 20,737.03 (around 0.6%) which proves the excellent stability of
the system.
Finally the extra head coverage rules are a real improvement on
the distributed algorithm alone. They introduce more constraints
and stability point is reached with less interactions and decision
making by the agents. It means that less messages are exchanged
in the system while obtaining a tree of higher quality for the 
ontologist.
5. DISCUSSION & PERSPECTIVES
5.1 Current Limitation of our Approach
The most important limitation of our current algorithm is that
the result depends on the order the data gets added. When the 
system works by itself on a fixed data set given during initialization,
the final result is equivalent to what we could obtain with a 
centralized algorithm. On the contrary, adding a new item after a first
stabilization has an impact on the final result.
Figure 11: Concept agent tree after autonomous stabilization
of the system
To illustrate our claims, we present another example of the 
working system. By using test data and letting the system work by itself,
we obtain the hierarchy of figure 11 after stabilization.
Figure 12: Concept agent tree after taking in account 
"hepatitis"
Then, the ontologist interacts with the system and adds a new
concept described by the term "hepatitis" and linked to the root.
The system reacts and stabilizes, we then obtain figure 12 as a 
result. "hepatitis" is located in the right branch, but we have not
obtained the same organization as the figure 6 of the previous 
example. We need to improve our distributed algorithm to allow a
concept to move along a branch. We are currently working on the
required rules, but the comparison with centralized algorithm will
become very difficult. In particular since they will take into account
criteria ignored by the centralized algorithm.
5.2 Pruning for Ontologies Building
In section 3, we presented the distributed clustering algorithm
used in the Dynamo system. Since this work was first based on this
algorithm, it introduced a clear bias toward binary trees as a result.
But we have to keep in mind that we are trying to obtain taxonomies
which are more refined and concise. Although the head coverage
rule is an improvement because it is based on how the ontologists
generally work, it only addresses low level structuring but not the
intermediate levels of the tree.
By looking at figure 7, it is clear that some pruning could be
done in the taxonomy. In particular, since "lésion" moved, 
"ConceptAgent:9" could be removed, it is not needed anymore. 
Moreover the branch starting with "ConceptAgent:8" clearly respects the
constraint to make a binary tree, but it would be more useful to the
user in a more compact and meaningful form. In this case 
"ConceptAgent:10" and "ConceptAgent:11" could probably be merged.
Currently, our system has the necessary rules to create 
intermediate levels in the taxonomy, or to have concepts shifting towards
the leaf. As we pointed, it is not enough, so new rules are needed to
allow removing nodes from the tree, or move them toward the root.
Most of the work needed to develop those rules consists in finding
the relevant statistic information that will support the ontologist.
6. CONCLUSION
After being presented as a promising solution, ensuring model
quality and their terminological richness, ontology building from
textual corpus analysis is difficult and costly. It requires analyst
supervising and taking in account the ontology aim. Using 
natural languages processing tools ease the knowledge localization in
texts through language uses. That said, those tools produce a huge
amount of lexical or grammatical data which is not trivial to 
examine in order to define conceptual elements. Our contribution lies in
this step of the modeling process from texts, before any attempts to
normalize or formalize the result.
We proposed an approach based on an adaptive multi-agent 
system to provide the ontologist with a first taxonomic structure of
concepts. Our system makes use of a terminological network 
resulting from an analysis made by Syntex. The current state of our
software allows to produce simple structures, to propose them to
the ontologist and to make them evolve depending on the 
modifications he made. Performances of the system are interesting and
some aspects are even comparable to their centralized counterpart.
Its strengths are mostly qualitative since it allows more subtle user
interactions and a progressive adaptation to new linguistic based
information.
From the point of view of ontology building, this work is a first
step showing the relevance of our approach. It must continue, both
to ensure a better robustness during classification, and to obtain
richer structures semantic wise than simple trees. From this 
improvements we are mostly focusing on the pruning to obtain better
taxonomies. We"re currently working on the criterion to trigger
the complementary actions of the structure changes applied by our
clustering algorithm. In other words this algorithm introduces 
inThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1291
termediate levels, and we need to be able to remove them if 
necessary, in order to reach a dynamic equilibrium.
Also from the multi-agent engineering point of view, their use
in a dynamic ontology context has shown its relevance. This 
dynamic ontologies can be seen as complex problem solving, in such
a case self-organization through cooperation has been an efficient
solution. And, more generally it"s likely to be interesting for other
design related tasks, even if we"re focusing only on knowledge 
engineering in this paper. Of course, our system still requires more
evaluation and validation work to accurately determine the 
advantages and flaws of this approach. We"re planning to work on such
benchmarking in the near future.
7. REFERENCES
[1] H. Assadi. Construction of a regional ontology from text and
its use within a documentary system. Proceedings of the
International Conference on Formal Ontology and
Information Systems - FOIS"98, pages 236-249, 1998.
[2] N. Aussenac-Gilles and D. Sörgel. Text analysis for ontology
and terminology engineering. Journal of Applied Ontology,
2005.
[3] J. Bao and V. Honavar. Collaborative ontology building with
wiki@nt. Proceedings of the Workshop on Evaluation of
Ontology-Based Tools (EON2004), 2004.
[4] C. Bernon, V. Camps, M.-P. Gleizes, and G. Picard.
Agent-Oriented Methodologies, chapter 7. Engineering
Self-Adaptive Multi-Agent Systems : the ADELFE
Methodology, pages 172-202. Idea Group Publishing, 2005.
[5] C. Brewster, F. Ciravegna, and Y. Wilks. Background and
foreground knowledge in dynamic ontology construction.
Semantic Web Workshop, SIGIR"03, August 2003.
[6] D. Faure and C. Nedellec. A corpus-based conceptual
clustering method for verb frames and ontology acquisition.
LREC workshop on adapting lexical and corpus resources to
sublanguages and applications, 1998.
[7] F. Gandon. Ontology Engineering: a Survey and a Return on
Experience. INRIA, 2002.
[8] J.-P. Georgé, G. Picard, M.-P. Gleizes, and P. Glize. Living
Design for Open Computational Systems. 12th IEEE
International Workshops on Enabling Technologies,
Infrastructure for Collaborative Enterprises, pages 389-394,
June 2003.
[9] M.-P. Gleizes, V. Camps, and P. Glize. A Theory of emergent
computation based on cooperative self-organization for
adaptive artificial systems. Fourth European Congress of
Systems Science, September 1999.
[10] J. Heflin and J. Hendler. Dynamic ontologies on the web.
American Association for Artificial Intelligence Conference,
2000.
[11] S. Le Moigno, J. Charlet, D. Bourigault, and M.-C. Jaulent.
Terminology extraction from text to build an ontology in
surgical intensive care. Proceedings of the AMIA 2002
annual symposium, 2002.
[12] K. Lister, L. Sterling, and K. Taveter. Reconciling
Ontological Differences by Assistant Agents. AAMAS"06,
May 2006.
[13] A. Maedche. Ontology learning for the Semantic Web.
Kluwer Academic Publisher, 2002.
[14] A. Maedche and S. Staab. Mining Ontologies from Text.
EKAW 2000, pages 189-202, 2000.
[15] C. D. Manning and H. Schütze. Foundations of Statistical
Natural Language Processing. The MIT Press, Cambridge,
Massachusetts, 1999.
[16] H. V. D. Parunak, R. Rohwer, T. C. Belding, and
S. Brueckner. Dynamic decentralized any-time hierarchical
clustering. 29th Annual International ACM SIGIR
Conference on Research & Development on Information
Retrieval, August 2006.
1292 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Organizational Self-Design in Semi-dynamic Environments
Sachin Kamboj
∗
and Keith S. Decker
Department of Computer and Information Sciences
University of Delaware
Newark, DE 19716
{kamboj, decker}@cis.udel.edu
ABSTRACT
Organizations are an important basis for coordination in 
multiagent systems. However, there is no best way to organize and all
ways of organizing are not equally effective. Attempting to 
optimize an organizational structure depends strongly on 
environmental features including problem characteristics, available resources,
and agent capabilities. If the environment is dynamic, the 
environmental conditions or the problem task structure may change over
time. This precludes the use of static, design-time generated, 
organizational structures in such systems. On the other hand, for many
real environments, the problems are not totally unique either: 
certain characteristics and conditions change slowly, if at all, and these
can have an important effect in creating stable organizational 
structures.
Organizational-Self Design (OSD) has been proposed as an 
approach for constructing suitable organizational structures at 
runtime. We extend the existing OSD approach to include 
worthoriented domains, model other resources in addition to only 
processor resources and build in robustness into the organization. We
then evaluate our approach against the contract-net approach and
show that our OSD agents perform better, are more efficient, and
more flexible to changes in the environment.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Multiagent systems
General Terms
Algorithms, Design, Performance, Experimentation
1. INTRODUCTION
In this paper, we are primarily interested in the organizational
design of a multiagent system - the roles enacted by the agents,
∗Primary author is a student
the coordination between the roles and the number and assignment
of roles and resources to the individual agents. The organizational
design is complicated by the fact that there is no best way to 
organize and all ways of organizing are not equally effective [2].
Instead, the optimal organizational structure depends both on the
problem at hand and the environmental conditions under which the
problem needs to be solved. The environmental conditions may
not be known a priori, or may change over time, which would 
preclude the use of a static organizational structure. On the other hand,
all problem instances and environmental conditions are not always
unique, which would render inefficient the use of a new, bespoke
organizational structure for every problem instance.
Organizational Self-Design (OSD) [4, 10] has been proposed
as an approach to designing organizations at run-time in which
the agents are responsible for generating their own organizational
structures. We believe that OSD is especially suited to the above
scenario in which the environment is semi-dynamic as the agents
can adapt to changes in the task structures and environmental 
conditions, while still being able to generate relatively stable 
organizational structures that exploit the common characteristics across
problem instances.
In our approach (as in [10]), we define two operators for OSD
- agent spawning and composition - when an agent becomes
overloaded, it spawns off a new agent to handle part of its task
load/responsibility; when an agent lies idle for an extended period
of time, it may decide to compose with another agent.
We use TÆMS as the underlying representation for our 
problem solving requests. TÆMS [11] (Task Analysis, Environment
Modeling and Simulation) is a computational framework for 
representing and reasoning about complex task environments in which
tasks (problems) are represented using extended hierarchical task
structures [3]. The root node of the task structure represents the
high-level goal that the agent is trying to achieve. The sub-nodes of
a node represent the subtasks and methods that make up the 
highlevel task. The leaf nodes are at the lowest level of abstraction and
represent executable methods - the primitive actions that the agents
can perform. The executable methods, themselves, may have 
multiple outcomes, with different probabilities and different 
characteristics such as quality, cost and duration. TÆMS also allows 
various mechanisms for specifying subtask variations and alternatives,
i.e. each node in TÆMS is labeled with a characteristic 
accumulation function that describes how many or which subgoals or sets of
subgoals need to be achieved in order to achieve a particular 
higherlevel goal. TÆMS has been used to model many different 
problemsolving environments including distributed sensor networks, 
information gathering, hospital scheduling, EMS, and military planning.
[5, 6, 3, 15].
The main contributions of this paper are as follows:
1. We extend existing OSD approaches to use TÆMS as the 
underlying problem representation, which allows us to model
and use OSD for worth-oriented domains. This in turn 
allows us to reason about (1) alternative task and role 
assignments that make different quality/cost tradeoffs and generate
different organizational structures and (2) uncertainties in the
execution of tasks.
2. We model the use of resources other than only processor 
resources.
3. We incorporate robustness into the organizational structures.
2. RELATED WORK
The concept of OSD is not new and has been around since
the work of Corkill and Lesser on the DVMT system[4], even
though the concept was not fully developed by them. More 
recently Dignum et. al.[8] have described OSD in the context of the
reorganization of agent societies and attempt to classify the various
kinds of reorganization possible according to the the reason for 
reorganization, the type of reorganization and who is responsible for
the reorganization decision. According to their scheme, the type of
reorganization done by our agents falls into the category of 
structural changes and the reorganization decision can be described as
shared command.
Our research primarily builds on the work done by Gasser and
Ishida [10], in which they use OSD in the context of a 
production system in order to perform adaptive work allocation and load
balancing. In their approach, they define two organizational 
primitives - composition and decomposition, which are similar to our 
organizational primitives for agent spawning and composition. The
main difference between their work and our work is that we use
TÆMS as the underlying representation for our problems, which
allows, firstly, the representation of a larger, more general class of
problems and, secondly, quantitative reasoning over task structures.
The latter also allows us to incorporate different design-to-criteria
schedulers [16].
Horling and Lesser [9] present a different, top-down approach to
OSD that also uses TÆMS as the underlying representation. 
However, their approach assumes a fixed number of agents with 
designated (and fixed) roles. OSD is used in their work to change the
interaction patterns between the agents and results in the agents 
using different subtasks or different resources to achieve their goals.
We also extend on the work done by Sycara et. al.,[13] on Agent
Cloning, which is another approach to resource allocation and load
balancing. In this approach, the authors present agent cloning as
a possible response to agent overload - if an agent detects that it
is overloaded and that there are spare (unused) resources in the
system, the agent clones itself and gives its clone some part of its
task load. Hence, agent cloning can be thought of as akin to agent
spawning in our approach. However, the two approaches are 
different in that there is no specialization of the agents in the 
formerthe cloned agents are perfect replicas of the original agents and 
fulfill the same roles and responsibilities as the original agents. In our
approach, on the other hand, the spawned agents are specialized on
a subpart of the spawning agent"s task structure, which is no longer
the responsibility of the spawning agent. Hence, our approach also
deals with explicit organization formation and the coordination of
the agents" tasks which are not handled by their approach.
Other approaches to OSD include the work of So and Durfee
[14], who describe a top-down model of OSD in the context of
Cooperative Distributive Problem Solving (CDPS) and Barber and
Martin [1], who describe an adaptive decision making framework
in which agents are able to reorganize decision-making groups by
dynamically changing (1) who makes the decisions for a particular
goal and (2) who must carry out these decisions.The latter work is
primarily concerned with coordination decisions and can be used
to complement our OSD work, which primarily deals with task and
resource allocation.
3. TASK AND RESOURCE MODEL
To ground our discussion of OSD, we now formally describe
our task and resource model. In our model, the primary input to
the multi-agent system (MAS) is an ordered set of problem 
solving requests or task instances, < P1, P2, P3, ..., Pn >, where each
problem solving request, Pi, can be represented using the tuple
< ti, ai, di >. In this scheme, ti is the underlying TÆMS task
structure, ai ∈ N+
is the arrival time and di ∈ N+
is the deadline
of the ith
task instance1
. The MAS has no prior knowledge about
the task ti before the arrival time, ai. In order for the MAS to
accrue quality, the task ti must be completed before the deadline,
di.
Furthermore, every underlying task structure, ti, can be 
represented using the tuple < T, τ, M, Q, E, R, ρ, C >, where:
• T is the set of tasks. The tasks are non-leaf nodes in a
TÆMS task structure and are used to denote goals that the
agents must achieve. Tasks have a characteristic 
accumulation function (see below) and are themselves composed of
other subtasks and/or methods that need to be achieved in
order to achieve the goal represented by that task. Formally,
each task Tj can be represented using the pair (qj, sj), where
qj ∈ Q and sj ⊂ (T ∪ M). For our convenience, we 
define two functions SUBTASKS(Task) : T → P(T ∪ M) and
SUPERTASKS(TÆMS node) : T ∪ M → P(T), that return
the subtasks and supertasks of a TÆMS node respectively2
.
• τ ∈ T, is the root of the task structure, i.e. the highest level
goal that the organization is trying to achieve. The quality
accrued on a problem is equal to the quality of task τ.
• M is the set executable methods, i.e., M =
{m1, m2, ..., mn}, where each method, mk,
is represented using the outcome distribution,
{(o1, p1), (o2, p2), ..., (om, pm)}. In the pair (ol, pl),
ol is an outcome and pl is the probability that executing mk
will result in the outcome ol. Furthermore, each outcome,
ol is represented using the triple (ql, cl, dl), where ql is the
quality distribution, cl is the cost distribution and dl is the
duration distribution of outcome ol. Each discrete 
distribution is itself a set of pairs, {(n1, p1), (n2, p2), ..., (nn, pn)},
where pi ∈ +
is the probability that the outcome will have
a quality/cost/duration of nl ∈ N depending on the type of
distribution and
Pm
i=1 pl = 1.
• Q is the set of quality/characteristic accumulation functions
(CAFs). The CAFs determine how a task group accrues 
quality given the quality accrued by its subtasks/methods. For
our research, we use four CAFs: MIN, MAX, SUM and 
EXACTLY ONE. See [5] for formal definitions.
• E is the set of (non-local) effects. Again, see [5] for formal
definitions.
• R is the set of resources.
• ρ is a mapping from an executable method and resource to
the quantity of that resource needed (by an agent) to 
schedule/execute that method. That is ρ(method, resource) :
M × R → N.
1
N is the set of natural numbers including zero and N+
is the set
of positive natural numbers excluding zero.
2
P is the power set of set, i.e., the set of all subsets of a set
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1229
• C is a mapping from a resource to the cost of that resource,
that is C(resource) : R → N+
We also make the following set of assumptions in our research:
1. The agents in the MAS are drawn from the infinite set A =
{a1, a2, a3, ...}. That is, we do not assume a fixed set of
agents - instead agents are created (spawned) and destroyed
(combined) as needed.
2. All problem solving requests have the same underlying task
structure, i.e. ∃t∀iti = t, where t is the task structure of
the problem that the MAS is trying to solve. We believe that
this assumption holds for many of the practical problems that
we have in mind because TÆMS task structures are 
basically high-level plans for achieving some goal in which the
steps required for achieving the goal-as well as the possible
contingency situations-have been pre-computed offline and
represented in the task structure. Because it represents many
contingencies, alternatives, uncertain characteristics and 
runtime flexible choices, the same underlying task structure
can play out very differently across specific instances.
3. All resources are exclusive, i.e., only one agent may use a
resource at any given time. Furthermore, we assume that
each agent has to own the set of resources that it 
needseven though the resource ownership can change during the
evolution of the organization.
4. All resources are non-consumable.
4. ORGANIZATIONAL SELF DESIGN
4.1 Agent Roles and Relationships
The organizational structure is primarily composed of roles and
the relationships between the roles. One or more agents may enact
a particular role and one or more roles must be enacted by every
agent. The roles may be thought of as the parts played by the agents
enacting the roles in the solution to the problem and reflect the
long-term commitments made by the agents in question to a certain
course of action (that includes task responsibility, authority, and
mechanisms for coordination). The relationships between the roles
are the coordination relationships that exist between the subparts of
a problem.
In our approach, the organizational design is directly contingent
on the task structure and the environmental conditions under which
the problems need to be solved. We define a role as a TÆMS 
subtree rooted at a particular node. Hence, the set (T ∪ M) 
encompasses the space of all possible roles. Note, by definition, a role
may consist of one or more other (sub-) roles as a particular TÆMS
node may itself be made up of one or more subtrees. Hence, we will
use the terms role, task node and task interchangeably.
We, also, differentiate between local and managed (non-local)
roles. Local roles are roles that are the sole responsibility of a 
single agent, that is, the agent concerned is responsible for solving all
the subproblems of the tree rooted at that node. For such roles, the
agent concerned can do one or more subtasks, solely at its 
discretion and without consultation with any other agent. Managed roles,
on the other hand, must be coordinated between two or more agents
as such roles will have two or more descendent local roles that are
the responsibility of two or more separate agents. Any of the 
existing coordination mechanisms (such as GPGP [11]) can be used to
achieve this coordination.
Formally, if the function TYPE(Agent, TÆMS Node) : A×(T ∪
M) → {Local, Managed, Unassigned}, returns the type of the 
responsibility of the agent towards the specified role, then
TYPE(a, r) = Local ⇐⇒
∀ri∈SUBTASKS(r)TYPE(a, ri) = Local
TYPE(a, r) = Managed ⇐⇒
[∃a1∃r1(r1 ∈ SUBTASKS(r)) ∧ (TYPE(a1, r1) = Managed)] ∨
[∃a2∃a3∃r2∃r3(a2 = a3) ∧ (r2 = r3) ∧
(r2 ∈ SUBTASKS(r)) ∧ (r3 ∈ SUBTASKS(r)) ∧
(TYPE(a2, r2) = Local) ∧ (TYPE(a3, r3) = Local)]
4.2 Organization Formation and Adaptation
To form or adapt their organizational structure, the agents use
two organizational primitives: agent spawning and composition.
These two primitives result in a change in the assignment of roles
to the agents. Agent spawning is the generation of a new agent to
handle a subset of the roles of the spawning agent. Agent 
composition, on the other hand, is orthogonal to agent spawning and
involves the merging of two or more agents together - the 
combined agent is responsible for enacting all the roles of the agents
being merged.
In order to participate in the formation and adaption of an 
organization, the agents need to explicitly represent and reason about
the role assignments. Hence, as a part of its organizational 
knowledge, each agent keeps a list of the local roles that it is enacting and
the non-local roles that it is managing. Note that each agent only
has limited organizational knowledge and is individually 
responsible for spawning off or combining with another agent, as needed,
based on its estimate of its performance so far.
To see how the organizational primitives work, we first describe
four rules that can be thought of as the organizational invariants
which will always hold before and after any organizational change:
1. For a local role, all the descendent nodes of that role will be
local.
TYPE(a, r) = Local =⇒
∀ri∈SUBTASKS(r)TYPE(a, ri) = Local
2. Similarly, for a managed (non-local) role, all the ascendent
nodes of that role will be managed.
TYPE(a, r) = Managed =⇒
∀ri∈SUPERTASKS(r)∃ai(ai ∈ A) ∧ (TYPE(ai, ri) = Managed)
3. If two local roles that are enacted by two different agents
share a common ancestor, that ancestor will be a managed
role.
(TYPE(a1, r1) = Local) ∧ (TYPE(a2, r2) = Local)∧
(a1 = a2) ∧ (r1 = r2) =⇒
∀ri∈(SUPERTASKS(r1)∩SUPERTASKS(r2))∃ai(ai ∈ A)∧
(TYPE(ai, ri) = Managed)
4. If all the direct descendants of a role are local and the sole
responsibility of a single agent, that role will be a local role.
∃a∃r∀ri∈SUBTASKS(r)(a ∈ A) ∧ (r ∈ (T ∪ M))∧
(TYPE(a, ri) = Local) =⇒
(TYPE(a, r) = Local)
When a new agent is spawned, the agent doing the spawning will
assign one or more of its local roles to the newly spawned agent
(Algorithm 1). To preserve invariant rules 2 and 3, the spawning
agent will change the type of all the ascendent roles of the nodes
assigned to the newly spawned agent from local to managed. Note
that the spawning agent is only changing its local organizational
knowledge and not the global organizational knowledge. At the
1230 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
same time, the spawning agent is taking on the task of managing
the previously local roles. Similarly, the newly spawned agent will
only know of its just assigned local roles.
When an agent (the composing agent) decides to compose with
another agent (the composed agent), the organizational knowledge
of the composing agent is merged with the organizational 
knowledge of the composed agent. To do this, the composed agent takes
on the roles of all the local and managed tasks of the composing
agent. Care is taken to preserve the organizational invariant rules 1
and 4.
Algorithm 1 SpawnAgent(SpawningAgent) : A → A
1: LocalRoles ← {r ⊆ (T ∪ M) | TYPE(SpawningAgent,
r)= Local}
2: NewAgent ← CREATENEWAGENT()
3: NewAgentRoles ← FINDROLESFORSPAWNEDAGENT
(LocalRoles)
4: for role in NewAgentRoles do
5: TYPE(NewAgent, role) ← Local
6: TYPE(SpawningAgent, role) ← Unassigned
7: PRESERVEORGANIZATIONALINVARIANTS()
8: return NewAgent
Algorithm 2 FINDROLESFORSPAWNEDAGENT 
(SpawningAgentRoles) : (T ∪ M) → (T ∪ M)
1: R ← SpawningAgentRoles
2: selectedRoles ← nil
3: for roleSet in [P(R) − {φ, R}] do
4: if COST(R, roleSet) < COST(R, selectedRoles) then
5: selectedRoles ← roleSet
6: return selectedRoles
Algorithm 3 GETRESOURCECOST(Roles) : (T ∪ M) →
1: M ← (Roles ∩ M)
2: cost ← 0
3: for resource in R do
4: maxResourceUsage ← 0
5: for method in M do
6: if ρ(method, resource) > maxResourceUsage then
7: max ← ρ(method, resource)
8: cost ← cost +
[C(resource) × maxResourceUsage]
9: return cost
4.2.1 Role allocation during spawning
One of the key questions that the agent doing the spawning needs
to answer is - which of its local-roles should it assign to the newly
spawned agent and which of its local roles should it keep to 
itself? The onus of answering this question falls on the 
FINDROLESFORSPAWNEDAGENT() function, shown in Algorithm 2 above. This
function takes the set of local roles that are the responsibility of the
spawning agent and returns a subset of those roles for allocation
to the newly spawned agent. This subset is selected based on the
results of a cost function as is evident from line 4 of the algorithm.
Since the use of different cost functions will result in different 
organizational structures and since we have no a priori reason to believe
that one cost function will out-perform the other, we evaluated the
performance of three different cost functions based on the 
following three different heuristics:
Algorithm 4 GETEXPECTEDDURATION(Roles) : (T ∪ M) → N+
1: M ← (Roles ∩ M)
2: exptDuration ← 0
3: for [outcome =< (q, c, d), outcomeProb >] in M do
4: exptOutcomeDuration ← 0
5: for (n,p) in d do
6: exptOutcomeDuration ← n × p
7: exptDuration ← exptDuration +
[exptOutcomeDuration × outcomeProb]
8: return exptDuration
Allocating top-most roles first: This heuristic always breaks
up at the top-most nodes first. That is, if the nodes of a task 
structure were numbered, starting from the root, in a breadth-first 
fashion, then this heuristic would select the local-role of the spawning
agent that had the lowest number and breakup that node (by 
allocating one of its subtasks to the newly spawned agent). We 
selected this heuristic because (a) it is the simplest to implement, (b)
fastest to run (the role allocation can be done in constant time 
without the need of a search through the task structure) and (c) it makes
sense from a human-organizational perspective as this heuristic 
corresponds to dividing an organization along functional lines.
Minimizing total resources: This heuristic attempts to 
minimize the total cost of the resources needed by the agents in the
organization to execute their roles. If R be the local roles of the
spawning agent and R be the subset of roles being evaluated for
allocation to the newly spawned agent, the cost function for this
heuristic is given by: COST(R, R ) ← GETRESOURCECOST(R −
R )+GETRESOURCECOST(R )
Balancing execution time: This heuristic attempts to allocate
roles in a way that tries to ensure that each agent has an equal
amount of work to do. For each potential role allocation, this
heuristic works by calculating the absolute value of the difference
between the expected duration of its own roles after spawning and
the expected duration of the roles of the newly spawned agent.
If this difference is close to zero, then the both the agents have
roughly the same amount of work to do. Formally, if R be the
local roles of the spawning agent and R be the subset of roles
being evaluated for allocation to the newly spawned agent, then
the cost function for this heuristic is given by: COST(R, R ) ←
|GETEXPECTEDDURATION(R−R )−GETEXPECTEDDURATION(R )|
To evaluate these heuristics, we ran a series of experiments that
tested the performance of the resultant organization on randomly
generated task structures. The results are given in Section 6.
4.3 Reasons for Organizational Change
As organizational change is expensive (requiring clock cycles,
allocation/deallocation of resources, etc.) we want a stable 
organizational structure that is suited to the task and environmental
conditions at hand. Hence, we wish to change the organizational
structure only if the task structure and/or environmental conditions
change. Also to allow temporary changes to the environmental 
conditions to be overlooked, we want the probability of an 
organizational change to be inversely proportional to the time since the last
organizational change. If this time is relatively short, the agents are
still adjusting to the changes in the environment - hence the 
probability of an agent initiating an organizational change should be
high. Similarly, if the time since the last organizational change is
relatively large, we wish to have a low probability of organizational
change.
To allow this variation in probability of organizational change,
we use simulated annealing to determine the probability of 
keepThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1231
ing an existing organizational structure. This probability is 
calculated using the annealing formula: p = e− ΔE
kT where ΔE is the
amount of overload/underload, T is the time since the last 
organizational change and k is a constant. The mechanism of 
computing ΔE is different for agent spawning than for agent composition
and is described below. From this formula, if T is large, p, or the
probability of keeping the existing organizational structure is large.
Note that the value of p is capped at a certain threshold in order to
prevent the organization from being too sluggish in its reaction to
environmental change.
To compute if agent spawning is necessary, we use the annealing
equation with ΔE = 1
α∗Slack
where α is a constant and Slack is
the difference between the total time available for completion of
the outstanding tasks and the sum of the expected time required for
completion of each task on the task queue. Also, if the amount of
Slack is negative, immediate agent spawning will occur without use
of the annealing equation.
To calculate if agent composition is necessary, we again use the
simulated annealing equation. However, in this case, ΔE = β ∗
Idle Time, where β is a constant and Idle Time is the amount
of time for which the agent was idle. If the agent has been sitting
idle for a long period of time, ΔE is large, which implies that p,
the probability of keeping the existing organizational structure, is
low.
5. ORGANIZATION AND ROBUSTNESS
There are two approaches commonly used to achieve robustness
in multiagent systems:
1. the Survivalist Approach [12], which involves replicating 
domain agents in order to allow the replicas to take over should
the original agents fail; and
2. the Citizen Approach [7], which involves the use of special
monitoring agents (called Sentinel Agents) in order to detect
agent failure and dynamically startup new agents in lieu of
the failed ones.
The advantage of the survivalist approach is that recovery is 
relatively fast, since the replicas are pre-existing in the organization
and can take over as soon as a failure is detected. The advantages
of the citizen approach are that it requires fewer resources, little
modification to the existing organizational structure and 
coordination protocol and is simpler to implement.
Both of these approaches can be applied to achieve robustness in
our OSD agents and it is not clear which approach would be better.
Rather a thorough empirical evaluation of both approaches would
be required. In this paper, we present the citizen approach as it has
been shown by [7], to have a better performance than the survivalist
approach in the Contract Net protocol, and leave the presentation
and evaluation of the survivalist approach to a future paper.
To implement the citizen approach, we designed special 
monitoring agents, that periodically poll the domain agents by sending
them are you alive messages that the agents must respond to. If
an agent fails, it will not respond to such messages - the 
monitoring agents can then create a new agent and delegate the 
responsibilities of the dead agent to the new agent.
This delegation of responsibilities is non-trivial as the 
monitoring agents do not have access to the internal state of the domain
agents, which is itself composed of two components - the 
organizational knowledge and the task information. The former consists
of the information about the local and managerial roles of the agent
while the latter is composed of the methods that are being 
scheduled and executed and the tasks that have been delegated to other
agents. This state information can only be deduced by monitoring
and recording the messages being sent and received by the domain
agents. For example, in order to deduce the organizational 
knowledge, the monitoring agents need to keep a track of the spawn and
compose messages sent by the agents in order to trigger the 
spawning and composition operations respectively. The deduction 
process is particularly complicated in the case of the task information
as the monitoring agents do not have access to the private 
schedules of the domain agents. The details are beyond the scope of this
paper.
6. EVALUATION
To evaluate our approach, we ran a series of experiments that
simulated the operation of both the OSD agents and the Contract
Net agents on various task structures with varied arrival rates and
deadlines. At the start of each experiment, a random TÆMS task
structure was generated with a specified depth and branching 
factor. During the course of the experiment, a series of task instances
(problems) arrive at the organization and must be completed by the
agents before their specified deadlines.
To directly compare the OSD approach with the Contract Net 
approach, each experiment was repeated several times - using OSD
agents on the first run and a different number of Contract Net agents
on each subsequent run. We were careful to use the same task 
structure, task arrival times, task deadlines and random numbers for each
of these trials.
We divided the experiments into two groups: experiments in
which the environment was static (fixed task arrival rates and 
deadlines) and experiments in which the environment was dynamic
(varying arrival rates and/or deadlines).
The two graphs in Figure 1, show the average performance of the
OSD organization against the Contract Net organizations with 8,
10, 12 and 14 agents. The results shown are the averages of running
40 experiments. 20 of those experiments had a static environment
with a fixed task arrival time of 15 cycles and a deadline window of
20 cycles. The remaining 20 experiments had a varying task arrival
rate - the task arrival rate was changed from 15 cycles to 30 cycles
and back to 15 cycles after every 20 tasks. In all the experiments,
the task structures were randomly generated with a maximum depth
of 4 and a maximum branching factor of 3. The runtime of all the
experiments was 2500 cycles.
We tested several hypotheses relating to the comparative 
performance of our OSD approach using the Wilcoxon Matched-Pair
Signed-Rank tests. Matched-Pair signifies that we are comparing
the performance of each system on precisely the same randomized
task set within each separate experiment. The tested hypothesis are:
The OSD organization requires fewer agents to complete an
equal or larger number of tasks when compared to the 
Contract Net organization: To test this hypothesis, we tested the
stronger null hypothesis that states that the contract net agents 
complete more tasks. This null hypothesis is rejected for all contract
net organizations with less than 14 agents (static: p < 0.0003; 
dynamic: p < 0.03). For large contract net organizations, the number
of tasks completed is statistically equivalent to the number 
completed by the OSD agents, however the number of agents used by
the OSD organization is smaller: 9.59 agents (in the static case) and
7.38 agents (in the dynamic case) versus 14 contract net agents3
.
Thus the original hypothesis, that OSD requires fewer agents to
3
These values should not be construed as an indication of the 
scalability of our approach. We have tested our approach on 
organizations with more than 300 agents, which is significantly greater than
the number of agents needed for the kind of applications that we
have in mind (i.e. web service choreography, efficient dynamic use
of grid computing, distributed information gathering, etc.).
1232 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Figure 1: Graph comparing the average performance of the
OSD organization with the Contract Net organizations (with
8, 10, 12 and 14 agents). The error bars show the standard
deviations.
complete an equal or larger number of tasks, is upheld.
The OSD organizations achieve an equal or greater average
quality than the Contract Net organizations: The null 
hypothesis is that the Contract Net agents achieve a greater average quality.
We can reject the null hypothesis for contract net organizations with
less than 12 agents (static: p < 0.01; dynamic: p < 0.05). For
larger contract net organizations, the average quality is statistically
equivalent to that achieved by OSD.
The OSD agents have a lower average response time as 
compared to the Contract Net agents: The null hypothesis that OSD
has the same or higher response time is rejected for all contract net
organizations (static: p < 0.0002; dynamic: p < 0.0004).
The OSD agents send less messages than the Contract Net
Agents: The null hypothesis that OSD sends the same or more
messages is rejected for all contract net organizations (p < .0003
in all cases except 8 contract net agents in a static environment
where p < 0.02)
Hence, as demonstrated by the above tests, our agents perform
better than the contract net agents as they complete a larger number
of tasks, achieve a greater quality and also have a lower response
time and communication overhead. These results make intuitive
sense given our goals for the OSD approach. We expected the OSD
organizations to have a faster average response time and to send
less messages because the agents in the OSD organization are not
wasting time and messages sending bid requests and replying to
bids. The quality gained on the tasks is directly dependent on the
Criteria/Heuristic BET TF MR Rand
Number of Agents 572 567 100 139
No-Org-Changes 641 51 5 177
Total-Messages-Sent 586 499 13 11
Resource-Cost 346 418 337 66
Tasks-Completed 427 560 154 166
Average-Quality 367 492 298 339
Average-Response-Time 356 321 370 283
Average-Runtime 543 323 74 116
Average-Turnaround-Time 560 314 74 126
Table 1: The number of times that each heuristic performed
the best or statistically equivalent to the best for each of the
performance criteria. Heuristic Key: BET is Balancing 
Execution Time, TF is Topmost First, MR is Minimizing Resources and
Rand is a random allocation strategy, in which every TÆMS
node has a uniform probability of being selected for allocation.
number of tasks completed, hence the more the number of tasks
completed, the greater average quality. The results of testing the
first hypothesis were slightly more surprising. It appears that due
to the inherent inefficiency of the contract net protocol in bidding
for each and every task instance, a greater number of agents are
needed to complete an equal number of tasks.
Next, we evaluated the performance of the three heuristics for 
allocating tasks. Some preliminary experiments (that are not reported
here due to space constraints) demonstrated the lack of a clear 
winner amongst the three heuristics for most of the performance 
criteria that we evaluated. We suspected this to be the case because 
different heuristics are better for different task structures and 
environmental conditions, and since each experiment starts with a different
random task structure, we couldn"t find one allocation strategy that
always dominated the other for all the performance criteria.
To determine which heuristic performs the best, given a set of
task structures, environmental conditions and performance criteria,
we performed a series of experiments that were controlled using
the following five variables:
• The depth of the task structure was varied from 3 to 5.
• The branching factor was varied from 3 to 5.
• The probability of any given task node having a MIN CAF
was varied from 0.0 to 1.0 in increments of 0.2. The 
probability of any node having a SUM CAF was in turn modified
to ensure that the probabilities add up to 14
.
• The arrival rate: from 10 to 40 cycles in increments of 10.
• The deadline slack: from 5 to 15 in increments of 5.
Each experiment was repeated 20 times, with a new task 
structure being generated each time - these 20 experiments formed an
experimental set. Hence, all the experiments in an experimental set
had the same values for the exogenous variables that were used to
control the experiment. Note that a static environment was used in
each of these experiments, as we wanted to see the performance of
the arrival rate and deadline slack on each of the three heuristics.
Also the results of any experiment in which the OSD organization
consisted of a single agent ware culled from the results. Similarly,
4
Since our preliminary analysis led is to believe that the number
of MAX and EXACTLY ONE CAFs in a task structure have a 
minimal effect on the performance of the allocation strategies being
evaluated, we set the probabilities of the MAX and EXACTLY ONE
CAFs to 0 in order to reduce the combinatorial explosion of the full
factorial experimental design.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1233
experiments in which the generated task structures were 
unsatisfiable (given the deadline constraints), were removed from the final
results. If any experimental set had more than 15 experiments thus
removed, the whole set was ignored for performing the evaluation.
The final evaluation was done on 673 experimental sets.
We tested the potential of these three heuristics on the following
performance criteria:
1. The average number of agents used.
2. The total number of organizational changes.
3. The total messages sent by all the agents.
4. The total resource cost of the organization.
5. The number of tasks completed.
6. The average quality accrued. The average quality is defined
as the total quality accrued during the experimental run 
divided by the sum of the number of tasks completed and the
number of tasks failed.
7. The average response time of the organization. The response
time of a task is defined as the difference between the time
at which any agent in the organization starts working on
the task (the start time) and the time at which the task was
generated (the generation time). Hence, the response time
is equivalent to the wait time. For tasks that are never 
attempted/started, the response time is set at final runtime 
minus the generation time.
8. The average runtime of the tasks attempted by the 
organization. This time is defined as the difference between the time
at which the task completed or failed and the start time. For
tasks that were never stated, this time is set to zero.
9. The turnaround time is defined as the sum of the response
time and runtime of a task.
Except for the number of tasks completed and the average 
quality accrued, lower values for the various performance criteria 
indicate better performance. Again we ran the Wilcoxon Matched-Pair
Signed-Rank tests on the experiments in each of the experimental
sets. The null hypothesis in each case was that there is no 
difference between the pair of heuristics for the performance criteria
under consideration. We were interested in the cases in which we
could reject the null hypothesis with 95% confidence (p < 0.05).
We noted the number of times that a heuristic performed the best
or was in a group that performed statistically better than the rest.
These counts are given in Tables 1 and 2.
The number of experimental sets in which each heuristic 
performed the best or statistically equivalent to the best is shown in
Table 1. The breakup of these numbers into (1) the number of times
that each heuristic performed better than all the other heuristics and
(2) the number of times each heuristic was statistically equivalent
to another group of heuristics, all of which performed the best, is
shown in Table 2. Both of these tables allow us to glean important
information about the performance of the three heuristics. 
Particularly interesting were the following results:
• Whereas Balancing Execution Time (BET) used the 
lowest number of agents in largest number of experimental sets
(572), in most of these cases (337 experimental sets) it was
statistically equivalent to Topmost First (TF). When these
two heuristics didn"t perform equally, there was an almost
even split between the number of experimental sets in which
one outperformed the other.
We believe this was the case because BET always bifurcates
the agents into two agents that have a more or less equal task
load. This often results in organizations that have an even
Figure 2: Graph demonstrating the robustness of the citizen
approach. The baseline shows the number of tasks completed
in the absence of any failure.
number of agents - none of which are small5
enough to
combine into a larger agent. With TF, on the other hand, a
large agent can successively spawn off smaller agents until it
and the spawned agents are small enough to complete their
tasks before the deadlines - this often results in 
organizations with an odd number of agents that is less than those
used by BET.
• As expected, BET achieved the lowest number of 
organizational changes in the largest number of experimental sets. In
fact, it was over ten times as good as its second best 
competitor (TF). This shows that if the agents are conscientious
in their initial task allocation, there is a lesser need for 
organizational change later on, especially for static environments.
• A particularly interesting, yet easily explainable, result was
that of the average response time. We found that the 
Minimizing Resources (MR) heuristic performed the best when it
came to minimizing the average response time! This can be
explained by the fact the MR heuristic is extremely greedy
and prefers to spawn off small agents that have a tiny 
resource footprint (so as to minimize the total increase in the
resource cost to the organization at the time of spawning).
Whereas most of these small agents might compose with
other agents over time, the presence of a single small agent
is sufficient to reduce the response time.
In fact the MR heuristic is not the most effective heuristic
when it comes to minimizing the resource-cost of the 
organization - in fact, it only outperforms a random task/resource
allocation. We believe this is in part due to the greedy 
nature of this heuristic and in part because of the fact that all
spawning and composition operations only use local 
information. We believe that using some non-local information
about the resource allocation might help in making better 
decisions, something that we plan to look at in the future.
Finally we evaluated the performance of the citizens approach to
robustness as applied to our OSD mechanism (Figure 2). As 
expected, as the probability of failure increases, the number of agents
failing during a run also increases. This results in a slight decrease
in the number of tasks completed, which can be explained by the
fact that whenever an agent fails, its looses whatever work it was
doing at the time. The newly created agent that fills in for the failed
5
For this discussion small agents are agents that have a low 
expected duration for their local roles (as calculated by Algorithm 4).
1234 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Criteria/Heuristic BET TF MR Rand BET+TF BET+Rand MR+Rand TF+MR BET+TF+MR All
Number of Agents 94 88 3 7 337 2 0 0 12 85
No-Org-Changes 480 0 0 29 16 113 0 0 0 5
Total-Messages-Sent 170 85 0 2 399 1 0 0 7 5
Resource-Cost 26 100 170 42 167 0 7 6 128 15
Tasks-Completed 77 197 4 28 184 1 3 9 36 99
Average-Quality 38 147 26 104 76 0 11 11 34 208
Average-Response-Time 104 74 162 43 31 20 16 8 7 169
Average-Runtime 322 110 0 12 121 13 1 1 1 69
Average-Turnaround-Time 318 94 1 11 125 26 1 0 7 64
Table 2: Table showing the number of times that each individual heuristic performed the best and the number of times that a certain
group of statistically equivalent heuristics performed the best. Only the more interesting heuristic groupings are shown. All shows
the number of experimental sets in which there was no statistical difference between the three heuristics and a random allocation
strategy
one must redo the work, thus wasting precious time which might
not be available close to a deadline.
As a part of our future research, we wish to, firstly, evaluate the
survivalist approach to robustness. The survivalist approach might
actually be better than the citizen approach for higher 
probabilities of agent failure, as the replicated agents may be processing the
task structures in parallel and can take over the moment the 
original agents fail - thus saving time around tight deadlines. Also,
we strongly believe that the optimal organizational structure may
vary, depending on the probability of failure and the desired level
of robustness. For example, one way of achieving a higher level
of robustness in the survivalist approach, given a large numbers of
agent failures, would be to relax the task deadlines. However, such
a relaxation would result in the system using fewer agents in order
to conserve resources, which in turn would have a detrimental 
effect on the robustness. Therefore, towards this end, we have begun
exploring the robustness properties of task structures and the ways
in which the organizational design can be modified to take such
properties into account.
7. CONCLUSION
In this paper, we have presented a run-time approach to 
organization in which the agents use Organizational Self-Design to come up
with a suitable organizational structure. We have also evaluated the
performance of the organizations generated by the agents following
our approach with the bespoke organization formation that takes
place in the Contract Net protocol and have demonstrated that our
approach is better than the Contract Net approach as evident by the
larger number of tasks completed, larger quality achieved and lower
response time. Finally, we tested the performance of three different
resource allocation heuristics on various performance metrics and
also evaluated the robustness of our approach.
8. REFERENCES
[1] K. S. Barber and C. E. Martin. Dynamic reorganization of
decision-making groups. In AGENTS "01, pages 513-520,
New York, NY, USA, 2001.
[2] K. M. Carley and L. Gasser. Computational organization
theory. In G. Wiess, editor, Multiagent Systems: A Modern
Approach to Distributed Artificial Intelligence, pages
299-330, MIT Press, 1999.
[3] W. Chen and K. S. Decker. The analysis of coordination in
an information system application - emergency medical
services. In Lecture Notes in Computer Science (LNCS),
number 3508, pages 36-51. Springer-Verlag, May 2005.
[4] D. Corkill and V. Lesser. The use of meta-level control for
coordination in a distributed problem solving network.
Proceedings of the Eighth International Joint Conference on
Artificial Intelligence, pages 748-756, August 1983.
[5] K. S. Decker. Environment centered analysis and design of
coordination mechanisms. Ph.D. Thesis, Dept. of Comp.
Science, University of Massachusetts, Amherst, May 1995.
[6] K. S. Decker and J. Li. Coordinating mutually exclusive
resources using GPGP. Autonomous Agents and Multi-Agent
Systems, 3(2):133-157, 2000.
[7] C. Dellarocas and M. Klein. An experimental evaluation of
domain-independent fault handling services in open
multi-agent systems. Proceedings of the International
Conference on Multi-Agent Systems (ICMAS-2000), July
2000.
[8] V. Dignum, F. Dignum, and L. Sonenberg. Towards Dynamic
Reorganization of Agent Societies. In Proceedings of CEAS:
Workshop on Coordination in Emergent Agent Societies at
ECAI, pages 22-27, Valencia, Spain, September 2004.
[9] B. Horling, B. Benyo, and V. Lesser. Using self-diagnosis to
adapt organizational structures. In AGENTS "01, pages
529-536, New York, NY, USA, 2001. ACM Press.
[10] T. Ishida, L. Gasser, and M. Yokoo. Organization self-design
of distributed production systems. IEEE Transactions on
Knowledge and Data Engineering, 4(2):123-134, 1992.
[11] V. R. Lesser et. al. Evolution of the gpgp/tæms
domain-independent coordination framework. Autonomous
Agents and Multi-Agent Systems, 9(1-2):87-143, 2004.
[12] O. Marin, P. Sens, J. Briot, and Z. Guessoum. Towards
adaptive fault tolerance for distributed multi-agent systems.
Proceedings of ERSADS 2001, May 2001.
[13] O. Shehory, K. Sycara, et. al. Agent cloning: an approach to
agent mobility and resource allocation. IEEE
Communications Magazine, 36(7):58-67, 1998.
[14] Y. So and E. Durfee. An organizational self-design model for
organizational change. In AAAI-93 Workshop on AI and
Theories of Groups and Organizations, pages 8-15,
Washington, D.C., July 1993.
[15] T. Wagner. Coordination decision support assistants
(coordinators). Technical Report 04-29, BAA, 2004.
[16] T. Wagner and V. Lesser. Design-to-criteria scheduling:
Real-time agent control. Proc. of AAAI 2000 Spring
Symposium on Real-Time Autonomous Systems, 89-96.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1235
Negotiation by Abduction and Relaxation
Chiaki Sakama
Dept. Computer and Communication Sciences
Wakayama University
Sakaedani, Wakayama 640 8510, Japan
sakama@sys.wakayama-u.ac.jp
Katsumi Inoue
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku
Tokyo 101 8430, Japan
ki@nii.ac.jp
ABSTRACT
This paper studies a logical framework for automated 
negotiation between two agents. We suppose an agent who has
a knowledge base represented by a logic program. Then,
we introduce methods of constructing counter-proposals in
response to proposals made by an agent. To this end, we
combine the techniques of extended abduction in artificial
intelligence and relaxation in cooperative query answering
for databases. These techniques are respectively used for
producing conditional proposals and neighborhood proposals
in the process of negotiation. We provide a negotiation 
protocol based on the exchange of these proposals and develop
procedures for computing new proposals.
Categories and Subject Descriptors
F.4.1 [Mathematical Logic]: Logic and constraint 
programming;; I.2.11 [Distributed Artificial Intelligence]:
Multiagent systems
General Terms
Theory
1. INTRODUCTION
Automated negotiation has been received increasing 
attention in multi-agent systems, and a number of frameworks
have been proposed in different contexts ([1, 2, 3, 5, 10, 11,
13, 14], for instance). Negotiation usually proceeds in a 
series of rounds and each agent makes a proposal at every
round. An agent that received a proposal responds in two
ways. One is a critique which is a remark as to whether
or not (parts of) the proposal is accepted. The other is a
counter-proposal which is an alternative proposal made in
response to a previous proposal [13].
To see these proposals in one-to-one negotiation, suppose
the following negotiation dialogue between a buyer agent B
and a seller agent S. (Bi (or Si) represents an utterance of
B (or S) in the i-th round.)
B1: I want to buy a personal computer of the brand b1,
with the specification of CPU:1GHz, Memory:512MB,
HDD: 80GB, and a DVD-RW driver. I want to get it
at the price under 1200 USD.
S1: We can provide a PC with the requested specification
if you pay for it by cash. In this case, however, service
points are not added for this special discount.
B2: I cannot pay it by cash.
S2: In a normal price, the requested PC costs 1300 USD.
B3: I cannot accept the price. My budget is under 1200
USD.
S3: We can provide another computer with the requested
specification, except that it is made by the brand b2.
The price is exactly 1200 USD.
B4: I do not want a PC of the brand b2. Instead, I can
downgrade a driver from DVD-RW to CD-RW in my
initial proposal.
S4: Ok, I accept your offer.
In this dialogue, in response to the opening proposal B1,
the counter-proposal S1 is returned. In the rest of the 
dialogue, B2, B3, S4 are critiques, while S2, S3, B4 are 
counterproposals.
Critiques are produced by evaluating a proposal in a 
knowledge base of an agent. In contrast, making counter-proposals
involves generating an alternative proposal which is more
favorable to the responding agent than the original one.
It is known that there are two ways of producing 
counterproposals: extending the initial proposal or amending part
of the initial proposal. According to [13], the first type 
appears in the dialogue: A: I propose that you provide me
with service X. B: I propose that I provide you with 
service X if you provide me with service Z. The second type
is in the dialogue: A: I propose that I provide you with
service Y if you provide me with service X. B: I propose
that I provide you with service X if you provide me with 
service Z. A negotiation proceeds by iterating such 
give-andtake dialogues until it reaches an agreement/disagreement.
In those dialogues, agents generate (counter-)proposals by
reasoning on their own goals or objectives. The objective
of the agent A in the above dialogues is to obtain service
X. The agent B proposes conditions to provide the 
service. In the process of negotiation, however, it may happen
that agents are obliged to weaken or change their initial
goals to reach a negotiated compromise. In the dialogue of
1022
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
a buyer agent and a seller agent presented above, a buyer
agent changes its initial goal by downgrading a driver from
DVD-RW to CD-RW. Such behavior is usually represented
as specific meta-knowledge of an agent or specified as 
negotiation protocols in particular problems. Currently, there is
no computational logic for automated negotiation which has
general inference rules for producing (counter-)proposals.
The purpose of this paper is to mechanize a process of
building (counter-)proposals in one-to-one negotiation 
dialogues. We suppose an agent who has a knowledge base
represented by a logic program. We then introduce 
methods for generating three different types of proposals. First,
we use the technique of extended abduction in artificial 
intelligence [8, 15] to construct a conditional proposal as an
extension of the original one. Second, we use the technique
of relaxation in cooperative query answering for databases
[4, 6] to construct a neighborhood proposal as an amendment
of the original one. Third, combining extended abduction
and relaxation, conditional neighborhood proposals are 
constructed as amended extensions of the original proposal. We
develop a negotiation protocol between two agents based on
the exchange of these counter-proposals and critiques. We
also provide procedures for computing proposals in logic 
programming.
This paper is organized as follows. Section 2 introduces
a logical framework used in this paper. Section 3 presents
methods for constructing proposals, and provides a 
negotiation protocol. Section 4 provides methods for computing
proposals in logic programming. Section 5 discusses related
works, and Section 6 concludes the paper.
2. PRELIMINARIES
Logic programs considered in this paper are extended 
disjunctive programs (EDP) [7]. An EDP (or simply a program)
is a set of rules of the form:
L1 ; · · · ; Ll ← Ll+1 , . . . , Lm, not Lm+1 , . . . , not Ln
(n ≥ m ≥ l ≥ 0) where each Li is a positive/negative 
literal, i.e., A or ¬A for an atom A, and not is negation as
failure (NAF). not L is called an NAF-literal. The symbol
; represents disjunction. The left-hand side of the rule
is the head, and the right-hand side is the body. For each
rule r of the above form, head(r), body+
(r) and body−
(r)
denote the sets of literals {L1, . . . , Ll}, {Ll+1, . . . , Lm}, and
{Lm+1, . . . , Ln}, respectively. Also, not body−
(r) denotes
the set of NAF-literals {not Lm+1, . . . , not Ln}. A 
disjunction of literals and a conjunction of (NAF-)literals in a rule
are identified with its corresponding sets of literals. A rule
r is often written as head(r) ← body+
(r), not body−
(r) or
head(r) ← body(r) where body(r) = body+
(r)∪not body−
(r).
A rule r is disjunctive if head(r) contains more than one 
literal. A rule r is an integrity constraint if head(r) = ∅; and
r is a fact if body(r) = ∅. A program is NAF-free if no
rule contains NAF-literals. Two rules/literals are identified
with respect to variable renaming. A substitution is a 
mapping from variables to terms θ = {x1/t1, . . . , xn/tn}, where
x1, . . . , xn are distinct variables and each ti is a term 
distinct from xi. Given a conjunction G of (NAF-)literals, Gθ
denotes the conjunction obtained by applying θ to G. A
program, rule, or literal is ground if it contains no variable.
A program P with variables is a shorthand of its ground
instantiation Ground(P), the set of ground rules obtained
from P by substituting variables in P by elements of its
Herbrand universe in every possible way.
The semantics of an EDP is defined by the answer set
semantics [7]. Let Lit be the set of all ground literals in
the language of a program. Suppose a program P and a
set of literals S(⊆ Lit). Then, the reduct P S
is the 
program which contains the ground rule head(r) ← body+
(r)
iff there is a rule r in Ground(P) such that body−
(r)∩S = ∅.
Given an NAF-free EDP P, Cn(P) denotes the smallest set
of ground literals which is (i) closed under P, i.e., for 
every ground rule r in Ground(P), body(r) ⊆ Cn(P) implies
head(r) ∩ Cn(P) = ∅; and (ii) logically closed, i.e., it is 
either consistent or equal to Lit. Given an EDP P and a set
S of literals, S is an answer set of P if S = Cn(P S
). A
program has none, one, or multiple answer sets in general.
An answer set is consistent if it is not Lit. A program P is
consistent if it has a consistent answer set; otherwise, P is
inconsistent.
Abductive logic programming [9] introduces a mechanism
of hypothetical reasoning to logic programming. An 
abductive framework used in this paper is the extended 
abduction introduced by Inoue and Sakama [8, 15]. An abductive
program is a pair P, H where P is an EDP and H is
a set of literals called abducibles. When a literal L ∈ H
contains variables, any instance of L is also an abducible.
An abductive program P, H is consistent if P is 
consistent. Throughout the paper, abductive programs are 
assumed to be consistent unless stated otherwise. Let G =
L1, . . . , Lm, not Lm+1, . . . , not Ln be a conjunction, where
all variables in G are existentially quantified at the front and
range-restricted, i.e., every variable in Lm+1, . . . , Ln appears
in L1, . . . , Lm. A set S of ground literals satisfies the 
conjunction G if { L1θ, . . . , Lmθ } ⊆ S and { Lm+1θ, . . . , Lnθ }∩
S = ∅ for some ground instance Gθ with a substitution θ.
Let P, H be an abductive program and G a conjunction
as above. A pair (E, F) is an explanation of an observation
G in P, H if1
1. (P \ F) ∪ E has an answer set which satisfies G,
2. (P \ F) ∪ E is consistent,
3. E and F are sets of ground literals such that E ⊆ H\P
and F ⊆ H ∩ P.
When (P \ F) ∪ E has an answer set S satisfying the above
three conditions, S is called a belief set of an abductive 
program P, H satisfying G (with respect to (E, F)). Note
that if P has a consistent answer set S satisfying G, S
is also a belief set of P, H satisfying G with respect to
(E, F) = (∅, ∅). Extended abduction introduces/removes
hypotheses to/from a program to explain an observation.
Note that normal abduction (as in [9]) considers only 
introducing hypotheses to explain an observation. An 
explanation (E, F) of an observation G is called minimal if for
any explanation (E , F ) of G, E ⊆ E and F ⊆ F imply
E = E and F = F.
Example 2.1. Consider the abductive program P, H :
P : flies(x) ← bird(x), not ab(x) ,
ab(x) ← broken-wing(x) ,
bird(tweety) ← , bird(opus) ← ,
broken-wing(tweety) ← .
H : broken-wing(x) .
The observation G = flies(tweety) has the minimal 
explanation (E, F) = (∅, {broken-wing(tweety)}).
1
This defines credulous explanations [15]. Skeptical 
explanations are used in [8].
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1023
3. NEGOTIATION
3.1 Conditional Proposals by Abduction
We suppose an agent who has a knowledge base 
represented by an abductive program P, H . A program P
consists of two types of knowledge, belief B and desire D,
where B represents objective knowledge of an agent, while
D represents subjective knowledge in general. We define
P = B ∪ D, but do not distinguish B and D if such 
distinction is not important in the context. In contrast, abducibles
H are used for representing permissible conditions to make
a compromise in the process of negotiation.
Definition 3.1. A proposal G is a conjunction of literals
and NAF-literals:
L1, . . . , Lm, not Lm+1, . . . , not Ln
where every variable in G is existentially quantified at the
front and range-restricted. In particular, G is called a 
critique if G = accept or G = reject where accept and reject
are the reserved propositions. A counter-proposal is a 
proposal made in response to a proposal.
Definition 3.2. A proposal G is accepted in an 
abductive program P, H if P has an answer set satisfying G.
When a proposal is not accepted, abduction is used for
seeking conditions to make it acceptable.
Definition 3.3. Let P, H be an abductive program
and G a proposal. If (E, F) is a minimal explanation of
Gθ for some substitution θ in P, H , the conjunction G :
Gθ, E, not F
is called a conditional proposal (for G), where E, not F
represents the conjunction: A1, . . . , Ak, not Ak+1, . . . , not Al
for E = {A1, . . . , Ak} and F = { Ak+1, . . . , Al }.
Proposition 3.1. Let P, H be an abductive program
and G a proposal. If G is a conditional proposal, there is a
belief set S of P, H satisfying G .
Proof. When G = Gθ, E, not F, (P \ F) ∪ E has a
consistent answer set S satisfying Gθ and E ∩ F = ∅. In
this case, S satisfies Gθ, E, not F.
A conditional proposal G provides a minimal requirement
for accepting the proposal G. If Gθ has multiple minimal 
explanations, several conditional proposals exist accordingly.
When (E, F) = (∅, ∅), a conditional proposal is used as a
new proposal made in response to the proposal G.
Example 3.1. An agent seeks a position of a research
assistant at the computer department of a university with
the condition that the salary is at least 50,000 USD per year.
The agent makes his/her request as the proposal:2
G = assist(compt dept), salary(x), x ≥ 50, 000.
The university has the abductive program P, H :
P : salary(40, 000) ← assist(compt dept), not has PhD,
salary(60, 000) ← assist(compt dept), has PhD,
salary(50, 000) ← assist(math dept),
salary(55, 000) ← system admin(compt dept),
2
For notational convenience, we often include mathematical
(in)equations in proposals/programs. They are written by
literals, for instance, x ≥ y by geq(x, y) with a suitable
definition of the predicate geq.
employee(x) ← assist(x),
employee(x) ← system admin(x),
assist(compt dept); assist(math dept)
; system admin(compt dept) ←,
H : has PhD,
where available positions are represented by disjunction. 
According to P, the base salary of a research assistant at the
computer department is 40,000 USD, but if he/she has PhD,
it is 60,000 USD. In this case, (E, F) = ({has PhD}, ∅) 
becomes the minimal explanation of Gθ = assist(compt dept),
salary(60, 000) with θ = { x/60, 000 }. Then, the 
conditional proposal made by the university becomes
assist(compt dept), salary(60, 000), has PhD .
3.2 Neighborhood Proposals by Relaxation
When a proposal is unacceptable, an agent tries to 
construct a new counter-proposal by weakening constraints in
the initial proposal. We use techniques of relaxation for
this purpose. Relaxation is used as a technique of 
cooperative query answering in databases [4, 6]. When an original
query fails in a database, relaxation expands the scope of
the query by relaxing the constraints in the query. This 
allows the database to return neighborhood answers which
are related to the original query. We use the technique for
producing proposals in the process of negotiation.
Definition 3.4. Let P, H be an abductive program
and G a proposal. Then, G is relaxed to G in the following
three ways:
Anti-instantiation: Construct G such that G θ = G for
some substitution θ.
Dropping conditions: Construct G such that G ⊂ G.
Goal replacement: If G is a conjunction G1, G2, where
G1 and G2 are conjunctions, and there is a rule L ←
G1 in P such that G1θ = G1 for some substitution θ,
then build G as Lθ, G2. Here, Lθ is called a replaced
literal.
In each case, every variable in G is existentially quantified
at the front and range-restricted.
Anti-instantiation replaces constants (or terms) with fresh
variables. Dropping conditions eliminates some conditions
in a proposal. Goal replacement replaces the condition G1
in G with a literal Lθ in the presence of a rule L ← G1 in P
under the condition G1θ = G1. All these operations 
generalize proposals in different ways. Each G obtained by these
operations is called a relaxation of G. It is worth noting
that these operations are also used in the context of 
inductive generalization [12]. The relaxed proposal can produce
new offers which are neighbor to the original proposal.
Definition 3.5. Let P, H be an abductive program
and G a proposal.
1. Let G be a proposal obtained by anti-instantiation. If
P has an answer set S which satisfies G θ for some 
substitution θ and G θ = G, G θ is called a neighborhood
proposal by anti-instantiation.
2. Let G be a proposal obtained by dropping conditions.
If P has an answer set S which satisfies G θ for some
substitution θ, G θ is called a neighborhood proposal by
dropping conditions.
1024 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
3. Let G be a proposal obtained by goal replacement.
For a replaced literal L ∈ G and a rule H ← B in
P such that L = Hσ and (G \ {L}) ∪ Bσ = G for
some substitution σ, put G = (G \ {L}) ∪ Bσ. If
P has an answer set S which satisfies G θ for some
substitution θ, G θ is called a neighborhood proposal
by goal replacement.
Example 3.2. (cont. Example 3.1) Given the proposal
G = assist(compt dept), salary(x), x ≥ 50, 000,
• G1 = assist(w), salary(x), x ≥ 50, 000 is produced by
substituting compt dept with a variable w. As
G1θ1 = assist(math dept), salary(50, 000)
with θ1 = { w/math dept } is satisfied by an answer
set of P, G1θ1 becomes a neighborhood proposal by
anti-instantiation.
• G2 = assist(compt dept), salary(x) is produced by
dropping the salary condition x ≥ 50, 000. As
G2θ2 = assist(compt dept), salary(40, 000)
with θ2 = { x/40, 000 } is satisfied by an answer set of
P, G2θ2 becomes a neighborhood proposal by 
dropping conditions.
• G3 = employee(compt dept), salary(x), x ≥ 50, 000 is
produced by replacing assist(compt dept) with
employee(compt dept) using the rule employee(x) ←
assist(x) in P. By G3 and the rule employee(x) ←
system admin(x) in P, G3 = sys admin(compt dept),
salary(x), x ≥ 50, 000 is produced. As
G3 θ3 = sys admin(compt dept), salary(55, 000)
with θ3 = { x/55, 000 } is satisfied by an answer set
of P, G3 θ3 becomes a neighborhood proposal by goal
replacement.
Finally, extended abduction and relaxation are combined to
produce conditional neighborhood proposals.
Definition 3.6. Let P, H be an abductive program
and G a proposal.
1. Let G be a proposal obtained by either anti-instantiation
or dropping conditions. If (E, F) is a minimal 
explanation of G θ(= G) for some substitution θ, the 
conjunction G θ, E, not F is called a conditional neighborhood
proposal by anti-instantiation/dropping conditions.
2. Let G be a proposal obtained by goal replacement.
Suppose G as in Definition 3.5(3). If (E, F) is a
minimal explanation of G θ for some substitution θ,
the conjunction G θ, E, not F is called a conditional
neighborhood proposal by goal replacement.
A conditional neighborhood proposal reduces to a 
neighborhood proposal when (E, F) = (∅, ∅).
3.3 Negotiation Protocol
A negotiation protocol defines how to exchange proposals
in the process of negotiation. This section presents a 
negotiation protocol in our framework. We suppose one-to-one
negotiation between two agents who have a common 
ontology and the same language for successful communication.
Definition 3.7. A proposal L1, ..., Lm, not Lm+1, ..., not Ln
violates an integrity constraint ← body+
(r), not body−
(r) if
for any substitution θ, there is a substitution σ such that
body+
(r)σ ⊆ { L1θ, . . . , Lmθ }, body−
(r)σ∩{ L1θ, . . . , Lmθ } =
∅, and body−
(r)σ ⊆ { Lm+1θ, . . . , Lnθ }.
Integrity constraints are conditions which an agent should
satisfy, so that they are used to explain why an agent does
not accept a proposal.
A negotiation proceeds in a series of rounds. Each i-th
round (i ≥ 1) consists of a proposal Gi
1 made by one agent
Ag1 and another proposal Gi
2 made by the other agent Ag2.
Definition 3.8. Let P1, H1 be an abductive program
of an agent Ag1 and Gi
2 a proposal made by Ag2 at the i-th
round. A critique set of Ag1 (at the i-th round) is a set
CSi
1(P1, Gj
2) = CSi−1
1 (P1, Gj−1
2 ) ∪ { r | r is an integrity
constraint in P1 and Gj
2 violates r }
where j = i − 1 or i, and CS0
1 (P1, G0
2) = CS1
1 (P1, G0
2) = ∅.
A critique set of an agent Ag1 accumulates integrity 
constraints which are violated by proposals made by another
agent Ag2. CSi
2(P2, Gj
1) is defined in the same manner.
Definition 3.9. Let Pk, Hk be an abductive program
of an agent Agk and Gj
a proposal, which is not a critique,
made by any agent at the j(≤ i)-th round. A negotiation set
of Agk (at the i-th round) is a triple NSi
k = (Si
c, Si
n, Si
cn),
where Si
c is the set of conditional proposals, Si
n is the set
of neighborhood proposals, and Si
cn is the set of conditional
neighborhood proposals, produced by Gj
and Pk, Hk .
A negotiation set represents the space of possible proposals
made by an agent. Si
x (x ∈ {c, n, cn}) accumulates proposals
produced by Gj
(1 ≤ j ≤ i) according to Definitions 3.3, 3.5,
and 3.6. Note that an agent can construct counter-proposals
by modifying its own previous proposals or another agent"s
proposals. An agent Agk accumulates proposals that are
made by Agk but are rejected by another agent, in the failed
proposal set FP i
k (at the i-th round), where FP 0
k = ∅.
Suppose two agents Ag1 and Ag2 who have abductive 
programs P1, H1 and P2, H2 , respectively. Given a 
proposal G1
1 which is satisfied by an answer set of P1, a 
negotiation starts. In response to the proposal Gi
1 made by Ag1
at the i-th round, Ag2 behaves as follows.
1. If Gi
1 = accept, an agreement is reached and 
negotiation ends in success.
2. Else if Gi
1 = reject, put FP i
2 = FPi−1
2 ∪{Gi−1
2 } where
{G0
2} = ∅. Proceed to the step 4(b).
3. Else if P2 has an answer set satisfying Gi
1, Ag2 returns
Gi
2 = accept to Ag1. Negotiation ends in success.
4. Otherwise, Ag2 behaves as follows. Put FP i
2 = FPi−1
2 .
(a) If Gi
1 violates an integrity constraint in P2, return
the critique Gi
2 = reject to Ag1, together with the
critique set CSi
2(P2, Gi
1).
(b) Otherwise, construct NSi
2 as follows.
(i) Produce Si
c. Let μ(Si
c) = { p | p ∈ Si
c \ FPi
2 and
p satisfies the constraints in CSi
1(P1, Gi−1
2 )}.
If μ(Si
c) = ∅, select one from μ(Si
c) and propose
it as Gi
2 to Ag1; otherwise, go to (ii).
(ii) Produce Si
n. If μ(Si
n) = ∅, select one from μ(Si
n)
and propose it as Gi
2 to Ag1; otherwise, go to (iii).
(iii) Produce Si
cn. If μ(Si
cn) = ∅, select one from
μ(Si
cn) and propose it as Gi
2 to Ag1; otherwise,
negotiation ends in failure. This means that Ag2
can make no counter-proposal or every 
counterproposal made by Ag2 is rejected by Ag1.
In the step 4(a), Ag2 rejects the proposal Gi
1 and returns
the reason of rejection as a critique set. This helps for Ag1
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1025
in preparing a next counter-proposal. In the step 4(b), Ag2
constructs a new proposal. In its construction, Ag2 should
take care of the critique set CSi
1(P1, Gi−1
2 ), which 
represents integrity constraints, if any, accumulated in previous
rounds, that Ag1 must satisfy. Also, FP i
2 is used for 
removing proposals which have been rejected. Construction of
Si
x (x ∈ {c, n, cn}) in NSi
2 is incrementally done by adding
new counter-proposals produced by Gi
1 or Gi−1
2 to Si−1
x . For
instance, Si
n in NSi
2 is computed as
Si
n = Si−1
n ∪{ p | p is a neighborhood proposal made by Gi
1 }
∪ { p | p is a neighborhood proposal made by Gi−1
2 },
where S0
n = ∅. That is, Si
n is constructed from Si−1
n by
adding new proposals which are obtained by modifying the
proposal Gi
1 made by Ag1 at the i-th round or modifying
the proposal Gi−1
2 made by Ag2 at the (i − 1)-th round. Si
c
and Si
cn are obtained as well.
In the above protocol, an agent produces Si
c at first, 
secondly Si
n, and finally Si
cn. This strategy seeks conditions
which satisfy the given proposal, prior to neighborhood 
proposals which change the original one. Another strategy,
which prefers neighborhood proposals to conditional ones,
is also considered. Conditional neighborhood proposals are
to be considered in the last place, since they differ from the
original one to the maximal extent. The above protocol 
produces the candidate proposals in Si
x for each x ∈ {c, n, cn}
at once. We can consider a variant of the protocol in which
each proposal in Si
x is constructed one by one (see 
Example 3.3).
The above protocol is repeatedly applied to each one of
the two negotiating agents until a negotiation ends in 
success/failure. Formally, the above negotiation protocol has
the following properties.
Theorem 3.2. Let Ag1 and Ag2 be two agents having 
abductive programs P1, H1 and P2, H2 , respectively.
1. If P1, H1 and P2, H2 are function-free (i.e., both
Pi and Hi contain no function symbol), any 
negotiation will terminate.
2. If a negotiation terminates with agreement on a 
proposal G, both P1, H1 and P2, H2 have belief sets
satisfying G.
Proof. 1. When an abductive program is function-free,
abducibles and negotiation sets are both finite. Moreover, if
a proposal is once rejected, it is not proposed again by the
function μ. Thus, negotiation will terminate in finite steps.
2. When a proposal G is made by Ag1, P1, H1 has a
belief set satisfying G. If the agent Ag2 accepts the proposal
G, it is satisfied by an answer set of P2 which is also a belief
set of P2, H2 .
Example 3.3. Suppose a buying-selling situation in the
introduction. A seller agent has the abductive program
Ps, Hs in which Ps consists of belief Bs and desire Ds:
Bs : pc(b1, 1G, 512M, 80G) ; pc(b2, 1G, 512M, 80G) ←,(1)
dvd-rw ; cd-rw ←, (2)
Ds : normal price(1300) ←
pc(b1, 1G, 512M, 80G), dvd-rw, (3)
normal price(1200) ←
pc(b1, 1G, 512M, 80G), cd-rw, (4)
normal price(1200) ←
pc(b2, 1G, 512M, 80G), dvd-rw, (5)
price(x) ← normal price(x), add point, (6)
price(x ∗ 0.9) ←
normal price(x), pay cash, not add point,(7)
add point ←, (8)
Hs : add point, pay cash.
Here, (1) and (2) represent selection of products. The atom
pc(b1, 1G, 512M, 80G) represents that the seller agent has
a PC of the brand b1 such that CPU is 1GHz, memory is
512MB, and HDD is 80GB. Prices of products are 
represented as desire of the seller. The rules (3) - (5) are normal
prices of products. A normal price is a selling price on the
condition that service points are added (6). On the other
hand, a discount price is applied if the paying method is cash
and no service point is added (7). The fact (8) represents
the addition of service points. This service would be 
withdrawn in case of discount prices, so add point is specified as
an abducible.
A buyer agent has the abductive program Pb, Hb in
which Pb consists of belief Bb and desire Db:
Bb : drive ← dvd-rw, (9)
drive ← cd-rw, (10)
price(x) ←, (11)
Db : pc(b1, 1G, 512M, 80G) ←, (12)
dvd-rw ←, (13)
cd-rw ← not dvd-rw, (14)
← pay cash, (15)
← price(x), x > 1200, (16)
Hb : dvd-rw.
Rules (12) - (16) are the buyer"s desire. Among them, (15)
and (16) impose constraints for buying a PC. A DVD-RW
is specified as an abducible which is subject to concession.
(1st round) First, the following proposal is given by the
buyer agent:
G1
b : pc(b1, 1G, 512M, 80G), dvd-rw, price(x), x ≤ 1200.
As Ps has no answer set which satisfies G1
b , the seller agent
cannot accept the proposal. The seller takes an action of
making a counter-proposal and performs abduction. As a
result, the seller finds the minimal explanation (E, F) =
({ pay cash }, { add point }) which explains G1
b θ1 with θ1 =
{ x/1170 }. The seller constructs the conditional proposal:
G1
s : pc(b1, 1G, 512M, 80G), dvd-rw, price(1170),
pay cash, not add point
and offers it to the buyer.
(2nd round) The buyer does not accept G1
s because he/she
cannot pay it by cash (15). The buyer then returns the 
critique G2
b = reject to the seller, together with the critique set
CS2
b (Pb, G1
s) = {(15)}. In response to this, the seller tries
to make another proposal which satisfies the constraint in
this critique set. As G1
s is stored in FP 2
s and no other 
conditional proposal satisfying the buyer"s requirement exists,
the seller produces neighborhood proposals. He/she relaxes
G1
b by dropping x ≤ 1200 in the condition, and produces
pc(b1, 1G, 512M, 80G), dvd-rw, price(x).
As Ps has an answer set which satisfies
G2
s : pc(b1, 1G, 512M, 80G), dvd-rw, price(1300),
1026 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
the seller offers G2
s as a new counter-proposal.
(3rd round) The buyer does not accept G2
s because he/she
cannot pay more than 1200USD (16). The buyer again 
returns the critique G3
b = reject to the seller, together with
the critique set CS3
b (Pb, G2
s) = CS2
b (Pb, G1
s) ∪ {(16)}. The
seller then considers another proposal by replacing b1 with
a variable w, G1
b now becomes
pc(w, 1G, 512M, 80G), dvd-rw, price(x), x ≤ 1200.
As Ps has an answer set which satisfies
G3
s : pc(b2, 1G, 512M, 80G), dvd-rw, price(1200),
the seller offers G3
s as a new counter-proposal.
(4th round) The buyer does not accept G3
s because a PC of
the brand b2 is out of his/her interest and Pb has no answer
set satisfying G3
s. Then, the buyer makes a concession by
changing his/her original goal. The buyer relaxes G1
b by goal
replacement using the rule (9) in Pb, and produces
pc(b1, 1G, 512M, 80G), drive, price(x), x ≤ 1200.
Using (10), the following proposal is produced:
pc(b1, 1G, 512M, 80G), cd-rw, price(x), x ≤ 1200.
As Pb \ { dvd-rw } has a consistent answer set satisfying the
above proposal, the buyer proposes the conditional 
neighborhood proposal
G4
b : pc(b1, 1G, 512M, 80G), cd-rw, not dvd-rw,
price(x), x ≤ 1200
to the seller agent. Since Ps also has an answer set satisfying
G4
b , the seller accepts it and sends the message G4
s = accept
to the buyer. Thus, the negotiation ends in success.
4. COMPUTATION
In this section, we provide methods of computing 
proposals in terms of answer sets of programs. We first introduce
some definitions from [15].
Definition 4.1. Given an abductive program P, H , the
set UR of update rules is defined as:
UR = { L ← not L, L ← not L | L ∈ H }
∪ { +L ← L | L ∈ H \ P }
∪ { −L ← not L | L ∈ H ∩ P } ,
where L, +L, and −L are new atoms uniquely associated
with every L ∈ H. The atoms +L and −L are called update
atoms.
By the definition, the atom L becomes true iff L is not
true. The pair of rules L ← not L and L ← not L specify
the situation that an abducible L is true or not. When
p(x) ∈ H and p(a) ∈ P but p(t) ∈ P for t = a, the rule
+L ← L precisely becomes +p(t) ← p(t) for any t = a. In
this case, the rule is shortly written as +p(x) ← p(x), x = a.
Generally, the rule becomes +p(x) ← p(x), x = t1, . . . , x =
tn for n such instances. The rule +L ← L derives the atom
+L if an abducible L which is not in P is to be true. In
contrast, the rule −L ← not L derives the atom −L if an
abducible L which is in P is not to be true. Thus, update
atoms represent the change of truth values of abducibles in
a program. That is, +L means the introduction of L, while
−L means the deletion of L. When an abducible L contains
variables, the associated update atom +L or −L is supposed
to have exactly the same variables. In this case, an update
atom is semantically identified with its ground instances.
The set of all update atoms associated with the abducibles
in H is denoted by UH, and UH = UH+
∪ UH−
where
UH+
(resp. UH−
) is the set of update atoms of the form
+L (resp. −L).
Definition 4.2. Given an abductive program P, H , its
update program UP is defined as the program
UP = (P \ H) ∪ UR .
An answer set S of UP is called U-minimal if there is no
answer set T of UP such that T ∩ UH ⊂ S ∩ UH.
By the definition, U-minimal answer sets exist whenever
UP has answer sets. Update programs are used for 
computing (minimal) explanations of an observation. Given an
observation G as a conjunction of literals and NAF-literals
possibly containing variables, we introduce a new ground
literal O together with the rule O ← G. In this case, O
has an explanation (E, F) iff G has the same explanation.
With this replacement, an observation is assumed to be a
ground literal without loss of generality. In what follows,
E+
= { +L | L ∈ E } and F −
= { −L | L ∈ F } for E ⊆ H
and F ⊆ H.
Proposition 4.1. ([15]) Let P, H be an abductive 
program, UP its update program, and G a ground literal 
representing an observation. Then, a pair (E, F) is an 
explanation of G iff UP ∪ { ← not G } has a consistent answer set
S such that E+
= S ∩ UH+
and F−
= S ∩ UH−
. In 
particular, (E, F) is a minimal explanation iff S is a U-minimal
answer set.
Example 4.1. To explain the observation G = flies(t)
in the program P of Example 2.1, first construct the update
program UP of P:3
UP : flies(x) ← bird(x), not ab(x),
ab(x) ← broken-wing(x) ,
bird(t) ← , bird(o) ← ,
broken-wing(x) ← not broken-wing(x),
broken-wing(x) ← not broken-wing(x),
+broken-wing(x) ← broken-wing(x), x = t ,
−broken-wing(t) ← not broken-wing(t) .
Next, consider the program UP ∪ { ← not flies(t) }. It has
the single U-minimal answer set: S = { bird(t), bird(o), flies(t),
flies(o), broken-wing(t), broken-wing(o), −broken-wing(t) }.
The unique minimal explanation (E, F) = (∅, {broken-wing(t)})
of G is expressed by the update atom −broken-wing(t) in
S ∩ UH−
.
Proposition 4.2. Let P, H be an abductive program
and G a ground literal representing an observation. If P ∪
{ ← not G } has a consistent answer set S, G has the 
minimal explanation (E, F) = (∅, ∅) and S satisfies G.
Now we provide methods for computing (counter-)proposals.
First, conditional proposals are computed as follows.
input : an abductive program P, H , a proposal G;
output : a set Sc of proposals.
If G is a ground literal, compute its minimal 
explanation (E, F) in P, H using the update program. Put
G, E, not F in Sc. Else if G is a conjunction possibly
containing variables, consider the abductive program
3
t represents tweety and o represents opus.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1027
P ∪{ O ← G }, H with a ground literal O. Compute
a minimal explanation of O in P ∪ { O ← G }, H
using its update program. If O has a minimal 
explanation (E, F) with a substitution θ for variables in G,
put Gθ, E, not F in Sc.
Next, neighborhood proposals are computed as follows.
input : an abductive program P, H , a proposal G;
output : a set Sn of proposals.
% neighborhood proposals by anti-instantiation;
Construct G by anti-instantiation. For a ground 
literal O, if P ∪ { O ← G } ∪ { ← not O } has a 
consistent answer set satisfying G θ with a substitution θ
and G θ = G, put G θ in Sn.
% neighborhood proposals by dropping conditions;
Construct G by dropping conditions. If G is a ground
literal and the program P ∪ { ← not G } has a 
consistent answer set, put G in Sn. Else if G is a 
conjunction possibly containing variables, do the following.
For a ground literal O, if P ∪{ O ← G }∪{ ← not O }
has a consistent answer set satisfying G θ with a 
substitution θ, put G θ in Sn.
% neighborhood proposals by goal replacement;
Construct G by goal replacement. If G is a ground 
literal and there is a rule H ← B in P such that G = Hσ
and Bσ = G for some substitution σ, put G = Bσ.
If P ∪ { ← not G } has a consistent answer set 
satisfying G θ with a substitution θ, put G θ in Sn. Else
if G is a conjunction possibly containing variables,
do the following. For a replaced literal L ∈ G , if
there is a rule H ← B in P such that L = Hσ and
(G \ {L}) ∪ Bσ = G for some substitution σ, put
G = (G \ {L}) ∪ Bσ. For a ground literal O, if
P ∪ { O ← G } ∪ { ← not O } has a consistent answer
set satisfying G θ with a substitution θ, put G θ in
Sn.
Theorem 4.3. The set Sc (resp. Sn) computed above 
coincides with the set of conditional proposals (resp. 
neighborhood proposals).
Proof. The result for Sc follows from Definition 3.3 and
Proposition 4.1. The result for Sn follows from Definition 3.5
and Proposition 4.2.
Conditional neighborhood proposals are computed by 
combining the above two procedures. Those proposals are 
computed at each round. Note that the procedure for computing
Sn contains some nondeterministic choices. For instance,
there are generally several candidates of literals to relax in
a proposal. Also, there might be several rules in a program
for the usage of goal replacement. In practice, an agent can
prespecify literals in a proposal for possible relaxation or
rules in a program for the usage of goal replacement.
5. RELATED WORK
As there are a number of literature on automated 
negotiation, this section focuses on comparison with negotiation
frameworks based on logic and argumentation.
Sadri et al. [14] use abductive logic programming as a 
representation language of negotiating agents. Agents negotiate
using common dialogue primitives, called dialogue moves.
Each agent has an abductive logic program in which a 
sequence of dialogues are specified by a program, a dialogue
protocol is specified as constraints, and dialogue moves are
specified as abducibles. The behavior of agents is regulated
by an observe-think-act cycle. Once a dialogue move is 
uttered by an agent, another agent that observed the utterance
thinks and acts using a proof procedure. Their approach
and ours both employ abductive logic programming as a
platform of agent reasoning, but the use of it is quite 
different. First, they use abducibles to specify dialogue primitives
of the form tell(utterer, receiver, subject, identifier, time),
while we use abducibles to specify arbitrary permissible 
hypotheses to construct conditional proposals. Second, a 
program pre-specifies a plan to carry out in order to achieve a
goal, together with available/missing resources in the 
context of resource-exchanging problems. This is in contrast
with our method in which possible counter-proposals are
newly constructed in response to a proposal made by an
agent. Third, they specify a negotiation policy inside a
program (as integrity constraints), while we give a protocol
independent of individual agents. They provide an 
operational model that completely specifies the behavior of agents
in terms of agent cycle. We do not provide such a complete
specification of the behavior of agents. Our primary interest
is to mechanize construction of proposals.
Bracciali and Torroni [2] formulate abductive agents that
have knowledge in abductive logic programs. To explain
an observation, two agents communicate by exchanging 
integrity constraints. In the process of communication, an
agent can revise its own integrity constraints according to
the information provided by the other agent. A set IC
of integrity constraints relaxes a set IC (or IC tightens
IC ) if any observation that can be proved with respect to
IC can also be proved with respect to IC . For instance,
IC : ← a, b, c relaxes IC : ← a, b. Thus, they use relaxation
for weakening the constraints in an abductive logic program.
In contrast, we use relaxation for weakening proposals and
three different relaxation methods, anti-instantiation, 
dropping conditions, and goal replacement, are considered. Their
goal is to explain an observation by revising integrity 
constraints of an agent through communication, while we use
integrity constraints for communication to explain critiques
and help other agents in making counter-proposals.
Meyer et al. [11] introduce a logical framework for 
negotiating agents. They introduce two different modes of 
negotiation: concession and adaptation. They provide rational
postulates to characterize negotiated outcomes between two
agents, and describe methods for constructing outcomes.
They provide logical conditions for negotiated outcomes to
satisfy, but they do not describe a process of negotiation nor
negotiation protocols. Moreover, they represent agents by
classical propositional theories, which is different from our
abductive logic programming framework.
Foo et al. [5] model one-to-one negotiation as a one-time
encounter between two extended logic programs. An agent
offers an answer set of its program, and their mutual deal is
regarded as a trade on their answer sets. Starting from the
initial agreement set S∩T for an answer set S of an agent and
an answer set T of another agent, each agent extends this
set to reflect its own demand while keeping consistency with
demand of the other agent. Their algorithm returns new
programs having answer sets which are consistent with each
other and keep the agreement set. The work is extended to
repeated encounters in [3]. In their framework, two agents
exchange answer sets to produce a common belief set, which
is different from our framework of exchanging proposals.
There are a number of proposals for negotiation based
1028 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
on argumentation. An advantage of argumentation-based
negotiation is that it constructs a proposal with arguments
supporting the proposal [1]. The existence of arguments is
useful to convince other agents of reasons why an agent offers
(counter-)proposals or returns critiques. Parsons et al. [13]
develop a logic of argumentation-based negotiation among
BDI agents. In one-to-one negotiation, an agent A generates
a proposal together with its arguments, and passes it to
another agent B. The proposal is evaluated by B which
attempts to build arguments against it. If it conflicts with
B"s interest, B informs A of its objection by sending back
its attacking argument. In response to this, A tries to find
an alternative way of achieving its original objective, or a
way of persuading B to drop its objection. If either type of
argument can be found, A will submit it to B. If B finds no
reason to reject the new proposal, it will be accepted and
the negotiation ends in success. Otherwise, the process is
iterated. In this negotiation processes, the agent A never
changes its original objective, so that negotiation ends in
failure if A fails to find an alternative way of achieving the
original objective. In our framework, when a proposal is
rejected by another agent, an agent can weaken or change
its objective by abduction and relaxation. Our framework
does not have a mechanism of argumentation, but reasons
for critiques can be informed by responding critique sets.
Kakas and Moraitis [10] propose a negotiation protocol
which integrates abduction within an argumentation 
framework. A proposal contains an offer corresponding to the 
negotiation object, together with supporting information 
representing conditions under which this offer is made. 
Supporting information is computed by abduction and is used
for constructing conditional arguments during the process
of negotiation. In their negotiation protocol, when an agent
cannot satisfy its own goal, the agent considers the other
agent"s goal and searches for conditions under which the
goal is acceptable. Our present approach differs from theirs
in the following points. First, they use abduction to seek
conditions to support arguments, while we use abduction
to seek conditions for proposals to accept. Second, in their
negotiation protocol, counter-proposals are chosen among
candidates based on preference knowledge of an agent at
meta-level, which represents policy under which an agent
uses its object-level decision rules according to situations.
In our framework, counter-proposals are newly constructed
using abduction and relaxation. The method of 
construction is independent of particular negotiation protocols. As
[2, 10, 14], abduction or abductive logic programming used
in negotiation is mostly based on normal abduction. In 
contrast, our approach is based on extended abduction which
can not only introduce hypotheses but remove them from a
program. This is another important difference.
Relaxation and neighborhood query answering are devised
to make databases cooperative with their users [4, 6]. In this
sense, those techniques have the spirit similar to cooperative
problem solving in multi-agent systems. As far as the 
authors know, however, there is no study which applies those
technique to agent negotiation.
6. CONCLUSION
In this paper we proposed a logical framework for 
negotiating agents. To construct proposals in the process of 
negotiation, we combined the techniques of extended abduction
and relaxation. It was shown that these two operations are
used for general inference rules in producing proposals. We
developed a negotiation protocol between two agents based
on exchange of proposals and critiques, and provided 
procedures for computing proposals in abductive logic 
programming. This enables us to realize automated negotiation on
top of the existing answer set solvers. The present 
framework does not have a mechanism of selecting an optimal
(counter-)proposal among different alternatives. To 
compare and evaluate proposals, an agent must have preference
knowledge of candidate proposals. Further elaboration to
maximize the utility of agents is left for future study.
7. REFERENCES
[1] L. Amgoud, S. Parsons, and N. Maudet. Arguments,
dialogue, and negotiation. In: Proc. ECAI-00,
pp. 338-342, IOS Press, 2000.
[2] A. Bracciali and P. Torroni. A new framework for
knowledge revision of abductive agents through their
interaction. In: Proc. CLIMA-IV, Computational Logic
in Multi-Agent Systems, LNAI 3259, pp. 159-177, 2004.
[3] W. Chen, M. Zhang, and N. Foo. Repeated negotiation
of logic programs. In: Proc. 7th Workshop on
Nonmonotonic Reasoning, Action and Change, 2006.
[4] W. W. Chu, Q. Chen, and R.-C. Lee. Cooperative
query answering via type abstraction hierarchy. In:
Cooperating Knowledge Based Systems, S. M. Deen ed.,
pp. 271-290, Springer, 1990.
[5] N. Foo, T. Meyer, Y. Zhang, and D. Zhang.
Negotiating logic programs. In: Proc. 6th Workshop on
Nonmonotonic Reasoning, Action and Change, 2005.
[6] T. Gaasterland, P. Godfrey, and J. Minker. Relaxation
as a platform for cooperative answering. Journal of
Intelligence Information Systems 1(3/4):293-321, 1992.
[7] M. Gelfond and V. Lifschitz. Classical negation in logic
programs and disjunctive databases. New Generation
Computing 9:365-385, 1991.
[8] K. Inoue and C. Sakama. Abductive framework for
nonmonotonic theory change. In: Proc. IJCAI-95,
pp. 204-210, Morgan Kaufmann.
[9] A. C. Kakas, R. A. Kowalski, and F. Toni, The role of
abduction in logic programming. In: Handbook of Logic
in AI and Logic Programming, D. M. Gabbay, et al.
(eds), vol. 5, pp. 235-324, Oxford University Press, 1998.
[10] A. C. Kakas and P. Moraitis. Adaptive agent
negotiation via argumentation. In: Proc. AAMAS-06,
pp. 384-391, ACM Press.
[11] T. Meyer, N. Foo, R. Kwok, and D. Zhang. Logical
foundation of negotiation: outcome, concession and
adaptation. In: Proc. AAAI-04, pp. 293-298, MIT Press.
[12] R. S. Michalski. A theory and methodology of
inductive learning. In: Machine Learning: An Artificial
Intelligence Approach, R. S. Michalski, et al. (eds),
pp. 83-134, Morgan Kaufmann, 1983.
[13] S. Parsons, C. Sierra and N. Jennings. Agents that
reason and negotiate by arguing. Journal of Logic and
Computation, 8(3):261-292, 1988.
[14] F. Sadri, F. Toni, and P. Torroni, An abductive logic
programming architecture for negotiating agents. In:
Proc. 8th European Conf. on Logics in AI, LNAI 2424,
pp. 419-431, Springer, 2002.
[15] C. Sakama and K. Inoue. An abductive framework for
computing knowledge base updates. Theory and Practice
of Logic Programming 3(6):671-715, 2003.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1029
A Q-decomposition and Bounded RTDP Approach to
Resource Allocation
Pierrick Plamondon and Brahim
Chaib-draa
Computer Science & Software Engineering Dept
Laval University
Québec, Canada
{plamon, chaib}@damas.ift.ulaval.ca
Abder Rezak Benaskeur
Decision Support Systems Section
Defence R&D Canada - Valcartier
Québec, Canada
abderrezak.benaskeur@drdc-rddc.gc.ca
ABSTRACT
This paper contributes to solve effectively stochastic 
resource allocation problems known to be NP-Complete. To
address this complex resource management problem, a 
Qdecomposition approach is proposed when the resources
which are already shared among the agents, but the actions
made by an agent may influence the reward obtained by
at least another agent. The Q-decomposition allows to 
coordinate these reward separated agents and thus permits to
reduce the set of states and actions to consider. On the other
hand, when the resources are available to all agents, no 
Qdecomposition is possible and we use heuristic search. In
particular, the bounded Real-time Dynamic Programming
(bounded rtdp) is used. Bounded rtdp concentrates the
planning on significant states only and prunes the action
space. The pruning is accomplished by proposing tight 
upper and lower bounds on the value function.
Categories and Subject Descriptors
I.2.8 [Artificial Intelligence]: Problem Solving, Control
Methods, and Search; I.2.11 [Artificial Intelligence]: 
Distributed Artificial Intelligence.
General Terms
Algorithms, Performance, Experimentation.
1. INTRODUCTION
This paper aims to contribute to solve complex stochastic
resource allocation problems. In general, resource 
allocation problems are known to be NP-Complete [12]. In such
problems, a scheduling process suggests the action (i.e. 
resources to allocate) to undertake to accomplish certain tasks,
according to the perfectly observable state of the 
environment. When executing an action to realize a set of tasks,
the stochastic nature of the problem induces probabilities
on the next visited state. In general, the number of states
is the combination of all possible specific states of each task
and available resources. In this case, the number of 
possible actions in a state is the combination of each individual
possible resource assignment to the tasks. The very high
number of states and actions in this type of problem makes
it very complex.
There can be many types of resource allocation problems.
Firstly, if the resources are already shared among the agents,
and the actions made by an agent does not influence the
state of another agent, the globally optimal policy can be
computed by planning separately for each agent. A second
type of resource allocation problem is where the resources
are already shared among the agents, but the actions made
by an agent may influence the reward obtained by at least
another agent. To solve this problem efficiently, we adapt 
Qdecomposition proposed by Russell and Zimdars [9]. In our
Q-decomposition approach, a planning agent manages each
task and all agents have to share the limited resources. The
planning process starts with the initial state s0. In s0, each
agent computes their respective Q-value. Then, the 
planning agents are coordinated through an arbitrator to find
the highest global Q-value by adding the respective possible
Q-values of each agents. When implemented with heuristic
search, since the number of states and actions to consider
when computing the optimal policy is exponentially reduced
compared to other known approaches, Q-decomposition 
allows to formulate the first optimal decomposed heuristic
search algorithm in a stochastic environments.
On the other hand, when the resources are available to
all agents, no Q-decomposition is possible. A common
way of addressing this large stochastic problem is by 
using Markov Decision Processes (mdps), and in particular
real-time search where many algorithms have been 
developed recently. For instance Real-Time Dynamic 
Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all
state-of-the-art heuristic search approaches in a stochastic
environment. Because of its anytime quality, an interesting
approach is rtdp introduced by Barto et al. [1] which 
updates states in trajectories from an initial state s0 to a goal
state sg. rtdp is used in this paper to solve efficiently a
constrained resource allocation problem.
rtdp is much more effective if the action space can be
pruned of sub-optimal actions. To do this, McMahan et
1212
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
al. [6], Smith and Simmons [11], and Singh and Cohn [10]
proposed solving a stochastic problem using a rtdp type
heuristic search with upper and lower bounds on the value
of states. McMahan et al. [6] and Smith and Simmons [11]
suggested, in particular, an efficient trajectory of state 
updates to further speed up the convergence, when given upper
and lower bounds. This efficient trajectory of state updates
can be combined to the approach proposed here since this
paper focusses on the definition of tight bounds, and efficient
state update for a constrained resource allocation problem.
On the other hand, the approach by Singh and Cohn is
suitable to our case, and extended in this paper using, in
particular, the concept of marginal revenue [7] to elaborate
tight bounds. This paper proposes new algorithms to define
upper and lower bounds in the context of a rtdp heuristic
search approach. Our marginal revenue bounds are 
compared theoretically and empirically to the bounds proposed
by Singh and Cohn. Also, even if the algorithm used to
obtain the optimal policy is rtdp, our bounds can be used
with any other algorithm to solve an mdp. The only 
condition on the use of our bounds is to be in the context of
stochastic constrained resource allocation. The problem is
now modelled.
2. PROBLEM FORMULATION
A simple resource allocation problem is one where there
are the following two tasks to realize: ta1 = {wash the
dishes}, and ta2 = {clean the floor}. These two tasks are 
either in the realized state, or not realized state. To realize the
tasks, two type of resources are assumed: res1 = {brush},
and res2 = {detergent}. A computer has to compute the 
optimal allocation of these resources to cleaner robots to realize
their tasks. In this problem, a state represents a 
conjunction of the particular state of each task, and the available
resources. The resources may be constrained by the amount
that may be used simultaneously (local constraint), and in
total (global constraint). Furthermore, the higher is the
number of resources allocated to realize a task, the higher is
the expectation of realizing the task. For this reason, when
the specific states of the tasks change, or when the number
of available resources changes, the value of this state may
change.
When executing an action a in state s, the specific states
of the tasks change stochastically, and the remaining 
resource are determined with the resource available in s, 
subtracted from the resources used by action a, if the resource
is consumable. Indeed, our model may consider 
consumable and non-consumable resource types. A consumable 
resource type is one where the amount of available resource
is decreased when it is used. On the other hand, a 
nonconsumable resource type is one where the amount of 
available resource is unchanged when it is used. For example, a
brush is a non-consumable resource, while the detergent is
a consumable resource.
2.1 Resource Allocation as a MDPs
In our problem, the transition function and the reward
function are both known. A Markov Decision Process (mdp)
framework is used to model our stochastic resource 
allocation problem. mdps have been widely adopted by researchers
today to model a stochastic process. This is due to the fact
that mdps provide a well-studied and simple, yet very 
expressive model of the world. An mdp in the context of a
resource allocation problem with limited resources is defined
as a tuple Res, T a, S, A, P, W, R, , where:
• Res = res1, ..., res|Res| is a finite set of resource
types available for a planning process. Each resource
type may have a local resource constraint Lres on
the number that may be used in a single step, and
a global resource constraint Gres on the number that
may be used in total. The global constraint only 
applies for consumable resource types (Resc) and the 
local constraints always apply to consumable and 
nonconsumable resource types.
• T a is a finite set of tasks with ta ∈ T a to be 
accomplished.
• S is a finite set of states with s ∈ S. A state s is
a tuple T a, res1, ..., res|Resc| , which is the 
characteristic of each unaccomplished task ta ∈ T a in the
environment, and the available consumable resources.
sta is the specific state of task ta. Also, S contains a
non empty set sg ⊆ S of goal states. A goal state is a
sink state where an agent stays forever.
• A is a finite set of actions (or assignments). The 
actions a ∈ A(s) applicable in a state are the 
combination of all resource assignments that may be executed,
according to the state s. In particular, a is simply an
allocation of resources to the current tasks, and ata is
the resource allocation to task ta. The possible actions
are limited by Lres and Gres.
• Transition probabilities Pa(s |s) for s ∈ S and a ∈
A(s).
• W = [wta] is the relative weight (criticality) of each
task.
• State rewards R = [rs] :
ta∈T a
rsta ← sta × wta. The
relative reward of the state of a task rsta is the product
of a real number sta by the weight factor wta. For
our problem, a reward of 1 × wta is given when the
state of a task (sta) is in an achieved state, and 0 in
all other cases.
• A discount (preference) factor γ, which is a real 
number between 0 and 1.
A solution of an mdp is a policy π mapping states s into
actions a ∈ A(s). In particular, πta(s) is the action (i.e.
resources to allocate) that should be executed on task ta,
considering the global state s. In this case, an optimal 
policy is one that maximizes the expected total reward for 
accomplishing all tasks. The optimal value of a state, V (s), is
given by:
V (s) = R(s) + max
a∈A(s)
γ
s ∈S
Pa(s |s)V (s ) (1)
where the remaining consumable resources in state s are
Resc \ res(a), where res(a) are the consumable resources
used by action a. Indeed, since an action a is a resource 
assignment, Resc \ res(a) is the new set of available resources
after the execution of action a. Furthermore, one may 
compute the Q-Values Q(a, s) of each state action pair using the
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213
following equation:
Q(a, s) = R(s) + γ
s ∈S
Pa(s |s) max
a ∈A(s )
Q(a , s ) (2)
where the optimal value of a state is V (s) = max
a∈A(s)
Q(a, s).
The policy is subjected to the local resource constraints
res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res. The global
constraint is defined according to all system trajectories
tra ∈ T RA. A system trajectory tra is a possible sequence
of state-action pairs, until a goal state is reached under the
optimal policy π. For example, state s is entered, which may
transit to s or to s , according to action a. The two 
possible system trajectories are (s, a), (s ) and (s, a), (s ) .
The global resource constraint is res(tra) ≤ Gres∀ tra ∈
T RA ,and ∀ res ∈ Resc where res(tra) is a function which
returns the resources used by trajectory tra. Since the 
available consumable resources are represented in the state space,
this condition is verified by itself. In other words, the model
is Markovian as the history has not to be considered in the
state space. Furthermore, the time is not considered in the
model description, but it may also include a time horizon
by using a finite horizon mdp. Since resource allocation in
a stochastic environment is NP-Complete, heuristics should
be employed. Q-decomposition which decomposes a 
planning problem to many agents to reduce the computational
complexity associated to the state and/or action spaces is
now introduced.
2.2 Q-decomposition for Resource Allocation
There can be many types of resource allocation problems.
Firstly, if the resources are already shared among the agents,
and the actions made by an agent does not influence the
state of another agent, the globally optimal policy can be
computed by planning separately for each agent.
A second type of resource allocation problem is where
the resources are already shared among the agents, but the
actions made by an agent may influence the reward obtained
by at least another agent. For instance, a group of agents
which manages the oil consummated by a country falls in
this group. These agents desire to maximize their specific
reward by consuming the right amount of oil. However, all
the agents are penalized when an agent consumes oil because
of the pollution it generates. Another example of this type
comes from our problem of interest, explained in Section
3, which is a naval platform which must counter incoming
missiles (i.e. tasks) by using its resources (i.e. weapons,
movements). In some scenarios, it may happens that the
missiles can be classified in two types: Those requiring a
set of resources Res1 and those requiring a set of resources
Res2. This can happen depending on the type of missiles,
their range, and so on. In this case, two agents can plan for
both set of tasks to determine the policy. However, there
are interaction between the resource of Res1 and Res2, so
that certain combination of resource cannot be assigned. IN
particular, if an agent i allocate resources Resi to the first
set of tasks T ai, and agent i allocate resources Resi to
second set of tasks T ai , the resulting policy may include
actions which cannot be executed together.
To result these conflicts, we use Q-decomposition 
proposed by Russell and Zimdars [9] in the context of 
reinforcement learning. The primary assumption underlying 
Qdecomposition is that the overall reward function R can be
additively decomposed into separate rewards Ri for each 
distinct agent i ∈ Ag, where |Ag| is the number of agents. That
is, R = i∈Ag Ri. It requires each agent to compute a value,
from its perspective, for every action. To coordinate with
each other, each agent i reports its action values Qi(ai, si)
for each state si ∈ Si to an arbitrator at each learning 
iteration. The arbitrator then chooses an action maximizing
the sum of the agent Q-values for each global state s ∈ S.
The next time state s is updated, an agent i considers the
value as its respective contribution, or Q-value, to the global
maximal Q-value. That is, Qi(ai, si) is the value of a state
such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si). The fact
that the agents use a determined Q-value as the value of a
state is an extension of the Sarsa on-policy algorithm [8] to
Q-decomposition. Russell and Zimdars called this approach
local Sarsa. In this way, an ideal compromise can be found
for the agents to reach a global optimum. Indeed, rather
than allowing each agent to choose the successor action, each
agent i uses the action ai executed by the arbitrator in the
successor state si:
Qi(ai, si) = Ri(si) + γ
si∈Si
Pai (si|si)Qi(ai, si)
(3)
where the remaining consumable resources in state si are
Resci \ resi(ai) for a resource allocation problem. Russell
and Zimdars [9] demonstrated that local Sarsa converges
to the optimum. Also, in some cases, this form of agent
decomposition allows the local Q-functions to be expressed
by a much reduced state and action space.
For our resource allocation problem described briefly in
this section, Q-decomposition can be applied to generate an
optimal solution. Indeed, an optimal Bellman backup can
be applied in a state as in Algorithm 1. In Line 5 of the
Qdec-backup function, each agent managing a task 
computes its respective Q-value. Here, Qi (ai, s ) determines the
optimal Q-value of agent i in state s . An agent i uses as
the value of a possible state transition s the Q-value for
this agent which determines the maximal global Q-value for
state s as in the original Q-decomposition approach. In
brief, for each visited states s ∈ S, each agent computes its
respective Q-values with respect to the global state s. So
the state space is the joint state space of all agents. Some
of the gain in complexity to use Q-decomposition resides in
the
si∈Si
Pai (si|s) part of the equation. An agent considers
as a possible state transition only the possible states of the
set of tasks it manages. Since the number of states is 
exponential with the number of tasks, using Q-decomposition
should reduce the planning time significantly. Furthermore,
the action space of the agents takes into account only their
available resources which is much less complex than a 
standard action space, which is the combination of all possible
resource allocation in a state for all agents.
Then, the arbitrator functionalities are in Lines 8 to 20.
The global Q-value is the sum of the Q-values produced by
each agent managing each task as shown in Line 11, 
considering the global action a. In this case, when an action of an
agent i cannot be executed simultaneously with an action
of another agent i , the global action is simply discarded
from the action space A(s). Line 14 simply allocate the 
current value with respect to the highest global Q-value, as in
a standard Bellman backup. Then, the optimal policy and
Q-value of each agent is updated in Lines 16 and 17 to the
sub-actions ai and specific Q-values Qi(ai, s) of each agent
1214 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
for action a.
Algorithm 1 The Q-decomposition Bellman Backup.
1: Function Qdec-backup(s)
2: V (s) ← 0
3: for all i ∈ Ag do
4: for all ai ∈ Ai(s) do
5: Qi(ai, s) ← Ri(s) + γ
si
∈Si
Pai (si|s)Qi (ai, s )
{where Qi (ai, s ) = hi(s ) when s is not yet visited,
and s has Resci \ resi(ai) remaining consumable
resources for each agent i}
6: end for
7: end for
8: for all a ∈ A(s) do
9: Q(a, s) ← 0
10: for all i ∈ Ag do
11: Q(a, s) ← Q(a, s) + Qi(ai, s)
12: end for
13: if Q(a, s) > V (s) then
14: V (s) ← Q(a, s)
15: for all i ∈ Ag do
16: πi(s) ← ai
17: Qi (ai, s) ← Qi(ai, s)
18: end for
19: end if
20: end for
A standard Bellman backup has a complexity of O(|A| ×
|SAg|), where |SAg| is the number of joint states for all agents
excluding the resources, and |A| is the number of joint 
actions. On the other hand, the Q-decomposition Bellman
backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| ×
|Ag|)), where |Si| is the number of states for an agent i,
excluding the resources and |Ai| is the number of actions
for an agent i. Since |SAg| is combinatorial with the 
number of tasks, so |Si| |S|. Also, |A| is combinatorial with
the number of resource types. If the resources are already
shared among the agents, the number of resource type for
each agent will usually be lower than the set of all 
available resource types for all agents. In these circumstances,
|Ai| |A|. In a standard Bellman backup, |A| is multiplied
by |SAg|, which is much more complex than multiplying |A|
by |Ag| with the Q-decomposition Bellman backup. Thus,
the Q-decomposition Bellman backup is much less complex
than a standard Bellman backup. Furthermore, the 
communication cost between the agents and the arbitrator is
null since this approach does not consider a geographically
separated problem.
However, when the resources are available to all agents,
no Q-decomposition is possible. In this case, Bounded 
RealTime Dynamic Programming (bounded-rtdp) permits to
focuss the search on relevant states, and to prune the action
space A by using lower and higher bound on the value of
states. bounded-rtdp is now introduced.
2.3 Bounded-RTDP
Bonet and Geffner [4] proposed lrtdp as an improvement
to rtdp [1]. lrtdp is a simple dynamic programming 
algorithm that involves a sequence of trial runs, each starting in
the initial state s0 and ending in a goal or a solved state.
Each lrtdp trial is the result of simulating the policy π while
updating the values V (s) using a Bellman backup (Equation
1) over the states s that are visited. h(s) is a heuristic which
define an initial value for state s. This heuristic has to be
admissible - The value given by the heuristic has to 
overestimate (or underestimate) the optimal value V (s) when
the objective function is maximized (or minimized). For
example, an admissible heuristic for a stochastic shortest
path problem is the solution of a deterministic shortest path
problem. Indeed, since the problem is stochastic, the 
optimal value is lower than for the deterministic version. It has
been proven that lrtdp, given an admissible initial 
heuristic on the value of states cannot be trapped in loops, and
eventually yields optimal values [4]. The convergence is 
accomplished by means of a labeling procedure called 
checkSolved(s, ). This procedure tries to label as solved each
traversed state in the current trajectory. When the initial
state is labelled as solved, the algorithm has converged.
In this section, a bounded version of rtdp 
(boundedrtdp) is presented in Algorithm 2 to prune the action space
of sub-optimal actions. This pruning enables to speed up the
convergence of lrtdp. bounded-rtdp is similar to rtdp
except there are two distinct initial heuristics for unvisited
states s ∈ S; hL(s) and hU (s). Also, the checkSolved(s,
) procedure can be omitted because the bounds can provide
the labeling of a state as solved. On the one hand, hL(s)
defines a lower bound on the value of s such that the optimal
value of s is higher than hL(s). For its part, hU (s) defines an
upper bound on the value of s such that the optimal value
of s is lower than hU (s).
The values of the bounds are computed in Lines 3 and
4 of the bounded-backup function. Computing these two
Q-values is made simultaneously as the state transitions are
the same for both Q-values. Only the values of the state
transitions change. Thus, having to compute two Q-values
instead of one does not augment the complexity of the 
approach. In fact, Smith and Simmons [11] state that the 
additional time to compute a Bellman backup for two bounds,
instead of one, is no more than 10%, which is also what we
obtained. In particular, L(s) is the lower bound of state s,
while U(s) is the upper bound of state s. Similarly, QL(a, s)
is the Q-value of the lower bound of action a in state s, while
QU (a, s) is the Q-value of the upper bound of action a in
state s. Using these two bounds allow significantly 
reducing the action space A. Indeed, in Lines 5 and 6 of the
bounded-backup function, if QU (a, s) ≤ L(s) then action
a may be pruned from the action space of s. In Line 13
of this function, a state can be labeled as solved if the 
difference between the lower and upper bounds is lower than
. When the execution goes back to the bounded-rtdp
function, the next state in Line 10 has a fixed number of
consumable resources available Resc, determined in Line 9.
In brief, pickNextState(res) selects a none-solved state
s reachable under the current policy which has the highest
Bellman error (|U(s) − L(s)|). Finally, in Lines 12 to 15, a
backup is made in a backward fashion on all visited state
of a trajectory, when this trajectory has been made. This
strategy has been proven as efficient [11] [6].
As discussed by Singh and Cohn [10], this type of 
algorithm has a number of desirable anytime characteristics: if
an action has to be picked in state s before the algorithm
has converged (while multiple competitive actions remains),
the action with the highest lower bound is picked. Since
the upper bound for state s is known, it may be estimated
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215
Algorithm 2 The bounded-rtdp algorithm. Adapted
from [4] and [10].
1: Function bounded-rtdp(S)
2: returns a value function V
3: repeat
4: s ← s0
5: visited ← null
6: repeat
7: visited.push(s)
8: bounded-backup(s)
9: Resc ← Resc \ {π(s)}
10: s ← s.pickNextState(Resc)
11: until s is a goal
12: while visited = null do
13: s ← visited.pop()
14: bounded-backup(s)
15: end while
16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from
s0
17: return V
Algorithm 3 The bounded Bellman backup.
1: Function bounded-backup(s)
2: for all a ∈ A(s) do
3: QU (a, s) ← R(s) + γ
s ∈S
Pa(s |s)U(s )
4: QL(a, s) ← R(s) + γ
s ∈S
Pa(s |s)L(s )
{where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s
is not yet visited and s has Resc \ res(a) remaining
consumable resources}
5: if QU (a, s) ≤ L(s) then
6: A(s) ← A(s) \ res(a)
7: end if
8: end for
9: L(s) ← max
a∈A(s)
QL(a, s)
10: U(s) ← max
a∈A(s)
QU (a, s)
11: π(s) ← arg max
a∈A(s)
QL(a, s)
12: if |U(s) − L(s)| < then
13: s ← solved
14: end if
how far the lower bound is from the optimal. If the 
difference between the lower and upper bound is too high, one
can choose to use another greedy algorithm of one"s choice,
which outputs a fast and near optimal solution. 
Furthermore, if a new task dynamically arrives in the environment,
it can be accommodated by redefining the lower and 
upper bounds which exist at the time of its arrival. Singh
and Cohn [10] proved that an algorithm that uses 
admissible lower and upper bounds to prune the action space is
assured of converging to an optimal solution.
The next sections describe two separate methods to define
hL(s) and hU (s). First of all, the method of Singh and Cohn
[10] is briefly described. Then, our own method proposes
tighter bounds, thus allowing a more effective pruning of
the action space.
2.4 Singh and Cohn"s Bounds
Singh and Cohn [10] defined lower and upper bounds to
prune the action space. Their approach is pretty 
straightforward. First of all, a value function is computed for all
tasks to realize, using a standard rtdp approach. Then,
using these task-value functions, a lower bound hL, and
upper bound hU can be defined. In particular, hL(s) =
max
ta∈T a
Vta(sta), and hU (s) =
ta∈T a
Vta(sta). For readability,
the upper bound by Singh and Cohn is named SinghU, and
the lower bound is named SinghL. The admissibility of these
bounds has been proven by Singh and Cohn, such that, the
upper bound always overestimates the optimal value of each
state, while the lower bound always underestimates the 
optimal value of each state. To determine the optimal policy
π, Singh and Cohn implemented an algorithm very similar
to bounded-rtdp, which uses the bounds to initialize L(s)
and U(s). The only difference between bounded-rtdp, and
the rtdp version of Singh and Cohn is in the stopping 
criteria. Singh and Cohn proposed that the algorithm terminates
when only one competitive action remains for each state, or
when the range of all competitive actions for any state are
bounded by an indifference parameter . bounded-rtdp 
labels states for which |U(s) − L(s)| < as solved and the
convergence is reached when s0 is solved or when only one
competitive action remains for each state. This stopping
criteria is more effective since it is similar to the one used
by Smith and Simmons [11] and McMahan et al. brtdp [6].
In this paper, the bounds defined by Singh and Cohn and
implemented using bounded-rtdp define the Singh-rtdp
approach. The next sections propose to tighten the bounds
of Singh-rtdp to permit a more effective pruning of the
action space.
2.5 Reducing the Upper Bound
SinghU includes actions which may not be possible to
execute because of resource constraints, which overestimates
the upper bound. To consider only possible actions, our
upper bound, named maxU is introduced:
hU (s) = max
a∈A(s)
ta∈T a
Qta(ata, sta) (4)
where Qta(ata, sta) is the Q-value of task ta for state sta,
and action ata computed using a standard lrtdp approach.
Theorem 2.1. The upper bound defined by Equation 4 is
admissible.
Proof: The local resource constraints are satisfied because
the upper bound is computed using all global possible 
actions a. However, hU (s) still overestimates V (s) because
the global resource constraint is not enforced. Indeed, each
task may use all consumable resources for its own purpose.
Doing this produces a higher value for each task, than the
one obtained when planning for all tasks globally with the
shared limited resources.
Computing the maxU bound in a state has a 
complexity of O(|A| × |T a|), and O(|T a|) for SinghU. A standard
Bellman backup has a complexity of O(|A| × |S|). Since
|A|×|T a| |A|×|S|, the computation time to determine the
upper bound of a state, which is done one time for each 
visited state, is much less than the computation time required
to compute a standard Bellman backup for a state, which is
usually done many times for each visited state. Thus, the
computation time of the upper bound is negligible.
1216 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
2.6 Increasing the Lower Bound
The idea to increase SinghL is to allocate the resources
a priori among the tasks. When each task has its own set
of resources, each task may be solved independently. The
lower bound of state s is hL(s) =
ta∈T a
Lowta(sta), where
Lowta(sta) is a value function for each task ta ∈ T a, such
that the resources have been allocated a priori. The 
allocation a priori of the resources is made using marginal revenue,
which is a highly used concept in microeconomics [7], and
has recently been used for coordination of a Decentralized
mdp [2]. In brief, marginal revenue is the extra revenue that
an additional unit of product will bring to a firm. Thus,
for a stochastic resource allocation problem, the marginal
revenue of a resource is the additional expected value it 
involves. The marginal revenue of a resource res for a task ta
in a state sta is defined as following:
mrta(sta) = max
ata∈A(sta)
Qta(ata, sta)−
max
ata∈A(sta)
Qta(ata|res /∈ ata, sta) (5)
The concept of marginal revenue of a resource is used in
Algorithm 4 to allocate the resources a priori among the
tasks which enables to define the lower bound value of a
state. In Line 4 of the algorithm, a value function is 
computed for all tasks in the environment using a standard
lrtdp [4] approach. These value functions, which are also
used for the upper bound, are computed considering that
each task may use all available resources. The Line 5 
initializes the valueta variable. This variable is the estimated
value of each task ta ∈ T a. In the beginning of the 
algorithm, no resources are allocated to a specific task, thus the
valueta variable is initialized to 0 for all ta ∈ T a. Then, in
Line 9, a resource type res (consumable or non-consumable)
is selected to be allocated. Here, a domain expert may 
separate all available resources in many types or parts to be
allocated. The resources are allocated in the order of its
specialization. In other words, the more a resource is 
efficient on a small group of tasks, the more it is allocated early.
Allocating the resources in this order improves the quality
of the resulting lower bound. The Line 12 computes the
marginal revenue of a consumable resource res for each task
ta ∈ T a. For a non-consumable resource, since the resource
is not considered in the state space, all other reachable states
from sta consider that the resource res is still usable. The
approach here is to sum the difference between the real value
of a state to the maximal Q-value of this state if resource res
cannot be used for all states in a trajectory given by the 
policy of task ta. This heuristic proved to obtain good results,
but other ones may be tried, for example Monte-Carlo 
simulation. In Line 21, the marginal revenue is updated in 
function of the resources already allocated to each task. R(sgta )
is the reward to realize task ta. Thus, Vta(sta)−valueta
R(sgta )
is
the residual expected value that remains to be achieved,
knowing current allocation to task ta, and normalized by
the reward of realizing the tasks. The marginal revenue is
multiplied by this term to indicate that, the more a task
has a high residual value, the more its marginal revenue is
going to be high. Then, a task ta is selected in Line 23 with
the highest marginal revenue, adjusted with residual value.
In Line 24, the resource type res is allocated to the group
of resources Resta of task ta. Afterwards, Line 29 
recomAlgorithm 4 The marginal revenue lower bound algorithm.
1: Function revenue-bound(S)
2: returns a lower bound LowT a
3: for all ta ∈ T a do
4: Vta ←lrtdp(Sta)
5: valueta ← 0
6: end for
7: s ← s0
8: repeat
9: res ← Select a resource type res ∈ Res
10: for all ta ∈ T a do
11: if res is consumable then
12: mrta(sta) ← Vta(sta) − Vta(sta(Res \ res))
13: else
14: mrta(sta) ← 0
15: repeat
16: mrta(sta) ← mrta(sta) + 
Vta(sta)max
(ata∈A(sta)|res/∈ata)
Qta(ata, sta)
17: sta ← sta.pickNextState(Resc)
18: until sta is a goal
19: s ← s0
20: end if
21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta
R(sgta )
22: end for
23: ta ← Task ta ∈ T a which maximize mrrvta(sta)
24: Resta ← Resta {res}
25: temp ← ∅
26: if res is consumable then
27: temp ← res
28: end if
29: valueta ← valueta + ((Vta(sta) − valueta)×
max
ata∈A(sta,res)
Qta(ata,sta(temp))
Vta(sta)
)
30: until all resource types res ∈ Res are assigned
31: for all ta ∈ T a do
32: Lowta ←lrtdp(Sta, Resta)
33: end for
34: return LowT a
putes valueta. The first part of the equation to compute
valueta represents the expected residual value for task ta.
This term is multiplied by
max
ata∈A(sta)
Qta(ata,sta(res))
Vta(sta)
, which
is the ratio of the efficiency of resource type res. In other
words, valueta is assigned to valueta + (the residual value
× the value ratio of resource type res). For a consumable
resource, the Q-value consider only resource res in the state
space, while for a non-consumable resource, no resources are
available.
All resource types are allocated in this manner until Res is
empty. All consumable and non-consumable resource types
are allocated to each task. When all resources are allocated,
the lower bound components Lowta of each task are 
computed in Line 32. When the global solution is computed,
the lower bound is as follow:
hL(s) = max(SinghL, max
a∈A(s)
ta∈T a
Lowta(sta)) (6)
We use the maximum of the SinghL bound and the sum
of the lower bound components Lowta, thus 
marginalrevenue ≥ SinghL. In particular, the SinghL bound may
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217
be higher when a little number of tasks remain. As the 
components Lowta are computed considering s0; for example, if
in a subsequent state only one task remains, the bound of
SinghL will be higher than any of the Lowta components.
The main difference of complexity between SinghL and
revenue-bound is in Line 32 where a value for each task
has to be computed with the shared resource. However,
since the resource are shared, the state space and action
space is greatly reduced for each task, reducing greatly the
calculus compared to the value functions computed in Line
4 which is done for both SinghL and revenue-bound.
Theorem 2.2. The lower bound of Equation 6 is 
admissible.
Proof: Lowta(sta) is computed with the resource being
shared. Summing the Lowta(sta) value functions for each
ta ∈ T a does not violates the local and global resource 
constraints. Indeed, as the resources are shared, the tasks 
cannot overuse them. Thus, hL(s) is a realizable policy, and an
admissible lower bound.
3. DISCUSSION AND EXPERIMENTS
The domain of the experiments is a naval platform which
must counter incoming missiles (i.e. tasks) by using its 
resources (i.e. weapons, movements). For the experiments,
100 randomly resource allocation problems were generated
for each approach, and possible number of tasks. In our
problem, |Sta| = 4, thus each task can be in four distinct
states. There are two types of states; firstly, states where
actions modify the transition probabilities; and then, there
are goal states. The state transitions are all stochastic 
because when a missile is in a given state, it may always transit
in many possible states. In particular, each resource type
has a probability to counter a missile between 45% and 65%
depending on the state of the task. When a missile is not
countered, it transits to another state, which may be 
preferred or not to the current state, where the most preferred
state for a task is when it is countered. The effectiveness
of each resource is modified randomly by ±15% at the start
of a scenario. There are also local and global resource 
constraints on the amount that may be used. For the local
constraints, at most 1 resource of each type can be 
allocated to execute tasks in a specific state. This constraint is
also present on a real naval platform because of sensor and
launcher constraints and engagement policies. Furthermore,
for consumable resources, the total amount of available 
consumable resource is between 1 and 2 for each type. The
global constraint is generated randomly at the start of a
scenario for each consumable resource type. The number of
resource type has been fixed to 5, where there are 3 
consumable resource types and 2 non-consumable resources types.
For this problem a standard lrtdp approach has been
implemented. A simple heuristic has been used where the
value of an unvisited state is assigned as the value of a goal
state such that all tasks are achieved. This way, the value
of each unvisited state is assured to overestimate its real
value since the value of achieving a task ta is the highest
the planner may get for ta. Since this heuristic is pretty
straightforward, the advantages of using better heuristics are
more evident. Nevertheless, even if the lrtdp approach uses
a simple heuristic, still a huge part of the state space is not
visited when computing the optimal policy. The approaches
described in this paper are compared in Figures 1 and 2.
Lets summarize these approaches here:
• Qdec-lrtdp: The backups are computed using the
Qdec-backup function (Algorithm 1), but in a lrtdp
context. In particular the updates made in the 
checkSolved function are also made using the the 
Qdecbackup function.
• lrtdp-up: The upper bound of maxU is used for
lrtdp.
• Singh-rtdp: The SinghL and SinghU bounds are
used for bounded-rtdp.
• mr-rtdp: The revenue-bound and maxU bounds
are used for bounded-rtdp.
To implement Qdec-lrtdp, we divided the set of tasks
in two equal parts. The set of task T ai, managed by agent
i, can be accomplished with the set of resources Resi, while
the second set of task T ai , managed by agent Agi , can be
accomplished with the set of resources Resi . Resi had one
consumable resource type and one non-consumable resource
type, while Resi had two consumable resource types and
one non-consumable resource type. When the number of
tasks is odd, one more task was assigned to T ai . There are
constraint between the group of resource Resi and Resi
such that some assignments are not possible. These 
constraints are managed by the arbitrator as described in 
Section 2.2. Q-decomposition permits to diminish the planning
time significantly in our problem settings, and seems a very
efficient approach when a group of agents have to allocate
resources which are only available to themselves, but the 
actions made by an agent may influence the reward obtained
by at least another agent.
To compute the lower bound of revenue-bound, all
available resources have to be separated in many types or
parts to be allocated. For our problem, we allocated each
resource of each type in the order of of its specialization like
we said when describing the revenue-bound function.
In terms of experiments, notice that the lrtdp lrtdp-up
and approaches for resource allocation, which doe not prune
the action space, are much more complex. For instance, it
took an average of 1512 seconds to plan for the lrtdp-up
approach with six tasks (see Figure 1). The Singh-rtdp
approach diminished the planning time by using a lower
and upper bound to prune the action space. mr-rtdp 
further reduce the planning time by providing very tight 
initial bounds. In particular, Singh-rtdp needed 231 seconds
in average to solve problem with six tasks and mr-rtdp
required 76 seconds. Indeed, the time reduction is quite
significant compared to lrtdp-up, which demonstrates the
efficiency of using bounds to prune the action space.
Furthermore, we implemented mr-rtdp with the SinghU
bound, and this was slightly less efficient than with the
maxU bound. We also implemented mr-rtdp with the
SinghL bound, and this was slightly more efficient than
Singh-rtdp. From these results, we conclude that the 
difference of efficiency between mr-rtdp and Singh-rtdp is
more attributable to the marginal-revenue lower bound
that to the maxU upper bound. Indeed, when the number
of task to execute is high, the lower bounds by Singh-rtdp
takes the values of a single task. On the other hand, the
lower bound of mr-rtdp takes into account the value of all
1218 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
0.01
0.1
1
10
100
1000
10000
100000
1 2 3 4 5 6 7 8 9 10 11 12 13
Timeinseconds
Number of tasks
LRTDP
QDEC-LRTDP
Figure 1: Efficiency of Q-decomposition LRTDP
and LRTDP.
0.01
0.1
1
10
100
1000
10000
1 2 3 4 5 6 7 8
Timeinseconds
Number of tasks
LRTDP
LRTDP-up
Singh-RTDP
MR-RTDP
Figure 2: Efficiency of MR-RTDP compared to
SINGH-RTDP.
task by using a heuristic to distribute the resources. 
Indeed, an optimal allocation is one where the resources are
distributed in the best way to all tasks, and our lower bound
heuristically does that.
4. CONCLUSION
The experiments have shown that Q-decomposition seems
a very efficient approach when a group of agents have to
allocate resources which are only available to themselves,
but the actions made by an agent may influence the reward
obtained by at least another agent.
On the other hand, when the available resource are
shared, no Q-decomposition is possible and we proposed
tight bounds for heuristic search. In this case, the 
planning time of bounded-rtdp, which prunes the action space,
is significantly lower than for lrtdp. Furthermore, The
marginal revenue bound proposed in this paper compares
favorably to the Singh and Cohn [10] approach. 
boundedrtdp with our proposed bounds may apply to a wide range
of stochastic environments. The only condition for the use
our bounds is that each task possesses consumable and/or
non-consumable limited resources.
An interesting research avenue would be to experiment
our bounds with other heuristic search algorithms. For 
instance, frtdp [11], and brtdp [6] are both efficient 
heuristic search algorithms. In particular, both these approaches
proposed an efficient state trajectory updates, when given
upper and lower bounds. Our tight bounds would enable,
for both frtdp and brtdp, to reduce the number of backup
to perform before convergence. Finally, the bounded-rtdp
function prunes the action space when QU (a, s) ≤ L(s), as
Singh and Cohn [10] suggested. frtdp and brtdp could
also prune the action space in these circumstances to 
further reduce their planning time.
5. REFERENCES
[1] A. Barto, S. Bradtke, and S. Singh. Learning to act
using real-time dynamic programming. Artificial
Intelligence, 72(1):81-138, 1995.
[2] A. Beynier and A. I. Mouaddib. An iterative algorithm
for solving constrained decentralized markov decision
processes. In Proceeding of the Twenty-First National
Conference on Artificial Intelligence (AAAI-06), 2006.
[3] B. Bonet and H. Geffner. Faster heuristic search
algorithms for planning with uncertainty and full
feedback. In Proceedings of the Eighteenth
International Joint Conference on Artificial
Intelligence (IJCAI-03), August 2003.
[4] B. Bonet and H. Geffner. Labeled lrtdp approach:
Improving the convergence of real-time dynamic
programming. In Proceeding of the Thirteenth
International Conference on Automated Planning &
Scheduling (ICAPS-03), pages 12-21, Trento, Italy,
2003.
[5] E. A. Hansen and S. Zilberstein. lao : A heuristic
search algorithm that finds solutions with loops.
Artificial Intelligence, 129(1-2):35-62, 2001.
[6] H. B. McMahan, M. Likhachev, and G. J. Gordon.
Bounded real-time dynamic programming: rtdp with
monotone upper bounds and performance guarantees.
In ICML "05: Proceedings of the Twenty-Second
International Conference on Machine learning, pages
569-576, New York, NY, USA, 2005. ACM Press.
[7] R. S. Pindyck and D. L. Rubinfeld. Microeconomics.
Prentice Hall, 2000.
[8] G. A. Rummery and M. Niranjan. On-line Q-learning
using connectionist systems. Technical report
CUED/FINFENG/TR 166, Cambridge University
Engineering Department, 1994.
[9] S. J. Russell and A. Zimdars. Q-decomposition for
reinforcement learning agents. In ICML, pages
656-663, 2003.
[10] S. Singh and D. Cohn. How to dynamically merge
markov decision processes. In Advances in Neural
Information Processing Systems, volume 10, pages
1057-1063, Cambridge, MA, USA, 1998. MIT Press.
[11] T. Smith and R. Simmons. Focused real-time dynamic
programming for mdps: Squeezing more out of a
heuristic. In Proceedings of the Twenty-First National
Conference on Artificial Intelligence (AAAI), Boston,
USA, 2006.
[12] W. Zhang. Modeling and solving a resource allocation
problem with soft constraint techniques. Technical
report: wucs-2002-13, Washington University,
Saint-Louis, Missouri, 2002.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219
Dynamics Based Control
with an Application to Area-Sweeping Problems
Zinovi Rabinovich
Engineering and Computer
Science
Hebrew University of
Jerusalem
Jerusalem, Israel
nomad@cs.huji.ac.il
Jeffrey S. Rosenschein
Engineering and Computer
Science
Hebrew University of
Jerusalem
Jerusalem, Israel
jeff@cs.huji.ac.il
Gal A. Kaminka
The MAVERICK Group
Department of Computer
Science
Bar Ilan University, Israel
galk@cs.biu.ac.il
ABSTRACT
In this paper we introduce Dynamics Based Control (DBC), an
approach to planning and control of an agent in stochastic 
environments. Unlike existing approaches, which seek to optimize 
expected rewards (e.g., in Partially Observable Markov Decision 
Problems (POMDPs)), DBC optimizes system behavior towards 
specified system dynamics. We show that a recently developed planning
and control approach, Extended Markov Tracking (EMT) is an 
instantiation of DBC. EMT employs greedy action selection to 
provide an efficient control algorithm in Markovian environments. We
exploit this efficiency in a set of experiments that applied 
multitarget EMT to a class of area-sweeping problems (searching for
moving targets). We show that such problems can be naturally 
defined and efficiently solved using the DBC framework, and its EMT
instantiation.
Categories and Subject Descriptors
I.2.8 [Problem Solving, Control Methods, and Search]: 
Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial 
Intelligence]: Intelligent Agents
General Terms
Algorithms, Theory
1. INTRODUCTION
Planning and control constitutes a central research area in 
multiagent systems and artificial intelligence. In recent years, Partially
Observable Markov Decision Processes (POMDPs) [12] have 
become a popular formal basis for planning in stochastic 
environments. In this framework, the planning and control problem is often
addressed by imposing a reward function, and computing a policy
(of choosing actions) that is optimal, in the sense that it will result
in the highest expected utility. While theoretically attractive, the
complexity of optimally solving a POMDP is prohibitive [8, 7].
We take an alternative view of planning in stochastic 
environments. We do not use a (state-based) reward function, but instead
optimize over a different criterion, a transition-based specification
of the desired system dynamics. The idea here is to view 
planexecution as a process that compels a (stochastic) system to change,
and a plan as a dynamic process that shapes that change according
to desired criteria. We call this general planning framework 
Dynamics Based Control (DBC).
In DBC, the goal of a planning (or control) process becomes to
ensure that the system will change in accordance with specific 
(potentially stochastic) target dynamics. As actual system behavior
may deviate from that which is specified by target dynamics (due
to the stochastic nature of the system), planning in such 
environments needs to be continual [4], in a manner similar to classical
closed-loop controllers [16]. Here, optimality is measured in terms
of probability of deviation magnitudes.
In this paper, we present the structure of Dynamics Based 
Control. We show that the recently developed Extended Markov 
Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT
employing greedy action selection, which is a specific 
parameterization among the options possible within DBC. EMT is an efficient
instantiation of DBC.
To evaluate DBC, we carried out a set of experiments applying
multi-target EMT to the Tag Game [11]; this is a variant on the
area sweeping problem, where an agent is trying to tag a moving
target (quarry) whose position is not known with certainty. 
Experimental data demonstrates that even with a simple model of the
environment and a simple design of target dynamics, high success
rates can be produced both in catching the quarry, and in surprising
the quarry (as expressed by the observed entropy of the controlled
agent"s position).
The paper is organized as follows. In Section 2 we motivate DBC
using area-sweeping problems, and discuss related work. Section 3
introduces the Dynamics Based Control (DBC) structure, and its
specialization to Markovian environments. This is followed by a
review of the Extended Markov Tracking (EMT) approach as a
DBC-structured control regimen in Section 4. That section also
discusses the limitations of EMT-based control relative to the 
general DBC framework. Experimental settings and results are then
presented in Section 5. Section 6 provides a short discussion of
the overall approach, and Section 7 gives some concluding remarks
and directions for future work.
790
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
2. MOTIVATION AND RELATED WORK
Many real-life scenarios naturally have a stochastic target 
dynamics specification, especially those domains where there exists
no ultimate goal, but rather system behavior (with specific 
properties) that has to be continually supported. For example, security
guards perform persistent sweeps of an area to detect any sign of
intrusion. Cunning thieves will attempt to track these sweeps, and
time their operation to key points of the guards" motion. It is thus
advisable to make the guards" motion dynamics appear irregular
and random.
Recent work by Paruchuri et al. [10] has addressed such 
randomization in the context of single-agent and distributed POMDPs. The
goal in that work was to generate policies that provide a measure of
action-selection randomization, while maintaining rewards within
some acceptable levels. Our focus differs from this work in that
DBC does not optimize expected rewards-indeed we do not 
consider rewards at all-but instead maintains desired dynamics 
(including, but not limited to, randomization).
The Game of Tag is another example of the applicability of the
approach. It was introduced in the work by Pineau et al. [11]. There
are two agents that can move about an area, which is divided into a
grid. The grid may have blocked cells (holes) into which no agent
can move. One agent (the hunter) seeks to move into a cell 
occupied by the other (the quarry), such that they are co-located (this is
a successful tag). The quarry seeks to avoid the hunter agent, and
is always aware of the hunter"s position, but does not know how the
hunter will behave, which opens up the possibility for a hunter to
surprise the prey. The hunter knows the quarry"s probabilistic law
of motion, but does not know its current location. Tag is an instance
of a family of area-sweeping (pursuit-evasion) problems.
In [11], the hunter modeled the problem using a POMDP. A 
reward function was defined, to reflect the desire to tag the quarry,
and an action policy was computed to optimize the reward 
collected over time. Due to the intractable complexity of determining
the optimal policy, the action policy computed in that paper was
essentially an approximation.
In this paper, instead of formulating a reward function, we use
EMT to solve the problem, by directly specifying the target 
dynamics. In fact, any search problem with randomized motion, the 
socalled class of area sweeping problems, can be described through
specification of such target system dynamics. Dynamics Based
Control provides a natural approach to solving these problems.
3. DYNAMICS BASED CONTROL
The specification of Dynamics Based Control (DBC) can be 
broken into three interacting levels: Environment Design Level, User
Level, and Agent Level.
• Environment Design Level is concerned with the formal
specification and modeling of the environment. For 
example, this level would specify the laws of physics within the
system, and set its parameters, such as the gravitation 
constant.
• User Level in turn relies on the environment model produced
by Environment Design to specify the target system 
dynamics it wishes to observe. The User Level also specifies the 
estimation or learning procedure for system dynamics, and the
measure of deviation. In the museum guard scenario above,
these would correspond to a stochastic sweep schedule, and a
measure of relative surprise between the specified and actual
sweeping.
• Agent Level in turn combines the environment model from
the Environment Design level, the dynamics estimation 
procedure, the deviation measure and the target dynamics 
specification from User Level, to produce a sequence of actions
that create system dynamics as close as possible to the 
targeted specification.
As we are interested in the continual development of a stochastic
system, such as happens in classical control theory [16] and 
continual planning [4], as well as in our example of museum sweeps,
the question becomes how the Agent Level is to treat the 
deviation measurements over time. To this end, we use a probability
threshold-that is, we would like the Agent Level to maximize the
probability that the deviation measure will remain below a certain
threshold.
Specific action selection then depends on system formalization.
One possibility would be to create a mixture of available system
trends, much like that which happens in Behavior-Based Robotic
architectures [1]. The other alternative would be to rely on the 
estimation procedure provided by the User Level-to utilize the 
Environment Design Level model of the environment to choose actions,
so as to manipulate the dynamics estimator into believing that a 
certain dynamics has been achieved. Notice that this manipulation is
not direct, but via the environment. Thus, for strong enough 
estimator algorithms, successful manipulation would mean a successful
simulation of the specified target dynamics (i.e., beyond discerning
via the available sensory input).
DBC levels can also have a back-flow of information (see 
Figure 1). For instance, the Agent Level could provide data about
target dynamics feasibility, allowing the User Level to modify the
requirement, perhaps focusing on attainable features of system 
behavior. Data would also be available about the system response to
different actions performed; combined with a dynamics estimator
defined by the User Level, this can provide an important tool for the
environment model calibration at the Environment Design Level.
UserEnv. Design Agent
Model
Ideal Dynamics
Estimator
Estimator
Dynamics Feasibility
System Response Data
Figure 1: Data flow of the DBC framework
Extending upon the idea of Actor-Critic algorithms [5], DBC
data flow can provide a good basis for the design of a learning 
algorithm. For example, the User Level can operate as an exploratory
device for a learning algorithm, inferring an ideal dynamics target
from the environment model at hand that would expose and verify
most critical features of system behavior. In this case, feasibility
and system response data from the Agent Level would provide key
information for an environment model update. In fact, the 
combination of feasibility and response data can provide a basis for the
application of strong learning algorithms such as EM [2, 9].
3.1 DBC for Markovian Environments
For a Partially Observable Markovian Environment, DBC can
be specified in a more rigorous manner. Notice how DBC discards
rewards, and replaces it by another optimality criterion (structural
differences are summarized in Table 1):
• Environment Design level is to specify a tuple
< S, A, T, O, Ω, s0 >, where:
- S is the set of all possible environment states;
- s0 is the initial state of the environment (which can also
be viewed as a probability distribution over S);
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791
- A is the set of all possible actions applicable in the 
environment;
- T is the environment"s probabilistic transition function:
T : S ×A → Π(S). That is, T(s |a, s) is the 
probability that the environment will move from state s to state
s under action a;
- O is the set of all possible observations. This is what
the sensor input would look like for an outside observer;
- Ω is the observation probability function:
Ω : S × A × S → Π(O).
That is, Ω(o|s , a, s) is the probability that one will
observe o given that the environment has moved from
state s to state s under action a.
• User Level, in the case of Markovian environment, operates
on the set of system dynamics described by a family of 
conditional probabilities F = {τ : S × A → Π(S)}. Thus
specification of target dynamics can be expressed by q ∈ F,
and the learning or tracking algorithm can be represented as
a function L : O×(A×O)∗
→ F; that is, it maps sequences
of observations and actions performed so far into an estimate
τ ∈ F of system dynamics.
There are many possible variations available at the User Level
to define divergence between system dynamics; several of
them are:
- Trace distance or L1 distance between two 
distributions p and q defined by
D(p(·), q(·)) =
1
2 x
|p(x) − q(x)|
- Fidelity measure of distance
F(p(·), q(·)) =
x
p(x)q(x)
- Kullback-Leibler divergence
DKL(p(·) q(·)) =
x
p(x) log
p(x)
q(x)
Notice that the latter two are not actually metrics over the
space of possible distributions, but nevertheless have 
meaningful and important interpretations. For instance, 
KullbackLeibler divergence is an important tool of information 
theory [3] that allows one to measure the price of encoding an
information source governed by q, while assuming that it is
governed by p.
The User Level also defines the threshold of dynamics 
deviation probability θ.
• Agent Level is then faced with a problem of selecting a 
control signal function a∗
to satisfy a minimization problem as
follows:
a∗
= arg min
a
Pr(d(τa, q) > θ)
where d(τa, q) is a random variable describing deviation of
the dynamics estimate τa, created by L under control signal
a, from the ideal dynamics q. Implicit in this minimization
problem is that L is manipulated via the environment, based
on the environment model produced by the Environment 
Design Level.
3.2 DBC View of the State Space
It is important to note the complementary view that DBC and
POMDPs take on the state space of the environment. POMDPs
regard state as a stationary snap-shot of the environment; 
whatever attributes of state sequencing one seeks are reached through
properties of the control process, in this case reward accumulation.
This can be viewed as if the sequencing of states and the attributes
of that sequencing are only introduced by and for the controlling
mechanism-the POMDP policy.
DBC concentrates on the underlying principle of state 
sequencing, the system dynamics. DBC"s target dynamics specification can
use the environment"s state space as a means to describe, discern,
and preserve changes that occur within the system. As a result,
DBC has a greater ability to express state sequencing properties,
which are grounded in the environment model and its state space
definition.
For example, consider the task of moving through rough terrain
towards a goal and reaching it as fast as possible. POMDPs would
encode terrain as state space points, while speed would be ensured
by negative reward for every step taken without reaching the 
goalaccumulating higher reward can be reached only by faster motion.
Alternatively, the state space could directly include the notion of
speed. For POMDPs, this would mean that the same concept is
encoded twice, in some sense: directly in the state space, and 
indirectly within reward accumulation. Now, even if the reward 
function would encode more, and finer, details of the properties of 
motion, the POMDP solution will have to search in a much larger
space of policies, while still being guided by the implicit concept
of the reward accumulation procedure.
On the other hand, the tactical target expression of variations and
correlations between position and speed of motion is now grounded
in the state space representation. In this situation, any further 
constraints, e.g., smoothness of motion, speed limits in different 
locations, or speed reductions during sharp turns, are explicitly and
uniformly expressed by the tactical target, and can result in faster
and more effective action selection by a DBC algorithm.
4. EMT-BASED CONTROL AS A DBC
Recently, a control algorithm was introduced called EMT-based
Control [13], which instantiates the DBC framework. Although it
provides an approximate greedy solution in the DBC sense, initial
experiments using EMT-based control have been encouraging [14,
15]. EMT-based control is based on the Markovian environment
definition, as in the case with POMDPs, but its User and Agent
Levels are of the Markovian DBC type of optimality.
• User Level of EMT-based control defines a limited-case 
target system dynamics independent of action:
qEMT : S → Π(S).
It then utilizes the Kullback-Leibler divergence measure to
compose a momentary system dynamics estimator-the 
Extended Markov Tracking (EMT) algorithm. The algorithm
keeps a system dynamics estimate τt
EMT that is capable of
explaining recent change in an auxiliary Bayesian system
state estimator from pt−1 to pt, and updates it conservatively
using Kullback-Leibler divergence. Since τt
EMT and pt−1,t
are respectively the conditional and marginal probabilities
over the system"s state space, explanation simply means
that
pt(s ) =
s
τt
EMT (s |s)pt−1(s),
and the dynamics estimate update is performed by solving a
792 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment
Level
Approach
MDP Markovian DBC
Environment
< S, A, T, O, Ω >,where
S - set of states
A - set of actions
Design
T : S × A → Π(S) - transition
O - observation set
Ω : S × A × S → Π(O)
User
r : S × A × S → R q : S × A → Π(S)
F(π∗
) → r L(o1, ..., ot) → τ
r - reward function q - ideal dynamics
F - reward remodeling L - dynamics estimator
θ - threshold
Agent π∗
= arg max
π
E[ γt
rt] π∗
= arg min
π
Prob(d(τ q) > θ)
minimization problem:
τt
EMT = H[pt, pt−1, τt−1
EMT ]
= arg min
τ
DKL(τ × pt−1 τt−1
EMT × pt−1)
s.t.
pt(s ) =
s
(τ × pt−1)(s , s)
pt−1(s) =
s
(τ × pt−1)(s , s)
• Agent Level in EMT-based control is suboptimal with 
respect to DBC (though it remains within the DBC 
framework), performing greedy action selection based on 
prediction of EMT"s reaction. The prediction is based on the 
environment model provided by the Environment Design level,
so that if we denote by Ta the environment"s transition 
function limited to action a, and pt−1 is the auxiliary Bayesian
system state estimator, then the EMT-based control choice is
described by
a∗
= arg min
a∈A
DKL(H[Ta × pt, pt, τt
EMT ] qEMT × pt−1)
Note that this follows the Markovian DBC framework precisely:
the rewarding optimality of POMDPs is discarded, and in its place
a dynamics estimator (EMT in this case) is manipulated via action
effects on the environment to produce an estimate close to the 
specified target system dynamics. Yet as we mentioned, naive 
EMTbased control is suboptimal in the DBC sense, and has several 
additional limitations that do not exist in the general DBC framework
(discussed in Section 4.2).
4.1 Multi-Target EMT
At times, there may exist several behavioral preferences. For
example, in the case of museum guards, some art items are more
heavily guarded, requiring that the guards stay more often in their
close vicinity. On the other hand, no corner of the museum is to
be left unchecked, which demands constant motion. Successful
museum security would demand that the guards adhere to, and 
balance, both of these behaviors. For EMT-based control, this would
mean facing several tactical targets {qk}K
k=1, and the question 
becomes how to merge and balance them. A balancing mechanism
can be applied to resolve this issue.
Note that EMT-based control, while selecting an action, creates
a preference vector over the set of actions based on their predicted
performance with respect to a given target. If these preference 
vectors are normalized, they can be combined into a single unified 
preference. This requires replacement of standard EMT-based action
selection by the algorithm below [15]:
• Given:
- a set of target dynamics {qk}K
k=1,
- vector of weights w(k)
• Select action as follows
- For each action a ∈ A predict the future state 
distribution ¯pa
t+1 = Ta ∗ pt;
- For each action, compute
Da = H(¯pa
t+1, pt, PDt)
- For each a ∈ A and qk tactical target, denote
V (a, k) = DKL (Da qk) pt
.
Let Vk(a) = 1
Zk
V (a, k), where Zk =
a∈A
V (a, k) is
a normalization factor.
- Select a∗
= arg min
a
k
k=1 w(k)Vk(a)
The weights vector w = (w1, ..., wK ) allows the additional
tuning of importance among target dynamics without the need
to redesign the targets themselves. This balancing method is also
seamlessly integrated into the EMT-based control flow of 
operation.
4.2 EMT-based Control Limitations
EMT-based control is a sub-optimal (in the DBC sense) 
representative of the DBC structure. It limits the User by forcing EMT to
be its dynamic tracking algorithm, and replaces Agent optimization
by greedy action selection. This kind of combination, however, is
common for on-line algorithms. Although further development of
EMT-based controllers is necessary, evidence so far suggests that
even the simplest form of the algorithm possesses a great deal of
power, and displays trends that are optimal in the DBC sense of the
word.
There are two further, EMT-specific, limitations to EMT-based
control that are evident at this point. Both already have partial 
solutions and are subjects of ongoing research.
The first limitation is the problem of negative preference. In the
POMDP framework for example, this is captured simply, through
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793
the appearance of values with different signs within the reward
structure. For EMT-based control, however, negative preference
means that one would like to avoid a certain distribution over 
system development sequences; EMT-based control, however, 
concentrates on getting as close as possible to a distribution. Avoidance is
thus unnatural in native EMT-based control.
The second limitation comes from the fact that standard 
environment modeling can create pure sensory actions-actions that do
not change the state of the world, and differ only in the way 
observations are received and the quality of observations received. Since
the world state does not change, EMT-based control would not be
able to differentiate between different sensory actions.
Notice that both of these limitations of EMT-based control are
absent from the general DBC framework, since it may have a 
tracking algorithm capable of considering pure sensory actions and, 
unlike Kullback-Leibler divergence, a distribution deviation measure
that is capable of dealing with negative preference.
5. EMT PLAYING TAG
The Game of Tag was first introduced in [11]. It is a single agent
problem of capturing a quarry, and belongs to the class of area
sweeping problems. An example domain is shown in Figure 2.
0 51 2 3 4 6
7 8 10 12 13
161514
17 18 19
2221
23
9 11Q A
20
Figure 2: Tag domain; an agent (A) attempts to seek and 
capture a quarry (Q)
The Game of Tag extremely limits the agent"s perception, so that
the agent is able to detect the quarry only if they are co-located in
the same cell of the grid world. In the classical version of the game,
co-location leads to a special observation, and the ‘Tag" action can
be performed. We slightly modify this setting: the moment that
both agents occupy the same cell, the game ends. As a result, both
the agent and its quarry have the same motion capability, which
allows them to move in four directions, North, South, East, and
West. These form a formal space of actions within a Markovian
environment.
The state space of the formal Markovian environment is described
by the cross-product of the agent and quarry"s positions. For 
Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.
The effects of an action taken by the agent are deterministic, but
the environment in general has a stochastic response due to the 
motion of the quarry. With probability q0
1
it stays put, and with 
probability 1 − q0 it moves to an adjacent cell further away from the
1
In our experiments this was taken to be q0 = 0.2.
agent. So for the instance shown in Figure 2 and q0 = 0.1:
P(Q = s9|Q = s9, A = s11) = 0.1
P(Q = s2|Q = s9, A = s11) = 0.3
P(Q = s8|Q = s9, A = s11) = 0.3
P(Q = s14|Q = s9, A = s11) = 0.3
Although the evasive behavior of the quarry is known to the
agent, the quarry"s position is not. The only sensory information
available to the agent is its own location.
We use EMT and directly specify the target dynamics. For the
Game of Tag, we can easily formulate three major trends: catching
the quarry, staying mobile, and stalking the quarry. This results in
the following three target dynamics:
Tcatch(At+1 = si|Qt = sj, At = sa) ∝
1 si = sj
0 otherwise
Tmobile(At+1 = si|Qt = so, At = sj) ∝
0 si = sj
1 otherwise
Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1
dist(si,so)
Note that none of the above targets are directly achievable; for
instance, if Qt = s9 and At = s11, there is no action that can move
the agent to At+1 = s9 as required by the Tcatch target dynamics.
We ran several experiments to evaluate EMT performance in the
Tag Game. Three configurations of the domain shown in Figure 3
were used, each posing a different challenge to the agent due to 
partial observability. In each setting, a set of 1000 runs was performed
with a time limit of 100 steps. In every run, the initial position of
both the agent and its quarry was selected at random; this means
that as far as the agent was concerned, the quarry"s initial position
was uniformly distributed over the entire domain cell space.
We also used two variations of the environment observability
function. In the first version, observability function mapped all
joint positions of hunter and quarry into the position of the hunter as
an observation. In the second, only those joint positions in which
hunter and quarry occupied different locations were mapped into
the hunter"s location. The second version in fact utilized and 
expressed the fact that once hunter and quarry occupy the same cell
the game ends.
The results of these experiments are shown in Table 2. 
Balancing [15] the catch, move, and stalk target dynamics described in
the previous section by the weight vector [0.8, 0.1, 0.1], EMT 
produced stable performance in all three domains.
Although direct comparisons are difficult to make, the EMT 
performance displayed notable efficiency vis-`a-vis the POMDP 
approach. In spite of a simple and inefficient Matlab implementation
of the EMT algorithm, the decision time for any given step 
averaged significantly below 1 second in all experiments. For the 
irregular open arena domain, which proved to be the most difficult, 1000
experiment runs bounded by 100 steps each, a total of 42411 steps,
were completed in slightly under 6 hours. That is, over 4 × 104
online steps took an order of magnitude less time than the offline
computation of POMDP policy in [11]. The significance of this 
differential is made even more prominent by the fact that, should the
environment model parameters change, the online nature of EMT
would allow it to maintain its performance time, while the POMDP
policy would need to be recomputed, requiring yet again a large
overhead of computation.
We also tested the behavior cell frequency entropy, empirical
measures from trial data. As Figure 4 and Figure 5 show, 
empir794 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
A
Q
Q A
0 1 2 3 4
5 6 7 8 9
10 11 12
13 14 15
16 17 18
A
Q
Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor
Table 2: Performance of the EMT-based solution in three Tag
Game domains and two observability models: I) omniposition
quarry, II) quarry is not at hunter"s position
Model Domain Capture% E(Steps) Time/Step
I
Dead-ends 100 14.8 72(mSec)
Arena 80.2 42.4 500(mSec)
Circle 91.4 34.6 187(mSec)
II
Dead-ends 100 13.2 91(mSec)
Arena 96.8 28.67 396(mSec)
Circle 94.4 31.63 204(mSec)
ical entropy grows with the length of interaction. For runs where
the quarry was not captured immediately, the entropy reaches 
between 0.85 and 0.952
for different runs and scenarios. As the agent
actively seeks the quarry, the entropy never reaches its maximum.
One characteristic of the entropy graph for the open arena 
scenario particularly caught our attention in the case of the 
omniposition quarry observation model. Near the maximum limit of trial
length (100 steps), entropy suddenly dropped. Further analysis of
the data showed that under certain circumstances, a fluctuating 
behavior occurs in which the agent faces equally viable versions of
quarry-following behavior. Since the EMT algorithm has greedy
action selection, and the state space does not encode any form of
commitment (not even speed or acceleration), the agent is locked
within a small portion of cells. It is essentially attempting to 
simultaneously follow several courses of action, all of which are 
consistent with the target dynamics. This behavior did not occur in our
second observation model, since it significantly reduced the set of
eligible courses of action-essentially contributing to tie-breaking
among them.
6. DISCUSSION
The design of the EMT solution for the Tag Game exposes the
core difference in approach to planning and control between EMT
or DBC, on the one hand, and the more familiar POMDP approach,
on the other. POMDP defines a reward structure to optimize, and
influences system dynamics indirectly through that optimization.
EMT discards any reward scheme, and instead measures and 
influences system dynamics directly.
2
Entropy was calculated using log base equal to the number of 
possible locations within the domain; this properly scales entropy 
expression into the range [0, 1] for all domains.
Thus for the Tag Game, we did not search for a reward function
that would encode and express our preference over the agent"s 
behavior, but rather directly set three (heuristic) behavior preferences
as the basis for target dynamics to be maintained. Experimental
data shows that these targets need not be directly achievable via the
agent"s actions. However, the ratio between EMT performance and
achievability of target dynamics remains to be explored.
The tag game experiment data also revealed the different 
emphasis DBC and POMDPs place on the formulation of an environment
state space. POMDPs rely entirely on the mechanism of reward
accumulation maximization, i.e., formation of the action selection
procedure to achieve necessary state sequencing. DBC, on the
other hand, has two sources of sequencing specification: through
the properties of an action selection procedure, and through direct
specification within the target dynamics. The importance of the
second source was underlined by the Tag Game experiment data,
in which the greedy EMT algorithm, applied to a POMDP-type
state space specification, failed, since target description over such a
state space was incapable of encoding the necessary behavior 
tendencies, e.g., tie-breaking and commitment to directed motion.
The structural differences between DBC (and EMT in 
particular), and POMDPs, prohibits direct performance comparison, and
places them on complementary tracks, each within a suitable niche.
For instance, POMDPs could be seen as a much more natural 
formulation of economic sequential decision-making problems, while
EMT is a better fit for continual demand for stochastic change, as
happens in many robotic or embodied-agent problems.
The complementary properties of POMDPs and EMT can be 
further exploited. There is recent interest in using POMDPs in hybrid
solutions [17], in which the POMDPs can be used together with
other control approaches to provide results not easily achievable
with either approach by itself. DBC can be an effective partner in
such a hybrid solution. For instance, POMDPs have prohibitively
large off-line time requirements for policy computation, but can
be readily used in simpler settings to expose beneficial behavioral
trends; this can serve as a form of target dynamics that are provided
to EMT in a larger domain for on-line operation.
7. CONCLUSIONS AND FUTURE WORK
In this paper, we have presented a novel perspective on the 
process of planning and control in stochastic environments, in the form
of the Dynamics Based Control (DBC) framework. DBC 
formulates the task of planning as support of a specified target system 
dynamics, which describes the necessary properties of change within
the environment. Optimality of DBC plans of action are measured
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795
0 20 40 60 80 100
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Steps
Entropy
Dead−ends
0 20 40 60 80 100
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Steps
Entropy
Arena
0 20 40 60 80 100
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Steps
Entropy
Circle
Figure 4: Observation Model I: Omniposition quarry. Entropy development with length of Tag Game for the three experiment
scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.
0 10 20 30 40 50 60
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Steps
Entropy
Dead−ends
0 20 40 60 80 100
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Steps
Entropy
Arena
0 20 40 60 80 100
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Steps
Entropy
Circle
Figure 5: Observation Model II: quarry not observed at hunter"s position. Entropy development with length of Tag Game for the
three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.
with respect to the deviation of actual system dynamics from the
target dynamics.
We show that a recently developed technique of Extended Markov
Tracking (EMT) [13] is an instantiation of DBC. In fact, EMT can
be seen as a specific case of DBC parameterization, which employs
a greedy action selection procedure.
Since EMT exhibits the key features of the general DBC 
framework, as well as polynomial time complexity, we used the 
multitarget version of EMT [15] to demonstrate that the class of area
sweeping problems naturally lends itself to dynamics-based 
descriptions, as instantiated by our experiments in the Tag Game 
domain.
As enumerated in Section 4.2, EMT has a number of 
limitations, such as difficulty in dealing with negative dynamic 
preference. This prevents direct application of EMT to such problems
as Rendezvous-Evasion Games (e.g., [6]). However, DBC in 
general has no such limitations, and readily enables the formulation
of evasion games. In future work, we intend to proceed with the
development of dynamics-based controllers for these problems.
8. ACKNOWLEDGMENT
The work of the first two authors was partially supported by 
Israel Science Foundation grant #898/05, and the third author was
partially supported by a grant from Israel"s Ministry of Science and
Technology.
9. REFERENCES
[1] R. C. Arkin. Behavior-Based Robotics. MIT Press, 1998.
[2] J. A. Bilmes. A gentle tutorial of the EM algorithm and its
application to parameter estimation for Gaussian mixture and
Hidden Markov Models. Technical Report TR-97-021,
Department of Electrical Engeineering and Computer
Science, University of California at Berkeley, 1998.
[3] T. M. Cover and J. A. Thomas. Elements of information
theory. Wiley, 1991.
[4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J.
Wolverton. A survey of research in distributed, continual
planning. AI Magazine, 4:13-22, 1999.
[5] V. R. Konda and J. N. Tsitsiklis. Actor-Critic algorithms.
SIAM Journal on Control and Optimization,
42(4):1143-1166, 2003.
[6] W. S. Lim. A rendezvous-evasion game on discrete locations
with joint randomization. Advances in Applied Probability,
29(4):1004-1017, December 1997.
[7] M. L. Littman, T. L. Dean, and L. P. Kaelbling. On the
complexity of solving Markov decision problems. In
Proceedings of the 11th Annual Conference on Uncertainty
in Artificial Intelligence (UAI-95), pages 394-402, 1995.
[8] O. Madani, S. Hanks, and A. Condon. On the undecidability
of probabilistic planning and related stochastic optimization
problems. Artificial Intelligence Journal, 147(1-2):5-34,
July 2003.
[9] R. M. Neal and G. E. Hinton. A view of the EM algorithm
796 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
that justifies incremental, sparse, and other variants. In M. I.
Jordan, editor, Learning in Graphical Models, pages
355-368. Kluwer Academic Publishers, 1998.
[10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus. Security
in multiagent systems by policy randomization. In
Proceeding of AAMAS 2006, 2006.
[11] J. Pineau, G. Gordon, and S. Thrun. Point-based value
iteration: An anytime algorithm for pomdps. In International
Joint Conference on Artificial Intelligence (IJCAI), pages
1025-1032, August 2003.
[12] M. L. Puterman. Markov Decision Processes. Wiley Series in
Probability and Mathematical Statistics: Applied Probability
and Statistics Section. Wiley-Interscience Publication, New
York, 1994.
[13] Z. Rabinovich and J. S. Rosenschein. Extended Markov
Tracking with an application to control. In The Workshop on
Agent Tracking: Modeling Other Agents from Observations,
at the Third International Joint Conference on Autonomous
Agents and Multiagent Systems, pages 95-100, New-York,
July 2004.
[14] Z. Rabinovich and J. S. Rosenschein. Multiagent
coordination by Extended Markov Tracking. In The Fourth
International Joint Conference on Autonomous Agents and
Multiagent Systems, pages 431-438, Utrecht, The
Netherlands, July 2005.
[15] Z. Rabinovich and J. S. Rosenschein. On the response of
EMT-based control to interacting targets and models. In The
Fifth International Joint Conference on Autonomous Agents
and Multiagent Systems, pages 465-470, Hakodate, Japan,
May 2006.
[16] R. F. Stengel. Optimal Control and Estimation. Dover
Publications, 1994.
[17] M. Tambe, E. Bowring, H. Jung, G. Kaminka,
R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce,
P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and
P. Varakantham. Conflicts in teamwork: Hybrids to the
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797
Rumours and Reputation: Evaluating Multi-Dimensional
Trust within a Decentralised Reputation System
Steven Reece1
, Alex Rogers2
, Stephen Roberts1
and Nicholas R. Jennings2
1
Department of Engineering Science, University of Oxford, Oxford, OX1 3PJ, UK.
{reece,sjrob}@robots.ox.ac.uk
2
Electronics and Computer Science, University of Southampton, Southampton, SO17 1BJ, UK.
{acr,nrj}@ecs.soton.ac.uk
ABSTRACT
In this paper we develop a novel probabilistic model of 
computational trust that explicitly deals with correlated multi-dimensional
contracts. Our starting point is to consider an agent attempting to
estimate the utility of a contract, and we show that this leads to a
model of computational trust whereby an agent must determine a
vector of estimates that represent the probability that any 
dimension of the contract will be successfully fulfilled, and a covariance
matrix that describes the uncertainty and correlations in these 
probabilities. We present a formalism based on the Dirichlet distribution
that allows an agent to calculate these probabilities and correlations
from their direct experience of contract outcomes, and we show that
this leads to superior estimates compared to an alternative approach
using multiple independent beta distributions. We then show how
agents may use the sufficient statistics of this Dirichlet distribution
to communicate and fuse reputation within a decentralised 
reputation system. Finally, we present a novel solution to the problem
of rumour propagation within such systems. This solution uses the
notion of private and shared information, and provides estimates
consistent with a centralised reputation system, whilst maintaining
the anonymity of the agents, and avoiding bias and overconfidence.
Categories and Subject Descriptors
I.2.11 [Distributed Artificial Intelligence]: Intelligent agents
General Terms
Algorithms, Design, Theory
1. INTRODUCTION
The role of computational models of trust within multi-agent 
systems in particular, and open distributed systems in general, has 
recently generated a great deal of research interest. In such systems,
agents must typically choose between interaction partners, and in
this context trust can be viewed to provide a means for agents to
represent and estimate the reliability with which these interaction
partners will fulfill their commitments. To date, however, much of
the work within this area has used domain specific or ad-hoc trust
metrics, and has focused on providing heuristics to evaluate and
update these metrics using direct experience and reputation reports
from other agents (see [8] for a review).
Recent work has attempted to place the notion of computational
trust within the framework of probability theory [6, 11]. This 
approach allows many of the desiderata of computational trust models
to be addressed through principled means. In particular: (i) it 
allows agents to update their estimates of the trustworthiness of a
supplier as they acquire direct experience, (ii) it provides a 
natural framework for agents to express their uncertainty this 
trustworthiness, and, (iii) it allows agents to exchange, combine and filter
reputation reports received from other agents.
Whilst this approach is attractive, it is somewhat limited in that it
has so far only considered single dimensional outcomes (i.e. whether
the contract has succeeded or failed in its entirety). However, in
many real world settings the success or failure of an interaction may
be decomposed into several dimensions [7]. This presents the 
challenge of combining these multiple dimensions into a single metric
over which a decision can be made. Furthermore, these dimensions
will typically also exhibit correlations. For example, a contract
within a supply chain may specify criteria for timeliness, quality
and quantity. A supplier who is suffering delays may attempt a
trade-off between these dimensions by supplying the full amount
late, or supplying as much as possible (but less than the quantity
specified within the contract) on time. Thus, correlations will 
naturally arise between these dimensions, and hence, between the 
probabilities that describe the successful fulfillment of each contract 
dimension. To date, however, no such principled framework exists
to describe these multi-dimensional contracts, nor the correlations
between these dimensions (although some ad-hoc models do exist
- see section 2 for more details).
To rectify this shortcoming, in this paper we develop a 
probabilistic model of computational trust that explicitly deals with 
correlated multi-dimensional contracts. The starting point for our work
is to consider how an agent can estimate the utility that it will derive
from interacting with a supplier. Here we use standard approaches
from the literature of data fusion (since this is a well developed
field where the notion of multi-dimensional correlated estimates is
well established1
) to show that this naturally leads to a trust model
where the agent must estimate probabilities and correlations over
1
In this context, the multiple dimensions typically represent the
physical coordinates of a target being tracked, and correlations arise
through the operation and orientation of sensors.
1070
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
multiple dimensions. Building upon this, we then devise a novel
trust model that addresses the three desiderata discussed above. In
more detail, in this paper we extend the state of the art in four key
ways:
1. We devise a novel multi-dimensional probabilistic trust model
that enables an agent to estimate the expected utility of a 
contract, by estimating (i) the probability that each contract 
dimension will be successfully fulfilled, and (ii) the 
correlations between these estimates.
2. We present an exact probabilistic model based upon the 
Dirichlet distribution that allows agents to use their direct 
experience of contract outcomes to calculate the probabilities and
correlations described above. We then benchmark this 
solution and show that it leads to good estimates.
3. We show that agents can use the sufficient statistics of this
Dirichlet distribution in order to exchange reputation reports
with one another. The sufficient statistics represent 
aggregations of their direct experience, and thus, express contract
outcomes in a compact format with no loss of information.
4. We show that, while being efficient, the aggregation of 
contract outcomes can lead to double counting, and rumour 
propagation, in decentralised reputation systems. Thus, we present
a novel solution based upon the idea of private and shared 
information. We show that it yields estimates consistent with a
centralised reputation system, whilst maintaining the anonymity
of the agents, and avoiding overconfidence.
The remainder of this paper is organised as follows: in section 2 we
review related work. In section 3 we present our notation for a 
single dimensional contract, before introducing our multi-dimensional
trust model in section 4. In sections 5 and 6 we discuss 
communicating reputation, and present our solution to rumour propagation
in decentralised reputation systems. We conclude in section 7.
2. RELATED WORK
The need for a multi-dimensional trust model has been recognised
by a number of researchers. Sabater and Sierra present a model of
reputation, in which agents form contracts based on multiple 
variables (such as delivery date and quality), and define impressions as
subjective evaluations of the outcome of these contracts. They 
provide heuristic approaches to combining these impressions to form
a measure they call subjective reputation.
Likewise, Griffiths decomposes overall trust into a number of
different dimensions such as success, cost, timeliness and 
quality [4]. In his case, each dimension is scored as a real number
that represents a comparative value with no strong semantic 
meaning. He develops an heuristic rule to update these values based
on the direct experiences of the individual agent, and an heuristic
function that takes the individual trust dimensions and generates a
single scalar that is then used to select between suppliers. Whilst,
he comments that the trust values could have some associated 
confidence level, heuristics for updating these levels are not presented.
Gujral et al. take a similar approach and present a trust model
over multiple domain specific dimensions [5]. They define 
multidimensional goal requirements, and evaluate an expected payoff
based on a supplier"s estimated behaviour. These estimates are,
however, simple aggregations over the direct experience of several
agents, and there is no measure of the uncertainty. Nevertheless,
they show that agents who select suppliers based on these multiple
dimensions outperform those who consider just a single one.
By contrast, a number of researchers have presented more 
principled computational trust models based on probability theory, albeit
limited to a single dimension. Jøsang and Ismail describe the Beta
Reputation System whereby the reputation of an agent is compiled
from the positive and negative reports from other agents who have
interacted with it [6]. The beta distribution represents a natural
choice for representing these binary outcomes, and it provides a
principled means of representing uncertainty. Moreover, they 
provide a number of extensions to this initial model including an 
approach to exchanging reputation reports using the sufficient 
statistics of the beta distribution, methods to discount the opinions of
agents who themselves have low reputation ratings, and techniques
to deal with reputations that may change over time.
Likewise, Teacy et al. use the beta distribution to describe an
agent"s belief in the probability that another agent will 
successfully fulfill its commitments [11]. They present a formalism using
a beta distribution that allows the agent to estimate this probability
based upon its direct experience, and again they use the sufficient
statistics of this distribution to communicate this estimate to other
agents. They provide a number of extensions to this initial model,
and, in particular, they consider that agents may not always 
truthfully report their trust estimates. Thus, they present a principled
approach to detecting and removing inconsistent reports.
Our work builds upon these more principled approaches. 
However, the starting point of our approach is to consider an agent that
is attempting to estimate the expected utility of a contract. We show
that estimating this expected utility requires that an agent must 
estimate the probability with which the supplier will fulfill its contract.
In the single-dimensional case, this naturally leads to a trust model
using the beta distribution (as per Jøsang and Ismail and Teacy et
al.). However, we then go on to extend this analysis to multiple
dimensions, where we use the natural extension of the beta 
distribution, namely the Dirichlet distribution, to represent the agent"s
belief over multiple dimensions.
3. SINGLE-DIMENSIONAL TRUST
Before presenting our multi-dimensional trust model, we first 
introduce the notation and formalism that we will use by describing the
more familiar single dimensional case. We consider an agent who
must decide whether to engage in a future contract with a supplier.
This contract will lead to some outcome, o, and we consider that
o = 1 if the contract is successfully fulfilled, and o = 0 if not2
.
In order for the agent to make a rational decision, it should 
consider the utility that it will derive from this contract. We assume
that in the case that the contract is successfully fulfilled, the agent
derives a utility u(o = 1), otherwise it receives no utility3
. Now,
given that the agent is uncertain of the reliability with which the
supplier will fulfill the contract, it should consider the expected
utility that it will derive, E[U], and this is given by:
E[U] = p(o = 1)u(o = 1) (1)
where p(o = 1) is the probability that the supplier will successfully
fulfill the contract. However, whilst u(o = 1) is known by the
agent, p(o = 1) is not. The best the agent can do is to determine
a distribution over possible values of p(o = 1) given its direct
experience of previous contract outcomes. Given that it has been
able to do so, it can then determine an estimate of the expected
utility4
of the contract, E[E[U]], and a measure of its uncertainty
in this expected utility, Var(E[U]). This uncertainty is important
since a risk averse agent may make a decision regarding a contract,
2
Note that we only consider binary contract outcomes, although
extending this to partial outcomes is part of our future work.
3
Clearly this can be extended to the case where some utility is 
derived from an unsuccessful outcome.
4
Note that this is often called the expected expected utility, and
this is the notation that we adopt here [2].
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1071
not only on its estimate of the expected utility of the contract, but
also on the probability that the expected utility will exceed some
minimum amount. These two properties are given by:
E[E[U]] = ˆp(o = 1)u(o = 1) (2)
Var(E[U]) = Var(p(o = 1))u(o = 1)2
(3)
where ˆp(o = 1) and Var(p(o = 1)) are the estimate and 
uncertainty of the probability that a contract will be successfully 
fulfilled, and are calculated from the distribution over possible values
of p(o = 1) that the agent determines from its direct experience.
The utility based approach that we present here provides an 
attractive motivation for this model of Teacy et al. [11].
Now, in the case of binary contract outcomes, the beta 
distribution is the natural choice to represent the distribution over possible
values of p(o = 1) since within Bayesian statistics this well known
to be the conjugate prior for binomial observations [3]. By adopting
the beta distribution, we can calculate ˆp(o = 1) and Var(p(o = 1))
using standard results, and thus, if an agent observed N previous
contracts of which n were successfully fulfilled, then:
ˆp(o = 1) =
n + 1
N + 2
and:
Var(p(o = 1)) =
(n + 1)(N − n + 1)
(N + 2)2(N + 3)
Note that as expected, the greater the number of contracts the agent
observes, the smaller the variance term Var(p(o = 1)), and, thus,
the less the uncertainty regarding the probability that a contract will
be successfully fulfilled, ˆp(o = 1).
4. MULTI-DIMENSIONAL TRUST
We now extend the description above, to consider contracts 
between suppliers and agents that are represented by multiple 
dimensions, and hence the success or failure of a contract can be
decomposed into the success or failure of each separate 
dimension. Consider again the example of the supply chain that 
specifies the timeliness, quantity, and quality of the goods that are to
be delivered. Thus, within our trust model oa = 1 now 
indicates a successful outcome over dimension a of the contract and
oa = 0 indicates an unsuccessful one. A contract outcome, X,
is now composed of a vector of individual contract part outcomes
(e.g. X = {oa = 1, ob = 0, oc = 0, . . .}).
Given a multi-dimensional contract whose outcome is described
by the vector X, we again consider that in order for an agent to
make a rational decision, it should consider the utility that it will
derive from this contract. To this end, we can make the general
statement that the expected utility of a contract is given by:
E[U] = p(X)U(X)T
(4)
where p(X) is a joint probability distribution over all possible 
contract outcomes:
p(X) =
⎛
⎜
⎜
⎜
⎝
p(oa = 1, ob = 0, oc = 0, . . .)
p(oa = 1, ob = 1, oc = 0, . . .)
p(oa = 0, ob = 1, oc = 0, . . .)
...
⎞
⎟
⎟
⎟
⎠
(5)
and U(X) is the utility derived from these possible outcomes:
U(X) =
⎛
⎜
⎜
⎜
⎝
u(oa = 1, ob = 0, oc = 0, . . .)
u(oa = 1, ob = 1, oc = 0, . . .)
u(oa = 0, ob = 1, oc = 0, . . .)
...
⎞
⎟
⎟
⎟
⎠
(6)
As before, whilst U(X) is known to the agent, the probability 
distribution p(X) is not. Rather, given the agent"s direct experience
of the supplier, the agent can determine a distribution over possible
values for p(X). In the single dimensional case, a beta distribution
was the natural choice over possible values of p(o = 1). In the
multi-dimensional case, where p(X) itself is a vector of 
probabilities, the corresponding natural choice is the Dirichlet distribution,
since this is a conjugate prior for multinomial proportions [3].
Given this distribution, the agent is then able to calculate an 
estimate of the expected utility of a contract. As before, this estimate
is itself represented by an expected value given by:
E[E[U]] = ˆp(X)U(X)T
(7)
and a variance, describing the uncertainty in this expected utility:
Var(E[U]) = U(X)Cov(p(X))U(X)T
(8)
where:
Cov(p(X)) E[(p(X) − ˆp(X))(p(X) − ˆp(X))T
] (9)
Thus, whilst the single dimensional case naturally leads to a trust
model in which the agents attempt to derive an estimate of 
probability that a contract will be successfully fulfilled, ˆp(o = 1), along
with a scalar variance that describes the uncertainty in this 
probability, Var(p(o = 1)), in this case, the agents must derive an 
estimate of a vector of probabilities, ˆp(X), along with a covariance
matrix, Cov(p(X)), that represents the uncertainty in p(X) given
the observed contractual outcomes. At this point, it is interesting
to note that the estimate in the single dimensional case, ˆp(o = 1),
has a clear semantic meaning in relation to trust; it is the agent"s
belief in the probability of a supplier successfully fulfilling a 
contract. However, in the multi-dimensional case the agent must 
determine ˆp(X), and since this describes the probability of all possible
contract outcomes, including those that are completely un-fulfilled,
this direct semantic interpretation is not present. In the next 
section, we describe the exemplar utility function that we shall use in
the remainder of this paper.
4.1 Exemplar Utility Function
The approach described so far is completely general, in that it 
applies to any utility function of the form described above, and also
applies to the estimation of any joint probability distribution. In
the remainder of this paper, for illustrative purposes, we shall limit
the discussion to the simplest possible utility function that exhibits
a dependence upon the correlations between the contract 
dimensions. That is, we consider the case that expected utility is 
dependent only on the marginal probabilities of each contract dimension
being successfully fulfilled, rather than the full joint probabilities:
U(X) =
⎛
⎜
⎜
⎜
⎝
u(oa = 1)
u(ob = 1)
u(oc = 1)
...
⎞
⎟
⎟
⎟
⎠
(10)
Thus, ˆp(X) is a vector estimate of the probability of each contract
dimension being successfully fulfilled, and maintains the clear 
semantic interpretation seen in the single dimensional case:
ˆp(X) =
⎛
⎜
⎜
⎜
⎝
ˆp(oa = 1)
ˆp(ob = 1)
ˆp(oc = 1)
...
⎞
⎟
⎟
⎟
⎠
(11)
The correlations between the contract dimensions affect the 
uncertainty in the expected utility. To see this, consider the covariance
1072 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
matrix that describes this uncertainty, Cov(p(X)), is now given by:
Cov(p(X)) =
⎛
⎜
⎜
⎜
⎝
Va Cab Cac . . .
Cab Vb Cbc . . .
Cac Cbc Vc . . .
...
...
...
⎞
⎟
⎟
⎟
⎠
(12)
In this matrix, the diagonal terms, Va, Vb and Vc, represent the
uncertainties in p(oa = 1), p(ob = 1) and p(oc = 1) within
p(X). The off-diagonal terms, Cab, Cac and Cbc, represent the
correlations between these probabilities. In the next section, we use
the Dirichlet distribution to calculate both ˆp(X) and Cov(p(X))
from an agent"s direct experience of previous contract outcomes.
We first illustrate why this is necessary by considering an 
alternative approach to modelling multi-dimensional contracts whereby
an agent na¨ıvely assumes that the dimensions are independent, and
thus, it models each individually by separate beta distributions (as
in the single dimensional case we presented in section 3). This
is actually equivalent to setting the off-diagonal terms within the
covariance matrix, Cov(p(X)), to zero. However, doing so can
lead an agent to assume that its estimate of the expected utility of
the contract is more accurate than it actually is. To illustrate this,
consider a specific scenario with the following values: u(oa =
1) = u(ob = 1) = 1 and Va = Vb = 0.2. In this case,
Var(E[U]) = 0.4(1 + Cab), and thus, if the correlation Cab is
ignored then the variance in the expected utility is 0.4. However, if
the contract outcomes are completely correlated then Cab = 1 and
Var(E[U]) is actually 0.8. Thus, in order to have an accurate 
estimate of the variance of the expected contract utility, and to make
a rational decision, it is essential that the agent is able to 
represent and calculate these correlation terms. In the next section, we
describe how an agent may do so using the Dirichlet distribution.
4.2 The Dirichlet Distribution
In this section, we describe how the agent may use its direct 
experience of previous contracts, and the standard results of the Dirichlet
distribution, to determine an estimate of the probability that each
contract dimension will be successful fulfilled, ˆp(X), and a 
measure of the uncertainties in these probabilities that expresses the
correlations between the contract dimensions, Cov(p(X)).
We first consider the calculation of ˆp(X) and the diagonal terms
of the covariance matrix Cov(p(X)). As described above, the
derivation of these results is identical to the case of the single 
dimensional beta distribution, where out of N contract outcomes,
n are successfully fulfilled. In the multi-dimensional case, 
however, we have a vector {na, nb, nc, . . .} that represents the number
of outcomes for which each of the individual contract dimensions
were successfully fulfilled. Thus, in terms of the standard Dirichlet
parameters where αa = na + 1 and α0 = N + 2, the agent can 
estimate the probability of this contract dimension being successfully
fulfilled:
ˆp(oa = 1) =
αa
α0
=
na + 1
N + 2
and can also calculate the variance in any contract dimension:
Va =
αa(α0 − αa)
α2
0(1 + α0)
=
(na + 1)(N − na + 1)
(N + 2)2(N + 3)
However, calculating the off-diagonal terms within Cov(p(X)) is
more complex since it is necessary to consider the correlations 
between the contract dimensions. Thus, for each pair of dimensions
(i.e. a and b), we must consider all possible combinations of 
contract outcomes, and thus we define nab
ij as the number of contract
outcomes for which both oa = i and ob = j. For example, nab
10
represents the number of contracts for which oa = 1 and ob = 0.
Now, using the standard Dirichlet notation, we can define αab
ij
nab
ij + 1 for all i and j taking values 0 and 1, and then, to calculate
the cross-correlations between contract pairs a and b, we note that
the Dirichlet distribution over pair-wise joint probabilities is:
Prob(pab) = Kab
i∈{0,1} j∈{0,1}
p(oa = i, ob = j)αab
ij −1
where:
i∈{0,1} j∈{0,1}
p(oa = i, ob = j) = 1
and Kab is a normalising constant [3]. From this we can derive
pair-wise probability estimates and variances:
E[p(oa = i, ob = j)] =
αab
ij
α0
(13)
V [p(oa = i, ob = j)] =
αab
ij (α0 − αab
ij )
α2
0(1 + α0)
(14)
where:
α0 =
i∈{0,1} j∈{0,1}
αab
ij (15)
and in fact, α0 = N + 2, where N is the total number of contracts
observed. Likewise, we can express the covariance in these 
pairwise probabilities in similar terms:
C[p(oa = i, ob = j), p(oa = m, ob = n)] =
−αab
ij αab
mn
α2
0(1 + α0)
Finally, we can use the expression:
p(oa = 1) =
j∈{0,1}
p(oa = 1, ob = j)
to determine the covariance Cab. To do so, we first simplify the
notation by defining V ab
ij V [p(oa = i, ob = j)] and Cab
ijmn
C[p(oa = i, ob = j), p(oa = m, ob = n)]. The covariance for the
probability of positive contract outcomes is then the covariance 
between j∈{0,1} p(oa = 1, ob = j) and i∈{0,1} p(oa = i, ob =
1), and thus:
Cab = Cab
1001 + Cab
1101 + Cab
1011 + V ab
11 .
Thus, given a set of contract outcomes that represent the agent"s
previous interactions with a supplier, we may use the Dirichlet 
distribution to calculate the mean and variance of the probability of
any contract dimension being successfully fulfilled (i.e. ˆp(oa = 1)
and Va). In addition, by a somewhat more complex procedure we
can also calculate the correlations between these probabilities (i.e.
Cab). This allows us to calculate an estimate of the probability that
any contract dimension will be successfully fulfilled, ˆp(X), and
also represent the uncertainty and correlations in these probabilities
by the covariance matrix, Cov(p(X)). In turn, these results may be
used to calculate the estimate and uncertainty in the expected 
utility of the contract. In the next section we present empirical results
that show that in practise this formalism yields significant 
improvements in these estimates compared to the na¨ıve approximation 
using multiple independent beta distributions.
4.3 Empirical Comparison
In order to evaluate the effectiveness of our formalism, and show
the importance of the off-diagonal terms in Cov(p(X)), we 
compare two approaches:
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1073
−1 −0.5 0 0.5 1
0.2
0.4
0.6
0.8
Correlation (ρ)
Var(E[U])
Dirichlet Distribution
Indepedent Beta Distributions
−1 −0.5 0 0.5 1
0.5
1
1.5
2
2.5
x 10
4
Correlation (ρ)
Information(I)
Dirichlet Distribution
Indepedent Beta Distributions
Figure 1: Plots showing (i) the variance of the expected contract
utility and (ii) the information content of the estimates 
computed using the Dirichlet distribution and multiple independent
beta distributions. Results are averaged over 106
runs, and the
error bars show the standard error in the mean.
• Dirichlet Distribution: We use the full Dirichlet 
distribution, as described above, to calculate ˆp(X) and Cov(p(X))
including all its off-diagonal terms that represent the 
correlations between the contract dimensions.
• Independent Beta Distributions: We use independent beta
distributions to represent each contract dimension, in order
to calculate ˆp(X), and then, as described earlier, we 
approximate Cov(p(X)) and ignore the correlations by setting all
the off-diagonal terms to zero.
We consider a two-dimensional case where u(oa = 1) = 6 and
u(ob = 1) = 2, since this allows us to plot ˆp(X) and Cov(p(X))
as ellipses in a two-dimensional plane, and thus explain the 
differences between the two approaches. Specifically, we initially 
allocate the agent some previous contract outcomes that represents its
direct experience with a supplier. The number of contracts is drawn
uniformly between 10 and 20, and the actual contract outcomes are
drawn from an arbitrary joint distribution intended to induce 
correlations between the contract dimensions. For each set of 
contracts, we use the approaches described above to calculate ˆp(X)
and Cov(p(X)), and hence, the variance in the expected contract
utility, Var(E[U]). In addition, we calculate a scalar measure of the
information content, I, of the covariance matrix Cov(p(X)), which
is a standard way of measuring the uncertainty encoded within the
covariance matrix [1]. More specifically, we calculate the 
determinant of the inverse of the covariance matrix:
I = det(Cov(p(X))−1
) (16)
and note that the larger the information content, the more precise
ˆp(X) will be, and thus, the better the estimate of the expected 
utility that the agent is able to calculate. Finally, we use the results
0.3 0.4 0.5 0.6 0.7 0.8
0.3
0.4
0.5
0.6
0.7
0.8
p(o =1)
p(o=1)
a
b
Dirichlet Distribution
Indepedent Beta Distributions
Figure 2: Examples of ˆp(X) and Cov(p(X)) plotted as second
standard error ellipses.
presented in section 4.2 to calculate the actual correlation, ρ, 
associated with this particular set of contract outcomes:
ρ =
Cab
√
VaVb
(17)
where Cab, Va and Vb are calculated as described in section 4.2.
The results of this analysis are shown in figure 1. Here we show
the values of I and Var(E[U]) calculated by the agents, plotted
against the correlation of the contract outcomes, ρ, that constituted
their direct experience. The results are averaged over 106

simulation runs. Note that as expected, when the dimensions of the 
contract outcomes are uncorrelated (i.e. ρ = 0), then both approaches
give the same results. However, the value of using our formalism
with the full Dirichlet distribution is shown when the correlation
between the dimensions increases (either negatively or positively).
As can be seen, if we approximate the Dirichlet distribution with
multiple independent beta distributions, all of the correlation 
information contained within the covariance matrix, Cov(p(X)), is
lost, and thus, the information content of the matrix is much lower.
The loss of this correlation information leads the variance of the
expected utility of the contract to be incorrect (either over or under
estimated depending on the correlation)5
, with the exact amount of
mis-estimation depending on the actual utility function chosen (i.e.
the values of u(oa = 1) and u(ob = 1)).
In addition, in figure 2 we illustrate an example of the estimates
calculated through both methods, for a single exemplar set of 
contract outcomes. We represent the probability estimates, ˆp(X), and
the covariance matrix, Cov(p(X)), in the standard way as an 
ellipse [1]. That is, ˆp(X) determines the position of the center of
the ellipse, Cov(p(X)) defines its size and shape. Note that whilst
the ellipse resulting from the full Dirichlet formalism accurately 
reflects the true distribution (samples of which are plotted as points),
that calculated by using multiple independent Beta distributions
(and thus ignoring the correlations) results in a much larger ellipse
that does not reflect the true distribution. The larger size of this
ellipse is a result of the off-diagonal terms of the covariance matrix
being set to zero, and corresponds to the agent miscalculating the
uncertainty in the probability of each contract dimension being 
fulfilled. This, in turn, leads it to miscalculate the uncertainty in the
expected utility of a contract (shown in figure 1 as Var(E[U]).
5. COMMUNICATING REPUTATION
Having described how an individual agent can use its own direct
experience of contract outcomes in order to estimate the 
probabil5
Note that the plots are not smooth due to the fact that given a
limited number of contract outcomes, then the mean of Va and Vb
do not vary smoothly with ρ.
1074 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
ity that a multi-dimensional contract will be successfully fulfilled,
we now go on to consider how agents within an open multi-agent
system can communicate these estimates to one another. This is
commonly referred to as reputation and allows agents with limited
direct experience of a supplier to make rational decisions.
Both Jøsang and Ismail, and Teacy et al. present models whereby
reputation is communicated between agents using the sufficient 
statistics of the beta distribution [6, 11]. This approach is attractive since
these sufficient statistics are simple aggregations of contract 
outcomes (more precisely, they are simply the total number of 
contracts observed, N, and the number of these that were successfully
fulfilled, n). Under the probabilistic framework of the beta 
distribution, reputation reports in this form may simply be aggregated with
an agent"s own direct experience, in order to gain a more precise
estimate based on a larger set of contract outcomes.
We can immediately extend this approach to the multi-dimensional
case considered here, by requiring that the agents exchange the 
sufficient statistics of the Dirichlet distribution instead of the beta 
distribution. In this case, for each pair of dimensions (i.e. a and b), the
agents must communicate a vector of contract outcomes, N, which
are the sufficient statistics of the Dirichlet distribution, given by:
N =< nab
ij > ∀a, b, i ∈ {0, 1}, j ∈ {0, 1} (18)
Thus, an agent is able to communicate the sufficient statistics of
its own Dirichlet distribution in terms of just 2d(d − 1) numbers
(where d is the number of contract dimensions). For instance, in
the case of three dimensions, N, is given by:
N =< nab
00, nab
01, nab
10, nab
11, nac
00, nac
01, nac
10, nac
11, nbc
00, nbc
01, nbc
10, nbc
11 >
and, hence, large sets of contract outcomes may be communicated
within a relatively small message size, with no loss of information.
Again, agents receiving these sufficient statistics may simply 
aggregate them with their own direct experience in order to gain a
more precise estimate of the trustworthiness of a supplier.
Finally, we note that whilst it is not the focus of our work here,
by adopting the same principled approach as Jøsang and Ismail, and
Teacy et al., many of the techniques that they have developed (such
as discounting reports from unreliable agents, and filtering 
inconsistent reports from selfish agents) may be directly applied within
this multi-dimensional model. However, we now go on to consider
a new issue that arises in both the single and multi-dimensional
models, namely the problems that arise when such aggregated 
sufficient statistics are propagated within decentralised agent networks.
6. RUMOUR PROPAGATION
WITHIN REPUTATION SYSTEMS
In the previous section, we described the use of sufficient 
statistics to communicate reputation, and we showed that by aggregating
contract outcomes together into these sufficient statistics, a large
number of contract outcomes can be represented and 
communicated in a compact form. Whilst, this is an attractive property, it
can be problematic in practise, since the individual provenance of
each contract outcome is lost in the aggregation. Thus, to ensure an
accurate estimate, the reputation system must ensure that each 
observation of a contract outcome is included within the aggregated
statistics no more than once.
Within a centralised reputation system, where all agents report
their direct experience to a trusted center, such double counting of
contract outcomes is easy to avoid. However, in a decentralised
reputation system, where agents communicate reputation to one
another, and aggregate their direct experience with these reputation
reports on-the-fly, avoiding double counting is much more difficult.
a1 a2
a3
¨
¨¨
¨¨
¨¨B
E
T
N1
N1
N1 + N2
Figure 3: Example of rumour propagation in a decentralised
reputation system.
For example, consider the case shown in figure 3 where three
agents (a1 . . . a3), each with some direct experience of a supplier,
share reputation reports regarding this supplier. If agent a1 were
to provide its estimate to agents a2 and a3 in the form of the 
sufficient statistics of its Dirichlet distribution, then these agents can
aggregate these contract outcomes with their own, and thus obtain
more precise estimates. If at a later stage, agent a2 were to send
its aggregate vector of contract outcomes to agent a3, then agent
a3 being unaware of the full history of exchanges, may attempt to
combine these contract outcomes with its own aggregated vector.
However, since both vectors contain a contribution from agent a1,
these will be counted twice in the final aggregated vector, and will
result in a biased and overconfident estimate. This is termed 
rumour propagation or data incest in the data fusion literature [9].
One possible solution would be to uniquely identify the source of
each contract outcome, and then propagate each vector, along with
its label, through the network. Agents can thus identify identical
observations that have arrived through different routes, and after
removing the duplicates, can aggregate these together to form their
estimates. Whilst this appears to be attractive in principle, for a
number of reasons, it is not always a viable solution in practise [12].
Firstly, agents may not actually wish to have their uniquely labelled
contract outcomes passed around an open system, since such 
information may have commercial or practical significance that could
be used to their disadvantage. As such, agents may only be willing
to exchange identifiable contract outcomes with a small number of
other agents (perhaps those that they have some sort of reciprocal
relationship with). Secondly, the fact that there is no aggregation
of the contract outcomes as they pass around the network means
that the message size increases over time, and the ultimate size of
these messages is bounded only by the number of agents within the
system (possibly an extremely large number for a global system).
Finally, it may actually be difficult to assign globally agreeable,
consistent, and unique labels for each agent within an open system.
In the next section, we develop a novel solution to the problem of
rumour propagation within decentralised reputation systems. Our
solution is based on an approach developed within the area of target
tracking and data fusion [9]. It avoids the need to uniquely identify
an agent, it allows agents to restrict the number of other agents
who they reveal their private estimates to, and yet it still allows
information to propagate throughout the network.
6.1 Private and Shared Information
Our solution to rumour propagation within decentralised reputation
systems introduces the notion of private information that an agent
knows it has not communicated to any other agent, and shared 
information that has been communicated to, or received from, 
another agent. Thus, the agent can decompose its contract outcome
vector, N, into two vectors, a private one, Np, that has not been
communicated to another agent, and a shared one, Ns, that has
been shared with, or received from, another agent:
N = Np + Ns (19)
Now, whenever an agent communicates reputation, it 
communicates both its private and shared vectors separately. Both the 
origThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1075
inating and receiving agents then update their two vectors 
appropriately. To understand this, consider the case that agent aα sends
its private and shared contract outcome vectors, Nα
p and Nα
s , to
agent aβ that itself has private and shared contract outcomes Nβ
p
and Nβ
s . Each agent updates its vectors of contract outcomes 
according to the following procedure:
• Originating Agent: Once the originating agent has sent both
its shared and private contract outcome vectors to another
agent, its private information is no longer private. Thus, it
must remove the contract outcomes that were in its private
vector, and add them into its shared vector:
Nα
s ← Nα
s + Nα
p
Nα
p ← ∅.
• Receiving Agent: The goal of the receiving agent is to 
accumulate the largest number contract outcomes (since this will
result in the most precise estimate) without including shared
information from both itself and the other agent (since this
may result in double counting of contract outcomes). It has
two choices depending on the total number of contract 
outcomes6
within its own shared vector, Nβ
s , and within that of
the originating agent, Nα
s . Thus, it updates its vector 
according to the procedure below:
- Nβ
s > Nα
s : If the receiving agent"s shared vector 
represents a greater number of contract outcomes than that
of the shared vector of the originating agent, then the
agent combines its shared vector with the private 
vector of the originating agent:
Nβ
s ← Nβ
s + Nα
p
Nβ
p unchanged.
- Nβ
s < Nα
s : Alternatively if the receiving agent"s shared
vector represents a smaller number contract outcomes
than that of the shared vector of the originating agent,
then the receiving agent discards its own shared vector
and forms a new one from both the private and shared
vectors of the originating agent:
Nβ
s ← Nα
s + Nα
p
Nβ
p unchanged.
In the case that Nβ
s = Nα
s then either option is 
appropriate. Once the receiving agent has updated its sets, it uses the
contract outcomes within both to form its trust estimate. If
agents receive several vectors simultaneously, this approach
generalises to the receiving agent using the largest shared
vector, and the private vectors of itself and all the originating
agents to form its new shared vector.
This procedure has a number of attractive properties. Firstly, since
contract outcomes in an agent"s shared vector are never combined
with those in the shared vector of another agent, outcomes that
originated from the same agent are never combined together, and
thus, rumour propagation is completely avoided. However, since
the receiving agent may discard its own shared vector, and adopt
the shared vector of the originating agent, information is still 
propagated around the network. Moreover, since contract outcomes are
aggregated together within the private and shared vectors, the 
message size is constant and does not increase as the number of 
interactions increases. Finally, an agent only communicates its own
private contract outcomes to its immediate neighbours. If this agent
6
Note that this may be calculated from N = nab
00 +nab
01 +nab
10 +nab
11.
subsequently passes it on, it does so as unidentifiable aggregated 
information within its shared information. Thus, an agent may limit
the number of agents with which it is willing to reveal 
identifiable contract outcomes, and yet these contract outcomes can still
propagate within the network, and thus, improve estimates of other
agents. Next, we demonstrate empirically that these properties can
indeed be realised in practise.
6.2 Empirical Comparison
In order to evaluate the effectiveness of this procedure we 
simulated random networks consisting of ten agents. Each agent has
some direct experience of interacting with a supplier (as described
in section 4.3). At each iteration of the simulation, it interacts with
its immediate neighbours and exchanges reputation reports through
the sufficient statistics of their Dirichlet distributions. We compare
our solution to two of the most obvious decentralised alternatives:
• Private and Shared Information: The agents follow the
procedure described in the previous section. That is, they
maintain separate private and shared vectors of contract 
outcomes, and at each iteration they communicate both these
vectors to their immediate neighbours.
• Rumour Propagation: The agents do not differentiate 
between private and shared contract outcomes. At the first 
iteration they communicate all of the contract outcomes that
constitute their direct experience. In subsequent iterations,
they propagate contract outcomes that they receive from any
of the neighbours, to all their other immediate neighbours.
• Private Information Only: The agents only communicate
the contract outcomes that constitute their direct experience.
In all cases, at each iteration, the agents use the Dirichlet 
distribution in order to calculate their trust estimates. We compare these
three decentralised approaches to a centralised reputation system:
• Centralised Reputation: All the agents pass their direct 
experience to a centralised reputation system that aggregates
them together, and passes this estimate back to each agent.
This centralised solution makes the most effective use of 
information available in the network. However, most real world 
problems demand decentralised solutions due to scalability, 
modularity and communication concerns. Thus, this centralised solution
is included since it represents the optimal case, and allows us to
benchmark our decentralised solution.
The results of these comparisons are shown in figure 4. Here
we show the sum of the information content of each agent"s 
covariance matrix (calculated as discussed earlier in section 4.3), for
each of these four different approaches. We first note that where
private information only is communicated, there is no change in 
information after the first iteration. Once each agent has received the
direct experience of its immediate neighbours, no further increase
in information can be achieved. This represents the minimum 
communication, and it exhibits the lowest total information of the four
cases. Next, we note that in the case of rumour propagation, the
information content increases continually, and rapidly exceeds the
centralised reputation result. The fact that the rumour propagation
case incorrectly exceeds this limit, indicates that it is continuously
counting the same contract outcomes as they cycle around the 
network, in the belief that they are independent events. Finally, we
note that using private and shared information represents a 
compromise between the private information only case and the centralised
reputation case. Information is still allowed to propagate around
the network, however rumours are eliminated.
As before, we also plot a single instance of the trust estimates
from one agent (i.e. ˆp(X) and Cov(p(X))) as a set of ellipses on a
1076 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
1 2 3 4 5
10
4
10
6
10
8
10
10
Iteration
Information(I)
Private & Shared Information
Rumour Propagation
Private Information Only
Centralised Reputation
Figure 4: Sum of information over all agents as a function of
the communication iteration.
two-dimensional plane (along with samples from the true 
distribution). As expected, the centralised reputation system achieves the
best estimate of the true distribution, since it uses the direct 
experience of all agents. The private information only case shows the
largest ellipse since it propagates the least information around the
network. The rumour propagation case shows the smallest ellipse,
but it is inconsistent with the actual distribution p(X). Thus, 
propagating rumours around the network and double counting contract
outcomes in the belief that they are independent events, results in
an overconfident estimate. However, we note that our solution, 
using separate vectors of private and shared information, allows us to
propagate more information than the private information only case,
but we completely avoid the problems of rumour propagation.
Finally, we consider the effect that this has on the agents" 
calculation of the expected utility of the contract. We assume the same
utility function as used in section 4.3 (i.e. u(oa = 1) = 6 and
u(ob = 1) = 2), and in table 1 we present the estimate of the 
expected utility, and its standard deviation calculated for all four cases
by a single agent at iteration five (after communication has ceased
to have any further effect for all methods other than rumour 
propagation). We note that the rumour propagation case is clearly 
inconsistent with the centralised reputation system, since its standard
deviation is too small and does not reflect the true uncertainty in
the expected utility, given the contract outcomes. However, we 
observe that our solution represents the closest case to the centralised
reputation system, and thus succeeds in propagating information
throughout the network, whilst also avoiding bias and 
overconfidence. The exact difference between it and the centralised 
reputation system depends upon the topology of the network, and the
history of exchanges that take place within it.
7. CONCLUSIONS
In this paper we addressed the need for a principled probabilistic
model of computational trust that deals with contracts that have
multiple correlated dimensions. Our starting point was an agent 
estimating the expected utility of a contract, and we showed that this
leads to a model of computational trust that uses the Dirichlet 
distribution to calculate a trust estimate from the direct experience of an
agent. We then showed how agents may use the sufficient statistics
of this Dirichlet distribution to represent and communicate 
reputation within a decentralised reputation system, and we presented a
solution to rumour propagation within these systems.
Our future work in this area is to extend the exchange of 
reputation to the case where contracts are not homogeneous. That
is, not all agents observe the same contract dimensions. This is
a challenging extension, since in this case, the sufficient statistics
of the Dirichlet distribution can not be used directly. However, by
0.2 0.3 0.4 0.5 0.6 0.7
0.1
0.2
0.3
0.4
0.5
0.6
0.7
p(o =1)
p(o=1)
a
b
Private & Shared Information
Rumour Propagation
Private Information Only
Centralised Reputation
Figure 5: Instances of ˆp(X) and Cov(p(X)) plotted as second
standard error ellipses after 5 communication iterations.
Method E[E[U]] ± Var(E[U])
Private and Shared Information 3.18 ± 0.54
Rumour Propagation 3.33 ± 0.07
Private Information Only 3.20 ± 0.65
Centralised Reputation 3.17 ± 0.42
Table 1: Estimated expected utility and its standard error as
calculated by a single agent after 5 communication iterations.
addressing this challenge, we hope to be able to apply these 
techniques to a setting in which a suppliers provides a range of services
whose failures are correlated, and agents only have direct 
experiences of different subsets of these services.
8. ACKNOWLEDGEMENTS
This research was undertaken as part of the ALADDIN (Autonomous
Learning Agents for Decentralised Data and Information Networks)
project and is jointly funded by a BAE Systems and EPSRC 
strategic partnership (EP/C548051/1).
9. REFERENCES
[1] Y. Bar-Shalom, X. R. Li, and T. Kirubarajan. Estimation with Applications to
Tracking and Navigation. Wiley Interscience, 2001.
[2] C. Boutilier. The foundations of expected expected utility. In Proc. of the 4th
Int. Joint Conf. on on Artificial Intelligence, pages 285-290, Acapulco,
Mexico, 2003.
[3] M. Evans, N. Hastings, and B. Peacock. Statistical Distributions. John Wiley
& Sons, Inc., 1993.
[4] N. Griffiths. Task delegation using experience-based multi-dimensional trust.
In Proc. of the 4th Int. Joint Conf. on Autonomous Agents and Multiagent
Systems, pages 489-496, New York, USA, 2005.
[5] N. Gukrai, D. DeAngelis, K. K. Fullam, and K. S. Barber. Modelling
multi-dimensional trust. In Proc. of the 9th Int. Workshop on Trust in Agent
Systems, Hakodate, Japan, 2006.
[6] A. Jøsang and R. Ismail. The beta reputation system. In Proc. of the 15th Bled
Electronic Commerce Conf., pages 324-337, Bled, Slovenia, 2002.
[7] E. M. Maximilien and M. P. Singh. Agent-based trust model involving
multiple qualities. In Proc. of the 4th Int. Joint Conf. on Autonomous Agents
and Multiagent Systems, pages 519-526, Utrecht, The Netherlands, 2005.
[8] S. D. Ramchurn, D. Hunyh, and N. R. Jennings. Trust in multi-agent systems.
Knowledge Engineering Review, 19(1):1-25, 2004.
[9] S. Reece and S. Roberts. Robust, low-bandwidth, multi-vehicle mapping. In
Proc. of the 8th Int. Conf. on Information Fusion, Philadelphia, USA, 2005.
[10] J. Sabater and C. Sierra. REGRET: A reputation model for gregarious
societies. In Proc. of the 4th Workshop on Deception, Fraud and Trust in Agent
Societies, pages 61-69, Montreal, Canada, 2001.
[11] W. T. L. Teacy, J. Patel, N. R. Jennings, and M. Luck. TRAVOS: Trust and
reputation in the context of inaccurate information sources. Autonomous
Agents and Multi-Agent Systems, 12(2):183-198, 2006.
[12] S. Utete. Network Management in Decentralised Sensing Systems. PhD thesis,
University of Oxford, UK, 1994.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1077
