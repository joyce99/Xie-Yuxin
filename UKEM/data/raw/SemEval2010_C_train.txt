Adapting Asynchronous Messaging Middleware
to ad-hoc Networking
Mirco Musolesi
Dept. of Computer Science,
University College London
Gower Street, London
WC1E 6BT, United Kingdom
m.musolesi@cs.ucl.ac.uk
Cecilia Mascolo
Dept. of Computer Science,
University College London
Gower Street, London
WC1E 6BT, United Kingdom
c.mascolo@cs.ucl.ac.uk
Stephen Hailes
Dept. of Computer Science,
University College London
Gower Street, London
WC1E 6BT, United Kingdom
s.hailes@cs.ucl.ac.uk
ABSTRACT
The characteristics of mobile environments, with the 
possibility of frequent disconnections and fluctuating bandwidth,
have forced a rethink of traditional middleware. In 
particular, the synchronous communication paradigms often 
employed in standard middleware do not appear to be 
particularly suited to ad-hoc environments, in which not even
the intermittent availability of a backbone network can be
assumed. Instead, asynchronous communication seems to
be a generally more suitable paradigm for such 
environments. Message oriented middleware for traditional systems
has been developed and used to provide an asynchronous
paradigm of communication for distributed systems, and,
recently, also for some specific mobile computing systems.
In this paper, we present our experience in designing,
implementing and evaluating EMMA (Epidemic Messaging
Middleware for ad-hoc networks), an adaptation of Java
Message Service (JMS) for mobile ad-hoc environments. We
discuss in detail the design challenges and some possible 
solutions, showing a concrete example of the feasibility and
suitability of the application of the asynchronous paradigm
in this setting and outlining a research roadmap for the 
coming years.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems-Distributed Applications; C.2.1 [Network 
Architecture and Design]: Wireless Communication
General Terms
DESIGN, ALGORITHMS
1. INTRODUCTION
With the increasing popularity of mobile devices and their
widespread adoption, there is a clear need to allow the 
development of a broad spectrum of applications that operate 
effectively over such an environment. Unfortunately, this is far
from simple: mobile devices are increasingly heterogeneous
in terms of processing capabilities, memory size, battery 
capacity, and network interfaces. Each such configuration has
substantially different characteristics that are both statically
different - for example, there is a major difference in 
capability between a Berkeley mote and an 802.11g-equipped
laptop - and that vary dynamically, as in situations of 
fluctuating bandwidth and intermittent connectivity. Mobile ad
hoc environments have an additional element of complexity
in that they are entirely decentralised.
In order to craft applications for such complex 
environments, an appropriate form of middleware is essential if cost
effective development is to be achieved. In this paper, we
examine one of the foundational aspects of middleware for
mobile ad-hoc environments: that of the communication
primitives.
Traditionally, the most frequently used middleware 
primitives for communication assume the simultaneous presence
of both end points on a network, since the stability and 
pervasiveness of the networking infrastructure is not an 
unreasonable assumption for most wired environments. In other
words, most communication paradigms are synchronous: 
object oriented middleware such as CORBA and Java RMI are
typical examples of middleware based on synchronous 
communication.
In recent years, there has been growing interest in 
platforms based on asynchronous communication paradigms, such
as publish-subscribe systems [6]: these have been exploited
very successfully where there is application level 
asynchronicity. From a Gartner Market Report [7]: Given 
messageoriented-middleware"s (MOM) popularity, scalability, 
flexibility, and affinity with mobile and wireless architectures,
by 2004, MOM will emerge as the dominant form of 
communication middleware for linking mobile and enterprise 
applications (0.7 probability).... Moreover, in mobile ad-hoc
systems, the likelihood of network fragmentation means that
synchronous communication may in any case be 
impracticable, giving situations in which delay tolerant asynchronous
traffic is the only form of traffic that could be supported.
121 Middleware 2004 Companion
Middleware for mobile ad-hoc environments must therefore
support semi-synchronous or completely asynchronous 
communication primitives if it is to avoid substantial 
limitations to its utility. Aside from the intellectual challenge in
supporting this model, this work is also interesting because
there are a number of practical application domains in 
allowing inter-community communication in undeveloped 
areas of the globe. Thus, for example, projects that have been
carried out to help populations that live in remote places of
the globe such as Lapland [3] or in poor areas that lack fixed
connectivity infrastructure [9].
There have been attempts to provide mobile middleware
with these properties, including STEAM, LIME, 
XMIDDLE, Bayou (see [11] for a more complete review of mobile
middleware). These models differ quite considerably from
the existing traditional middleware in terms of primitives
provided. Furthermore, some of them fail in providing a
solution for the true ad-hoc scenarios.
If the projected success of MOM becomes anything like
a reality, there will be many programmers with experience
of it. The ideal solution to the problem of middleware for
ad-hoc systems is, then, to allow programmers to utilise the
same paradigms and models presented by common forms of
MOM and to ensure that these paradigms are supportable
within the mobile environment. This approach has clear
advantages in allowing applications developed on standard
middleware platforms to be easily deployed on mobile 
devices. Indeed, some research has already led to the 
adaptation of traditional middleware platforms to mobile settings,
mainly to provide integration between mobile devices and
existing fixed networks in a nomadic (i.e., mixed) 
environment [4]. With respect to message oriented middleware, the
current implementations, however, either assume the 
existence of a backbone network to which the mobile hosts 
connect from time to time while roaming [10], or assume that
nodes are always somehow reachable through a path [18].
No adaptation to heterogeneous or completely ad-hoc 
scenarios, with frequent disconnection and periodically isolated
clouds of hosts, has been attempted.
In the remainder of this paper we describe an initial 
attempt to adapt message oriented middleware to suit mobile
and, more specifically, mobile ad-hoc networks. In our case,
we elected to examine JMS, as one of the most widely known
MOM systems. In the latter part of this paper, we explore
the limitations of our results and describe the plans we have
to take the work further.
2. MESSAGE ORIENTED MIDDLEWARE
AND JAVA MESSAGE SERVICE (JMS)
Message-oriented middleware systems support 
communication between distributed components via message-passing:
the sender sends a message to identified queues, which 
usually reside on a server. A receiver retrieves the message from
the queue at a different time and may acknowledge the reply
using the same asynchronous mechanism. Message-oriented
middleware thus supports asynchronous communication in
a very natural way, achieving de-coupling of senders and
receivers. A sender is able to continue processing as soon
as the middleware has accepted the message; eventually,
the receiver will send an acknowledgment message and the
sender will be able to collect it at a convenient time. 
However, given the way they are implemented, these middleware
systems usually require resource-rich devices, especially in
terms of memory and disk space, where persistent queues
of messages that have been received but not yet processed,
are stored. Sun Java Message Service [5], IBM WebSphere
MQ [6], Microsoft MSMQ [12] are examples of very 
successful message-oriented middleware for traditional distributed
systems.
The Java Messaging Service (JMS) is a collection of 
interfaces for asynchronous communication between distributed
components. It provides a common way for Java programs
to create, send and receive messages. JMS users are usually
referred to as clients. The JMS specification further defines
providers as the components in charge of implementing the
messaging system and providing the administrative and 
control functionality (i.e., persistence and reliability) required
by the system. Clients can send and receive messages, 
asynchronously, through the JMS provider, which is in charge of
the delivery and, possibly, of the persistence of the messages.
There are two types of communication supported: point
to point and publish-subscribe models. In the point to point
model, hosts send messages to queues. Receivers can be 
registered with some specific queues, and can asynchronously
retrieve the messages and then acknowledge them. The
publish-subscribe model is based on the use of topics that
can be subscribed to by clients. Messages are sent to topics
by other clients and are then received in an asynchronous
mode by all the subscribed clients. Clients learn about the
available topics and queues through Java Naming and 
Directory Interface (JNDI) [14]. Queues and topics are created
by an administrator on the provider and are registered with
the JNDI interface for look-up.
In the next section, we introduce the challenges of mobile
networks, and show how JMS can be adapted to cope with
these requirements.
3. JMS FOR MOBILE COMPUTING
Mobile networks vary very widely in their characteristics,
from nomadic networks in which modes relocate whilst 
offline through to ad-hoc networks in which modes move freely
and in which there is no infrastructure. Mobile ad-hoc 
networks are most generally applicable in situations where 
survivability and instant deployability are key: most notably in
military applications and disaster relief. In between these
two types of "mobile" networks, there are, however, a number
of possible heterogeneous combinations, where nomadic and
ad-hoc paradigms are used to interconnect totally unwired
areas to more structured networks (such as a LAN or the
Internet).
Whilst the JMS specification has been extensively 
implemented and used in traditional distributed systems, 
adaptations for mobile environments have been proposed only
recently. The challenges of porting JMS to mobile settings
are considerable; however, in view of its widespread 
acceptance and use, there are considerable advantages in allowing
the adaptation of existing applications to mobile 
environments and in allowing the interoperation of applications in
the wired and wireless regions of a network.
In [10], JMS was adapted to a nomadic mobile setting,
where mobile hosts can be JMS clients and communicate
through the JMS provider that, however, sits on a 
backbone network, providing reliability and persistence. The
client prototype presented in [10] is very lightweight, due
to the delegation of all the heavyweight functionality to the
Middleware for Pervasive and ad-hoc Computing 122
provider on the wired network. However, this approach is
somewhat limited in terms of widespread applicability and
scalability as a consequence of the concentration of 
functionality in the wired portion of the network.
If JMS is to be adapted to completely ad-hoc 
environments, where no fixed infrastructure is available, and where
nodes change location and status very dynamically, more
issues must be taken into consideration. Firstly, discovery
needs to use a resilient but distributed model: in this 
extremely dynamic environment, static solutions are 
unacceptable. As discussed in Section 2, a JMS administrator defines
queues and topics on the provider. Clients can then learn
about them using the Java Naming and Directory Interface
(JNDI). However, due to the way JNDI is designed, a JNDI
node (or more than one) needs to be in reach in order to 
obtain a binding of a name to an address (i.e., knowing where
a specific queue/topic is). In mobile ad-hoc environments,
the discovery process cannot assume the existence of a fixed
set of discovery servers that are always reachable, as this
would not match the dynamicity of ad-hoc networks.
Secondly, a JMS Provider, as suggested by the JMS 
specification, also needs to be reachable by each node in the
network, in order to communicate. This assumes a very
centralised architecture, which again does not match the 
requirements of a mobile ad-hoc setting, in which nodes may
be moving and sparse: a more distributed and dynamic 
solution is needed. Persistence is, however, essential 
functionality in asynchronous communication environments as hosts
are, by definition, connected at different times.
In the following section, we will discuss our experience
in designing and implementing JMS for mobile ad-hoc 
networks.
4. JMSFOR MOBILE ad-hoc NETWORKS
4.1 Adaptation of JMS for Mobile ad-hoc
Networks
Developing applications for mobile networks is yet more
challenging: in addition to the same considerations as for
infrastructured wireless environments, such as the limited
device capabilities and power constraints, there are issues
of rate of change of network connectivity, and the lack of a
static routing infrastructure. Consequently, we now describe
an initial attempt to adapt the JMS specification to target
the particular requirements related to ad-hoc scenarios. As
discussed in Section 3, a JMS application can use either the
point to point and the publish-subscribe styles of messaging.
Point to Point Model The point to point model is based
on the concept of queues, that are used to enable 
asynchronous communication between the producer of a message
and possible different consumers. In our solution, the 
location of queues is determined by a negotiation process that
is application dependent. For example, let us suppose that
it is possible to know a priori, or it is possible to determine
dynamically, that a certain host is the receiver of the most
part of messages sent to a particular queue. In this case, the
optimum location of the queue may well be on this 
particular host. In general, it is worth noting that, according to the
JMS specification and suggested design patterns, it is 
common and preferable for a client to have all of its messages
delivered to a single queue.
Queues are advertised periodically to the hosts that are
within transmission range or that are reachable by means of
the underlying synchronous communication protocol, if 
provided. It is important to note that, at the middleware level,
it is logically irrelevant whether or not the network layer 
implements some form of ad-hoc routing (though considerably
more efficient if it does); the middleware only considers 
information about which nodes are actively reachable at any
point in time. The hosts that receive advertisement 
messages add entries to their JNDI registry. Each entry is 
characterized by a lease (a mechanism similar to that present
in Jini [15]). A lease represents the time of validity of a
particular entry. If a lease is not refreshed (i.e, its life is
not extended), it can expire and, consequently, the entry
is deleted from the registry. In other words, the host 
assumes that the queue will be unreachable from that point
in time. This may be caused, for example, if a host storing
the queue becomes unreachable. A host that initiates a 
discovery process will find the topics and the queues present
in its connected portion of the network in a straightforward
manner.
In order to deliver a message to a host that is not 
currently in reach1
, we use an asynchronous epidemic routing
protocol that will be discussed in detail in Section 4.2. If two
hosts are in the same cloud (i.e., a connected path exists 
between them), but no synchronous protocol is available, the
messages are sent using the epidemic protocol. In this case,
the delivery latency will be low as a result of the rapidity of
propagation of the infection in the connected cloud (see also
the simulation results in Section 5). Given the existence of
an epidemic protocol, the discovery mechanism consists of
advertising the queues to the hosts that are currently 
unreachable using analogous mechanisms.
Publish-Subscribe Model In the publish-subscribe model,
some of the hosts are similarly designated to hold topics and
store subscriptions, as before. Topics are advertised through
the registry in the same way as are queues, and a client
wishing to subscribe to a topic must register with the client
holding the topic. When a client wishes to send a message
to the topic list, it sends it to the topic holder (in the same
way as it would send a message to a queue). The topic
holder then forwards the message to all subscribers, using
the synchronous protocol if possible, the epidemic protocol
otherwise. It is worth noting that we use a single message
with multiple recipients, instead of multiple messages with
multiple recipients. When a message is delivered to one of
the subscribers, this recipient is deleted from the list. In
order to delete the other possible replicas, we employ 
acknowledgment messages (discussed in Section 4.4), returned
in the same way as a normal message.
We have also adapted the concepts of durable and non
durable subscriptions for ad-hoc settings. In fixed platforms,
durable subscriptions are maintained during the 
disconnections of the clients, whether these are intentional or are the
result of failures. In traditional systems, while a durable
subscriber is disconnected from the server, it is responsible
for storing messages. When the durable subscriber 
reconnects, the server sends it all unexpired messages. The 
problem is that, in our scenario, disconnections are the norm
1
In theory, it is not possible to send a message to a peer that
has never been reachable in the past, since there can be no
entry present in the registry. However, to overcome this
possible limitation, we provide a primitive through which
information can be added to the registry without using the
normal channels.
123 Middleware 2004 Companion
rather than the exception. In other words, we cannot 
consider disconnections as failures. For these reasons, we adopt
a slightly different semantics. With respect to durable 
subscriptions, if a subscriber becomes disconnected, 
notifications are not stored but are sent using the epidemic 
protocol rather than the synchronous protocol. In other words,
durable notifications remain valid during the possible 
disconnections of the subscriber.
On the other hand, if a non-durable subscriber becomes
disconnected, its subscription is deleted; in other words, 
during disconnections, notifications are not sent using the 
epidemic protocol but exploit only the synchronous protocol. If
the topic becomes accessible to this host again, it must make
another subscription in order to receive the notifications.
Unsubscription messages are delivered in the same way
as are subscription messages. It is important to note that
durable subscribers have explicitly to unsubscribe from a
topic in order to stop the notification process; however, all
durable subscriptions have a predefined expiration time in
order to cope with the cases of subscribers that do not meet
again because of their movements or failures. This feature
is clearly provided to limit the number of the unnecessary
messages sent around the network.
4.2 Message Delivery using Epidemic Routing
In this section, we examine one possible mechanism that
will allow the delivery of messages in a partially connected
network. The mechanism we discuss is intended for the 
purposes of demonstrating feasibility; more efficient 
communication mechanisms for this environment are themselves 
complex, and are the subject of another paper [13].
The asynchronous message delivery described above is
based on a typical pure epidemic-style routing protocol [16].
A message that needs to be sent is replicated on each host in
reach. In this way, copies of the messages are quickly spread
through connected networks, like an infection. If a host 
becomes connected to another cloud of mobile nodes, during
its movement, the message spreads through this collection
of hosts. Epidemic-style replication of data and messages
has been exploited in the past in many fields starting with
the distributed database systems area [2].
Within epidemic routing, each host maintains a buffer
containing the messages that it has created and the replicas
of the messages generated by the other hosts. To improve
the performance, a hash-table indexes the content of the
buffer. When two hosts connect, the host with the smaller
identifier initiates a so-called anti-entropy session, sending
a list containing the unique identifiers of the messages that
it currently stores. The other host evaluates this list and
sends back a list containing the identifiers it is storing that
are not present in the other host, together with the messages
that the other does not have. The host that has started the
session receives the list and, in the same way, sends the 
messages that are not present in the other host. Should buffer
overflow occur, messages are dropped.
The reliability offered by this protocol is typically best 
effort, since there is no guarantee that a message will 
eventually be delivered to its recipient. Clearly, the delivery ratio
of the protocol increases proportionally to the maximum 
allowed delay time and the buffer size in each host (interesting
simulation results may be found in [16]).
4.3 Adaptation of the JMS Message Model
In this section, we will analyse the aspects of our 
adaptation of the specification related to the so-called JMS Message
Model [5]. According to this, JMS messages are 
characterised by some properties defined using the header field,
which contains values that are used by both clients and
providers for their delivery. The aspects discussed in the
remainder of this section are valid for both models (point to
point and publish-subscribe).
A JMS message can be persistent or non-persistent. 
According to the JMS specification, persistent messages must
be delivered with a higher degree of reliability than the 
nonpersistent ones. However, it is worth noting that it is not
possible to ensure once-and-only-once reliability for 
persistent messages as defined in the specification, since, as we 
discussed in the previous subsection, the underlying epidemic
protocol can guarantee only best-effort delivery. However,
clients maintain a list of the identifiers of the recently 
received messages to avoid the delivery of message duplicates.
In other words, we provide the applications with 
at-mostonce reliability for both types of messages.
In order to implement different levels of reliability, EMMA
treats persistent and non-persistent messages differently, 
during the execution of the anti-entropy epidemic protocol. Since
the message buffer space is limited, persistent messages are
preferentially replicated using the available free space. If
this is insufficient and non-persistent messages are present
in the buffer, these are replaced. Only the successful 
deliveries of the persistent messages are notified to the senders.
According to the JMS specification, it is possible to assign
a priority to each message. The messages with higher 
priorities are delivered in a preferential way. As discussed above,
persistent messages are prioritised above the non-persistent
ones. Further selection is based on their priorities. Messages
with higher priorities are treated in a preferential way. In
fact, if there is not enough space to replicate all the 
persistent messages, a mechanism based on priorities is used to
delete and replicate non-persistent messages (and, if 
necessary, persistent messages).
Messages are deleted from the buffers using the expiration
time value that can be set by senders. This is a way to free
space in the buffers (one preferentially deletes older 
messages in cases of conflict); to eliminate stale replicas in the
system; and to limit the time for which destinations must
hold message identifiers to dispose of duplicates.
4.4 Reliability and Acknowledgment 
Mechanisms
As already discussed, at-most-once message delivery is the
best that can be achieved in terms of delivery semantics in
partially connected ad-hoc settings. However, it is 
possible to improve the reliability of the system with efficient
acknowledgment mechanisms. EMMA provides a 
mechanism for failure notification to applications if the 
acknowledgment is not received within a given timeout (that can
be configured by application developers). This mechanism
is the one that distinguishes the delivery of persistent and
non-persistent messages in our JMS implementation: the
deliveries of the former are notified to the senders, whereas
the latter are not.
We use acknowledgment messages not only to inform senders
about the successful delivery of messages but also to delete
the replicas of the delivered messages that are still present
in the network. Each host maintains a list of the messages
Middleware for Pervasive and ad-hoc Computing 124
successfully delivered that is updated as part of the normal
process of information exchange between the hosts. The lists
are exchanged during the first steps of the anti-entropic 
epidemic protocol with a certain predefined frequency. In the
case of messages with multiple recipients, a list of the actual
recipients is also stored. When a host receives the list, it
checks its message buffer and updates it according to the
following rules: (1) if a message has a single recipient and
it has been delivered, it is deleted from the buffer; (2) if a
message has multiple recipients, the identifiers of the 
delivered hosts are deleted from the associated list of recipients.
If the resulting length of the list of recipients is zero, the
message is deleted from the buffer.
These lists have, clearly, finite dimensions and are 
implemented as circular queues. This simple mechanism, together
with the use of expiration timestamps, guarantees that the
old acknowledgment notifications are deleted from the 
system after a limited period of time.
In order to improve the reliability of EMMA, a design
mechanism for intelligent replication of queues and topics
based on the context information could be developed. 
However this is not yet part of the current architecture of EMMA.
5. IMPLEMENTATION AND PRELIMINARY
EVALUATION
We implemented a prototype of our platform using the
J2ME Personal Profile. The size of the executable is about
250KB including the JMS 1.1 jar file; this is a perfectly 
acceptable figure given the available memory of the current
mobile devices on the market. We tested our prototype on
HP IPaq PDAs running Linux, interconnected with 
WaveLan, and on a number of laptops with the same network
interface.
We also evaluated the middleware platform using the 
OMNET++ discrete event simulator [17] in order to explore a
range of mobile scenarios that incorporated a more realistic
number of hosts than was achievable experimentally. More
specifically, we assessed the performance of the system in
terms of delivery ratio and average delay, varying the 
density of population and the buffer size, and using persistent
and non-persistent messages with different priorities.
The simulation results show that the EMMA"s 
performance, in terms of delivery ratio and delay of persistent
messages with higher priorities, is good. In general, it is
evident that the delivery ratio is strongly related to the 
correct dimensioning of the buffers to the maximum acceptable
delay. Moreover, the epidemic algorithms are able to 
guarantee a high delivery ratio if one evaluates performance over
a time interval sufficient for the dissemination of the replicas
of messages (i.e., the infection spreading) in a large portion
of the ad-hoc network.
One consequence of the dimensioning problem is that 
scalability may be seriously impacted in peer-to-peer 
middleware for mobile computing due to the resource poverty of
the devices (limited memory to store temporarily messages)
and the number of possible interconnections in ad-hoc 
settings. What is worse is that common forms of commercial
and social organisation (six degrees of separation) mean that
even modest TTL values on messages will lead to widespread
flooding of epidemic messages. This problem arises because
of the lack of intelligence in the epidemic protocol, and can
be addressed by selecting carrier nodes for messages with
greater care. The details of this process are, however, 
outside the scope of this paper (but may be found in [13]) and do
not affect the foundation on which the EMMA middleware
is based: the ability to deliver messages asynchronously.
6. CRITICAL VIEW OF THE STATE OF
THE ART
The design of middleware platforms for mobile 
computing requires researchers to answer new and fundamentally
different questions; simply assuming the presence of wired
portions of the network on which centralised functionality
can reside is not generalisable. Thus, it is necessary to 
investigate novel design principles and to devise architectural
patterns that differ from those traditionally exploited in the
design of middleware for fixed systems.
As an example, consider the recent cross-layering trend in
ad-hoc networking [1]. This is a way of re-thinking software
systems design, explicitly abandoning the classical forms of
layering, since, although this separation of concerns afford
portability, it does so at the expense of potential efficiency
gains. We believe that it is possible to view our approach
as an instance of cross-layering. In fact, we have added the
epidemic network protocol at middleware level and, at the
same time, we have used the existing synchronous network
protocol if present both in delivering messages (traditional
layering) and in informing the middleware about when 
messages may be delivered by revealing details of the forwarding
tables (layer violation). For this reason, we prefer to 
consider them jointly as the communication layer of our 
platform together providing more efficient message delivery.
Another interesting aspect is the exploitation of context
and system information to improve the performance of 
mobile middleware platforms. Again, as a result of adopting
a cross-layering methodology, we are able to build systems
that gather information from the underlying operating 
system and communication components in order to allow for
adaptation of behaviour. We can summarise this conceptual
design approach by saying that middleware platforms must
be not only context-aware (i.e., they should be able to 
extract and analyse information from the surrounding context)
but also system-aware (i.e., they should be able to gather
information from the software and hardware components of
the mobile system).
A number of middleware systems have been developed to
support ad-hoc networking with the use of asynchronous
communication (such as LIME, XMIDDLE, STEAM [11]).
In particular, the STEAM platform is an interesting 
example of event-based middleware for ad-hoc networks, 
providing location-aware message delivery and an effective solution
for event filtering.
A discussion of JMS, and its mobile realisation, has 
already been conducted in Sections 4 and 2. The Swiss 
company Softwired has developed the first JMS middleware for
mobile computing, called iBus Mobile [10]. The main 
components of this typically infrastructure-based architecture
are the JMS provider, the so-called mobile JMS gateway,
which is deployed on a fixed host and a lightweight JMS
client library. The gateway is used for the communication
between the application server and mobile hosts. The 
gateway is seen by the JMS provider as a normal JMS client. The
JMS provider can be any JMS-enabled application server,
such as BEA Weblogic. Pronto [19] is an example of 
mid125 Middleware 2004 Companion
dleware system based on messaging that is specifically 
designed for mobile environments. The platform is composed
of three classes of components: mobile clients implementing
the JMS specification, gateways that control traffic, 
guaranteeing efficiency and possible user customizations using
different plug-ins and JMS servers. Different configurations
of these components are possible; with respect to mobile ad
hoc networks applications, the most interesting is 
Serverless JMS. The aim of this configuration is to adapt JMS
to a decentralized model. The publish-subscribe model 
exploits the efficiency and the scalability of the underlying IP
multicast protocol. Unreliable and reliable message delivery
services are provided: reliability is provided through a 
negative acknowledgment-based protocol. Pronto represents a
good solution for infrastructure-based mobile networks but
it does not adequately target ad-hoc settings, since mobile
nodes rely on fixed servers for the exchange of messages.
Other MOM implemented for mobile environments exist;
however, they are usually straightforward extensions of 
existing middleware [8]. The only implementation of MOM
specifically designed for mobile ad-hoc networks was 
developed at the University of Newcastle [18]. This work is again
a JMS adaptation; the focus of that implementation is on
group communication and the use of application level 
routing algorithms for topic delivery of messages. However, there
are a number of differences in the focus of our work. The
importance that we attribute to disconnections makes 
persistence a vital requirement for any middleware that needs
to be used in mobile ad-hoc networks. The authors of [18]
signal persistence as possible future work, not considering
the fact that routing a message to a non-connected host will
result in delivery failure. This is a remarkable limitation in
mobile settings where unpredictable disconnections are the
norm rather than the exception.
7. ROADMAP AND CONCLUSIONS
Asynchronous communication is a useful communication
paradigm for mobile ad-hoc networks, as hosts are allowed to
come, go and pick up messages when convenient, also taking
account of their resource availability (e.g., power, 
connectivity levels). In this paper we have described the state of the
art in terms of MOM for mobile systems. We have also
shown a proof of concept adaptation of JMS to the extreme
scenario of partially connected mobile ad-hoc networks.
We have described and discussed the characteristics and
differences of our solution with respect to traditional JMS
implementations and the existing adaptations for mobile 
settings. However, trade-offs between application-level routing
and resource usage should also be investigated, as mobile
devices are commonly power/resource scarce. A key 
limitation of this work is the poorly performing epidemic 
algorithm and an important advance in the practicability of
this work requires an algorithm that better balances the
needs of efficiency and message delivery probability. We
are currently working on algorithms and protocols that, 
exploiting probabilistic and statistical techniques on the basis
of small amounts of exchanged information, are able to 
improve considerably the efficiency in terms of resources 
(memory, bandwidth, etc) and the reliability of our middleware
platform [13].
One futuristic research development, which may take these
ideas of adaptation of messaging middleware for mobile 
environments further is the introduction of more mobility 
oriented communication extensions, for instance the support
of geocast (i.e., the ability to send messages to specific 
geographical areas).
8. REFERENCES
[1] M. Conti, G. Maselli, G. Turi, and S. Giordano.
Cross-layering in Mobile ad-hoc Network Design. IEEE
Computer, 37(2):48-51, February 2004.
[2] A. Demers, D. Greene, C. Hauser, W. Irish, J. Larson,
S. Shenker, H. Sturgis, D. Swinehart, and D. Terry.
Epidemic Algorithms for Replicated Database
Maintenance. In Sixth Symposium on Principles of
Distributed Computing, pages 1-12, August 1987.
[3] A. Doria, M. Uden, and D. P. Pandey. Providing
connectivity to the Saami nomadic community. In
Proceedings of the Second International Conference on
Open Collaborative Design for Sustainable Innovation,
December 2002.
[4] M. Haahr, R. Cunningham, and V. Cahill. Supporting
CORBA applications in a Mobile Environment. In 5th
International Conference on Mobile Computing and
Networking (MOBICOM99), pages 36-47. ACM, August
1999.
[5] M. Hapner, R. Burridge, R. Sharma, J. Fialli, and
K. Stout. Java Message Service Specification Version 1.1.
Sun Microsystems, Inc., April 2002.
http://java.sun.com/products/jms/.
[6] J. Hart. WebSphere MQ: Connecting your applications
without complex programming. IBM WebSphere Software
White Papers, 2003.
[7] S. Hayward and M. Pezzini. Marrying Middleware and
Mobile Computing. Gartner Group Research Report,
September 2001.
[8] IBM. WebSphere MQ EveryPlace Version 2.0, November
2002. http://www-3.ibm.com/software/integration/wmqe/.
[9] ITU. Connecting remote communities. Documents of the
World Summit on Information Society, 2003.
http://www.itu.int/osg/spu/wsis-themes.
[10] S. Maffeis. Introducing Wireless JMS. Softwired AG,
www.sofwired-inc.com, 2002.
[11] C. Mascolo, L. Capra, and W. Emmerich. Middleware for
Mobile Computing. In E. Gregori, G. Anastasi, and
S. Basagni, editors, Advanced Lectures on Networking,
volume 2497 of Lecture Notes in Computer Science, pages
20-58. Springer Verlag, 2002.
[12] Microsoft. Microsoft Message Queuing (MSMQ) Version
2.0 Documentation.
[13] M. Musolesi, S. Hailes, and C. Mascolo. Adaptive routing
for intermittently connected mobile ad-hoc networks.
Technical report, UCL-CS Research Note, July 2004.
Submitted for Publication.
[14] Sun Microsystems. Java Naming and Directory Interface
(JNDI) Documentation Version 1.2. 2003.
http://java.sun.com/products/jndi/.
[15] Sun Microsystems. Jini Specification Version 2.0, 2003.
http://java.sun.com/products/jini/.
[16] A. Vahdat and D. Becker. Epidemic routing for Partially
Connected ad-hoc Networks. Technical Report CS-2000-06,
Department of Computer Science, Duke University, 2000.
[17] A. Vargas. The OMNeT++ discrete event simulation
system. In Proceedings of the European Simulation
Multiconference (ESM"2001), Prague, June 2001.
[18] E. Vollset, D. Ingham, and P. Ezhilchelvan. JMS on Mobile
ad-hoc Networks. In Personal Wireless Communications
(PWC), pages 40-52, Venice, September 2003.
[19] E. Yoneki and J. Bacon. Pronto: Mobilegateway with
publish-subscribe paradigm over wireless network.
Technical Report 559, University of Cambridge, Computer
Laboratory, February 2003.
Middleware for Pervasive and ad-hoc Computing 126
Evaluating Adaptive Resource Management for
Distributed Real-Time Embedded Systems
Nishanth Shankaran,
∗
Xenofon Koutsoukos, Douglas C. Schmidt, and Aniruddha Gokhale
Dept. of EECS, Vanderbilt University, Nashville
ABSTRACT
A challenging problem faced by researchers and developers
of distributed real-time and embedded (DRE) systems is 
devising and implementing effective adaptive resource 
management strategies that can meet end-to-end quality of service
(QoS) requirements in varying operational conditions. This
paper presents two contributions to research in adaptive 
resource management for DRE systems. First, we describe the
structure and functionality of the Hybrid Adaptive 
Resourcemanagement Middleware (HyARM), which provides 
adaptive resource management using hybrid control techniques
for adapting to workload fluctuations and resource 
availability. Second, we evaluate the adaptive behavior of HyARM
via experiments on a DRE multimedia system that distributes
video in real-time. Our results indicate that HyARM yields
predictable, stable, and high system performance, even in the
face of fluctuating workload and resource availability.
Categories and Subject Descriptors
C.2.4 [Distributed Systems]: Distributed Applications;
D.4.7 [Organization and Design]: Real-time Systems and
Embedded Systems
1. INTRODUCTION
Achieving end-to-end real-time quality of service (QoS)
is particularly important for open distributed real-time and
embedded (DRE) systems that face resource constraints, such
as limited computing power and network bandwidth. 
Overutilization of these system resources can yield unpredictable
and unstable behavior, whereas under-utilization can yield
excessive system cost. A promising approach to meeting
these end-to-end QoS requirements effectively, therefore, is
to develop and apply adaptive middleware [10, 15], which is
software whose functional and QoS-related properties can be
modified either statically or dynamically. Static 
modifications are carried out to reduce footprint, leverage 
capabilities that exist in specific platforms, enable functional 
subsetting, and/or minimize hardware/software infrastructure
dependencies. Objectives of dynamic modifications include
optimizing system responses to changing environments or 
requirements, such as changing component interconnections,
power-levels, CPU and network bandwidth availability, 
latency/jitter, and workload.
In open DRE systems, adaptive middleware must make
such modifications dependably, i.e., while meeting 
stringent end-to-end QoS requirements, which requires the 
specification and enforcement of upper and lower bounds on
system resource utilization to ensure effective use of 
system resources. To meet these requirements, we have 
developed the Hybrid Adaptive Resource-management 
Middleware (HyARM), which is an open-source1
distributed 
resource management middleware.
HyARM is based on hybrid control theoretic techniques [8],
which provide a theoretical framework for designing 
control of complex system with both continuous and discrete
dynamics. In our case study, which involves a distributed
real-time video distribution system, the task of adaptive 
resource management is to control the utilization of the 
different resources, whose utilizations are described by 
continuous variables. We achieve this by adapting the resolution
of the transmitted video, which is modeled as a continuous
variable, and by changing the frame-rate and the 
compression, which are modeled by discrete actions. We have 
implemented HyARM atop The ACE ORB (TAO) [13], which
is an implementation of the Real-time CORBA 
specification [12]. Our results show that (1) HyARM ensures 
effective system resource utilization and (2) end-to-end QoS
requirements of higher priority applications are met, even in
the face of fluctuations in workload.
The remainder of the paper is organized as follows: 
Section 2 describes the architecture, functionality, and resource
utilization model of our DRE multimedia system case study;
Section 3 explains the structure and functionality of HyARM;
Section 4 evaluates the adaptive behavior of HyARM via 
experiments on our multimedia system case study; Section 5
compares our research on HyARM with related work; and
Section 6 presents concluding remarks.
1
The code and examples for HyARM are available at www.
dre.vanderbilt.edu/∼nshankar/HyARM/.
Article 7
2. CASE STUDY: DRE MULTIMEDIA 
SYSTEM
This section describes the architecture and QoS 
requirements of our DRE multimedia system.
2.1 Multimedia System Architecture
Wireless Link
Wireless Link
Wireless
Link
`
`
`
Physical Link
Physical Link
Physical Link
Base Station
End Receiver
End Receiver
End Receiver`
Physical Link
End Receiver
UAV
Camera
Video
Encoder
Camera
Video
Encoder
Camera
Video
Encoder
UAV
Camera
Video
Encoder
Camera
Video
Encoder
Camera
Video
Encoder
UAV
Camera
Video
Encoder
Camera
Video
Encoder
Camera
Video
Encoder
Figure 1: DRE Multimedia System Architecture
The architecture for our DRE multimedia system is shown
in Figure 1 and consists of the following entities: (1)Data
source (video capture by UAV), where video is captured
(related to subject of interest) by camera(s) on each UAV,
followed by encoding of raw video using a specific encoding
scheme and transmitting the video to the next stage in the
pipeline. (2)Data distributor (base station), where the
video is processed to remove noise, followed by 
retransmission of the processed video to the next stage in the pipeline.
(3) Sinks (command and control center), where the
received video is again processed to remove noise, then 
decoded and finally rendered to end user via graphical displays.
Significant improvements in video encoding/decoding and
(de)compression techniques have been made as a result of
recent advances in video encoding and compression 
techniques [14]. Common video compression schemes are 
MPEG1, MPEG-2, Real Video, and MPEG-4. Each compression
scheme is characterized by its resource requirement, e.g., the
computational power to (de)compress the video signal and
the network bandwidth required to transmit the compressed
video signal. Properties of the compressed video, such as 
resolution and frame-rate determine both the quality and the
resource requirements of the video.
Our multimedia system case study has the following 
endto-end real-time QoS requirements: (1) latency, (2) 
interframe delay (also know as jitter), (3) frame rate, and (4)
picture resolution. These QoS requirements can be 
classified as being either hard or soft. Hard QoS requirements
should be met by the underlying system at all times, whereas
soft QoS requirements can be missed occasionally.2
For our
case study, we treat QoS requirements such as latency and
jitter as harder QoS requirements and strive to meet these
requirements at all times. In contrast, we treat QoS 
requirements such as video frame rate and picture resolution as
softer QoS requirements and modify these video properties
adaptively to handle dynamic changes in resource 
availabil2
Although hard and soft are often portrayed as two discrete
requirement sets, in practice they are usually two ends of
a continuum ranging from softer to harder rather than
two disjoint points.
ity effectively.
2.2 DRE Multimedia System Rresources
There are two primary types of resources in our DRE
multimedia system: (1) processors that provide 
computational power available at the UAVs, base stations, and end
receivers and (2) network links that provide communication
bandwidth between UAVs, base stations, and end receivers.
The computing power required by the video capture and
encoding tasks depends on dynamic factors, such as speed
of the UAV, speed of the subject (if the subject is mobile),
and distance between UAV and the subject. The wireless
network bandwidth available to transmit video captured by
UAVs to base stations also depends on the wireless 
connectivity between the UAVs and the base station, which in-turn
depend on dynamic factors such as the speed of the UAVs
and the relative distance between UAVs and base stations.
The bandwidth of the link between the base station and
the end receiver is limited, but more stable than the 
bandwidth of the wireless network. Resource requirements and
availability of resources are subjected to dynamic changes.
Two classes of applications - QoS-enabled and best-effort
- use the multimedia system infrastructure described above
to transmit video to their respective receivers. QoS-enabled
class of applications have higher priority over best-effort
class of application. In our study, emergency response 
applications belong to QoS-enabled and surveillance applications
belong to best-effort class. For example, since a stream from
an emergency response application is of higher importance
than a video stream from a surveillance application, it 
receives more resources end-to-end.
Since resource availability significantly affects QoS, we use
current resource utilization as the primary indicator of 
system performance. We refer to the current level of system
resource utilization as the system condition. Based on this
definition, we can classify system conditions as being either
under, over, or effectively utilized.
Under-utilization of system resources occurs when the 
current resource utilization is lower than the desired lower bound
on resource utilization. In this system condition, residual
system resources (i.e., network bandwidth and 
computational power) are available in large amounts after meeting
end-to-end QoS requirements of applications. These 
residual resources can be used to increase the QoS of the 
applications. For example, residual CPU and network bandwidth
can be used to deliver better quality video (e.g., with greater
resolution and higher frame rate) to end receivers.
Over-utilization of system resources occurs when the 
current resource utilization is higher than the desired upper
bound on resource utilization. This condition can arise
from loss of resources - network bandwidth and/or 
computing power at base station, end receiver or at UAV - or
may be due to an increase in resource demands by 
applications. Over-utilization is generally undesirable since the
quality of the received video (such as resolution and frame
rate) and timeliness properties (such as latency and jitter)
are degraded and may result in an unstable (and thus 
ineffective) system.
Effective resource utilization is the desired system 
condition since it ensures that end-to-end QoS requirements of
the UAV-based multimedia system are met and utilization of
both system resources, i.e., network bandwidth and 
computational power, are within their desired utilization bounds.
Article 7
Section 3 describes techniques we applied to achieve effective
utilization, even in the face of fluctuating resource 
availability and/or demand.
3. OVERVIEW OF HYARM
This section describes the architecture of the Hybrid 
Adaptive Resource-management Middleware (HyARM). HyARM
ensures efficient and predictable system performance by 
providing adaptive resource management, including monitoring
of system resources and enforcing bounds on application 
resource utilization.
3.1 HyARM Structure and Functionality
Resource Utilization
Legend
Resource Allocation
Application Parameters
Figure 2: HyARM Architecture
HyARM is composed of three types of entities shown in
Figure 2 and described below:
Resource monitors observe the overall resource 
utilization for each type of resource and resource utilization per
application. In our multimedia system, there are resource
monitors for CPU utilization and network bandwidth. CPU
monitors observe the CPU resource utilization of UAVs, base
station, and end receivers. Network bandwidth monitors 
observe the network resource utilization of (1) wireless network
link between UAVs and the base station and (2) wired 
network link between the base station and end receivers.
The central controller maintains the system resource
utilization below a desired bound by (1) processing periodic
updates it receives from resource monitors and (2) 
modifying the execution of applications accordingly, e.g., by 
using different execution algorithms or operating the 
application with increased/decreased QoS. This adaptation 
process ensures that system resources are utilized efficiently and
end-to-end application QoS requirements are met. In our
multimedia system, the HyARM controller determines the
value of application parameters such as (1) video 
compression schemes, such as Real Video and MPEG-4, and/or (2)
frame rate, and (3) picture resolution. From the perspective
of hybrid control theoretic techniques [8], the different video
compression schemes and frame rate form the discrete 
variables of application execution and picture resolution forms
the continuous variables.
Application adapters modify application execution 
according to parameters recommended by the controller and
ensures that the operation of the application is in accordance
with the recommended parameters. In the current 
mplementation of HyARM, the application adapter modifies the
input parameters to the application that affect application
QoS and resource utilization - compression scheme, frame
rate, and picture resolution. In our future implementations,
we plan to use resource reservation mechanisms such as 
Differentiated Service [7, 3] and Class-based Kernel Resource
Management [4] to provision/reserve network and CPU 
resources. In our multimedia system, the application adapter
ensures that the video is encoded at the recommended frame
rate and resolution using the specified compression scheme.
3.2 Applying HyARM to the Multimedia 
System Case Study
HyARM is built atop TAO [13], a widely used open-source
implementation of Real-time CORBA [12]. HyARM can be
applied to ensure efficient, predictable and adaptive resource
management of any DRE system where resource availability
and requirements are subject to dynamic change.
Figure 3 shows the interaction of various parts of the
DRE multimedia system developed with HyARM, TAO,
and TAO"s A/V Streaming Service. TAO"s A/V Streaming
service is an implementation of the CORBA A/V 
Streaming Service specification. TAO"s A/V Streaming Service is
a QoS-enabled video distribution service that can transfer
video in real-time to one or more receivers. We use the A/V
Streaming Service to transmit the video from the UAVs to
the end receivers via the base station. Three entities of
Receiver
UAV
TAO
Resource
Utilization
HyARM
Central
Controller
A/V Streaming
Service : Sender
MPEG1
MPEG4
Real
Video
HyARM
Resource
Monitor
A/V Streaming
Service : Receiver
Compressed
Video Compressed
Video
Application
HyARM
Application
Adapter
Remote Object Call
Control
Inputs Resource
Utilization
Resource
Utilization /
Control Inputs
Control
Inputs
Legend
Figure 3: Developing the DRE Multimedia System
with HyARM
HyARM, namely the resource monitors, central controller,
and application adapters are built as CORBA servants, so
they can be distributed throughout a DRE system. 
Resource monitors are remote CORBA objects that update
the central controller periodically with the current resource
utilization. Application adapters are collocated with 
applications since the two interact closely.
As shown in Figure 3, UAVs compress the data using 
various compression schemes, such as MPEG1, MPEG4, and
Real Video, and uses TAO"s A/V streaming service to 
transmit the video to end receivers. HyARM"s resource monitors
continuously observe the system resource utilization and 
notify the central controller with the current utilization. 3
The interaction between the controller and the resource
monitors uses the Observer pattern [5]. When the controller
receives resource utilization updates from monitors, it 
computes the necessary modifications to application(s) 
parameters and notifies application adapter(s) via a remote 
operation call. Application adapter(s), that are collocated with
the application, modify the input parameters to the 
application - in our case video encoder - to modify the application
resource utilization and QoS.
3
The base station is not included in the figure since it only
retransmits the video received from UAVs to end receivers.
Article 7
4. PERFORMANCE RESULTS AND 
ANALYSIS
This section first describes the testbed that provides the
infrastructure for our DRE multimedia system, which was
used to evaluate the performance of HyARM. We then 
describe our experiments and analyze the results obtained to
empirically evaluate how HyARM behaves during 
underand over-utilization of system resources.
4.1 Overview of the Hardware and Software
Testbed
Our experiments were performed on the Emulab testbed
at University of Utah. The hardware configuration consists
of two nodes acting as UAVs, one acting as base station,
and one as end receiver. Video from the two UAVs were
transmitted to a base station via a LAN configured with
the following properties: average packet loss ratio of 0.3 and
bandwidth 1 Mbps. The network bandwidth was chosen to
be 1 Mbps since each UAV in the DRE multimedia system
is allocated 250 Kbps. These parameters were chosen to 
emulate an unreliable wireless network with limited bandwidth
between the UAVs and the base station. From the base 
station, the video was retransmitted to the end receiver via a
reliable wireline link of 10 Mbps bandwidth with no packet
loss.
The hardware configuration of all the nodes was chosen as
follows: 600 MHz Intel Pentium III processor, 256 MB 
physical memory, 4 Intel EtherExpress Pro 10/100 Mbps Ethernet
ports, and 13 GB hard drive. A real-time version of Linux
- TimeSys Linux/NET 3.1.214 based on RedHat Linux 
9was used as the operating system for all nodes. The 
following software packages were also used for our experiments: (1)
Ffmpeg 0.4.9-pre1, which is an open-source library (http:
//www.ffmpeg.sourceforge.net/download.php) that 
compresses video into MPEG-2, MPEG-4, Real Video, and many
other video formats. (2) Iftop 0.16, which is an 
opensource library (http://www.ex-parrot.com/∼pdw/iftop/)
we used for monitoring network activity and bandwidth 
utilization. (3) ACE 5.4.3 + TAO 1.4.3, which is an 
opensource (http://www.dre.vanderbilt.edu/TAO) 
implementation of the Real-time CORBA [12] specification upon which
HyARM is built. TAO provides the CORBA Audio/Video
(A/V) Streaming Service that we use to transmit the video
from the UAVs to end receivers via the base station.
4.2 Experiment Configuration
Our experiment consisted of two (emulated) UAVs that 
simultaneously send video to the base station using the 
experimentation setup described in Section 4.1. At the base 
station, video was retransmitted to the end receivers (without
any modifications), where it was stored to a file. Each UAV
hosted two applications, one QoS-enabled application 
(emergency response), and one best-effort application 
(surveillance). Within each UAV, computational power is shared
between the applications, while the network bandwidth is
shared among all applications.
To evaluate the QoS provided by HyARM, we monitored
CPU utilization at the two UAVs, and network bandwidth
utilization between the UAV and the base station. CPU 
resource utilization was not monitored at the base station and
the end receiver since they performed no 
computationallyintensive operations. The resource utilization of the 10 Mpbs
physical link between the base station and the end receiver
does not affect QoS of applications and is not monitored by
HyARM since it is nearly 10 times the 1 MB bandwidth
of the LAN between the UAVs and the base station. The
experiment also monitors properties of the video that affect
the QoS of the applications, such as latency, jitter, frame
rate, and resolution.
The set point on resource utilization for each resource was
specified at 0.69, which is the upper bound typically 
recommended by scheduling techniques, such as rate monotonic
algorithm [9]. Since studies [6] have shown that human eyes
can perceive delays more than 200ms, we use this as the
upper bound on jitter of the received video. QoS 
requirements for each class of application is specified during system
initialization and is shown in Table 1.
4.3 Empirical Results and Analysis
This section presents the results obtained from running
the experiment described in Section 4.2 on our DRE 
multimedia system testbed. We used system resource utilization
as a metric to evaluate the adaptive resource management
capabilities of HyARM under varying input work loads. We
also used application QoS as a metric to evaluate HyARM"s
capabilities to support end-to-end QoS requirements of the
various classes of applications in the DRE multimedia 
system. We analyze these results to explain the significant 
differences in system performance and application QoS.
Comparison of system performance is decomposed into
comparison of resource utilization and application QoS. For
system resource utilization, we compare (1) network 
bandwidth utilization of the local area network and (2) CPU
utilization at the two UAV nodes. For application QoS, we
compare mean values of video parameters, including (1) 
picture resolution, (2) frame rate, (3) latency, and (4) jitter.
Comparison of resource utilization. Over-utilization
of system resources in DRE systems can yield an unstable
system. In contrast, under-utilization of system resources
increases system cost. Figure 4 and Figure 5 compare the
system resource utilization with and without HyARM. 
Figure 4 shows that HyARM maintains system utilization close
to the desired utilization set point during fluctuation in 
input work load by transmitting video of higher (or lower) QoS
for QoS-enabled (or best-effort) class of applications during
over (or under) utilization of system resources.
Figure 5 shows that without HyARM, network 
utilization was as high as 0.9 during increase in workload 
conditions, which is greater than the utilization set point of 0.7
by 0.2. As a result of over-utilization of resources, QoS of
the received video, such as average latency and jitter, was
affected significantly. Without HyARM, system resources
were either under-utilized or over-utilized, both of which
are undesirable. In contrast, with HyARM, system resource
utilization is always close to the desired set point, even
during fluctuations in application workload. During 
sudden fluctuation in application workload, system conditions
may be temporarily undesirable, but are restored to the 
desired condition within several sampling periods. Temporary
over-utilization of resources is permissible in our multimedia
system since the quality of the video may be degraded for
a short period of time, though application QoS will be 
degraded significantly if poor quality video is transmitted for
a longer period of time.
Comparison of application QoS. Figures 6, Figure 7,
and Table 2 compare latency, jitter, resolution, and 
frameArticle 7
Class Resolution Frame Rate Latency (msec ) Jitter (msec)
QoS Enabled 1024 x 768 25 200 200
Best-effort 320 x 240 15 300 250
Table 1: Application QoS Requirements
Figure 4: Resource utilization with HyARM Figure 5: Resource utilization without HyARM
rate of the received video, respectively. Table 2 shows that
HyARM increases the resolution and frame video of 
QoSenabled applications, but decreases the resolution and frame
rate of best effort applications. During over utilization of
system resources, resolution and frame rate of lower priority
applications are reduced to adapt to fluctuations in 
application workload and to maintain the utilization of resources
at the specified set point.
It can be seen from Figure 6 and Figure 7 that HyARM
reduces the latency and jitter of the received video 
significantly. These figures show that the QoS of QoS-enabled
applications is greatly improved by HyARM. Although 
application parameters, such as frame rate and resolutions,
which affect the soft QoS requirements of best-effort 
applications may be compromised, the hard QoS requirements,
such as latency and jitter, of all applications are met.
HyARM responds to fluctuation in resource availability
and/or demand by constant monitoring of resource 
utilization. As shown in Figure 4, when resources utilization 
increases above the desired set point, HyARM lowers the 
utilization by reducing the QoS of best-effort applications. This
adaptation ensures that enough resources are available for
QoS-enabled applications to meet their QoS needs. 
Figures 6 and 7 show that the values of latency and jitter of
the received video of the system with HyARM are nearly half
of the corresponding value of the system without HyARM.
With HyARM, values of these parameters are well below
the specified bounds, whereas without HyARM, these value
are significantly above the specified bounds due to 
overutilization of the network bandwidth, which leads to network
congestion and results in packet loss. HyARM avoids this
by reducing video parameters such as resolution, frame-rate,
and/or modifying the compression scheme used to compress
the video.
Our conclusions from analyzing the results described above
are that applying adaptive middleware via hybrid control to
DRE system helps to (1) improve application QoS, (2) 
increase system resource utilization, and (3) provide better
predictability (lower latency and inter-frame delay) to 
QoSenabled applications. These improvements are achieved largely
due to monitoring of system resource utilization, efficient
system workload management, and adaptive resource 
provisioning by means of HyARM"s network/CPU resource 
monitors, application adapter, and central controller, 
respectively.
5. RELATED WORK
A number of control theoretic approaches have been 
applied to DRE systems recently. These techniques aid in 
overcoming limitations with traditional scheduling approaches
that handle dynamic changes in resource availability poorly
and result in a rigidly scheduled system that adapts poorly
to change. A survey of these techniques is presented in [1].
One such approach is feedback control scheduling (FCS) [2,
11]. FCS algorithms dynamically adjust resource allocation
by means of software feedback control loops. FCS 
algorithms are modeled and designed using rigorous 
controltheoretic methodologies. These algorithms provide robust
and analytical performance assurances despite uncertainties
in resource availability and/or demand. Although existing
FCS algorithms have shown promise, these algorithms often
assume that the system has continuous control variable(s)
that can continuously be adjusted. While this assumption
holds for certain classes of systems, there are many classes
of DRE systems, such as avionics and total-ship computing
environments that only support a finite a priori set of 
discrete configurations. The control variables in such systems
are therefore intrinsically discrete.
HyARM handles both continuous control variables, such
as picture resolution, and discrete control variable, such as
discrete set of frame rates. HyARM can therefore be applied
to system that support continuous and/or discrete set of
control variables. The DRE multimedia system as described
in Section 2 is an example DRE system that offers both 
continuous (picture resolution) and discrete set (frame-rate) of
control variables. These variables are modified by HyARM
to achieve efficient resource utilization and improved 
application QoS.
6. CONCLUDING REMARKS
Article 7
Figure 6: Comparison of Video Latency Figure 7: Comparison of Video Jitter
Source Picture Size / Frame Rate
With HyARM Without HyARM
UAV1 QoS Enabled Application 1122 X 1496 / 25 960 X 720 / 20
UAV1 Best-effort Application 288 X 384 / 15 640 X 480 / 20
UAV2 QoS Enabled Application 1126 X 1496 / 25 960 X 720 / 20
UAV2 Best-effort Application 288 X 384 / 15 640 X 480 / 20
Table 2: Comparison of Video Quality
Many distributed real-time and embedded (DRE) systems
demand end-to-end quality of service (QoS) enforcement
from their underlying platforms to operate correctly. These
systems increasingly run in open environments, where 
resource availability is subject to dynamic change. To meet
end-to-end QoS in dynamic environments, DRE systems can
benefit from an adaptive middleware that monitors system
resources, performs efficient application workload 
management, and enables efficient resource provisioning for 
executing applications.
This paper described HyARM, an adaptive middleware,
that provides effective resource management to DRE 
systems. HyARM employs hybrid control techniques to 
provide the adaptive middleware capabilities, such as resource
monitoring and application adaptation that are key to 
providing the dynamic resource management capabilities for
open DRE systems. We employed HyARM to a 
representative DRE multimedia system that is implemented using
Real-time CORBA and CORBA A/V Streaming Service.
We evaluated the performance of HyARM in a system
composed of three distributed resources and two classes of
applications with two applications each. Our empirical 
results indicate that HyARM ensures (1) efficient resource 
utilization by maintaining the resource utilization of system 
resources within the specified utilization bounds, (2) QoS 
requirements of QoS-enabled applications are met at all times.
Overall, HyARM ensures efficient, predictable, and adaptive
resource management for DRE systems.
7. REFERENCES
[1] T. F. Abdelzaher, J. Stankovic, C. Lu, R. Zhang, and Y. Lu.
Feddback Performance Control in Software Services. IEEE:
Control Systems, 23(3), June 2003.
[2] L. Abeni, L. Palopoli, G. Lipari, and J. Walpole. Analysis of a
reservation-based feedback scheduler. In IEEE Real-Time
Systems Symposium, Dec. 2002.
[3] S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang, and
W. Weiss. An architecture for differentiated services. Network
Information Center RFC 2475, Dec. 1998.
[4] H. Franke, S. Nagar, C. Seetharaman, and V. Kashyap.
Enabling Autonomic Workload Management in Linux. In
Proceedings of the International Conference on Autonomic
Computing (ICAC), New York, New York, May 2004. IEEE.
[5] E. Gamma, R. Helm, R. Johnson, and J. Vlissides. Design
Patterns: Elements of Reusable Object-Oriented Software.
Addison-Wesley, Reading, MA, 1995.
[6] G. Ghinea and J. P. Thomas. Qos impact on user perception
and understanding of multimedia video clips. In
MULTIMEDIA "98: Proceedings of the sixth ACM
international conference on Multimedia, pages 49-54, Bristol,
United Kingdom, 1998. ACM Press.
[7] Internet Engineering Task Force. Differentiated Services
Working Group (diffserv) Charter.
www.ietf.org/html.charters/diffserv-charter.html, 2000.
[8] X. Koutsoukos, R. Tekumalla, B. Natarajan, and C. Lu. Hybrid
Supervisory Control of Real-Time Systems. In 11th IEEE
Real-Time and Embedded Technology and Applications
Symposium, San Francisco, California, Mar. 2005.
[9] J. Lehoczky, L. Sha, and Y. Ding. The Rate Monotonic
Scheduling Algorithm: Exact Characterization and Average
Case Behavior. In Proceedings of the 10th IEEE Real-Time
Systems Symposium (RTSS 1989), pages 166-171. IEEE
Computer Society Press, 1989.
[10] J. Loyall, J. Gossett, C. Gill, R. Schantz, J. Zinky, P. Pal,
R. Shapiro, C. Rodrigues, M. Atighetchi, and D. Karr.
Comparing and Contrasting Adaptive Middleware Support in
Wide-Area and Embedded Distributed Object Applications. In
Proceedings of the 21st International Conference on
Distributed Computing Systems (ICDCS-21), pages 625-634.
IEEE, Apr. 2001.
[11] C. Lu, J. A. Stankovic, G. Tao, and S. H. Son. Feedback
Control Real-Time Scheduling: Framework, Modeling, and
Algorithms. Real-Time Systems Journal, 23(1/2):85-126, July
2002.
[12] Object Management Group. Real-time CORBA Specification,
OMG Document formal/02-08-02 edition, Aug. 2002.
[13] D. C. Schmidt, D. L. Levine, and S. Mungee. The Design and
Performance of Real-Time Object Request Brokers. Computer
Communications, 21(4):294-324, Apr. 1998.
[14] Thomas Sikora. Trends and Perspectives in Image and Video
Coding. In Proceedings of the IEEE, Jan. 2005.
[15] X. Wang, H.-M. Huang, V. Subramonian, C. Lu, and C. Gill.
CAMRIT: Control-based Adaptive Middleware for Real-time
Image Transmission. In Proc. of the 10th IEEE Real-Time and
Embedded Tech. and Applications Symp. (RTAS), Toronto,
Canada, May 2004.
Article 7
Context Awareness for Group Interaction Support
Alois Ferscha, Clemens Holzmann, Stefan Oppl
Institut für Pervasive Computing, Johannes Kepler Universität Linz
Altenbergerstraße 69, A-4040 Linz
{ferscha,holzmann,oppl}@soft.uni-linz.ac.at
ABSTRACT
In this paper, we present an implemented system for supporting
group interaction in mobile distributed computing environments.
First, an introduction to context computing and a motivation for
using contextual information to facilitate group interaction is
given. We then present the architecture of our system, which
consists of two parts: a subsystem for location sensing that
acquires information about the location of users as well as spatial
proximities between them, and one for the actual context-aware
application, which provides services for group interaction.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems - distributed applications.
H.1.2 [Models and Principles]: User/Machine Systems - human
factors.
H.5.3 [Information Interfaces and Presentation]: Group and
Organization Interfaces - asynchronous interaction, collaborative
computing, theory and models, synchronous interaction.
General Terms
Design, Experimentation
1. INTRODUCTION
Today"s computing environments are characterized by an
increasing number of powerful, wirelessly connected mobile
devices. Users can move throughout an environment while
carrying their computers with them and having remote access to
information and services, anytime and anywhere. New situations
appear, where the user"s context - for example his current
location or nearby people - is more dynamic; computation does
not occur at a single location and in a single context any longer,
but comprises a multitude of situations and locations. This
development leads to a new class of applications, which are
aware of the context in which they run in and thus bringing virtual
and real worlds together.
Motivated by this and the fact, that only a few studies have been
done for supporting group communication in such computing
environments [12], we have developed a system, which we refer
to as Group Interaction Support System (GISS). It supports group
interaction in mobile distributed computing environments in a
way that group members need not to at the same place any longer
in order to interact with each other or just to be aware of the
others situation.
In the following subchapters, we will give a short overview on
context aware computing and motivate its benefits for supporting
group interaction. A software framework for developing 
contextsensitive applications is presented, which serves as middleware
for GISS. Chapter 2 presents the architecture of GISS, and chapter
3 and 4 discuss the location sensing and group interaction
concepts of GISS in more detail. Chapter 5 gives a final summary
of our work.
1.1 What is Context Computing?
According to Merriam-Webster"s Online Dictionary1
, context is
defined as the interrelated conditions in which something exists
or occurs. Because this definition is very general, many
approaches have been made to define the notion of context with
respect to computing environments.
Most definitions of context are done by enumerating examples or
by choosing synonyms for context. The term context-aware has
been introduced first in [10] where context is referred to as
location, identities of nearby people and objects, and changes to
those objects. In [2], context is also defined by an enumeration of
examples, namely location, identities of the people around the
user, the time of the day, season, temperature etc. [9] defines
context as the user"s location, environment, identity and time.
Here we conform to a widely accepted and more formal
definition, which defines context as any information than can be
used to characterize the situation of an entity. An entity is a
person, place, or object that is considered relevant to the
interaction between a user and an application, including the user
and applications themselves. [4]
[4] identifies four primary types of context information
(sometimes referred to as context dimensions), that are - with
respect to characterizing the situation of an entity - more
important than others. These are location, identity, time and
activity, which can also be used to derive other sources of
contextual information (secondary context types). For example, if
we know a person"s identity, we can easily derive related
information about this person from several data sources (e.g. day
of birth or e-mail address).
According to this definition, [4] defines a system to be 
contextaware if it uses context to provide relevant information and/or
services to the user, where relevancy depends on the user"s task.
[4] also gives a classification of features for context-aware
applications, which comprises presentation of information and
services to a user, automatic execution of a service and tagging of
context to information for later retrieval.
Figure 1. Layers of a context-aware system
Context computing is based on two major issues, namely
identifying relevant context (identity, location, time, activity) and
using obtained context (automatic execution, presentation,
tagging). In order to do this, there are a few layers between (see
Figure 1). First, the obtained low-level context information has to
be transformed, aggregated and interpreted (context
transformation) and represented in an abstract context world
model (context representation), either centralized or
decentralized. Finally, the stored context information is used to
trigger certain context events (context triggering). [7]
1.2 Group Interaction in Context
After these abstract and formal definitions about what context and
context computing is, we will now focus on the main goal of this
work, namely how the interaction of mobile group members can
be supported by using context information.
In [6] we have identified organizational systems to be crucial for
supporting mobile groups (see Figure 2). First, there has to be an
Information and Knowledge Management System, which is
capable of supporting a team with its information processing- and
knowledge gathering needs. The next part is the Awareness
System, which is dedicated to the perceptualisation of the effects
of team activity. It does this by communicating work context,
agenda and workspace information to the users. The Interaction
Systems provide support for the communication among team
members, either synchronous or asynchronous, and for the shared
access to artefacts, such as documents. Mobility Systems deploy
mechanisms to enable any-place access to team memory as well
as the capturing and delivery of awareness information from and
to any places. Finally yet importantly, the organisational
innovation system integrates aspects of the team itself, like roles,
leadership and shared facilities.
With respect to these five aspects of team support, we focus on
interaction and partly cover mobility- and awareness-support.
Group interaction includes all means that enable group members
to communicate freely with all the other members. At this point,
the question how context information can be used for supporting
group interaction comes up. We believe that information about
the current situation of a person provides a surplus value to
existing group interaction systems. Context information facilitates
group interaction by allowing each member to be aware of the
availability status or the current location of each other group
member, which again makes it possible to form groups
dynamically, to place virtual post-its in the real world or to
determine which people are around.
Figure 2. Support for Mobile Groups [6]
Most of today"s context-aware applications use location and time
only, and location is referred to as a crucial type of context
information [3]. We also see the importance of location
information in mobile and ubiquitous environments, wherefore a
main focus of our work is on the utilization of location
information and information about users in spatial proximity.
Nevertheless, we believe that location, as the only used type of
context information, is not sufficient to support group interaction,
wherefore we also take advantage of the other three primary
types, namely identity, time and activity. This provides a
comprehensive description of a user"s current situation and thus
enabling numerous means for supporting group interaction, which
are described in detail in chapter 4.4.
When we look at the types of context information stated above,
we can see that all of them are single user-centred, taking into
account only the context of the user itself. We believe, that for the
support of group interaction, the status of the group itself has also
be taken into account. Therefore, we have added a fifth 
contextdimension group-context, which comprises more than the sum of
the individual member"s contexts. Group context includes any
information about the situation of a whole group, for example
how many members a group currently has or if a certain group
meets right now.
1.3 Context Middleware
The Group Interaction Support System (GISS) uses the 
softwareframework introduced in [1], which serves as a middleware for
developing context-sensitive applications. This so-called Context
Framework is based on a distributed communication architecture
and it supports different kinds of transport protocols and message
coding mechanisms.
89
A main feature of the framework is the abstraction of context
information retrieval via various sensors and its delivery to a level
where no difference appears, for the application designer,
between these different kinds of context retrieval mechanisms; the
information retrieval is hidden from the application developer.
This is achieved by so-called entities, which describe 
objectse.g. a human user - that are important for a certain context
scenario.
Entities express their functionality by the use of so-called
attributes, which can be loaded into the entity. These attributes
are complex pieces of software, which are implemented as Java
classes. Typical attributes are encapsulations of sensors, but they
can also be used to implement context services, for example to
notify other entities about location changes of users.
Each entity can contain a collection of such attributes, where an
entity itself is an attribute. The initial set of attributes an entity
contains can change dynamically at runtime, if an entity loads or
unloads attributes from the local storage or over the network. In
order to load and deploy new attributes, an entity has to reference
a class loader and a transport and lookup layer, which manages
the lookup mechanism for discovering other entities and the
transport. XML configuration files specify which initial set of
entities should be loaded and which attributes these entities own.
The communication between entities and attributes is based on
context events. Each attribute is able to trigger events, which are
addressed to other attributes and entities respectively,
independently on which physical computer they are running.
Among other things, and event contains the name of the event and
a list of parameters delivering information about the event itself.
Related with this event-based architecture is the use of ECA
(Event-Condition-Action)-rules for defining the behaviour of the
context system. Therefore, every entity has a rule-interpreter,
which catches triggered events, checks conditions associated with
them and causes certain actions. These rules are referenced by the
entity"s XML configuration. A rule itself is even able to trigger
the insertion of new rules or the unloading of existing rules at
runtime in order to change the behaviour of the context system
dynamically.
To sum up, the context framework provides a flexible, distributed
architecture for hiding low-level sensor data from high-level
applications and it hides external communication details from the
application developer. Furthermore, it is able to adapt its
behaviour dynamically by loading attributes, entities or 
ECArules at runtime.
2. ARCHITECTURE OVERVIEW
As GISS uses the Context Framework described in chapter 1.3 as
middleware, every user is represented by an entity, as well as the
central server, which is responsible for context transformation,
context representation and context triggering (cf. Figure 1).
A main part of our work is about the automated acquisition of
position information and its sensor-independent provision at
application level. We do not only sense the current location of
users, but also determine spatial proximities between them.
Developing the architecture, we focused on keeping the client as
simple as possible and reducing the communication between
client and server to a minimum.
Each client may have various location and/or proximity sensors
attached, which are encapsulated by respective Context
Framework-attributes (Sensor Encapsulation). These attributes
are responsible for integrating native sensor-implementations into
the Context Framework and sending sensor-dependent position
information to the server. We consider it very important to
support different types of sensors even at the same time, in order
to improve location accuracy on the one hand, while providing a
pervasive location-sensing environment with seamless transition
between different location sensing techniques on the other hand.
All location- and proximity-sensors supported are represented by
server-side context-attributes, which correspond to the client-side
sensor encapsulation-attributes and abstract the sensor-dependent
position information received from all users via the wireless
network (sensor abstraction). This requires a context repository,
where the mapping of diverse physical positions to standardized
locations is stored.
The standardized location- and proximity-information of each
user is then passed to the so-called Sensor Fusion-attributes,
one for symbolic locations and a second one for spatial
proximities. Their job is to merge location- and 
proximityinformation of clients, respectively, which is described in detail in
Chapter 3.3. Every time the symbolic location of a user or the
spatial proximity between two users changes, the Sensor
Fusion-attributes notify the GISS Core-attribute, which
controls the application.
Because of the abstraction of sensor-dependent position
information, the system can easily be extended by additional
sensors, just by implementing the (typically two) attributes for
encapsulating sensors (some sensors may not need a client-side
part), abstracting physical positions and observing the interface to
GISS Core.
Figure 3. Architecture of the Group Interaction Support
System (GISS)
The GISS Core-attribute is the central coordinator of the
application as it shows to the user. It not only serves as an
interface to the location-sensing subsystem, but also collects
further context information in other dimensions (time, identity or
activity).
90
Every time a change in the context of one or more users is
detected, GISS Core evaluates the effect of these changes on
the user, on the groups he belongs to and on the other members of
these groups. Whenever necessary, events are thrown to the
affected clients to trigger context-aware activities, like changing
the presentation of awareness information or the execution of
services.
The client-side part of the application is kept as simple as
possible. Furthermore, modular design was not only an issue on
the sensor side but also when designing the user interface
architecture. Thus, the complete user interface can be easily
exchanged, if all of the defined events are taken into account and
understood by the new interface-attribute.
The currently implemented user interface is split up in two parts,
which are also represented by two attributes. The central attribute
on client-side is the so-called Instant Messenger Encapsulation,
which on the one hand interacts with the server through events
and on the other hand serves as a proxy for the external
application the user interface is built on.
As external application, we use an existing open source instant
messenger - the ICQ2
-compliant Simple Instant Messenger
(SIM)3
. We have chosen and instant messenger as front-end
because it provides a well-known interface for most users and
facilitates a seamless integration of group interaction support, thus
increasing acceptance and ease of use. As the basic functionality
of the instant messenger - to serve as a client in an instant
messenger network - remains fully functional, our application is
able to use the features already provided by the messenger. For
example, the contexts activity and identity are derived from the
messenger network as it is described later.
The Instant Messenger Encapsulation is also responsible for
supporting group communication. Through the interface of the
messenger, it provides means of synchronous and asynchronous
communication as well as a context-aware reminder system and
tools for managing groups and the own availability status.
The second part of the user interface is a visualisation of the
user"s locations, which is implemented in the attribute Viewer.
The current implementation provides a two-dimensional map of
the campus, but it can easily be replaced by other visualisations, a
three-dimensional VRML-model for example. Furthermore, this
visualisation is used to show the artefacts for asynchronous
communication. Based on a floor plan-view of the geographical
area the user currently resides in, it gives a quick overview of
which people are nearby, their state and provides means to
interact with them.
In the following chapters 3 and 4, we describe the location
sensing-backend and the application front-end for supporting
group interaction in more detail.
3. LOCATION SENSING
In the following chapter, we will introduce a location model,
which is used for representing locations; afterwards, we will
describe the integration of location- and proximity-sensors in
2
http://www.icq.com/
3
http://sim-icq.sourceforge.net
more detail. Finally, we will have a closer look on the fusion of
location- and proximity-information, acquired by various sensors.
3.1 Location Model
A location model (i.e. a context representation for the 
contextinformation location) is needed to represent the locations of users,
in order to be able to facilitate location-related queries like given
a location, return a list of all the objects there or given an
object, return its current location. In general, there are two
approaches [3,5]: symbolic models, which represent location as
abstract symbols, and a geometric model, which represent
location as coordinates.
We have chosen a symbolic location model, which refers to
locations as abstract symbols like Room P111 or Physics
Building, because we do not require geometric location data.
Instead, abstract symbols are more convenient for human
interaction at application level. Furthermore, we use a symbolic
location containment hierarchy similar to the one introduced in
[11], which consists of top-level regions, which contain buildings,
which contain floors, and the floors again contain rooms. We also
distinguish four types, namely region (e.g. a whole campus),
section (e.g. a building or an outdoor section), level (e.g. a certain
floor in a building) and area (e.g. a certain room). We introduce a
fifth type of location, which we refer to as semantic. These 
socalled semantic locations can appear at any level in the hierarchy
and they can be nested, but they do not necessarily have a
geographic representation. Examples for such semantic locations
are tagged objects within a room (e.g. a desk and a printer on this
desk) or the name of a department, which contains certain rooms.
Figure 4. Symbolic Location Containment Hierarchy
The hierarchy of symbolic locations as well as the type of each
position is stored in the context repository.
3.2 Sensors
Our architecture supports two different kinds of sensors: location
sensors, which acquire location information, and proximity
sensors, which detect spatial proximities between users.
As described above, each sensor has a server- and in most cases a
corresponding client-side-implementation, too. While the 
clientattributes (Sensor Abstraction) are responsible for acquiring
low-level sensor-data and transmitting it to the server, the
corresponding Sensor Encapsulation-attributes transform them
into a uniform and sensor-independent format, namely symbolic
locations and IDs of users in spatial proximity, respectively.
91
Afterwards, the respective attribute Sensor Fusion is being
triggered with this sensor-independent information of a certain
user, detected by a particular sensor. Such notifications are
performed every time the sensor acquired new information.
Accordingly, Sensor Abstraction-attributes are responsible to
detect when a certain sensor is no longer available on the client
side (e.g. if it has been unplugged by the user) or when position
respectively proximity could not be determined any longer (e.g.
RFID reader cannot detect tags) and notify the corresponding
sensor fusion about this.
3.2.1 Location Sensors
In order to sense physical positions, the Sensor 
Encapsulationattributes asynchronously transmit sensor-dependent position
information to the server. The corresponding location Sensor
Abstraction-attributes collect these physical positions delivered
by the sensors of all users, and perform a repository-lookup in
order to get the associated symbolic location. This requires certain
tables for each sensor, which map physical positions to symbolic
locations. One physical position may have multiple symbolic
locations at different accuracy-levels in the location hierarchy
assigned to, for example if a sensor covers several rooms. If such
a mapping could be found, an event is thrown in order to notify
the attribute Location Sensor Fusion about the symbolic
locations a certain sensor of a particular user determined.
We have prototypically implemented three kinds of location
sensors, which are based on WLAN (IEEE 802.11), Bluetooth and
RFID (Radio Frequency Identification). We have chosen these
three completely different sensors because of their differences
concerning accuracy, coverage and administrative effort, in order
to evaluate the flexibility of our system (see Table 1).
The most accurate one is an RFID sensor, which is based on an
active RFID-reader. As soon as the reader is plugged into the
client, it scans for active RFID tags in range and transmits their
serial numbers to the server, where they are mapped to symbolic
locations. We also take into account RSSI (Radio Signal Strength
Information), which provides position accuracy of few
centimetres and thus enables us to determine which RFID-tag is
nearest. Due to this high accuracy, RFID is used for locating users
within rooms. The administration is quite simple; once a new
RFID tag is placed, its serial number simply has to be assigned to
a single symbolic location. A drawback is the poor availability,
which can be traced back to the fact that RFID readers are still
very expensive.
The second one is an 802.11 WLAN sensor. Therefore, we
integrated a purely software-based, commercial WLAN
positioning system for tracking clients on the university 
campuswide WLAN infrastructure. The reached position accuracy is in
the range of few meters and thus is suitable for location sensing at
the granularity of rooms. A big disadvantage is that a map of the
whole area has to be calibrated with measuring points at a
distance of 5 meters each. Because most mobile computers are
equipped with WLAN technology and the positioning-system is a
software-only solution, nearly everyone is able to use this kind of
sensor.
Finally, we have implemented a Bluetooth sensor, which detects
Bluetooth tags (i.e. Bluetooth-modules with known position) in
range and transmits them to the server that maps to symbolic
locations. Because of the fact that we do not use signal 
strengthinformation in the current implementation, the accuracy is above
10 meters and therefore a single Bluetooth MAC address is
associated with several symbolic locations, according to the
physical locations such a Bluetooth module covers. This leads to
the disadvantage that the range of each Bluetooth-tag has to be
determined and mapped to symbolic locations within this range.
Table 1. Comparison of implemented sensors
Sensor Accuracy Coverage Administration
RFID < 10 cm poor easy
WLAN 1-4 m very well
very 
timeconsuming
Bluetooth ~ 10 m well time-consuming
3.2.2 Proximity Sensors
Any sensor that is able to detect whether two users are in spatial
proximity is referred to as proximity sensor. Similar to the
location sensors, the Proximity Sensor Abstraction-attributes
collect physical proximity information of all users and transform
them to mappings of user-IDs.
We have implemented two types of proximity-sensors, which are
based on Bluetooth on the one hand and on fused symbolic
locations (see chapter 3.3.1) on the other hand.
The Bluetooth-implementation goes along with the
implementation of the Bluetooth-based location sensor. The
already determined Bluetooth MAC addresses in range of a
certain client are being compared with those of all other clients,
and each time the attribute Bluetooth Sensor Abstraction
detects congruence, it notifies the proximity sensor fusion about
this.
The second sensor is based on symbolic locations processed by
Location Sensor Fusion, wherefore it does not need a client-side
implementation. Each time the fused symbolic location of a
certain user changes, it checks whether he is at the same symbolic
location like another user and again notifies the proximity sensor
fusion about the proximity between these two users. The range
can be restricted to any level of the location containment
hierarchy, for example to room granularity.
A currently unresolved issue is the incomparable granularity of
different proximity sensors. For example, the symbolic locations
at same level in the location hierarchy mostly do not cover the
same geographic area.
3.3 Sensor Fusion
Core of the location sensing subsystem is the sensor fusion. It
merges data of various sensors, while coping with differences
concerning accuracy, coverage and sample-rate. According to the
two kinds of sensors described in chapter 3.2, we distinguish
between fusion of location sensors on the one hand, and fusion of
proximity sensors on the other hand.
The fusion of symbolic locations as well as the fusion of spatial
proximities operates on standardized information (cf. Figure 3).
This has the advantage, that additional position- and 
proximitysensors can be added easily or the fusion algorithms can be
replaced by ones that are more sophisticated.
92
Fusion is performed for each user separately and takes into
account the measurements at a single point in time only (i.e. no
history information is used for determining the current location of
a certain user). The algorithm collects all events thrown by the
Sensor Abstraction-attributes, performs fusion and triggers the
GISS Core-attribute if the symbolic location of a certain user or
the spatial proximity between users changed.
An important feature is the persistent storage of location- and
proximity-history in a database in order to allow future retrieval.
This enables applications to visualize the movement of users for
example.
3.3.1 Location Sensor Fusion
Goal of the fusion of location information is to improve precision
and accuracy by merging the set of symbolic locations supplied
by various location sensors, in order to reduce the number of
these locations to a minimum, ideally to a single symbolic
location per user. This is quite difficult, because different sensors
may differ in accuracy and sample rate as well.
The Location Sensor Fusion-attribute is triggered by events,
which are thrown by the Location Sensor 
Abstractionattributes. These events contain information about the identity of
the user concerned, his current location and the sensor by which
the location has been determined.
If the attribute Location Sensor Fusion receives such an event,
it checks if the amount of symbolic locations of the user
concerned has changed (compared with the last event). If this is
the case, it notifies the GISS Core-attribute about all symbolic
locations this user is currently associated with.
However, this information is not very useful on its own if a
certain user is associated with several locations. As described in
chapter 3.2.1, a single location sensor may deliver multiple
symbolic locations. Moreover, a certain user may have several
location sensors, which supply symbolic locations differing in
accuracy (i.e. different levels in the location containment
hierarchy). To cope with this challenge, we implemented a fusion
algorithm in order to reduce the number of symbolic locations to a
minimum (ideally to a single location).
In a first step, each symbolic location is associated with its
number of occurrences. A symbolic location may occur several
times if it is referred to by more than one sensor or if a single
sensor detects multiple tags, which again refer to several
locations. Furthermore, this number is added to the previously
calculated number of occurrences of each symbolic location,
which is a child-location of the considered one in the location
containment hierarchy. For example, if - in Figure 4 - room2
occurs two times and desk occurs a single time, the value 2 of
room2 is added to the value 1 of desk, whereby desk finally
gets the value 3. In a final step, only those symbolic locations are
left which are assigned with the highest number of occurrences.
A further reduction can be achieved by assigning priorities to
sensors (based on accuracy and confidence) and cumulating these
priorities for each symbolic location instead of just counting the
number of occurrences.
If the remaining fused locations have changed (i.e. if they differ
from the fused locations the considered user is currently
associated with), they are provided with the current timestamp,
written to the database and the GISS-attribute is notified about
where the user is probably located.
Finally, the most accurate, common location in the location
hierarchy is calculated (i.e. the least upper bound of these
symbolic locations) in order to get a single symbolic location. If it
changes, the GISS Core-attribute is triggered again.
3.3.2 Proximity Sensor Fusion
Proximity sensor fusion is much simpler than the fusion of
symbolic locations. The corresponding proximity sensor 
fusionattribute is triggered by events, which are thrown by the
Proximity Sensor Abstraction-attributes. These special events
contain information about the identity of the two users concerned,
if they are currently in spatial proximity or if proximity no longer
persists, and by which proximity-sensor this has been detected.
If the sensor fusion-attribute is notified by a certain Proximity
Sensor Abstraction-attribute about an existing spatial proximity,
it first checks if these two users are already known to be in
proximity (detected either by another user or by another
proximity-sensor of the user, which caused the event). If not, this
change in proximity is written to the context repository with
current timestamp. Similarly, if the attribute Proximity Fusion
is notified about an ended proximity, it checks if the users are still
known to be in proximity, and writes this change to the repository
if not.
Finally, if spatial proximity between the two users actually
changed, an event is thrown to notify the GISS Core-attribute
about this.
4. CONTEXTSENSITIVE INTERACTION
4.1 Overview
In most of today"s systems supporting interaction in groups, the
provided means lack any awareness of the user"s current context,
thus being unable to adapt to his needs.
In our approach, we use context information to enhance
interaction and provide further services, which offer new
possibilities to the user. Furthermore, we believe that interaction
in groups also has to take into account the current context of the
group itself and not only the context of individual group
members. For this reason, we also retrieve information about the
group"s current context, derived from the contexts of the group
members together with some sort of meta-information (see
chapter 4.3).
The sources of context used for our application correspond with
the four primary context types given in chapter 1.1 - identity (I),
location (L), time (T) and activity (A). As stated before, we also
take into account the context of the group the user is interaction
with, so that we could add a fifth type of context 
informationgroup awareness (G) - to the classification. Using this context
information, we can trigger context-aware activities in all of the
three categories described in chapter 1.1 - presentation of
information (P), automatic execution of services (A) and tagging
of context to information for later retrieval (T).
Table 2 gives an overview of activities we have already
implemented; they are described comprehensively in chapter 4.4.
The table also shows which types of context information are used
for each activity and the category the activity could be classified
in.
93
Table 2. Classification of implemented context-aware
activities
Service L T I A G P A T
Location Visualisation X X X
Group Building Support X X X X
Support for Synchronous
Communication
X X X X
Support for Asynchronous
Communication
X X X X X X X
Availability Management X X X
Task Management Support X X X X
Meeting Support X X X X X X
Reasons for implementing these very features are to take
advantage of all four types of context information in order to
support group interaction by utilizing a comprehensive knowledge
about the situation a single user or a whole group is in.
A critical issue for the user acceptance of such a system is the
usability of its interface. We have evaluated several ways of
presenting context-aware means of interaction to the user, until
we came to the solution we use right now. Although we think that
the user interface that has been implemented now offers the best
trade-off between seamless integration of features and ease of use,
it would be no problem to extend the architecture with other user
interfaces, even on different platforms.
The chosen solution is based on an existing instant messenger,
which offers several possibilities to integrate our system (see
chapter 4.2). The biggest advantage of this approach is that the
user is confronted with a graphical user interface he is already
used to in most cases. Furthermore, our system uses an instant
messenger account as an identifier, so that the user does not have
to register a further account anywhere else (for example, the user
can use his already existing ICQ2
-account).
4.2 Instant Messenger Integration
Our system is based upon an existing instant messenger, the 
socalled Simple Instant Messenger (SIM)3
. The implementation of
this messenger is carried out as a project at Sourceforge4
.
SIM supports multiple messenger protocols such as AIM5
, ICQ2
and MSN6
. It also supports connections to multiple accounts at
the same time. Furthermore, full support for SMS-notification
(where provided from the used protocol) is given.
SIM is based on a plug-in concept. All protocols as well as parts
of the user-interface are implemented as plug-ins. Its architecture
is also used to extend the application"s abilities to communicate
with external applications. For this purpose, a remote control
plug-in is provided, by which SIM can be controlled from
external applications via socket connection. This remote control
interface is extensively used by GISS for retrieving the contact
list, setting the user"s availability-state or sending messages. The
4
http://sourceforge.net/
5
http://www.aim.com/
6
http://messenger.msn.com/
functionality of the plug-in was extended in several ways, for
example to accept messages for an account (as if they would have
been sent via the messenger network).
The messenger, more exactly the contact list (i.e. a list of profiles
of all people registered with the instant messenger, which is
visualized by listing their names as it can be seen in Figure 5), is
also used to display locations of other members of the groups a
user belongs to. This provides location awareness without taking
too much space or requesting the user"s full attention. A more
comprehensive description of these features is given in chapter
4.4.
4.3 Sources of Context Information
While the location-context of a user is obtained from our location
sensing subsystem described in chapter 3, we consider further
types of context than location relevant for the support of group
interaction, too.
Local time as a very important context dimension can be easily
retrieved from the real time clock of the user"s system. Besides
location and time, we also use context information of user"s
activity and identity, where we exploit the functionality provided
by the underlying instant messenger system. Identity (or more
exactly, the mapping of IDs to names as well as additional
information from the user"s profile) can be distilled out of the
contents of the user"s contact list.
Information about the activity or a certain user is only available in
a very restricted area, namely the activity at the computer itself.
Other activities like making a phone call or something similar,
cannot be recognized with the current implementation of the
activity sensor. The only context-information used is the instant
messenger"s availability state, thus only providing a very coarse
classification of the user"s activity (online, offline, away, busy
etc.). Although this may not seem to be very much information, it
is surely relevant and can be used to improve or even enable
several services.
Having collected the context information from all available users,
it is now possible to distil some information about the context of a
certain group. Information about the context of a group includes
how many members the group currently has, if the group meets
right now, which members are participating at a meeting, how
many members have read which of the available posts from other
team members and so on.
Therefore, some additional information like a list of members for
each group is needed. These lists can be assembled manually (by
users joining and leaving groups) or retrieved automatically. The
context of a group is secondary context and is aggregated from
the available contexts of the group members. Every time the
context of a single group member changes, the context of the
whole group is changing and has to be recalculated.
With knowledge about a user"s context and the context of the
groups he belongs to, we can provide several context-aware
services to the user, which enhance his interaction abilities. A
brief description of these services is given in chapter 4.4.
94
4.4 Group Interaction Support
4.4.1 Visualisation of Location Information
An important feature is the visualisation of location information,
thus allowing users to be aware of the location of other users and
members of groups he joined, respectively.
As already described in chapter 2, we use two different forms of
visualisation. The maybe more important one is to display
location information in the contact list of the instant messenger,
right beside the name, thus being always visible while not
drawing the user"s attention on it (compared with a 
twodimensional view for example, which requires a own window for
displaying a map of the environment).
Due to the restricted space in the contact list, it has been
necessary to implement some sort of level-of-detail concept. As
we use a hierarchical location model, we are able to determine the
most accurate common location of two users. In the contact list,
the current symbolic location one level below the previously
calculated common location is then displayed. If, for example,
user A currently resides in room P121 at the first floor of a
building and user B, which has to be displayed in the contact list
of user A, is in room P304 at the third floor, the most accurate
common location of these two users is the building they are in.
For that reason, the floor (i.e. one level beyond the common
location, namely the building) of user B is displayed in the
contact list of user A. If both people reside on the same floor or
even in the same room, the room would be taken.
Figure 5 shows a screenshot of the Simple Instant Messenger3
,
where the current location of those people, whose location is
known by GISS, is displayed in brackets right beside their name.
On top of the image, the heightened, integrated GISS-toolbar is
shown, which currently contains the following, implemented
functionality (from left to right): Asynchronous communication
for groups (see chapter 4.4.4), context-aware reminders (see
chapter 4.4.6), two-dimensional visualisation of 
locationinformation, forming and managing groups (see chapter 4.4.2),
context-aware availability-management (see chapter 4.4.5) and
finally a button for terminating GISS.
Figure 5. GISS integration in Simple Instant Messenger3
As displaying just this short form of location may not be enough
for the user, because he may want to see the most accurate
position available, a fully qualified position is shown if a name
in the contact-list is clicked (e.g. in the form of
desk@room2@department1@1stfloor@building 1@campus).
The second possible form of visualisation is a graphical one. We
have evaluated a three-dimensional view, which was based on a
VRML model of the respective area (cf. Figure 6). Due to lacks in
navigational and usability issues, we decided to use a 
twodimensional view of the floor (it is referred to as level in the
location hierarchy, cf. Figure 4). Other levels of granularity like
section (e.g. building) and region (e.g. campus) are also provided.
In this floor-plan-based view, the current locations are shown in
the manner of ICQ2
contacts, which are placed at the currently
sensed location of the respective person. The availability-status of
a user, for example away if he is not on the computer right now,
or busy if he does not want to be disturbed, is visualized by
colour-coding the ICQ2
-flower left beside the name. Furthermore,
the floor-plan-view shows so-called the virtual post-its, which are
virtual counterparts of real-life post-its and serve as our means of
asynchronous communication (more about virtual post-its can be
found in chapter 4.4.4).
Figure 6. 3D-view of the floor (VRML)
Figure 7 shows the two-dimensional map of a certain floor, where
several users are currently located (visualized by their name and
the flower left beside). The location of the client, on which the
map is displayed, is visualized by a green circle. Down to the
right, two virtual post-its can be seen.
Figure 7. 2D view of the floor
Another feature of the 2D-view is the visualisation of 
locationhistory of users. As we store the complete history of a user"s
locations together with a timestamp, we are able to provide
information about the locations he has been back in time. When
the mouse is moved over the name of a certain user in the 
2Dview, footprints of a user, placed at the locations he has been,
are faded out the stronger, the older the location information is.
95
4.4.2 Forming and Managing Groups
To support interaction in groups, it is first necessary to form
groups. As groups can have different purposes, we distinguish two
types of groups.
So-called static groups are groups, which are built up manually
by people joining and leaving them. Static groups can be further
divided into two subtypes. In open static groups, everybody can
join and leave anytime, useful for example to form a group of
lecture attendees of some sort of interest group. Closed static
groups have an owner, who decides, which persons are allowed to
join, although everybody could leave again at any time. Closed
groups enable users for example to create a group of their friends,
thus being able to communicate with them easily.
In contrast to that, we also support the creation of dynamic
groups. They are formed among persons, who are at the same
location at the same time. The creation of dynamic groups is only
performed at locations, where it makes sense to form groups, for
example in lecture halls or meeting rooms, but not on corridors or
outdoor. It would also be not very meaningful to form a group
only of the people residing in the left front sector of a hall;
instead, the complete hall should be considered. For these
reasons, all the defined locations in the hierarchy are tagged,
whether they allow the formation of groups or not. Dynamic
groups are also not only formed granularity of rooms, but also on
higher levels in the hierarchy, for example with the people
currently residing in the area of a department.
As the members of dynamic groups constantly change, it is
possible to create an open static group out of them.
4.4.3 Synchronous Communication for Groups
The most important form of synchronous communication on
computers today is instant messaging; some people even see
instant messaging to be the real killer application on the Internet.
This has also motivated the decision to build GISS upon an
instant messaging system.
In today"s messenger systems, peer-to-peer-communication is
extensively supported. However, when it comes to
communication in groups, the support is rather poor most of the
time. Often, only sending a message to multiple recipients is
supported, lacking means to take into account the current state of
the recipients. Furthermore, groups can only be formed of
members in one"s contact list, thus being not able to send
messages to a group, where not all of its members are known
(which may be the case in settings, where the participants of a
lecture form a group).
Our approach does not have the mentioned restrictions. We
introduce group-entries in the user"s contact list; enable him or
his to send messages to this group easily, without knowing who
exactly is currently a member of this group. Furthermore, group
messages are only delivered to persons, who are currently not
busy, thus preventing a disturbance by a message, which is
possibly unimportant for the user.
These features cannot be carried out in the messenger network
itself, so whenever a message to a group account is sent, we
intercept it and route it through our system to all the recipients,
which are available at a certain time. Communication via a group
account is also stored centrally, enabling people to query missed
messages or simply viewing the message history.
4.4.4 Asynchronous Communication for Groups
Asynchronous communication in groups is not a new idea. The
goal of this approach is not to reinvent the wheel, as email is
maybe the most widely used form of asynchronous
communication on computers and is broadly accepted and
standardized. In out work, we aim at the combination of
asynchronous communication with location awareness.
For this reason, we introduce the concept of so-called virtual 
postits (cp. [13]), which are messages that are bound to physical
locations. These virtual post-its could be either visible for all
users that are passing by or they can be restricted to be visible for
certain groups of people only. Moreover, a virtual post-it can also
have an expiry date after which it is dropped and not displayed
anymore. Virtual post-its can also be commented by others, thus
providing some from of forum-like interaction, where each post-it
forms a thread.
Virtual post-its are displayed automatically, whenever a user
(available) passes by the first time. Afterwards, post-its can be
accessed via the 2D-viewer, where all visible post-its are shown.
All readers of a post-it are logged and displayed when viewing it,
providing some sort of awareness about the group members"
activities in the past.
4.4.5 Context-aware Availability Management
Instant messengers in general provide some kind of availability
information about a user. Although this information can be only
defined in a very coarse granularity, we have decided to use these
means of gathering activity context, because the introduction of
an additional one would strongly decrease the usability of the
system.
To support the user managing his availability, we provide an
interface that lets the user define rules to adapt his availability to
the current context. These rules follow the form on event (E) if
condition (C) then action (A), which is directly supported by the
ECA-rules of the Context Framework described in chapter 1.3.
The testing of conditions is periodically triggered by throwing
events (whenever the context of a user changes). The condition
itself is defined by the user, who can demand the change of his
availability status as the action in the rule. As a condition, the user
can define his location, a certain time (also triggering daily, every
week or every month) or any logical combination of these criteria.
4.4.6 Context-Aware Reminders
Reminders [14] are used to give the user the opportunity of
defining tasks and being reminded of those, when certain criteria
are fulfilled. Thus, a reminder can be seen as a post-it to oneself,
which is only visible in certain cases. Reminders can be bound to
a certain place or time, but also to spatial proximity of users or
groups. These criteria can be combined with Boolean operators,
thus providing a powerful means to remind the user of tasks that
he wants to carry out when a certain context occurs.
A reminder will only pop up the first time the actual context
meets the defined criterion. On showing up the reminder, the user
has the chance to resubmit it to be reminded again, for example
five minutes later or the next time a certain user is in spatial
proximity.
96
4.4.7 Context-Aware Recognition and Notification of
Group Meetings
With the available context information, we try to recognize
meetings of a group. The determination of the criteria, when the
system recognizes a group having a meeting, is part of the
ongoing work. In a first approach, we use the location- and
activity-context of the group members to determine a meeting.
Whenever more than 50 % of the members of a group are
available at a location, where a meeting is considered to make
sense (e.g. not on a corridor), a meeting minutes post-it is created
at this location and all absent group members are notified of the
meeting and the location it takes place.
During the meeting, the comment-feature of virtual post-its
provides a means to take notes for all of the participants. When
members are joining or leaving the meeting, this is automatically
added as a note to the list of comments.
Like the recognition of the beginning of a meeting, the
recognition of its end is still part of ongoing work. If the end of
the meeting is recognized, all group members get the complete list
of comments as a meeting protocol at the end of the meeting.
5. CONCLUSIONS
This paper discussed the potentials of support for group
interaction by using context information. First, we introduced the
notions of context and context computing and motivated their
value for supporting group interaction.
An architecture is presented to support context-aware group
interaction in mobile, distributed environments. It is built upon a
flexible and extensible framework, thus enabling an easy adoption
to available context sources (e.g. by adding additional sensors) as
well as the required form of representation.
We have prototypically developed a set of services, which
enhance group interaction by taking into account the current
context of the users as well as the context of groups itself.
Important features are dynamic formation of groups, visualization
of location on a two-dimensional map as well as unobtrusively
integrated in an instant-messenger, asynchronous communication
by virtual post-its, which are bound to certain locations, and a
context-aware availability-management, which adapts the
availability-status of a user to his current situation.
To provide location information, we have implemented a
subsystem for automated acquisition of location- and 
proximityinformation provided by various sensors, which provides a
technology-independent presentation of locations and spatial
proximities between users and merges this information using
sensor-independent fusion algorithms. A history of locations as
well as of spatial proximities is stored in a database, thus enabling
context history-based services.
6. REFERENCES
[1] Beer, W., Christian, V., Ferscha, A., Mehrmann, L.
Modeling Context-aware Behavior by Interpreted ECA
Rules. In Proceedings of the International Conference on
Parallel and Distributed Computing (EUROPAR"03).
(Klagenfurt, Austria, August 26-29, 2003). Springer Verlag,
LNCS 2790, 1064-1073.
[2] Brown, P.J., Bovey, J.D., Chen X. Context-Aware
Applications: From the Laboratory to the Marketplace.
IEEE Personal Communications, 4(5) (1997), 58-64.
[3] Chen, H., Kotz, D. A Survey of Context-Aware Mobile
Computing Research. Technical Report TR2000-381,
Computer Science Department, Dartmouth College,
Hanover, New Hampshire, November 2000.
[4] Dey, A. Providing Architectural Support for Building
Context-Aware Applications. Ph.D. Thesis, Department of
Computer Science, Georgia Institute of Technology,
Atlanta, November 2000.
[5] Svetlana Domnitcheva. Location Modeling: State of the Art
and Challenges. In Proceedings of the Workshop on
Location Modeling for Ubiquitous Computing. (Atlanta,
Georgia, United States, September 30, 2001). 13-19.
[6] Ferscha, A. Workspace Awareness in Mobile Virtual Teams.
In Proceedings of the IEEE 9th
International Workshop on
Enabling Technologies: Infrastructure for Collaborative
Enterprises (WETICE"00). (Gaithersburg, Maryland, March
14-16, 2000). IEEE Computer Society Press, 272-277.
[7] Ferscha, A. Coordination in Pervasive Computing
Environments. In Proceedings of the Twelfth International
IEEE Workshop on Enabling Technologies: Infrastructure
for Collaborative Enterprises (WETICE-2003). (June 9-11,
2003). IEEE Computer Society Press, 3-9.
[8] Leonhard, U. Supporting Location Awareness in Open
Distributed Systems. Ph.D. Thesis, Department of
Computing, Imperial College, London, May 1998.
[9] Ryan, N., Pascoe, J., Morse, D. Enhanced Reality
Fieldwork: the Context-Aware Archaeological Assistant.
Gaffney, V., Van Leusen, M., Exxon, S. (eds.) Computer
Applications in Archaeology (1997)
[10] Schilit, B.N., Theimer, M. Disseminating Active Map
Information to Mobile Hosts. IEEE Network, 8(5) (1994),
22-32.
[11] Schilit, B.N. A System Architecture for Context-Aware
Mobile Computing. Ph.D. Thesis, Columbia University,
Department of Computer Science, May 1995.
[12] Wang, B., Bodily, J., Gupta, S.K.S. Supporting Persistent
Social Groups in Ubiquitous Computing Environments
Using Context-Aware Ephemeral Group Service. In
Proceedings of the Second IEEE International Conference
on Pervasive Computing and Communications
(PerCom"04). (March 14-17, 2004). IEEE Computer Society
Press, 287-296.
[13] Pascoe, J. The Stick-e Note Architecture: Extending the
Interface Beyond the User. Proceedings of the 2nd
International Conference of Intelligent User Interfaces
(IUI"97). (Orlando, USA, 1997), 261-264.
[14] Dey, A., Abowd, G. CybreMinder: A Context-Aware System
for Supporting Re-minders. Proceedings of the 2nd
International Symposium on Handheld and Ubiquitous
Computing (HUC"00). (Bristol, UK, 2000), 172-186.
97
A Cross-Layer Approach to Resource Discovery
and Distribution in Mobile ad-hoc Networks
Chaiporn Jaikaeo
Computer Engineering
Kasetsart University, Thailand
(+662) 942-8555 Ext 1424
cpj@cpe.ku.ac.th
Xiang Cao
Computer and Information Sciences
University of Delaware, USA
(+1) 302-831-1131
cao@cis.udel.edu
Chien-Chung Shen
Computer and Information Sciences
University of Delaware, USA
(+1) 302-831-1951
cshen@cis.udel.edu
ABSTRACT
This paper describes a cross-layer approach to designing robust
P2P system over mobile ad-hoc networks. The design is based on
simple functional primitives that allow routing at both P2P and
network layers to be integrated to reduce overhead. With these
primitives, the paper addresses various load balancing techniques.
Preliminary simulation results are also presented.
Categories and Subject Descriptors
C.2.4 [Distributed Systems]: Distributed Applications
General Terms
Algorithms and design
1. INTRODUCTION
Mobile ad-hoc networks (MANETs) consist of mobile nodes that
autonomously establish connectivity via multi-hop wireless 
communications. Without relying on any existing, pre-configured
network infrastructure or centralized control, MANETs are useful
in situations where impromptu communication facilities are 
required, such as battlefield communications and disaster relief
missions. As MANET applications demand collaborative 
processing and information sharing among mobile nodes, resource 
(service) discovery and distribution have become indispensable 
capabilities.
One approach to designing resource discovery and distribution
schemes over MANETs is to construct a peer-to-peer (P2P) 
system (or an overlay) which organizes peers of the system into a
logical structure, on top of the actual network topology. However,
deploying such P2P systems over MANETs may result in either a
large number of flooding operations triggered by the reactive 
routing process, or inefficiency in terms of bandwidth utilization in
proactive routing schemes. Either way, constructing an overlay
will potentially create a scalability problem for large-scale
MANETs.
Due to the dynamic nature of MANETs, P2P systems should be
robust by being scalable and adaptive to topology changes. These
systems should also provide efficient and effective ways for peers
to interact, as well as other desirable application specific features.
This paper describes a design paradigm that uses the following
two functional primitives to design robust resource discovery and
distribution schemes over MANETs.
1. Positive/negative feedback. Query packets are used to 
explore a route to other peers holding resources of interest.
Optionally, advertisement packets are sent out to advertise
routes from other peers about available resources. When
traversing a route, these control packets measure goodness
of the route and leave feedback information on each node
along the way to guide subsequent control packets to 
appropriate directions.
2. Sporadic random walk. As the network topology and/or
the availability of resources change, existing routes may 
become stale while better routes become available. Sporadic
random walk allows a control packet to explore different
paths and opportunistically discover new and/or better
routes.
Adopting this paradigm, the whole MANET P2P system operates
as a collection of autonomous entities which consist of different
types of control packets such as query and advertisement packets.
These packets work collaboratively, but indirectly, to achieve
common tasks, such as resource discovery, routing, and load 
balancing. With collaboration among these entities, a MANET P2P
system is able to ‘learn" the network dynamics by itself and adjust
its behavior accordingly, without the overhead of organizing peers
into an overlay.
The remainder of this paper is organized as follows. Related work
is described in the next section. Section 3 describes the resource
discovery scheme. Section 4 describes the resource distribution
scheme. The replica invalidation scheme is described in Section 5,
followed by it performance evaluation in Section 6. Section 7
concludes the paper.
2. RELATED WORK
For MANETs, P2P systems can be classified based on the design
principle, into layered and cross-layer approaches. A layered 
approach adopts a P2P-like [1] solution, where resource discovery is
facilitated as an application layer protocol and query/reply 
messages are delivered by the underlying MANET routing protocols.
For instance, Konark [2] makes use of a underlying multicast
protocol such that service providers and queriers advertise and
search services via a predefined multicast group, respectively.
Proem [3] is a high-level mobile computing platform for P2P
systems over MANETs. It defines a transport protocol that sits on
top of the existing TCP/IP stack, hence relying on an existing
routing protocol to operate. With limited control over how control
and data packets are routed in the network, it is difficult to avoid
the inefficiency of the general-purpose routing protocols which
are often reactive and flooding-based.
In contrast, cross-layer approaches either relies on its own routing
mechanism or augments existing MANET routing algorithms to
support resource discovery. 7DS [4], which is the pioneering
work deploying P2P system on mobile devices, exploits data 
locality and node mobility to dissemination data in a single-hop
fashion. Hence, long search latency may be resulted as a 7DS
node can get data of interest only if the node that holds the data is
in its radio coverage. Mohan et al. [5] propose an adaptive service
discovery algorithm that combines both push and pull models.
Specifically, a service provider/querier broadcasts 
advertisement/query only when the number of nodes advertising or 
querying, which is estimated by received control packets, is below a
threshold during a period of time. In this way, the number of 
control packets on the network is constrained, thus providing good
scalability. Despite the mechanism to reduce control packets, high
overhead may still be unavoidable, especially when there are
many clients trying to locate different services, due to the fact that
the algorithm relies on flooding,
For resource replication, Yin and Cao [6] design and evaluate
cooperative caching techniques for MANETs. Caching, however,
is performed reactively by intermediate nodes when a querier
requests data from a server. Data items or resources are never
pushed into other nodes proactively. Thanedar et al. [7] propose a
lightweight content replication scheme using an expanding ring
technique. If a server detects the number of requests exceed a
threshold within a time period, it begins to replicate its data onto
nodes capable of storing replicas, whose hop counts from the
server are of certain values. Since data replication is triggered by
the request frequency alone, it is possible that there are replicas
unnecessarily created in a large scope even though only nodes
within a small range request this data. Our proposed resource
replication mechanism, in contrast, attempts to replicate a data
item in appropriate areas, instead of a large area around the server,
where the item is requested frequently.
3. RESOURCE DISCOVERY
We propose a cross-layer, hybrid resource discovery scheme that
relies on the multiple interactions of query, reply and 
advertisement packets. We assume that each resource is associated with a
unique ID1
. Initially, when a node wants to discover a resource, it
deploys query packets, which carry the corresponding resource
ID, and randomly explore the network to search for the requested
resource. Upon receiving such a query packet, a reply packet is
generated by the node providing the requested resource. 
Advertisement packets can also be used to proactively inform other
nodes about what resources are available at each node. In addition
to discovering the ‘identity" of the node providing the requested
resource, it may be also necessary to discover a ‘route" leading to
this node for further interaction.
To allow intermediate nodes to make a decision on where to 
forward query packets, each node maintains two tables: neighbor
1
The assumption of unique ID is made for brevity in exposition,
and resources could be specified via attribute-value assertions.
table and pheromone table. The neighbor table maintains a list of
all current neighbors obtained via a neighbor discovery protocol.
The pheromone table maintains the mapping of a resource ID and
a neighbor ID to a pheromone value. This table is initially empty,
and is updated by a reply packet generated by a successful query.
Figure 1 illustrates an example of a neighbor table and a 
pheromone table maintained by node A having four neighbors. When
node A receives a query packet searching for a resource, it makes
a decision to which neighbor it should forward the query packet
by computing the desirability of each of the neighbors that have
not been visited before by the same query packet. For a resource
ID r, the desirability of choosing a neighbor n, δ(r,n), is obtained
from the pheromone value of the entry whose neighbor and 
resource ID fields are n and r, respectively. If no such entry exists in
the pheromone table, δ(r,n) is set to zero.
Once the desirabilities of all valid next hops have been calculated,
they are normalized to obtain the probability of choosing each
neighbor. In addition, a small probability is also assigned to those
neighbors with zero desirability to exercise the sporadic random
walk primitive. Based on these probabilities, a next hop is 
selected to forward the query packet to. When a query packet 
encounters a node with a satisfying resource, a reply packet is 
returned to the querying node. The returning reply packet also 
updates the pheromone table at each node on its return trip by 
increasing the pheromone value in the entry whose resource ID and
neighbor ID fields match the ID of the discovered resource and
the previous hop, respectively. If such an entry does not exist, a
new entry is added into the table. Therefore, subsequent query
packets looking for the same resource, when encountering this
pheromone information, are then guided toward the same 
destination with a small probability of taking an alternate path.
Since the hybrid discovery scheme neither relies on a MANET
routing protocol nor arranges nodes into a logical overlay, query
packets are to traverse the actual network topology. In dense 
networks, relatively large nodal degrees can have potential impacts
on this random exploring mechanism. To address this issue, the
hybrid scheme also incorporates proactive advertisement in 
addition to the reactive query. To perform proactive advertisement,
each node periodically deploys an advertising packet containing a
list of its available resources" IDs. These packets will traverse
away from the advertising node in a random walk manner up to a
limited number of hops and advertise resource information to
surrounding nodes in the same way as reply packets. In the hybrid
scheme, an increase of pheromone serves as a positive feedback
which indirectly guides query packets looking for similar 
resources. Intuitively, the amount of pheromone increased is 
inversely proportional to the distance the reply packet has traveled
back, and other metrics, such as quality of the resource, could
contribute to this amount as well. Each node also performs an
implicit negative feedback for resources that have not been given
a positive feedback for some time by regularly decreasing the
pheromone in all of its pheromone table entries over time. In 
addition, pheromone can be reduced by an explicit negative response,
for instance, a reply packet returning from a node that is not 
willing to provide a resource due to excessive workload. As a result,
load balancing can be achieved via positive and negative 
feedback. A node serving too many nodes can either return fewer 
responses to query packets or generate negative responses.
2 The 3rd International Conference on Mobile Technology, Applications and Systems - Mobility 2006
Figure 1: Example illustrating neighbor and pheromone tables maintained by node A: (a) wireless connectivity around A showing
that it currently has four neighbors, (b) A"s neighbor table, and (c) a possible pheromone table of A
Figure 2: Sample scenarios illustrating the three mechanisms supporting load-balancing: (a) resource replication, (b) resource 
relocation, and (c) resource division
4. RESOURCE DISTRIBUTION
In addition to resource discovery, a querying node usually 
attempts to access and retrieve the contents of a resource after a
successful discovery. In certain situations, it is also beneficial to
make a resource readily available at multiple nodes when the 
resource can be relocated and/or replicated, such as data files. 
Furthermore, in MANETs, we should consider not only the amount
of load handled by a resource provider, but also the load on those
intermediate nodes that are located on the communication paths
between the provider and other nodes as well. Hence, we describe
a cross-layer, hybrid resource distribution scheme to achieve load
balancing by incorporating the functionalities of resource 
relocation, resource replication, and resource division.
4.1 Resource Replication
Multiple replicas of a resource in the network help prevent a 
single node, as well as nodes surrounding it, from being overloaded
by a large number of requests and data transfers. An example is
when a node has obtained a data file from another node, the 
requesting node and the intermediate nodes can cache the file and
start sharing that file with other surrounding nodes right away. In
addition, replicable resources can also be proactively replicated at
other nodes which are located in certain strategic areas. For 
instance, to help nodes find a resource quickly, we could replicate
the resource so that it becomes reachable by random walk for a
specific number of hops from any node with some probability, as
depicted in Figure 2(a).
To realize this feature, the hybrid resource distribution scheme
employs a different type of control packet, called resource 
replication packet, which is responsible for finding an appropriate
place to create a replica of a resource. A resource replication
packet of type R is deployed by a node that is providing the 
resource R itself. Unlike a query packet which follows higher
pheromone upstream toward a resource it is looking for, a 
resource replication packet tends to be propelled away from similar
resources by moving itself downstream toward weaker 
pheromone. When a resource replication packet finds itself in an area
with sufficiently low pheromone, it makes a decision whether it
should continue exploring or turn back. The decision depends on
conditions such as current workload and/or remaining energy of
the node being visited, as well as popularity of the resource itself.
4.2 Resource Relocation
In certain situations, a resource may be required to transfer from
one node to another. For example, a node may no longer want to
possess a file due to the shortage of storage space, but it cannot
simply delete the file since other nodes may still need it in the
future. In this case, the node can choose to create replicas of the
file by the aforementioned resource replication mechanism and
then delete its own copy. Let us consider a situation where a 
majority of nodes requesting for a resource are located far away from
a resource provider, as shown on the top of Figure 2(b). If the
resource R is relocatable, it is preferred to be relocated to another
area that is closer to those nodes, similar to the bottom of the
same figure. Hence network bandwidth is more efficiently 
utilized.
The 3rd Conference on Mobile Technology, Applications and Systems - Mobility 2006 3
The hybrid resource distribution scheme incorporates resource
relocation algorithms that are adaptive to user requests and aim to
reduce communication overhead. Specifically, by following the
same pheromone maintenance concept, the hybrid resource 
distribution scheme introduces another type of pheromone which 
corresponds to user requests, instead of resources. This type of
pheromone, called request pheromone, is setup by query packets
that are in their exploring phases (not returning ones) to guide a
resource to a new location.
4.3 Resource Division
Certain types of resources can be divided into smaller 
subresources (e.g., a large file being broken into smaller files) and
distributed to multiple locations to avoid overloading a single
node, as depicted in Figure 2(c). The hybrid resource distribution
scheme incorporates a resource division mechanism that operates
at a thin layer right above all the other mechanisms described
earlier. The resource division mechanism is responsible for 
decomposing divisible resources into sub-resources, and then adds
an extra keyword to distinguish each sub-resource from one 
another. Therefore, each of these sub-resources will be seen by the
other mechanisms as one single resource which can be 
independently discovered, replicated, and relocated. The resource division
mechanism is also responsible for combining data from these 
subresources together (e.g., merging pieces of a file) and delivering
the final result to the application.
5. REPLICA INVALIDATION
Although replicas improve accessibility and balance load, replica
invalidation becomes a critical issue when nodes caching 
updatable resources may concurrently update their own replicas,
which renders replicas held by other nodes obsolete. Most 
existing solutions to the replica invalidation problem either impose
constrains that only the data source could perform update and
invalidate other replicas, or resort to network-wide flooding which
results in heavy network traffic and leads to scalability problem,
or both. The lack of infrastructure supports and frequent topology
changes in MANETs further challenge the issue.
We apply the same cross-layer paradigm to invalidating replicas
in MANETs which allows concurrent updates performed by 
multiple replicas. To coordinate concurrent updates and disseminate
replica invalidations, a special infrastructure, called validation
mesh or mesh for short, is adaptively maintained among nodes
possessing ‘valid" replicas of a resource. Once a node has updated
its replica, an invalidation packet will only be disseminated over
the validation mesh to inform other replica-possessing nodes that
their replicas become invalid and should be deleted. The structure
(topology) of the validation mesh keeps evolving (1) when nodes
request and cache a resource, (2) when nodes update their 
respective replicas and invalidate other replicas, and (3) when nodes
move. To accommodate the dynamics, our scheme integrates the
components of swarm intelligence to adaptively maintain the 
validation mesh without relying on any underlying MANET routing
protocol. In particular, the scheme takes into account concurrent
updates initiated by multiple nodes to ensure the consistency
among replicas. In addition, version number is used to distinguish
new from old replicas when invalidating any stale replica. 
Simulation results show that the proposed scheme effectively facilitates
concurrent replica updates and efficiently perform replica 
invalidation without incurring network-wide flooding.
Figure 3 depicts the idea of ‘validation mesh" which maintains
connectivity among nodes holding valid replicas of a resource to
avoid network-wide flooding when invalidating replicas.
Figure 3: Examples showing maintenance of validation mesh
There are eight nodes in the sample network, and we start with
only node A holding the valid file, as shown in Figure 3(a). Later
on, node G issues a query packet for the file and eventually 
obtains the file from A via nodes B and D. Since intermediate nodes
are allowed to cache forwarded data, nodes B, D, and G will now
hold valid replicas of the file. As a result, a validation mesh is
established among nodes A, B, D, and G, as depicted in Figure
3(b). In Figure 3(c), another node, H, has issued a query packet
for the same file and obtained it from node B"s cache via node E.
At this point, six nodes hold valid replicas and are connected
through the validation mesh. Now we assume node G updates its
replica of the file and informs the other nodes by sending an 
invalidation packet over the validation mesh. Consequently, all
other nodes except G remove their replicas of the file from their
storage and the validation mesh is torn down. However, query
forwarding pheromone, as denoted by the dotted arrows in Figure
3(d), is setup at these nodes via the ‘reverse paths" in which the
invalidation packets have traversed, so that future requests for this
file will be forwarded to node G. In Figure 3(e), node H makes a
new request for the file again. This time, its query packet follows
the pheromone toward node G, where the updated file can be
obtained. Eventually, a new validation mesh is established over
nodes G, B, D, E, and H.
To maintain a validation mesh among the nodes holding valid
replicas, one of them is designated to be the focal node. Initially,
the node that originally holds the data is the focal node. As nodes
update replicas, the node that last (or most recently) updates a
4 The 3rd International Conference on Mobile Technology, Applications and Systems - Mobility 2006
corresponding replica assumes the role of focal node. We also
name nodes, such as G and H, who originate requests to replicate
data as clients, and nodes B, D, and E who locally cache passing
data as data nodes. For instance, in Figures 3(a), 3(b), and 3(c),
node A is the focal node; in Figures 3(d), 3(e), and 3(f), node G
becomes the focal node. In addition, to accommodate newly 
participating nodes and mobility of nodes, the focal node periodically
floods the validation mesh with a keep-alive packet, so that nodes
who can hear this packet are considered themselves to be part of
the validation mesh. If a node holding a valid/updated replica
doesn"t hear a keep-alive packet for a certain time interval, it will
deploy a search packet using the resource discovery mechanism
described in Section 3 to find another node (termed attachment
point) currently on the validation mesh so that it can attach itself
to. Once an attachment point is found, a search_reply packet is
returned to the disconnected node who originated the search. 
Intermediate nodes who forward the search_reply packet will 
become part of the validation mesh as well. To illustrate the effect of
node mobility, in Figure 3(f), node H has moved to a location
where it is not directly connected to the mesh. Via the resource
discovery mechanism, node H relies on an intermediate node F to
connect itself to the mesh. Here, node F, although part of the 
validation mesh, doesn"t hold data replica, and hence is termed 
nondata node.
Client and data node who keep hearing the keep-alive packets
from the focal node act as if they are holding a valid replica, so
that they can reply to query packets, like node B in Figure 3(c)
replying a request from node H. While a disconnected node 
attempting to discover an attachment point to reattach itself to the
mesh, the disconnected node can"t reply to a query packet. For
instance, in Figure 3(f), node H does not reply to any query packet
before it reattaches itself to the mesh.
Although validation mesh provides a conceptual topology that (1)
connects all replicas together, (2) coordinates concurrent updates,
and (3) disseminates invalidation packets, the technical issue is
how such a mesh topology could be effectively and efficiently
maintained and evolved when (a) nodes request and cache a 
resource, (b) when nodes update their respective replicas and 
invalidate other replicas, and (c) when nodes move. Without relying
on any MANET routing protocols, the two primitives work 
together to facilitate efficient search and adaptive maintenance.
6. PERFORMANCE EVALUATION
We have conducted simulation experiments using the QualNet
simulator to evaluate the performance of the described resource
discovery, resource distribution, and replica invalidation schemes.
However, due to space limitation only the performance of the
replica invalidation is reported. In our experiments, eighty nodes
are uniformly distributed over a terrain of size 1000×1000 m2
.
Each node has a communication range of approximately 250 m
over a 2 Mbps wireless channel, using IEEE802.11 as the MAC
layer. We use the random-waypoint mobility model with a pause
time of 1 second. Nodes may move at the minimum and maximum
speeds of 1 m/s and 5 m/s, respectively. Table 1 lists other 
parameter settings used in the simulation. Initially, there is one 
resource server node in network. Two nodes are randomly picked
up every 10 seconds as clients. Every β seconds, we check the
number of nodes, N, which have gotten data. Then we randomly
pickup Min(γ,N) nodes from them to initiate data update. Each
experiment is run for 10 minutes.
Table 1: Simulation Settings
HOP_LIMIT 10
ADVERTISE_HOP_LIMIT 1
KEEPALIVE_INTERVAL 3 second
NUM_SEARCH 1
ADVERTISE_INTERVAL 5 second
EXPIRATION_INTERVAL 10 second
Average Query Generation Rate 2 query/ 10 sec
Max # of Concurrent Update (γ) 2
Frequency of Update (β) 3s
We evaluate the performance under different mobility speed, the
density, the maximum number of concurrent update nodes, and
update frequency using two metrics:
• Average overhead per update measures the average number of
packets transmitted per update in the network.
• Average delay per update measures how long our approach
takes to finish an update on average.
All figures shown present the results with a 70% confidence 
interval.
Figure 4: Overhead vs. speed
for 80 nodes
Figure 5: Overhead vs. density
Figure 6: Overhead vs. max
#concurrent updates
Figure 7: Overhead vs. freq.
Figure 8: Delay vs. speed Figure 9: Delay vs. density
The 3rd Conference on Mobile Technology, Applications and Systems - Mobility 2006 5
Figure 10: Delay vs. max
#concurrent updates
Figure 11: Delay vs. freq.
Figures 4, 5, 6, and 7 show the overhead versus various parameter
values. In Figure 4, the overhead increases as the speed increase,
which is due to the fact that as the speed increase, nodes move out
of mesh more frequently, and will send out more search packets.
However, the overhead is not high, and even in speed 10m/sec,
the overhead is below 100 packets. In contrast, the packets will be
expected to be more than 200 packets at various speeds when
flooding is used.
Figure 5 shows that the overhead almost remains the same under
various densities. That is attributed to only flooding over the mesh
instead of the whole network. The size of mesh doesn"t vary much
on various densities, so that the overhead doesn"t vary much.
Figure 6 shows that overhead also almost remains the same under
various maximum number of concurrent updates. That"s because
one more node just means one more flood over the mesh during
update process so that the impact is limited.
Figure 7 shows that if updates happen more frequently, the 
overhead is higher. This is because the more quickly updates happen,
(1) there will be more keep_alive message over the mesh between
two updates, and (2) nodes move out of mesh more frequently and
send out more search packets.
Figures 8, 9, 10, and 11 show the delay versus various parameter
values. From Figure 8, we know the delay increases as the speed
increases, which is due to the fact that with increasing speed, 
clients will move out of mesh with higher probability. When these
clients want to update data, they will spend time to first search the
mesh. The faster the speed, the more time clients need to spend to
search the mesh.
Figure 9 shows that delay is negligibly affected by the density.
Delay decreases slightly as the number of nodes increases, due to
the fact that the more nodes in the network, the more nodes 
receives the advertisement packets which helps the search packet
find the target so that the delay of update decreases.
Figure 10 shows that delay decreases slightly as the maximum
number of concurrent updates increases. The larger the maximum
number of concurrent updates is, the more nodes are picked up to
do update. Then with higher probability, one of these nodes is still
in mesh and finishes the update immediately (don"t need to search
mesh first), which decreases the delay.
Figure 11 shows how the delay varies with the update frequency.
When updates happen more frequently, the delay will higher.
Because the less frequently, the more time nodes in mesh have to
move out of mesh then they need to take time to search the mesh
when they do update, which increases the delay.
The simulation results show that the replica invalidation scheme
can significantly reduce the overhead with an acceptable delay.
7. CONCLUSION
To facilitate resource discovery and distribution over MANETs,
one approach is to designing peer-to-peer (P2P) systems over
MANETs which constructs an overlay by organizing peers of the
system into a logical structure on the top of MANETs" physical
topology. However, deploying overlay over MANETs may result
in either a large number of flooding operations triggered by the
routing process, or inefficiency in terms of bandwidth usage. 
Specifically, overlay routing relies on the network-layer routing 
protocols. In the case of a reactive routing protocol, routing on the
overlay may cause a large number of flooded route discovery
message since the routing path in each routing step must be 
discovered on demand. On the other hand, if a proactive routing
protocol is adopted, each peer has to periodically broadcast 
control messages, which leads to poor efficiency in terms of 
bandwidth usage. Either way, constructing an overlay will potentially
suffer from the scalability problem. The paper describes a design
paradigm that uses the functional primitives of positive/negative
feedback and sporadic random walk to design robust resource
discovery and distribution schemes over MANETs. In particular,
the scheme offers the features of (1) cross-layer design of P2P
systems, which allows the routing process at both the P2P and the
network layers to be integrated to reduce overhead, (2) scalability
and mobility support, which minimizes the use of global flooding
operations and adaptively combines proactive resource 
advertisement and reactive resource discovery, and (3) load balancing,
which facilitates resource replication, relocation, and division to
achieve load balancing.
8. REFERENCES
[1] A. Oram, Peer-to-Peer: Harnessing the Power of Disruptive
Technologies. O"Reilly, March 2000.
[2] S. Helal, N. Desai, V. Verma, and C. Lee, Konark - A 
Service Discovery and Delivery Protocol for ad-hoc 
Networks, in the Third IEEE Conference on Wireless 
Communication Networks (WCNC), New Orleans, Louisiana, 2003.
[3] G. Krotuem, Proem: A Peer-to-Peer Computing Platform
for Mobile ad-hoc Networks, in Advanced Topic Workshop
Middleware for Mobile Computing, Germany, 2001.
[4] M. Papadopouli and H. Schulzrinne, A Performance 
Analysis of 7DS a Peer-to-Peer Data Dissemination and 
Prefetching Tool for Mobile Users, in Advances in wired and 
wireless communications, IEEE Sarnoff Symposium Digest, 
Ewing, NJ, 2001, (Best student paper & poster award).
[5] U. Mohan, K. Almeroth, and E. Belding-Royer, Scalable
Service Discovery in Mobile ad-hoc Networks, in IFIP
Networking Conference, Athens, Greece, May 2004.
[6] L. Yin and G. Cao, Supporting Cooperative Caching in Ad
Hoc Networks, in IEEE INFOCOM, 2004.
[7] V. Thanedar, K. Almeroth, and E. Belding-Royer, A 
Lightweight Content Replication Scheme for Mobile ad-hoc 
Environments, in IFIP Networking Conference, Athens, Greece,
May 2004.
6 The 3rd International Conference on Mobile Technology, Applications and Systems - Mobility 2006
A Scalable Distributed Information Management System∗
Praveen Yalagandula
ypraveen@cs.utexas.edu
Mike Dahlin
dahlin@cs.utexas.edu
Department of Computer Sciences
The University of Texas at Austin
Austin, TX 78712
ABSTRACT
We present a Scalable Distributed Information Management 
System (SDIMS) that aggregates information about large-scale 
networked systems and that can serve as a basic building block for a
broad range of large-scale distributed applications by providing 
detailed views of nearby information and summary views of global 
information. To serve as a basic building block, a SDIMS should have
four properties: scalability to many nodes and attributes, flexibility
to accommodate a broad range of applications, administrative 
isolation for security and availability, and robustness to node and 
network failures. We design, implement and evaluate a SDIMS that (1)
leverages Distributed Hash Tables (DHT) to create scalable 
aggregation trees, (2) provides flexibility through a simple API that lets
applications control propagation of reads and writes, (3) provides
administrative isolation through simple extensions to current DHT
algorithms, and (4) achieves robustness to node and network 
reconfigurations through lazy reaggregation, on-demand reaggregation,
and tunable spatial replication. Through extensive simulations and
micro-benchmark experiments, we observe that our system is an 
order of magnitude more scalable than existing approaches, achieves
isolation properties at the cost of modestly increased read latency
in comparison to flat DHTs, and gracefully handles failures.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed 
Systems-Network Operating Systems, Distributed Databases
General Terms
Management, Design, Experimentation
1. INTRODUCTION
The goal of this research is to design and build a Scalable 
Distributed Information Management System (SDIMS) that aggregates
information about large-scale networked systems and that can serve
as a basic building block for a broad range of large-scale distributed
applications. Monitoring, querying, and reacting to changes in
the state of a distributed system are core components of 
applications such as system management [15, 31, 37, 42], service 
placement [14, 43], data sharing and caching [18, 29, 32, 35, 46], sensor
monitoring and control [20, 21], multicast tree formation [8, 9, 33,
36, 38], and naming and request routing [10, 11]. We therefore
speculate that a SDIMS in a networked system would provide a
distributed operating systems backbone and facilitate the 
development and deployment of new distributed services.
For a large scale information system, hierarchical aggregation
is a fundamental abstraction for scalability. Rather than expose all
information to all nodes, hierarchical aggregation allows a node to
access detailed views of nearby information and summary views of
global information. In a SDIMS based on hierarchical aggregation,
different nodes can therefore receive different answers to the query
find a [nearby] node with at least 1 GB of free memory or find
a [nearby] copy of file foo. A hierarchical system that aggregates
information through reduction trees [21, 38] allows nodes to access
information they care about while maintaining system scalability.
To be used as a basic building block, a SDIMS should have
four properties. First, the system should be scalable: it should
accommodate large numbers of participating nodes, and it should
allow applications to install and monitor large numbers of data 
attributes. Enterprise and global scale systems today might have tens
of thousands to millions of nodes and these numbers will increase
over time. Similarly, we hope to support many applications, and
each application may track several attributes (e.g., the load and
free memory of a system"s machines) or millions of attributes (e.g.,
which files are stored on which machines).
Second, the system should have flexibility to accommodate a
broad range of applications and attributes. For example, 
readdominated attributes like numCPUs rarely change in value, while
write-dominated attributes like numProcesses change quite often.
An approach tuned for read-dominated attributes will consume high
bandwidth when applied to write-dominated attributes. Conversely,
an approach tuned for write-dominated attributes will suffer from
unnecessary query latency or imprecision for read-dominated 
attributes. Therefore, a SDIMS should provide mechanisms to handle
different types of attributes and leave the policy decision of tuning
replication to the applications.
Third, a SDIMS should provide administrative isolation. In a
large system, it is natural to arrange nodes in an organizational or
an administrative hierarchy. A SDIMS should support 
administraSession 10: Distributed Information Systems
379
tive isolation in which queries about an administrative domain"s 
information can be satisfied within the domain so that the system can
operate during disconnections from other domains, so that an 
external observer cannot monitor or affect intra-domain queries, and
to support domain-scoped queries efficiently.
Fourth, the system must be robust to node failures and 
disconnections. A SDIMS should adapt to reconfigurations in a timely
fashion and should also provide mechanisms so that applications
can tradeoff the cost of adaptation with the consistency level in the
aggregated results when reconfigurations occur.
We draw inspiration from two previous works: Astrolabe [38]
and Distributed Hash Tables (DHTs).
Astrolabe [38] is a robust information management system. 
Astrolabe provides the abstraction of a single logical aggregation tree
that mirrors a system"s administrative hierarchy. It provides a 
general interface for installing new aggregation functions and provides
eventual consistency on its data. Astrolabe is robust due to its use
of an unstructured gossip protocol for disseminating information
and its strategy of replicating all aggregated attribute values for a
subtree to all nodes in the subtree. This combination allows any
communication pattern to yield eventual consistency and allows
any node to answer any query using local information. This high
degree of replication, however, may limit the system"s ability to
accommodate large numbers of attributes. Also, although the 
approach works well for read-dominated attributes, an update at one
node can eventually affect the state at all nodes, which may limit
the system"s flexibility to support write-dominated attributes.
Recent research in peer-to-peer structured networks resulted in
Distributed Hash Tables (DHTs) [18, 28, 29, 32, 35, 46]-a data
structure that scales with the number of nodes and that distributes
the read-write load for different queries among the participating
nodes. It is interesting to note that although these systems export
a global hash table abstraction, many of them internally make use
of what can be viewed as a scalable system of aggregation trees
to, for example, route a request for a given key to the right DHT
node. Indeed, rather than export a general DHT interface, Plaxton
et al."s [28] original application makes use of hierarchical 
aggregation to allow nodes to locate nearby copies of objects. It seems
appealing to develop a SDIMS abstraction that exposes this internal
functionality in a general way so that scalable trees for aggregation
can be a basic system building block alongside the DHTs.
At a first glance, it might appear to be obvious that simply 
fusing DHTs with Astrolabe"s aggregation abstraction will result in a
SDIMS. However, meeting the SDIMS requirements forces a 
design to address four questions: (1) How to scalably map different
attributes to different aggregation trees in a DHT mesh? (2) How to
provide flexibility in the aggregation to accommodate different 
application requirements? (3) How to adapt a global, flat DHT mesh
to attain administrative isolation property? and (4) How to provide
robustness without unstructured gossip and total replication?
The key contributions of this paper that form the foundation of
our SDIMS design are as follows.
1. We define a new aggregation abstraction that specifies both
attribute type and attribute name and that associates an 
aggregation function with a particular attribute type. This 
abstraction paves the way for utilizing the DHT system"s internal
trees for aggregation and for achieving scalability with both
nodes and attributes.
2. We provide a flexible API that lets applications control the
propagation of reads and writes and thus trade off update
cost, read latency, replication, and staleness.
3. We augment an existing DHT algorithm to ensure path 
convergence and path locality properties in order to achieve 
administrative isolation.
4. We provide robustness to node and network reconfigurations
by (a) providing temporal replication through lazy 
reaggregation that guarantees eventual consistency and (b) 
ensuring that our flexible API allows demanding applications gain
additional robustness by using tunable spatial replication of
data aggregates or by performing fast on-demand 
reaggregation to augment the underlying lazy reaggregation or by
doing both.
We have built a prototype of SDIMS. Through simulations and
micro-benchmark experiments on a number of department machines
and PlanetLab [27] nodes, we observe that the prototype achieves
scalability with respect to both nodes and attributes through use
of its flexible API, inflicts an order of magnitude lower maximum
node stress than unstructured gossiping schemes, achieves isolation
properties at a cost of modestly increased read latency compared to
flat DHTs, and gracefully handles node failures.
This initial study discusses key aspects of an ongoing system
building effort, but it does not address all issues in building a SDIMS.
For example, we believe that our strategies for providing robustness
will mesh well with techniques such as supernodes [22] and other
ongoing efforts to improve DHTs [30] for further improving 
robustness. Also, although splitting aggregation among many trees
improves scalability for simple queries, this approach may make
complex and multi-attribute queries more expensive compared to
a single tree. Additional work is needed to understand the 
significance of this limitation for real workloads and, if necessary, to
adapt query planning techniques from DHT abstractions [16, 19]
to scalable aggregation tree abstractions.
In Section 2, we explain the hierarchical aggregation 
abstraction that SDIMS provides to applications. In Sections 3 and 4, we
describe the design of our system for achieving the flexibility, 
scalability, and administrative isolation requirements of a SDIMS. In
Section 5, we detail the implementation of our prototype system.
Section 6 addresses the issue of adaptation to the topological 
reconfigurations. In Section 7, we present the evaluation of our 
system through large-scale simulations and microbenchmarks on real
networks. Section 8 details the related work, and Section 9 
summarizes our contribution.
2. AGGREGATION ABSTRACTION
Aggregation is a natural abstraction for a large-scale distributed
information system because aggregation provides scalability by 
allowing a node to view detailed information about the state near it
and progressively coarser-grained summaries about progressively
larger subsets of a system"s data [38].
Our aggregation abstraction is defined across a tree spanning all
nodes in the system. Each physical node in the system is a leaf and
each subtree represents a logical group of nodes. Note that logical
groups can correspond to administrative domains (e.g., department
or university) or groups of nodes within a domain (e.g., 10 
workstations on a LAN in CS department). An internal non-leaf node,
which we call virtual node, is simulated by one or more physical
nodes at the leaves of the subtree for which the virtual node is the
root. We describe how to form such trees in a later section.
Each physical node has local data stored as a set of (attributeType,
attributeName, value) tuples such as (configuration, numCPUs,
16), (mcast membership, session foo, yes), or (file stored, foo, 
myIPaddress). The system associates an aggregation function ftype
with each attribute type, and for each level-i subtree Ti in the 
system, the system defines an aggregate value Vi,type,name for each 
(at380
tributeType, attributeName) pair as follows. For a (physical) leaf
node T0 at level 0, V0,type,name is the locally stored value for the 
attribute type and name or NULL if no matching tuple exists. Then
the aggregate value for a level-i subtree Ti is the aggregation 
function for the type, ftype computed across the aggregate values of
each of Ti"s k children:
Vi,type,name = ftype(V0
i−1,type,name,V1
i−1,type,name,...,Vk−1
i−1,type,name).
Although SDIMS allows arbitrary aggregation functions, it is 
often desirable that these functions satisfy the hierarchical 
computation property [21]: f(v1,...,vn)= f(f(v1,...,vs1 ), f(vs1+1,...,vs2 ),
..., f(vsk+1,...,vn)), where vi is the value of an attribute at node
i. For example, the average operation, defined as avg(v1,...,vn) =
1/n.∑n
i=0 vi, does not satisfy the property. Instead, if an attribute
stores values as tuples (sum,count), the attribute satisfies the 
hierarchical computation property while still allowing the applications
to compute the average from the aggregate sum and count values.
Finally, note that for a large-scale system, it is difficult or 
impossible to insist that the aggregation value returned by a probe
corresponds to the function computed over the current values at the
leaves at the instant of the probe. Therefore our system provides
only weak consistency guarantees - specifically eventual 
consistency as defined in [38].
3. FLEXIBILITY
A major innovation of our work is enabling flexible aggregate
computation and propagation. The definition of the aggregation
abstraction allows considerable flexibility in how, when, and where
aggregate values are computed and propagated. While previous
systems [15, 29, 38, 32, 35, 46] implement a single static strategy,
we argue that a SDIMS should provide flexible computation and
propagation to efficiently support wide variety of applications with
diverse requirements. In order to provide this flexibility, we 
develop a simple interface that decomposes the aggregation 
abstraction into three pieces of functionality: install, update, and probe.
This definition of the aggregation abstraction allows our system
to provide a continuous spectrum of strategies ranging from lazy
aggregate computation and propagation on reads to aggressive 
immediate computation and propagation on writes. In Figure 1, we
illustrate both extreme strategies and an intermediate strategy. 
Under the lazy Update-Local computation and propagation strategy,
an update (or write) only affects local state. Then, a probe (or read)
that reads a level-i aggregate value is sent up the tree to the issuing
node"s level-i ancestor and then down the tree to the leaves. The
system then computes the desired aggregate value at each layer up
the tree until the level-i ancestor that holds the desired value. 
Finally, the level-i ancestor sends the result down the tree to the 
issuing node. In the other extreme case of the aggressive Update-All
immediate computation and propagation on writes [38], when an
update occurs, changes are aggregated up the tree, and each new
aggregate value is flooded to all of a node"s descendants. In this
case, each level-i node not only maintains the aggregate values for
the level-i subtree but also receives and locally stores copies of all
of its ancestors" level- j ( j > i) aggregation values. Also, a leaf 
satisfies a probe for a level-i aggregate using purely local data. In an
intermediate Update-Up strategy, the root of each subtree maintains
the subtree"s current aggregate value, and when an update occurs,
the leaf node updates its local state and passes the update to its
parent, and then each successive enclosing subtree updates its 
aggregate value and passes the new value to its parent. This strategy
satisfies a leaf"s probe for a level-i aggregate value by sending the
probe up to the level-i ancestor of the leaf and then sending the 
aggregate value down to the leaf. Finally, notice that other strategies
exist. In general, an Update-Upk-Downj strategy aggregates up to
parameter description optional
attrType Attribute Type
aggrfunc Aggregation Function
up How far upward each update is
sent (default: all)
X
down How far downward each 
aggregate is sent (default: none)
X
domain Domain restriction (default: none) X
expTime Expiry Time
Table 1: Arguments for the install operation
the kth level and propagates the aggregate values of a node at level
l (s.t. l ≤ k) downward for j levels.
A SDIMS must provide a wide range of flexible computation and
propagation strategies to applications for it to be a general 
abstraction. An application should be able to choose a particular 
mechanism based on its read-to-write ratio that reduces the bandwidth
consumption while attaining the required responsiveness and 
precision. Note that the read-to-write ratio of the attributes that 
applications install vary extensively. For example, a read-dominated
attribute like numCPUs rarely changes in value, while a 
writedominated attribute like numProcesses changes quite often. An 
aggregation strategy like Update-All works well for read-dominated
attributes but suffers high bandwidth consumption when applied for
write-dominated attributes. Conversely, an approach like 
UpdateLocal works well for write-dominated attributes but suffers from
unnecessary query latency or imprecision for read-dominated 
attributes.
SDIMS also allows non-uniform computation and propagation
across the aggregation tree with different up and down parameters
in different subtrees so that applications can adapt with the 
spatial and temporal heterogeneity of read and write operations. With
respect to spatial heterogeneity, access patterns may differ for 
different parts of the tree, requiring different propagation strategies
for different parts of the tree. Similarly with respect to temporal
heterogeneity, access patterns may change over time requiring 
different strategies over time.
3.1 Aggregation API
We provide the flexibility described above by splitting the 
aggregation API into three functions: Install() installs an aggregation
function that defines an operation on an attribute type and 
specifies the update strategy that the function will use, Update() inserts
or modifies a node"s local value for an attribute, and Probe() 
obtains an aggregate value for a specified subtree. The install 
interface allows applications to specify the k and j parameters of the
Update-Upk-Downj strategy along with the aggregation function.
The update interface invokes the aggregation of an attribute on the
tree according to corresponding aggregation function"s aggregation
strategy. The probe interface not only allows applications to obtain
the aggregated value for a specified tree but also allows a probing
node to continuously fetch the values for a specified time, thus 
enabling an application to adapt to spatial and temporal heterogeneity.
The rest of the section describes these three interfaces in detail.
3.1.1 Install
The Install operation installs an aggregation function in the 
system. The arguments for this operation are listed in Table 1. The
attrType argument denotes the type of attributes on which this 
aggregation function is invoked. Installed functions are soft state that
must be periodically renewed or they will be garbage collected at
expTime.
The arguments up and down specify the aggregate computation
381
Update Strategy On Update On Probe for Global Aggregate Value On Probe for Level-1 Aggregate Value
Update-Local
Update-Up
Update-All
Figure 1: Flexible API
parameter description optional
attrType Attribute Type
attrName Attribute Name
mode Continuous or One-shot (default:
one-shot)
X
level Level at which aggregate is sought
(default: at all levels)
X
up How far up to go and re-fetch the
value (default: none)
X
down How far down to go and 
reaggregate (default: none)
X
expTime Expiry Time
Table 2: Arguments for the probe operation
and propagation strategy Update-Upk-Downj. The domain 
argument, if present, indicates that the aggregation function should be
installed on all nodes in the specified domain; otherwise the 
function is installed on all nodes in the system.
3.1.2 Update
The Update operation takes three arguments attrType, attrName,
and value and creates a new (attrType, attrName, value) tuple or
updates the value of an old tuple with matching attrType and 
attrName at a leaf node.
The update interface meshes with installed aggregate 
computation and propagation strategy to provide flexibility. In particular,
as outlined above and described in detail in Section 5, after a leaf
applies an update locally, the update may trigger re-computation
of aggregate values up the tree and may also trigger propagation
of changed aggregate values down the tree. Notice that our 
abstraction associates an aggregation function with only an attrType
but lets updates specify an attrName along with the attrType. This
technique helps achieve scalability with respect to nodes and 
attributes as described in Section 4.
3.1.3 Probe
The Probe operation returns the value of an attribute to an 
application. The complete argument set for the probe operation is shown
in Table 2. Along with the attrName and the attrType arguments, a
level argument specifies the level at which the answers are required
for an attribute. In our implementation we choose to return results
at all levels k < l for a level-l probe because (i) it is inexpensive as
the nodes traversed for level-l probe also contain level k aggregates
for k < l and as we expect the network cost of transmitting the 
additional information to be small for the small aggregates which we
focus and (ii) it is useful as applications can efficiently get several
aggregates with a single probe (e.g., for domain-scoped queries as
explained in Section 4.2).
Probes with mode set to continuous and with finite expTime 
enable applications to handle spatial and temporal heterogeneity. When
node A issues a continuous probe at level l for an attribute, then 
regardless of the up and down parameters, updates for the attribute
at any node in A"s level-l ancestor"s subtree are aggregated up to
level l and the aggregated value is propagated down along the path
from the ancestor to A. Note that continuous mode enables SDIMS
to support a distributed sensor-actuator mechanism where a 
sensor monitors a level-i aggregate with a continuous mode probe and
triggers an actuator upon receiving new values for the probe.
The up and down arguments enable applications to perform 
ondemand fast re-aggregation during reconfigurations, where a forced
re-aggregation is done for the corresponding levels even if the 
aggregated value is available, as we discuss in Section 6. When
present, the up and down arguments are interpreted as described
in the install operation.
3.1.4 Dynamic Adaptation
At the API level, the up and down arguments in install API can be
regarded as hints, since they suggest a computation strategy but do
not affect the semantics of an aggregation function. A SDIMS 
implementation can dynamically adjust its up/down strategies for an
attribute based on its measured read/write frequency. But a virtual
intermediate node needs to know the current up and down 
propagation values to decide if the local aggregate is fresh in order to
answer a probe. This is the key reason why up and down need to be
statically defined at the install time and can not be specified in the
update operation. In dynamic adaptation, we implement a 
leasebased mechanism where a node issues a lease to a parent or a child
denoting that it will keep propagating the updates to that parent or
child. We are currently evaluating different policies to decide when
to issue a lease and when to revoke a lease.
4. SCALABILITY
Our design achieves scalability with respect to both nodes and 
attributes through two key ideas. First, it carefully defines the 
aggregation abstraction to mesh well with its underlying scalable DHT
system. Second, it refines the basic DHT abstraction to form an
Autonomous DHT (ADHT) to achieve the administrative isolation
properties that are crucial to scaling for large real-world systems.
In this section, we describe these two ideas in detail.
4.1 Leveraging DHTs
In contrast to previous systems [4, 15, 38, 39, 45], SDIMS"s 
aggregation abstraction specifies both an attribute type and attribute
name and associates an aggregation function with a type rather than
just specifying and associating a function with a name. Installing a
single function that can operate on many different named attributes
matching a type improves scalability for sparse attribute types
with large, sparsely-filled name spaces. For example, to construct
a file location service, our interface allows us to install a single
function that computes an aggregate value for any named file. A
subtree"s aggregate value for (FILELOC, name) would be the ID of
a node in the subtree that stores the named file. Conversely, 
Astrolabe copes with sparse attributes by having aggregation functions
compute sets or lists and suggests that scalability can be improved
by representing such sets with Bloom filters [6]. Supporting sparse
names within a type provides at least two advantages. First, when
the value associated with a name is updated, only the state 
associ382
001 010100
000
011 101
111
110
011 111 001 101 000 100 110010
L0
L1
L2
L3
Figure 2: The DHT tree corresponding to key 111 (DHTtree111)
and the corresponding aggregation tree.
ated with that name needs to be updated and propagated to other
nodes. Second, splitting values associated with different names
into different aggregation values allows our system to leverage 
Distributed Hash Tables (DHTs) to map different names to different
trees and thereby spread the function"s logical root node"s load and
state across multiple physical nodes.
Given this abstraction, scalably mapping attributes to DHTs is
straightforward. DHT systems assign a long, random ID to each
node and define an algorithm to route a request for key k to a
node rootk such that the union of paths from all nodes forms a tree
DHTtreek rooted at the node rootk. Now, as illustrated in Figure 2,
by aggregating an attribute along the aggregation tree 
corresponding to DHTtreek for k =hash(attribute type, attribute name), 
different attributes will be aggregated along different trees.
In comparison to a scheme where all attributes are aggregated
along a single tree, aggregating along multiple trees incurs lower
maximum node stress: whereas in a single aggregation tree 
approach, the root and the intermediate nodes pass around more 
messages than leaf nodes, in a DHT-based multi-tree, each node acts as
an intermediate aggregation point for some attributes and as a leaf
node for other attributes. Hence, this approach distributes the onus
of aggregation across all nodes.
4.2 Administrative Isolation
Aggregation trees should provide administrative isolation by 
ensuring that for each domain, the virtual node at the root of the
smallest aggregation subtree containing all nodes of that domain is
hosted by a node in that domain. Administrative isolation is 
important for three reasons: (i) for security - so that updates and probes
flowing in a domain are not accessible outside the domain, (ii) for
availability - so that queries for values in a domain are not affected
by failures of nodes in other domains, and (iii) for efficiency - so
that domain-scoped queries can be simple and efficient.
To provide administrative isolation to aggregation trees, a DHT
should satisfy two properties:
1. Path Locality: Search paths should always be contained in
the smallest possible domain.
2. Path Convergence: Search paths for a key from different
nodes in a domain should converge at a node in that domain.
Existing DHTs support path locality [18] or can easily support it
by using the domain nearness as the distance metric [7, 17], but they
do not guarantee path convergence as those systems try to optimize
the search path to the root to reduce response latency. For example,
Pastry [32] uses prefix routing in which each node"s routing table
contains one row per hexadecimal digit in the nodeId space where
the ith row contains a list of nodes whose nodeIds differ from the
current node"s nodeId in the ith digit with one entry for each 
possible digit value. Given a routing topology, to route a packet to
an arbitrary destination key, a node in Pastry forwards a packet to
the node with a nodeId prefix matching the key in at least one more
digit than the current node. If such a node is not known, the 
current node uses an additional data structure, the leaf set containing
110XX
010XX
011XX
100XX
101XX
univ
dep1 dep2
key = 111XX
011XX 100XX 101XX 110XX 010XX
L1
L0
L2
Figure 3: Example shows how isolation property is violated
with original Pastry. We also show the corresponding 
aggregation tree.
110XX
010XX
011XX
100XX
101XX
univ
dep1 dep2
key = 111XX
X
011XX 100XX 101XX 110XX 010XX
L0
L1
L2
Figure 4: Autonomous DHT satisfying the isolation property.
Also the corresponding aggregation tree is shown.
L immediate higher and lower neighbors in the nodeId space, and
forwards the packet to a node with an identical prefix but that is
numerically closer to the destination key in the nodeId space. This
process continues until the destination node appears in the leaf set,
after which the message is routed directly. Pastry"s expected 
number of routing steps is logn, where n is the number of nodes, but
as Figure 3 illustrates, this algorithm does not guarantee path 
convergence: if two nodes in a domain have nodeIds that match a key
in the same number of bits, both of them can route to a third node
outside the domain when routing for that key.
Simple modifications to Pastry"s route table construction and
key-routing protocols yield an Autonomous DHT (ADHT) that 
satisfies the path locality and path convergence properties. As Figure 4
illustrates, whenever two nodes in a domain share the same prefix
with respect to a key and no other node in the domain has a longer
prefix, our algorithm introduces a virtual node at the boundary of
the domain corresponding to that prefix plus the next digit of the
key; such a virtual node is simulated by the existing node whose id
is numerically closest to the virtual node"s id. Our ADHT"s routing
table differs from Pastry"s in two ways. First, each node maintains
a separate leaf set for each domain of which it is a part. Second,
nodes use two proximity metrics when populating the routing tables
- hierarchical domain proximity is the primary metric and network
distance is secondary. Then, to route a packet to a global root for a
key, ADHT routing algorithm uses the routing table and the leaf set
entries to route to each successive enclosing domain"s root (the 
virtual or real node in the domain matching the key in the maximum
number of digits). Additional details about the ADHT algorithm
are available in an extended technical report [44].
Properties. Maintaining a different leaf set for each 
administrative hierarchy level increases the number of neighbors that each
node tracks to (2b)∗lgb n+c.l from (2b)∗lgb n+c in unmodified
Pastry, where b is the number of bits in a digit, n is the number of
nodes, c is the leaf set size, and l is the number of domain levels.
Routing requires O(lgbn + l) steps compared to O(lgbn) steps in
Pastry; also, each routing hop may be longer than in Pastry because
the modified algorithm"s routing table prefers same-domain nodes
over nearby nodes. We experimentally quantify the additional 
routing costs in Section 7.
In a large system, the ADHT topology allows domains to 
im383
A1 A2 B1
((B1.B.,1),
(B.,1),(.,1))
((B1.B.,1),
(B.,1),(.,1))
L2
L1
L0
((B1.B.,1),
(B.,1),(.,3))
((A1.A.,1),
(A.,2),(.,2))
((A1.A.,1),
(A.,1),(.,1))
((A2.A.,1),
(A.,1),(.,1))
Figure 5: Example for domain-scoped queries
prove security for sensitive attribute types by installing them only
within a specified domain. Then, aggregation occurs entirely within
the domain and a node external to the domain can neither observe
nor affect the updates and aggregation computations of the attribute
type. Furthermore, though we have not implemented this feature
in the prototype, the ADHT topology would also support 
domainrestricted probes that could ensure that no one outside of a domain
can observe a probe for data stored within the domain.
The ADHT topology also enhances availability by allowing the
common case of probes for data within a domain to depend only on
a domain"s nodes. This, for example, allows a domain that becomes
disconnected from the rest of the Internet to continue to answer
queries for local data.
Aggregation trees that provide administrative isolation also 
enable the definition of simple and efficient domain-scoped 
aggregation functions to support queries like what is the average load
on machines in domain X? For example, consider an 
aggregation function to count the number of machines in an example 
system with three machines illustrated in Figure 5. Each leaf node
l updates attribute NumMachines with a value vl containing a set
of tuples of form (Domain, Count) for each domain of which the
node is a part. In the example, the node A1 with name A1.A. 
performs an update with the value ((A1.A.,1),(A.,1),(.,1)). An 
aggregation function at an internal virtual node hosted on node N with
child set C computes the aggregate as a set of tuples: for each 
domain D that N is part of, form a tuple (D,∑c∈C(count|(D,count) ∈
vc)). This computation is illustrated in the Figure 5. Now a query
for NumMachines with level set to MAX will return the 
aggregate values at each intermediate virtual node on the path to the
root as a set of tuples (tree level, aggregated value) from which
it is easy to extract the count of machines at each enclosing 
domain. For example, A1 would receive ((2, ((B1.B.,1),(B.,1),(.,3))),
(1, ((A1.A.,1),(A.,2),(.,2))), (0, ((A1.A.,1),(A.,1),(.,1)))). Note that
supporting domain-scoped queries would be less convenient and
less efficient if aggregation trees did not conform to the system"s
administrative structure. It would be less efficient because each 
intermediate virtual node will have to maintain a list of all values at
the leaves in its subtree along with their names and it would be less
convenient as applications that need an aggregate for a domain will
have to pick values of nodes in that domain from the list returned
by a probe and perform computation.
5. PROTOTYPE IMPLEMENTATION
The internal design of our SDIMS prototype comprises of two
layers: the Autonomous DHT (ADHT) layer manages the overlay
topology of the system and the Aggregation Management Layer
(AML) maintains attribute tuples, performs aggregations, stores
and propagates aggregate values. Given the ADHT construction
described in Section 4.2, each node implements an Aggregation
Management Layer (AML) to support the flexible API described in
Section 3. In this section, we describe the internal state and 
operation of the AML layer of a node in the system.
local
MIB
MIBs
ancestor
reduction MIB
(level 1)MIBs
ancestor
MIB from
child 0X...
MIB from
child 0X...
Level 2
Level 1
Level 3
Level 0
1XXX...
10XX...
100X...
From parents0X..
To parent 0X...
−− aggregation functions
From parents
To parent 10XX...
1X..
1X..
1X..
To parent 11XX...
Node Id: (1001XXX)
1001X..
100X..
10X..
1X..
Virtual Node
Figure 6: Example illustrating the data structures and the 
organization of them at a node.
We refer to a store of (attribute type, attribute name, value) tuples
as a Management Information Base or MIB, following the 
terminology from Astrolabe [38] and SNMP [34]. We refer an (attribute
type, attribute name) tuple as an attribute key.
As Figure 6 illustrates, each physical node in the system acts as
several virtual nodes in the AML: a node acts as leaf for all attribute
keys, as a level-1 subtree root for keys whose hash matches the
node"s ID in b prefix bits (where b is the number of bits corrected
in each step of the ADHT"s routing scheme), as a level-i subtree
root for attribute keys whose hash matches the node"s ID in the
initial i ∗ b bits, and as the system"s global root for attribute keys
whose hash matches the node"s ID in more prefix bits than any
other node (in case of a tie, the first non-matching bit is ignored
and the comparison is continued [46]).
To support hierarchical aggregation, each virtual node at the root
of a level-i subtree maintains several MIBs that store (1) child MIBs
containing raw aggregate values gathered from children, (2) a 
reduction MIB containing locally aggregated values across this raw
information, and (3) an ancestor MIB containing aggregate values
scattered down from ancestors. This basic strategy of maintaining
child, reduction, and ancestor MIBs is based on Astrolabe [38],
but our structured propagation strategy channels information that
flows up according to its attribute key and our flexible propagation
strategy only sends child updates up and ancestor aggregate results
down as far as specified by the attribute key"s aggregation 
function. Note that in the discussion below, for ease of explanation, we
assume that the routing protocol is correcting single bit at a time
(b = 1). Our system, built upon Pastry, handles multi-bit correction
(b = 4) and is a simple extension to the scheme described here.
For a given virtual node ni at level i, each child MIB contains the
subset of a child"s reduction MIB that contains tuples that match
ni"s node ID in i bits and whose up aggregation function attribute is
at least i. These local copies make it easy for a node to recompute
a level-i aggregate value when one child"s input changes. Nodes
maintain their child MIBs in stable storage and use a simplified
version of the Bayou log exchange protocol (sans conflict detection
and resolution) for synchronization after disconnections [26].
Virtual node ni at level i maintains a reduction MIB of tuples
with a tuple for each key present in any child MIB containing the
attribute type, attribute name, and output of the attribute type"s 
aggregate functions applied to the children"s tuples.
A virtual node ni at level i also maintains an ancestor MIB to
store the tuples containing attribute key and a list of aggregate 
values at different levels scattered down from ancestors. Note that the
384
list for a key might contain multiple aggregate values for a same
level but aggregated at different nodes (see Figure 4). So, the 
aggregate values are tagged not only with level information, but are
also tagged with ID of the node that performed the aggregation.
Level-0 differs slightly from other levels. Each level-0 leaf node
maintains a local MIB rather than maintaining child MIBs and a
reduction MIB. This local MIB stores information about the local
node"s state inserted by local applications via update() calls. We 
envision various sensor programs and applications insert data into
local MIB. For example, one program might monitor local 
configuration and perform updates with information such as total memory,
free memory, etc., A distributed file system might perform update
for each file stored on the local node.
Along with these MIBs, a virtual node maintains two other 
tables: an aggregation function table and an outstanding probes 
table. An aggregation function table contains the aggregation 
function and installation arguments (see Table 1) associated with an 
attribute type or an attribute type and name. Each aggregate 
function is installed on all nodes in a domain"s subtree, so the aggregate
function table can be thought of as a special case of the ancestor
MIB with domain functions always installed up to a root within a
specified domain and down to all nodes within the domain. The
outstanding probes table maintains temporary information 
regarding in-progress probes.
Given these data structures, it is simple to support the three API
functions described in Section 3.1.
Install The Install operation (see Table 1) installs on a domain an
aggregation function that acts on a specified attribute type. 
Execution of an install operation for function aggrFunc on attribute type
attrType proceeds in two phases: first the install request is passed
up the ADHT tree with the attribute key (attrType, null) until it
reaches the root for that key within the specified domain. Then, the
request is flooded down the tree and installed on all intermediate
and leaf nodes.
Update When a level i virtual node receives an update for an
attribute from a child below: it first recomputes the level-i 
aggregate value for the specified key, stores that value in its reduction
MIB and then, subject to the function"s up and domain parameters,
passes the updated value to the appropriate parent based on the 
attribute key. Also, the level-i (i ≥ 1) virtual node sends the updated
level-i aggregate to all its children if the function"s down parameter
exceeds zero. Upon receipt of a level-i aggregate from a parent,
a level k virtual node stores the value in its ancestor MIB and, if
k ≥ i−down, forwards this aggregate to its children.
Probe A Probe collects and returns the aggregate value for a
specified attribute key for a specified level of the tree. As Figure 1
illustrates, the system satisfies a probe for a level-i aggregate value
using a four-phase protocol that may be short-circuited when 
updates have previously propagated either results or partial results up
or down the tree. In phase 1, the route probe phase, the system
routes the probe up the attribute key"s tree to either the root of the
level-i subtree or to a node that stores the requested value in its 
ancestor MIB. In the former case, the system proceeds to phase 2 and
in the latter it skips to phase 4. In phase 2, the probe scatter phase,
each node that receives a probe request sends it to all of its children
unless the node"s reduction MIB already has a value that matches
the probe"s attribute key, in which case the node initiates phase 3
on behalf of its subtree. In phase 3, the probe aggregation phase,
when a node receives values for the specified key from each of its
children, it executes the aggregate function on these values and 
either (a) forwards the result to its parent (if its level is less than i)
or (b) initiates phase 4 (if it is at level i). Finally, in phase 4, the
aggregate routing phase the aggregate value is routed down to the
node that requested it. Note that in the extreme case of a function
installed with up = down = 0, a level-i probe can touch all nodes
in a level-i subtree while in the opposite extreme case of a 
function installed with up = down = ALL, probe is a completely local
operation at a leaf.
For probes that include phases 2 (probe scatter) and 3 (probe
aggregation), an issue is how to decide when a node should stop
waiting for its children to respond and send up its current 
aggregate value. A node stops waiting for its children when one of three
conditions occurs: (1) all children have responded, (2) the ADHT
layer signals one or more reconfiguration events that mark all 
children that have not yet responded as unreachable, or (3) a watchdog
timer for the request fires. The last case accounts for nodes that
participate in the ADHT protocol but that fail at the AML level.
At a virtual node, continuous probes are handled similarly as
one-shot probes except that such probes are stored in the 
outstanding probe table for a time period of expTime specified in the probe.
Thus each update for an attribute triggers re-evaluation of 
continuous probes for that attribute.
We implement a lease-based mechanism for dynamic adaptation.
A level-l virtual node for an attribute can issue the lease for 
levell aggregate to a parent or a child only if up is greater than l or it
has leases from all its children. A virtual node at level l can issue
the lease for level-k aggregate for k > l to a child only if down≥
k −l or if it has the lease for that aggregate from its parent. Now a
probe for level-k aggregate can be answered by level-l virtual node
if it has a valid lease, irrespective of the up and down values. We
are currently designing different policies to decide when to issue a
lease and when to revoke a lease and are also evaluating them with
the above mechanism.
Our current prototype does not implement access control on 
install, update, and probe operations but we plan to implement 
Astrolabe"s [38] certificate-based restrictions. Also our current 
prototype does not restrict the resource consumption in executing the
aggregation functions; but, ‘techniques from research on resource
management in server systems and operating systems [2, 3] can be
applied here.
6. ROBUSTNESS
In large scale systems, reconfigurations are common. Our two
main principles for robustness are to guarantee (i) read availability
- probes complete in finite time, and (ii) eventual consistency - 
updates by a live node will be visible to probes by connected nodes
in finite time. During reconfigurations, a probe might return a stale
value for two reasons. First, reconfigurations lead to incorrectness
in the previous aggregate values. Second, the nodes needed for
aggregation to answer the probe become unreachable. Our 
system also provides two hooks that applications can use for improved
end-to-end robustness in the presence of reconfigurations: (1) 
Ondemand re-aggregation and (2) application controlled replication.
Our system handles reconfigurations at two levels - adaptation at
the ADHT layer to ensure connectivity and adaptation at the AML
layer to ensure access to the data in SDIMS.
6.1 ADHT Adaptation
Our ADHT layer adaptation algorithm is same as Pastry"s 
adaptation algorithm [32] - the leaf sets are repaired as soon as a 
reconfiguration is detected and the routing table is repaired lazily. Note
that maintaining extra leaf sets does not degrade the fault-tolerance
property of the original Pastry; indeed, it enhances the resilience
of ADHTs to failures by providing additional routing links. Due
to redundancy in the leaf sets and the routing table, updates can be
routed towards their root nodes successfully even during failures.
385
Reconfig
reconfig
notices
DHT
partial
DHT
complete
DHT
ends
Lazy
Time
Data
3 7 81 2 4 5 6starts
Lazy
Data
starts
Lazy
Data
starts
Lazy
Data
repairrepair
reaggr reaggr reaggr reaggr
happens
Figure 7: Default lazy data re-aggregation time line
Also note that the administrative isolation property satisfied by our
ADHT algorithm ensures that the reconfigurations in a level i 
domain do not affect the probes for level i in a sibling domain.
6.2 AML Adaptation
Broadly, we use two types of strategies for AML adaptation in
the face of reconfigurations: (1) Replication in time as a 
fundamental baseline strategy, and (2) Replication in space as an 
additional performance optimization that falls back on replication in
time when the system runs out of replicas. We provide two 
mechanisms for replication in time. First, lazy re-aggregation propagates
already received updates to new children or new parents in a lazy
fashion over time. Second, applications can reduce the probability
of probe response staleness during such repairs through our flexible
API with appropriate setting of the down parameter.
Lazy Re-aggregation: The DHT layer informs the AML layer
about reconfigurations in the network using the following three
function calls - newParent, failedChild, and newChild. On 
newParent(parent, prefix), all probes in the outstanding-probes table 
corresponding to prefix are re-evaluated. If parent is not null, then 
aggregation functions and already existing data are lazily transferred
in the background. Any new updates, installs, and probes for this
prefix are sent to the parent immediately. On failedChild(child, 
prefix), the AML layer marks the child as inactive and any outstanding
probes that are waiting for data from this child are re-evaluated.
On newChild(child, prefix), the AML layer creates space in its data
structures for this child.
Figure 7 shows the time line for the default lazy re-aggregation
upon reconfiguration. Probes initiated between points 1 and 2 and
that are affected by reconfigurations are reevaluated by AML upon
detecting the reconfiguration. Probes that complete or start between
points 2 and 8 may return stale answers.
On-demand Re-aggregation: The default lazy aggregation
scheme lazily propagates the old updates in the system. 
Additionally, using up and down knobs in the Probe API, applications can
force on-demand fast re-aggregation of updates to avoid staleness
in the face of reconfigurations. In particular, if an application 
detects or suspects an answer as stale, then it can re-issue the probe
increasing the up and down parameters to force the refreshing of
the cached data. Note that this strategy will be useful only after the
DHT adaptation is completed (Point 6 on the time line in Figure 7).
Replication in Space: Replication in space is more 
challenging in our system than in a DHT file location application because
replication in space can be achieved easily in the latter by just 
replicating the root node"s contents. In our system, however, all internal
nodes have to be replicated along with the root.
In our system, applications control replication in space using up
and down knobs in the Install API; with large up and down values,
aggregates at the intermediate virtual nodes are propagated to more
nodes in the system. By reducing the number of nodes that have to
be accessed to answer a probe, applications can reduce the 
probability of incorrect results occurring due to the failure of nodes that
do not contribute to the aggregate. For example, in a file location
application, using a non-zero positive down parameter ensures that
a file"s global aggregate is replicated on nodes other than the root.
0.1
1
10
100
1000
10000
0.0001 0.01 1 100 10000
Avg.numberofmessagesperoperation
Read to Write ratio
Update-All
Up=ALL, Down=9
Up=ALL, Down=6
Update-Up
Update-Local
Up=2, Down=0
Up=5, Down=0
Figure 8: Flexibility of our approach. With different UP and
DOWN values in a network of 4096 nodes for different 
readwrite ratios.
Probes for the file location can then be answered without accessing
the root; hence they are not affected by the failure of the root. 
However, note that this technique is not appropriate in some cases. An
aggregated value in file location system is valid as long as the node
hosting the file is active, irrespective of the status of other nodes
in the system; whereas an application that counts the number of
machines in a system may receive incorrect results irrespective of
the replication. If reconfigurations are only transient (like a node
temporarily not responding due to a burst of load), the replicated
aggregate closely or correctly resembles the current state.
7. EVALUATION
We have implemented a prototype of SDIMS in Java using the
FreePastry framework [32] and performed large-scale simulation
experiments and micro-benchmark experiments on two real 
networks: 187 machines in the department and 69 machines on the
PlanetLab [27] testbed. In all experiments, we use static up and
down values and turn off dynamic adaptation. Our evaluation 
supports four main conclusions. First, flexible API provides different
propagation strategies that minimize communication resources at
different read-to-write ratios. For example, in our simulation we
observe Update-Local to be efficient for read-to-write ratios 
below 0.0001, Update-Up around 1, and Update-All above 50000.
Second, our system is scalable with respect to both nodes and 
attributes. In particular, we find that the maximum node stress in
our system is an order lower than observed with an Update-All,
gossiping approach. Third, in contrast to unmodified Pastry which
violates path convergence property in upto 14% cases, our system
conforms to the property. Fourth, the system is robust to 
reconfigurations and adapts to failures with in a few seconds.
7.1 Simulation Experiments
Flexibility and Scalability: A major innovation of our system
is its ability to provide flexible computation and propagation of 
aggregates. In Figure 8, we demonstrate the flexibility exposed by the
aggregation API explained in Section 3. We simulate a system with
4096 nodes arranged in a domain hierarchy with branching factor
(bf) of 16 and install several attributes with different up and down
parameters. We plot the average number of messages per operation
incurred for a wide range of read-to-write ratios of the operations
for different attributes. Simulations with other sizes of networks
with different branching factors reveal similar results. This graph
clearly demonstrates the benefit of supporting a wide range of 
computation and propagation strategies. Although having a small UP
386
1
10
100
1000
10000
100000
1e+06
1e+07
1 10 100 1000 10000 100000
MaximumNodeStress
Number of attributes installed
Gossip 256
Gossip 4096
Gossip 65536
DHT 256
DHT 4096
DHT 65536
Figure 9: Max node stress for a gossiping approach vs. ADHT
based approach for different number of nodes with increasing
number of sparse attributes.
value is efficient for attributes with low read-to-write ratios (write
dominated applications), the probe latency, when reads do occur,
may be high since the probe needs to aggregate the data from all
the nodes that did not send their aggregate up. Conversely, 
applications that wish to improve probe overheads or latencies can increase
their UP and DOWN propagation at a potential cost of increase in
write overheads.
Compared to an existing Update-all single aggregation tree 
approach [38], scalability in SDIMS comes from (1) leveraging DHTs
to form multiple aggregation trees that split the load across nodes
and (2) flexible propagation that avoids propagation of all updates
to all nodes. Figure 9 demonstrates the SDIMS"s scalability with
nodes and attributes. For this experiment, we build a simulator to
simulate both Astrolabe [38] (a gossiping, Update-All approach)
and our system for an increasing number of sparse attributes. Each
attribute corresponds to the membership in a multicast session with
a small number of participants. For this experiment, the session
size is set to 8, the branching factor is set to 16, the propagation
mode for SDIMS is Update-Up, and the participant nodes perform
continuous probes for the global aggregate value. We plot the 
maximum node stress (in terms of messages) observed in both schemes
for different sized networks with increasing number of sessions
when the participant of each session performs an update operation.
Clearly, the DHT based scheme is more scalable with respect to 
attributes than an Update-all gossiping scheme. Observe that at some
constant number of attributes, as the number of nodes increase in
the system, the maximum node stress increases in the gossiping
approach, while it decreases in our approach as the load of 
aggregation is spread across more nodes. Simulations with other session
sizes (4 and 16) yield similar results.
Administrative Hierarchy and Robustness: Although the
routing protocol of ADHT might lead to an increased number of
hops to reach the root for a key as compared to original Pastry, the
algorithm conforms to the path convergence and locality properties
and thus provides administrative isolation property. In Figure 10,
we quantify the increased path length by comparisons with 
unmodified Pastry for different sized networks with different branching
factors of the domain hierarchy tree. To quantify the path 
convergence property, we perform simulations with a large number of
probe pairs - each pair probing for a random key starting from two
randomly chosen nodes. In Figure 11, we plot the percentage of
probe pairs for unmodified pastry that do not conform to the path
convergence property. When the branching factor is low, the 
domain hierarchy tree is deeper resulting in a large difference between
0
1
2
3
4
5
6
7
10 100 1000 10000 100000
PathLength
Number of Nodes
ADHT bf=4
ADHT bf=16
ADHT bf=64
PASTRY bf=4,16,64
Figure 10: Average path length to root in Pastry versus ADHT
for different branching factors. Note that all lines 
corresponding to Pastry overlap.
0
2
4
6
8
10
12
14
16
10 100 1000 10000 100000
Percentageofviolations
Number of Nodes
bf=4
bf=16
bf=64
Figure 11: Percentage of probe pairs whose paths to the root
did not conform to the path convergence property with Pastry.
U
pdate-All
U
pdate-U
p
U
pdate-Local
0
200
400
600
800
Latency(inms)
Average Latency
U
pdate-All
U
pdate-U
p
U
pdate-Local
0
1000
2000
3000
Latency(inms) Average Latency
(a) (b)
Figure 12: Latency of probes for aggregate at global root level
with three different modes of aggregate propagation on (a) 
department machines, and (b) PlanetLab machines
Pastry and ADHT in the average path length; but it is at these small
domain sizes, that the path convergence fails more often with the
original Pastry.
7.2 Testbed experiments
We run our prototype on 180 department machines (some 
machines ran multiple node instances, so this configuration has a 
total of 283 SDIMS nodes) and also on 69 machines of the 
PlanetLab [27] testbed. We measure the performance of our system with
two micro-benchmarks. In the first micro-benchmark, we install
three aggregation functions of types Update-Local, Update-Up, and
Update-All, perform update operation on all nodes for all three 
aggregation functions, and measure the latencies incurred by probes
for the global aggregate from all nodes in the system. Figure 12
387
0
20
40
60
80
100
120
140
0 5 10 15 20 25
2700
2720
2740
2760
2780
2800
2820
2840
Latency(inms)
ValuesObserved
Time(in sec)
Values
latency
Node Killed
Figure 13: Micro-benchmark on department network showing
the behavior of the probes from a single node when failures are
happening at some other nodes. All 283 nodes assign a value of
10 to the attribute.
10
100
1000
10000
100000
0 50 100 150 200 250 300 350 400 450 500
500
550
600
650
700
Latency(inms)
ValuesObserved
Time(in sec)
Values
latency
Node Killed
Figure 14: Probe performance during failures on 69 machines
of PlanetLab testbed
shows the observed latencies for both testbeds. Notice that the 
latency in Update-Local is high compared to the Update-UP policy.
This is because latency in Update-Local is affected by the presence
of even a single slow machine or a single machine with a high 
latency network connection.
In the second benchmark, we examine robustness. We install one
aggregation function of type Update-Up that performs sum 
operation on an integer valued attribute. Each node updates the attribute
with the value 10. Then we monitor the latencies and results 
returned on the probe operation for global aggregate on one chosen
node, while we kill some nodes after every few probes. Figure 13
shows the results on the departmental testbed. Due to the nature
of the testbed (machines in a department), there is little change in
the latencies even in the face of reconfigurations. In Figure 14, we
present the results of the experiment on PlanetLab testbed. The
root node of the aggregation tree is terminated after about 275 
seconds. There is a 5X increase in the latencies after the death of the
initial root node as a more distant node becomes the root node after
repairs. In both experiments, the values returned on probes start
reflecting the correct situation within a short time after the failures.
From both the testbed benchmark experiments and the 
simulation experiments on flexibility and scalability, we conclude that (1)
the flexibility provided by SDIMS allows applications to tradeoff
read-write overheads (Figure 8), read latency, and sensitivity to
slow machines (Figure 12), (2) a good default aggregation 
strategy is Update-Up which has moderate overheads on both reads and
writes (Figure 8), has moderate read latencies (Figure 12), and is
scalable with respect to both nodes and attributes (Figure 9), and
(3) small domain sizes are the cases where DHT algorithms fail to
provide path convergence more often and SDIMS ensures path 
convergence with only a moderate increase in path lengths (Figure 11).
7.3 Applications
SDIMS is designed as a general distributed monitoring and 
control infrastructure for a broad range of applications. Above, we 
discuss some simple microbenchmarks including a multicast 
membership service and a calculate-sum function. Van Renesse et al. [38]
provide detailed examples of how such a service can be used for a
peer-to-peer caching directory, a data-diffusion service, a 
publishsubscribe system, barrier synchronization, and voting. 
Additionally, we have initial experience using SDIMS to construct two 
significant applications: the control plane for a large-scale distributed
file system [12] and a network monitor for identifying heavy 
hitters that consume excess resources.
Distributed file system control: The PRACTI (Partial 
Replication, Arbitrary Consistency, Topology Independence) replication
system provides a set of mechanisms for data replication over which
arbitrary control policies can be layered. We use SDIMS to provide
several key functions in order to create a file system over the 
lowlevel PRACTI mechanisms.
First, nodes use SDIMS as a directory to handle read misses.
When a node n receives an object o, it updates the (ReadDir, o)
attribute with the value n; when n discards o from its local store,
it resets (ReadDir, o) to NULL. At each virtual node, the ReadDir
aggregation function simply selects a random non-null child value
(if any) and we use the Update-Up policy for propagating updates.
Finally, to locate a nearby copy of an object o, a node n1 issues a
series of probe requests for the (ReadDir, o) attribute, starting with
level = 1 and increasing the level value with each repeated probe
request until a non-null node ID n2 is returned. n1 then sends a
demand read request to n2, and n2 sends the data if it has it. 
Conversely, if n2 does not have a copy of o, it sends a nack to n1,
and n1 issues a retry probe with the down parameter set to a value
larger than used in the previous probe in order to force on-demand
re-aggregation, which will yield a fresher value for the retry.
Second, nodes subscribe to invalidations and updates to interest
sets of files, and nodes use SDIMS to set up and maintain 
perinterest-set network-topology-sensitive spanning trees for 
propagating this information. To subscribe to invalidations for interest
set i, a node n1 first updates the (Inval, i) attribute with its 
identity n1, and the aggregation function at each virtual node selects
one non-null child value. Finally, n1 probes increasing levels of the
the (Inval, i) attribute until it finds the first node n2 = n1; n1 then
uses n2 as its parent in the spanning tree. n1 also issues a 
continuous probe for this attribute at this level so that it is notified of any
change to its spanning tree parent. Spanning trees for streams of
pushed updates are maintained in a similar manner.
In the future, we plan to use SDIMS for at least two additional
services within this replication system. First, we plan to use SDIMS
to track the read and write rates to different objects; prefetch 
algorithms will use this information to prioritize replication [40, 41].
Second, we plan to track the ranges of invalidation sequence 
numbers seen by each node for each interest set in order to augment
the spanning trees described above with additional hole filling to
allow nodes to locate specific invalidations they have missed.
Overall, our initial experience with using SDIMS for the 
PRACTII replication system suggests that (1) the general aggregation
interface provided by SDIMS simplifies the construction of 
distributed applications-given the low-level PRACTI mechanisms,
388
we were able to construct a basic file system that uses SDIMS for
several distinct control tasks in under two weeks and (2) the weak
consistency guarantees provided by SDIMS meet the requirements
of this application-each node"s controller effectively treats 
information from SDIMS as hints, and if a contacted node does not have
the needed data, the controller retries, using SDIMS on-demand 
reaggregation to obtain a fresher hint.
Distributed heavy hitter problem: The goal of the heavy hitter
problem is to identify network sources, destinations, or protocols
that account for significant or unusual amounts of traffic. As noted
by Estan et al. [13], this information is useful for a variety of 
applications such as intrusion detection (e.g., port scanning), denial of
service detection, worm detection and tracking, fair network 
allocation, and network maintenance. Significant work has been done
on developing high-performance stream-processing algorithms for
identifying heavy hitters at one router, but this is just a first step;
ideally these applications would like not just one router"s views of
the heavy hitters but an aggregate view.
We use SDIMS to allow local information about heavy hitters
to be pooled into a view of global heavy hitters. For each 
destination IP address IPx, a node updates the attribute (DestBW,IPx)
with the number of bytes sent to IPx in the last time window. The
aggregation function for attribute type DestBW is installed with the
Update-UP strategy and simply adds the values from child nodes.
Nodes perform continuous probe for global aggregate of the 
attribute and raise an alarm when the global aggregate value goes
above a specified limit. Note that only nodes sending data to a 
particular IP address perform probes for the corresponding attribute.
Also note that techniques from [25] can be extended to hierarchical
case to tradeoff precision for communication bandwidth.
8. RELATED WORK
The aggregation abstraction we use in our work is heavily 
influenced by the Astrolabe [38] project. Astrolabe adopts a 
PropagateAll and unstructured gossiping techniques to attain robustness [5].
However, any gossiping scheme requires aggressive replication of
the aggregates. While such aggressive replication is efficient for
read-dominated attributes, it incurs high message cost for attributes
with a small read-to-write ratio. Our approach provides a 
flexible API for applications to set propagation rules according to their
read-to-write ratios. Other closely related projects include 
Willow [39], Cone [4], DASIS [1], and SOMO [45]. Willow, DASIS
and SOMO build a single tree for aggregation. Cone builds a tree
per attribute and requires a total order on the attribute values.
Several academic [15, 21, 42] and commercial [37] distributed
monitoring systems have been designed to monitor the status of
large networked systems. Some of them are centralized where all
the monitoring data is collected and analyzed at a central host. 
Ganglia [15, 23] uses a hierarchical system where the attributes are
replicated within clusters using multicast and then cluster 
aggregates are further aggregated along a single tree. Sophia [42] is
a distributed monitoring system designed with a declarative logic
programming model where the location of query execution is both
explicit in the language and can be calculated during evaluation.
This research is complementary to our work. TAG [21] collects
information from a large number of sensors along a single tree.
The observation that DHTs internally provide a scalable forest
of reduction trees is not new. Plaxton et al."s [28] original paper 
describes not a DHT, but a system for hierarchically aggregating and
querying object location data in order to route requests to nearby
copies of objects. Many systems-building upon both Plaxton"s
bit-correcting strategy [32, 46] and upon other strategies [24, 29,
35]-have chosen to hide this power and export a simple and 
general distributed hash table abstraction as a useful building block for
a broad range of distributed applications. Some of these systems
internally make use of the reduction forest not only for routing but
also for caching [32], but for simplicity, these systems do not 
generally export this powerful functionality in their external interface.
Our goal is to develop and expose the internal reduction forest of
DHTs as a similarly general and useful abstraction.
Although object location is a predominant target application for
DHTs, several other applications like multicast [8, 9, 33, 36] and
DNS [11] are also built using DHTs. All these systems implicitly
perform aggregation on some attribute, and each one of them must
be designed to handle any reconfigurations in the underlying DHT.
With the aggregation abstraction provided by our system, designing
and building of such applications becomes easier.
Internal DHT trees typically do not satisfy domain locality 
properties required in our system. Castro et al. [7] and Gummadi et
al. [17] point out the importance of path convergence from the 
perspective of achieving efficiency and investigate the performance of
Pastry and other DHT algorithms, respectively. SkipNet [18] 
provides domain restricted routing where a key search is limited to the
specified domain. This interface can be used to ensure path 
convergence by searching in the lowest domain and moving up to the next
domain when the search reaches the root in the current domain. 
Although this strategy guarantees path convergence, it loses the 
aggregation tree abstraction property of DHTs as the domain constrained
routing might touch a node more than once (as it searches forward
and then backward to stay within a domain).
9. CONCLUSIONS
This paper presents a Scalable Distributed Information 
Management System (SDIMS) that aggregates information in large-scale
networked systems and that can serve as a basic building block
for a broad range of applications. For large scale systems, 
hierarchical aggregation is a fundamental abstraction for scalability.
We build our system by extending ideas from Astrolabe and DHTs
to achieve (i) scalability with respect to both nodes and attributes
through a new aggregation abstraction that helps leverage DHT"s
internal trees for aggregation, (ii) flexibility through a simple API
that lets applications control propagation of reads and writes, (iii)
administrative isolation through simple augmentations of current
DHT algorithms, and (iv) robustness to node and network 
reconfigurations through lazy reaggregation, on-demand reaggregation,
and tunable spatial replication.
Acknowlegements
We are grateful to J.C. Browne, Robert van Renessee, Amin 
Vahdat, Jay Lepreau, and the anonymous reviewers for their helpful
comments on this work.
10. REFERENCES
[1] K. Albrecht, R. Arnold, M. Gahwiler, and R. Wattenhofer.
Join and Leave in Peer-to-Peer Systems: The DASIS
approach. Technical report, CS, ETH Zurich, 2003.
[2] G. Back, W. H. Hsieh, and J. Lepreau. Processes in KaffeOS:
Isolation, Resource Management, and Sharing in Java. In
Proc. OSDI, Oct 2000.
[3] G. Banga, P. Druschel, and J. Mogul. Resource Containers:
A New Facility for Resource Management in Server
Systems. In OSDI99, Feb. 1999.
[4] R. Bhagwan, P. Mahadevan, G. Varghese, and G. M. Voelker.
Cone: A Distributed Heap-Based Approach to Resource
Selection. Technical Report CS2004-0784, UCSD, 2004.
389
[5] K. P. Birman. The Surprising Power of Epidemic
Communication. In Proceedings of FuDiCo, 2003.
[6] B. Bloom. Space/time tradeoffs in hash coding with
allowable errors. Comm. of the ACM, 13(7):422-425, 1970.
[7] M. Castro, P. Druschel, Y. C. Hu, and A. Rowstron.
Exploiting Network Proximity in Peer-to-Peer Overlay
Networks. Technical Report MSR-TR-2002-82, MSR.
[8] M. Castro, P. Druschel, A.-M. Kermarrec, A. Nandi,
A. Rowstron, and A. Singh. SplitStream: High-bandwidth
Multicast in a Cooperative Environment. In SOSP, 2003.
[9] M. Castro, P. Druschel, A.-M. Kermarrec, and A. Rowstron.
SCRIBE: A Large-scale and Decentralised Application-level
Multicast Infrastructure. IEEE JSAC (Special issue on
Network Support for Multicast Communications), 2002.
[10] J. Challenger, P. Dantzig, and A. Iyengar. A scalable and
highly available system for serving dynamic data at
frequently accessed web sites. In In Proceedings of
ACM/IEEE, Supercomputing "98 (SC98), Nov. 1998.
[11] R. Cox, A. Muthitacharoen, and R. T. Morris. Serving DNS
using a Peer-to-Peer Lookup Service. In IPTPS, 2002.
[12] M. Dahlin, L. Gao, A. Nayate, A. Venkataramani,
P. Yalagandula, and J. Zheng. PRACTI replication for
large-scale systems. Technical Report TR-04-28, The
University of Texas at Austin, 2004.
[13] C. Estan, G. Varghese, and M. Fisk. Bitmap algorithms for
counting active flows on high speed links. In Internet
Measurement Conference 2003, 2003.
[14] Y. Fu, J. Chase, B. Chun, S. Schwab, and A. Vahdat.
SHARP: An architecture for secure resource peering. In
Proc. SOSP, Oct. 2003.
[15] Ganglia: Distributed Monitoring and Execution System.
http://ganglia.sourceforge.net.
[16] S. Gribble, A. Halevy, Z. Ives, M. Rodrig, and D. Suciu.
What Can Peer-to-Peer Do for Databases, and Vice Versa? In
Proceedings of the WebDB, 2001.
[17] K. Gummadi, R. Gummadi, S. D. Gribble, S. Ratnasamy,
S. Shenker, and I. Stoica. The Impact of DHT Routing
Geometry on Resilience and Proximity. In SIGCOMM, 2003.
[18] N. J. A. Harvey, M. B. Jones, S. Saroiu, M. Theimer, and
A. Wolman. SkipNet: A Scalable Overlay Network with
Practical Locality Properties. In USITS, March 2003.
[19] R. Huebsch, J. M. Hellerstein, N. Lanham, B. T. Loo,
S. Shenker, and I. Stoica. Querying the Internet with PIER.
In Proceedings of the VLDB Conference, May 2003.
[20] C. Intanagonwiwat, R. Govindan, and D. Estrin. Directed
diffusion: a scalable and robust communication paradigm for
sensor networks. In MobiCom, 2000.
[21] S. R. Madden, M. J. Franklin, J. M. Hellerstein, and
W. Hong. TAG: a Tiny AGgregation Service for ad-hoc
Sensor Networks. In OSDI, 2002.
[22] D. Malkhi. Dynamic Lookup Networks. In FuDiCo, 2002.
[23] M. L. Massie, B. N. Chun, and D. E. Culler. The ganglia
distributed monitoring system: Design, implementation, and
experience. In submission.
[24] P. Maymounkov and D. Mazieres. Kademlia: A Peer-to-peer
Information System Based on the XOR Metric. In
Proceesings of the IPTPS, March 2002.
[25] C. Olston and J. Widom. Offering a precision-performance
tradeoff for aggregation queries over replicated data. In
VLDB, pages 144-155, Sept. 2000.
[26] K. Petersen, M. Spreitzer, D. Terry, M. Theimer, and
A. Demers. Flexible Update Propagation for Weakly
Consistent Replication. In Proc. SOSP, Oct. 1997.
[27] Planetlab. http://www.planet-lab.org.
[28] C. G. Plaxton, R. Rajaraman, and A. W. Richa. Accessing
Nearby Copies of Replicated Objects in a Distributed
Environment. In ACM SPAA, 1997.
[29] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and
S. Shenker. A Scalable Content Addressable Network. In
Proceedings of ACM SIGCOMM, 2001.
[30] S. Ratnasamy, S. Shenker, and I. Stoica. Routing Algorithms
for DHTs: Some Open Questions. In IPTPS, March 2002.
[31] T. Roscoe, R. Mortier, P. Jardetzky, and S. Hand. InfoSpect:
Using a Logic Language for System Health Monitoring in
Distributed Systems. In Proceedings of the SIGOPS
European Workshop, 2002.
[32] A. Rowstron and P. Druschel. Pastry: Scalable, Distributed
Object Location and Routing for Large-scale Peer-to-peer
Systems. In Middleware, 2001.
[33] S.Ratnasamy, M.Handley, R.Karp, and S.Shenker.
Application-level Multicast using Content-addressable
Networks. In Proceedings of the NGC, November 2001.
[34] W. Stallings. SNMP, SNMPv2, and CMIP. Addison-Wesley,
1993.
[35] I. Stoica, R. Morris, D. Karger, F. Kaashoek, and
H. Balakrishnan. Chord: A scalable Peer-To-Peer lookup
service for internet applications. In ACM SIGCOMM, 2001.
[36] S.Zhuang, B.Zhao, A.Joseph, R.Katz, and J.Kubiatowicz.
Bayeux: An Architecture for Scalable and Fault-tolerant
Wide-Area Data Dissemination. In NOSSDAV, 2001.
[37] IBM Tivoli Monitoring.
www.ibm.com/software/tivoli/products/monitor.
[38] R. VanRenesse, K. P. Birman, and W. Vogels. Astrolabe: A
Robust and Scalable Technology for Distributed System
Monitoring, Management, and Data Mining. TOCS, 2003.
[39] R. VanRenesse and A. Bozdog. Willow: DHT, Aggregation,
and Publish/Subscribe in One Protocol. In IPTPS, 2004.
[40] A. Venkataramani, P. Weidmann, and M. Dahlin. Bandwidth
constrained placement in a wan. In PODC, Aug. 2001.
[41] A. Venkataramani, P. Yalagandula, R. Kokku, S. Sharif, and
M. Dahlin. Potential costs and benefits of long-term
prefetching for content-distribution. Elsevier Computer
Communications, 25(4):367-375, Mar. 2002.
[42] M. Wawrzoniak, L. Peterson, and T. Roscoe. Sophia: An
Information Plane for Networked Systems. In HotNets-II,
2003.
[43] R. Wolski, N. Spring, and J. Hayes. The network weather
service: A distributed resource performance forecasting
service for metacomputing. Journal of Future Generation
Computing Systems, 15(5-6):757-768, Oct 1999.
[44] P. Yalagandula and M. Dahlin. SDIMS: A scalable
distributed information management system. Technical
Report TR-03-47, Dept. of Computer Sciences, UT Austin,
Sep 2003.
[45] Z. Zhang, S.-M. Shi, and J. Zhu. SOMO: Self-Organized
Metadata Overlay for Resource Management in P2P DHT. In
IPTPS, 2003.
[46] B. Y. Zhao, J. D. Kubiatowicz, and A. D. Joseph. Tapestry:
An Infrastructure for Fault-tolerant Wide-area Location and
Routing. Technical Report UCB/CSD-01-1141, UC
Berkeley, Apr. 2001.
390
Globally Synchronized Dead-Reckoning with Local Lag for
Continuous Distributed Multiplayer Games
Yi Zhang1
, Ling Chen1, 2
, Gencai Chen1
1College of Computer Science, Zhejiang University, Hangzhou 310027, P.R. China
2School of Computer Science and IT, The University of Nottingham, Nottingham NG8 1BB, UK
{m05zhangyi, lingchen, chengc}@cs.zju.edu.cn
ABSTRACT
Dead-Reckoning (DR) is an effective method to maintain
consistency for Continuous Distributed Multiplayer Games
(CDMG). Since DR can filter most unnecessary state updates and
improve the scalability of a system, it is widely used in
commercial CDMG. However, DR cannot maintain high
consistency, and this constrains its application in highly
interactive games. With the help of global synchronization, DR
can achieve higher consistency, but it still cannot eliminate before
inconsistency. In this paper, a method named Globally
Synchronized DR with Local Lag (GS-DR-LL), which combines
local lag and Globally Synchronized DR (GS-DR), is presented.
Performance evaluation shows that GS-DR-LL can effectively
decrease before inconsistency, and the effects increase with the
lag.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems - distributed applications.
General Terms
Algorithms, Performance, Experimentation.
1. INTRODUCTION
Nowadays, many distributed multiplayer games adopt replicated
architectures. In such games, the states of entities are changed not
only by the operations of players, but also by the passing of time
[1, 2]. These games are referred to as Continuous Distributed
Multiplayer Games (CDMG). Like other distributed applications,
CDMG also suffer from the consistency problem caused by
network transmission delay. Although new network techniques
(e.g. QoS) can reduce or at least bound the delay, they can not
completely eliminate it, as there exists the physical speed
limitation of light, for instance, 100 ms is needed for light to
propagate from Europe to Australia [3]. There are many studies
about the effects of network transmission delay in different
applications [4, 5, 6, 7]. In replication based games, network
transmission delay makes the states of local and remote sites to be
inconsistent, which can cause serious problems, such as reducing
the fairness of a game and leading to paradoxical situations etc. In
order to maintain consistency for distributed systems, many
different approaches have been proposed, among which local lag
and Dead-Reckoning (DR) are two representative approaches.
Mauve et al [1] proposed local lag to maintain high consistency
for replicated continuous applications. It synchronizes the
physical clocks of all sites in a system. After an operation is
issued at local site, it delays the execution of the operation for a
short time. During this short time period the operation is
transmitted to remote sites, and all sites try to execute the
operation at a same physical time. In order to tackle the
inconsistency caused by exceptional network transmission delay,
a time warp based mechanism is proposed to repair the state.
Local lag can achieve significant high consistency, but it is based
on operation transmission, which forwards every operation on a
shared entity to remote sites. Since operation transmission
mechanism requests that all operations should be transmitted in a
reliable way, message filtering is difficult to be deployed and the
scalability of a system is limited.
DR is based on state transmission mechanism. In addition to the
high fidelity model that maintains the accurate states of its own
entities, each site also has a DR model that estimates the states of
all entities (including its own entities). After each update of its
own entities, a site compares the accurate state with the estimated
one. If the difference exceeds a pre-defined threshold, a state
update would be transmitted to all sites and all DR models would
be corrected. Through state estimation, DR can not only maintain
consistency but also decrease the number of transmitted state
updates. Compared with aforementioned local lag, DR cannot
maintain high consistency. Due to network transmission delay,
when a remote site receives a state update of an entity the state of
the entity might have changed at the site sending the state update.
In order to make DR maintain high consistency, Aggarwal et al [8]
proposed Globally Synchronized DR (GS-DR), which
synchronizes the physical clocks of all sites in a system and adds
time stamps to transmitted state updates. Detailed description of
GS-DR can be found in Section 3.
When a state update is available, GS-DR immediately updates the
state of local site and then transmits the state update to remote
sites, which causes the states of local site and remote sites to be
inconsistent in the transmission procedure. Thus with the
synchronization of physical clocks, GS-DR can eliminate after
inconsistency, but it cannot tackle before inconsistency [8]. In this
paper, we propose a new method named globally synchronized
DR with Local Lag (GS-DR-LL), which combines local lag and
GS-DR. By delaying the update to local site, GS-DR-LL can
achieve higher consistency than GS-DR. The rest of this paper is
organized as follows: Section 2 gives the definition of consistency
and corresponding metrics; the cause of the inconsistency of DR
is analyzed in Section 3; Section 4 describes how GS-DR-LL
works; performance evaluation is presented in Section 5; Section
6 concludes the paper.
2. CONSISTENCY DEFINITIONS AND
METRICS
The consistency of replicated applications has already been well
defined in discrete domain [9, 10, 11, 12], but few related work
has been done in continuous domain. Mauve et al [1] have given a
definition of consistency for replicated applications in continuous
domain, but the definition is based on operation transmission and
it is difficult for the definition to describe state transmission based
methods (e.g. DR). Here, we present an alternative definition of
consistency in continuous domain, which suits state transmission
based methods well.
Given two distinct sites i and j, which have replicated a shared
entity e, at a given time t, the states of e at sites i and j are Si(t)
and Sj(t).
DEFINITION 1: the states of e at sites i and j are consistent at
time t, iff:
De(i, j, t) = |Si(t) - Sj(t)| = 0 (1)
DEFINITION 2: the states of e at sites i and j are consistent
between time t1 and t2 (t1 < t2), iff:
De(i, j, t1, t2) = dt|)t(S)t(S|
t2
t1
ji = 0 (2)
In this paper, formulas (1) and (2) are used to determine whether
the states of shared entities are consistent between local and
remote sites. Due to network transmission delay, it is difficult to
maintain the states of shared entities absolutely consistent.
Corresponding metrics are needed to measure the consistency of
shared entities between local and remote sites.
De(i, j, t) can be used as a metric to measure the degree of
consistency at a certain time point. If De(i, j, t1) > De(i, j, t2), it
can be stated that between sites i and j, the consistency of the
states of entity e at time point t1 is lower than that at time point t2.
If De(i, j, t) > De(l, k, t), it can be stated that, at time point t, the
consistency of the states of entity e between sites i and j is lower
than that between sites l and k.
Similarly, De(i, j, t1, t2) can been used as a metric to measure the
degree of consistency in a certain time period. If De(i, j, t1, t2) >
De(i, j, t3, t4) and |t1 - t2| = |t3 - t4|, it can be stated that between
sites i and j, the consistency of the states of entity e between time
points t1 and t2 is lower than that between time points t3 and t4. If
De(i, j, t1, t2) > De(l, k, t1, t2), it can be stated that between time
points t1 and t2, the consistency of the states of entity e between
sites i and j is lower than that between sites l and k.
In DR, the states of entities are composed of the positions and
orientations of entities and some prediction related parameters
(e.g. the velocities of entities). Given two distinct sites i and j,
which have replicated a shared entity e, at a given time point t, the
positions of e at sites i and j are (xit, yit, zit) and (xjt, yjt, zjt), De(i, j,
t) and D (i, j, t1, t2) could be calculated as:
De(i, j, t) = )zz()yy()xx( jtit
2
jtit
2
jtit
2
(3)
De(i, j, t1, t2)
= dt)zz()yy()xx(
2t
1t jtit
2
jtit
2
jtit
2
(4)
In this paper, formulas (3) and (4) are used as metrics to measure
the consistency of shared entities between local and remote sites.
3. INCONSISTENCY IN DR
The inconsistency in DR can be divided into two sections by the
time point when a remote site receives a state update. The
inconsistency before a remote site receives a state update is
referred to as before inconsistency, and the inconsistency after a
remote site receives a state update is referred to as after
inconsistency. Before inconsistency and after inconsistency are
similar with the terms before export error and after export error
[8].
After inconsistency is caused by the lack of synchronization
between the physical clocks of all sites in a system. By employing
physical clock synchronization, GS-DR can accurately calculate
the states of shared entities after receiving state updates, and it
can eliminate after inconsistency. Before inconsistency is caused
by two reasons. The first reason is the delay of sending state
updates, as local site does not send a state update unless the
difference between accurate state and the estimated one is larger
than a predefined threshold. The second reason is network
transmission delay, as a shared entity can be synchronized only
after remote sites receiving corresponding state update.
Figure 1. The paths of a shared entity by using GS-DR.
For example, it is assumed that the velocity of a shared entity is
the only parameter to predict the entity"s position, and current
position of the entity can be calculated by its last position and
current velocity. To simplify the description, it is also assumed
that there are only two sites i and j in a game session, site i acts as
2 The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006
local site and site j acts as remote site, and t1 is the time point the
local site updates the state of the shared entity. Figure 1 illustrates
the paths of the shared entity at local site and remote site in x axis
by using GS-DR. At the beginning, the positions of the shared
entity are the same at sites i and j and the velocity of the shared
entity is 0. Before time point t0, the paths of the shared entity at
sites i and j in x coordinate are exactly the same. At time point t0,
the player at site i issues an operation, which changes the velocity
in x axis to v0. Site i first periodically checks whether the
difference between the accurate position of the shared entity and
the estimated one, 0 in this case, is larger than a predefined
threshold. At time point t1, site i finds that the difference is larger
than the threshold and it sends a state update to site j. The state
update contains the position and velocity of the shared entity at
time point t1 and time point t1 is also attached as a timestamp. At
time point t2, the state update reaches site j, and the received state
and the time deviation between time points t1 and t2 are used to
calculate the current position of the shared entity. Then site j
updates its replicated entity"s position and velocity, and the paths
of the shared entity at sites i and j overlap again.
From Figure 1, it can be seen that the after inconsistency is 0, and
the before consistency is composed of two parts, D1 and D2. D1
is De(i, j, t0, t1) and it is caused by the state filtering mechanism
of DR. D2 is De(i, j, t1, t2) and it is caused by network
transmission delay.
4. GLOBALLY SYNCHRONIZED DR
WITH LOCAL LAG
From the analysis in Section 3, It can be seen that GS-DR can
eliminate after inconsistency, but it cannot effectively tackle
before inconsistency. In order to decrease before inconsistency,
we propose GS-DR-LL, which combines GS-DR with local lag
and can effectively decrease before inconsistency.
In GS-DR-LL, the state of a shared entity at a certain time point t
is notated as S = (t, pos, par 1, par 2, ……, par n), in which pos
means the position of the entity and par 1 to par n means the
parameters to calculate the position of the entity. In order to
simplify the description of GS-DR-LL, it is assumed that there are
only one shared entity and one remote site.
At the beginning of a game session, the states of the shared entity
are the same at local and remote sites, with the same position p0
and parameters pars0 (pars represents all the parameters). Local
site keeps three states: the real state of the entity Sreal, the
predicted state at remote site Sp-remote, and the latest state updated
to remote site Slate. Remote site keep only one state Sremote, which
is the real state of the entity at remote site. Therefore, at the
beginning of a game session Sreal = Sp-remote = Slate = Sremote = (t0,
p0, pars0). In GS-DR-LL, it is assumed that the physical clocks of
all sites are synchronized with a deviation of less than 50 ms
(using NTP or GPS clock). Furthermore, it is necessary to make
corrections to a physical clock in a way that does not result in
decreasing the value of the clock, for example by slowing down
or halting the clock for a period of time. Additionally it is
assumed that the game scene is updated at a fixed frequency and
T stands for the time interval between two consecutive updates,
for example, if the scene update frequency is 50 Hz, T would be
20 ms. n stands for the lag value used by local lag, and t stands for
current physical time.
After updating the scene, local site waits for a constant amount of
time T. During this time period, local site receives the operations
of the player and stores them in a list L. All operations in L are
sorted by their issue time. At the end of time period T, local site
executes all stored operations, whose issue time is between t - T
and t, on Slate to get the new Slate, and it also executes all stored
operations, whose issue time is between t - (n + T) and t - n, on
Sreal to get the new Sreal. Additionally, local site uses Sp-remote and
corresponding prediction methods to estimate the new Sp-remote.
After new Slate, Sreal, and Sp-remote are calculated, local site
compares whether the difference between the new Slate and 
Spremote exceeds the predefined threshold. If YES, local site sends
new Slate to remote site and Sp-remote is updated with new Slate. Note
that the timestamp of the sent state update is t. After that, local
site uses Sreal to update local scene and deletes the operations,
whose issue time is less than t - n, from L.
After updating the scene, remote site waits for a constant amount
of time T. During this time period, remote site stores received
state update(s) in a list R. All state updates in R are sorted by their
timestamps. At the end of time period T, remote site checks
whether R contains state updates whose timestamps are less than t
- n. Note that t is current physical time and it increases during the
transmission of state updates. If YES, it uses these state updates
and corresponding prediction methods to calculate the new Sremote,
else they use Sremote and corresponding prediction methods to
estimate the new Sremote. After that, local site uses Sremote to update
local scene and deletes the sate updates, whose timestamps are
less than t - n, from R.
From the above description, it can been see that the main
difference between GS-DR and GS-DR-LL is that GS-DR-LL
uses the operations, whose issue time is less than t - n, to
calculate Sreal. That means that the scene seen by local player is
the results of the operations issued a period of time (i.e. n) ago.
Meanwhile, if the results of issued operations make the difference
between Slate and Sp-remote exceed a predefined threshold,
corresponding state updates are sent to remote sites immediately.
The aforementioned is the basic mechanism of GS-DR-LL. In the
case with multiple shared entities and remote sites, local site
calculates Slate, Sreal, and Sp-remote for different shared entities
respectively, if there are multiple Slate need to be transmitted, local
site packets them in one state update and then send it to all remote
sites.
Figure 2 illustrates the paths of a shared entity at local site and
remote site while using GS-DR and GS-DR-LL. All conditions
are the same with the conditions used in the aforementioned
example describing GS-DR. Compared with t1, t2, and n, T (i.e.
the time interval between two consecutive updates) is quite small
and it is ignored in the following description.
At time point t0, the player at site i issues an operation, which
changes the velocity of the shared entity form 0 to v0. By using
GS-DR-LL, the results of the operation are updated to local scene
at time point t0 + n. However the operation is immediately used
to calculate Slate, thus in spite of GS-DR or GS-DR-LL, at time
point t1 site i finds that the difference between accurate position
and the estimated one is larger than the threshold and it sends a
state update to site j. At time point t2, the state update is received
by remote site j. Assuming that the timestamp of the state update
is less than t - n, site j uses it to update local scene immediately.
The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 3
With GS-DR, the time period of before inconsistency is (t2 - t1) +
(t1 - t0), whereas it decreases to (t2 - t1 - n) + (t1 - t0) with the
help of GS-DR-LL. Note that t2 - t1 is caused by network
transmission delay and t1 - t0 is caused by the state filtering
mechanism of DR. If n is larger than t2 - t1, GS-DR-LL can
eliminate the before inconsistency caused by network
transmission delay, but it cannot eliminate the before
inconsistency caused by the state filtering mechanism of DR
(unless the threshold is set to 0). In highly interactive games,
which request high consistency and GS-DR-LL might be
employed, the results of operations are quite difficult to be
estimated and a small threshold must be used. Thus, in practice,
most before inconsistency is caused by network transmission
delay and GS-DR-LL has the capability to eliminate such before
inconsistency.
Figure 2. The paths of a shared entity by using GS-DR and
GS-DR-LL.
To GS-DR-LL, the selection of lag value n is very important, and
both network transmission delay and the effects of local lag on
interaction should be considered. According to the results of HCI
related researches, humans cannot perceive the delay imposed on
a system when it is smaller than a specific value, and the specific
value depends on both the system and the task. For example, in a
graphical user interface a delay of approximately 150 ms cannot
be noticed for keyboard interaction and the threshold increases to
195 ms for mouse interaction [13], and a delay of up to 50 ms is
uncritical for a car-racing game [5]. Thus if network transmission
delay is less than the specific value of a game system, n can be set
to the specific value. Else n can be set in terms of the effects of
local lag on the interaction of a system [14]. In the case that a
large n must be used, some HCI methods (e.g. echo [15]) can be
used to relieve the negative effects of the large lag. In the case
that n is larger than the network transmission delay, GS-DR-LL
can eliminate most before inconsistency. Traditional local lag
requests that the lag value must be larger than typical network
transmission delay, otherwise state repairs would flood the system.
However GS-DR-LL allows n to be smaller than typical network
transmission delay. In this case, the before inconsistency caused
by network transmission delay still exists, but it can be decreased.
5. PERFORMANCE EVALUATION
In order to evaluate GS-DR-LL and compare it with GS-DR in a
real application, we had implemented both two methods in a
networked game named spaceship [1]. Spaceship is a very simple
networked computer game, in which players can control their
spaceships to accelerate, decelerate, turn, and shoot spaceships
controlled by remote players with laser beams. If a spaceship is
hit by a laser beam, its life points decrease one. If the life points
of a spaceship decrease to 0, the spaceship is removed from the
game and the player controlling the spaceship loses the game.
In our practical implementation, GS-DR-LL and GS-DR
coexisted in the game system, and the test bed was composed of
two computers connected by 100 M switched Ethernet, with one
computer acted as local site and the other acted as remote site. In
order to simulate network transmission delay, a specific module
was developed to delay all packets transmitted between the two
computers in terms of a predefined delay value.
The main purpose of performance evaluation is to study the
effects of GS-DR-LL on decreasing before inconsistency in a
particular game system under different thresholds, lags, and
network transmission delays. Two different thresholds were used
in the evaluation, one is 10 pixels deviation in position or 15
degrees deviation in orientation, and the other is 4 pixels or 5
degrees. Six different combinations of lag and network
transmission delay were used in the evaluation and they could be
divided into two categories. In one category, the lag was fixed at
300 ms and three different network transmission delays (100 ms,
300 ms, and 500 ms) were used. In the other category, the
network transmission delay was fixed at 800 ms and three
different lags (100 ms, 300 ms, and 500 ms) were used. Therefore
the total number of settings used in the evaluation was 12 (2 × 6).
The procedure of performance evaluation was composed of three
steps. In the first step, two participants were employed to play the
game, and the operation sequences were recorded. Based on the
records, a sub operation sequence, which lasted about one minute
and included different operations (e.g. accelerate, decelerate, and
turn), was selected. In the second step, the physical clocks of the
two computers were synchronized first. Under different settings
and consistency maintenance approaches, the selected sub
operation sequence was played back on one computer, and it
drove the two spaceships, one was local and the other was remote,
to move. Meanwhile, the tracks of the spaceships on the two
computers were recorded separately and they were called as a
track couple. Since there are 12 settings and 2 consistency
maintenance approaches, the total number of recorded track
couples was 24. In the last step, to each track couple, the
inconsistency between them was calculated, and the unit of
inconsistency was pixel. Since the physical clocks of the two
computers were synchronized, the calculation of inconsistency
was quite simple. The inconsistency at a particular time point was
the distance between the positions of the two spaceships at that
time point (i.e. formula (3)).
In order to show the results of inconsistency in a clear way, only
parts of the results, which last about 7 seconds, are used in the
following figures, and the figures show almost the same parts of
the results. Figures 3, 4, and 5 show the results of inconsistency
when the lag is fixed at 300 ms and the network transmission
delays are 100, 300, and 500 ms. It can been seen that
inconsistency does exist, but in most of the time it is 0.
Additionally, inconsistency increases with the network
transmission delay, but decreases with the threshold. Compared
with GS-DR, GS-DR-LL can decrease more inconsistency, and it
eliminates most inconsistency when the network transmission
delay is 100 ms and the threshold is 4 pixels or 5 degrees.
4 The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006
According to the prediction and state filtering mechanisms of DR,
inconsistency cannot be completely eliminated if the threshold is
not 0. With the definitions of before inconsistency and after
inconsistency, it can be indicated that GS-DR and GS-DR-LL
both can eliminate after inconsistency, and GS-DR-LL can
effectively decrease before inconsistency. It can be foreseen that
with proper lag and threshold (e.g. the lag is larger than the
network transmission delay and the threshold is 0), GS-DR-LL
even can eliminate before inconsistency.
0
10
20
30
40
0.0 1.5 3.1 4.6 6.1
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 10 pixels or 15degrees
0
10
20
30
40
0.0 1.5 3.1 4.6 6.1
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 4 pixels or 5degrees
Figure 3. Inconsistency when the network transmission delay is 100 ms and the lag is 300 ms.
0
10
20
30
40
0.0 1.5 3.1 4.6 6.1
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 10 pixels or 15degrees
0
10
20
30
40
0.0 1.5 3.1 4.6 6.1
Time (seconds)
Inconsistency(pixels) GS-DR-LL GS-DR
The threshold is 4 pixels or 5degrees
Figure 4. Inconsistency when the network transmission delay is 300 ms and the lag is 300 ms.
0
10
20
30
40
0.0 1.5 3.1 4.6 6.2
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 10 pixels or 15degrees
0
10
20
30
40
0.0 1.5 3.1 4.6 6.1
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 4 pixels or 5degrees
Figure 5. Inconsistency when the network transmission delay is 500 ms and the lag is 300 ms.
Figures 6, 7, and 8 show the results of inconsistency when the
network transmission delay is fixed at 800 ms and the lag are 100,
300, and 500 ms. It can be seen that with GS-DR-LL before
inconsistency decreases with the lag. In traditional local lag, the
lag must be set to a value larger than typical network transmission
delay, otherwise the state repairs would flood the system. From
the above results it can be seen that there does not exist any
constraint on the selection of the lag, with GS-DR-LL a system
would work fine even if the lag is much smaller than the network
transmission delay.
The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 5
From all above results, it can be indicated that GS-DR and 
GSDR-LL both can eliminate after inconsistency, and GS-DR-LL
can effectively decrease before inconsistency, and the effects
increase with the lag.
0
10
20
30
40
0.0 1.5 3.1 4.7 6.2
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 10 pixels or 15degrees
0
10
20
30
40
0.0 1.5 3.1 4.6 6.1
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 4 pixels or 5degrees
Figure 6. Inconsistency when the network transmission delay is 800 ms and the lag is 100 ms.
0
10
20
30
40
0.0 1.5 3.1 4.6 6.1
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 10 pixels or 15degrees
0
10
20
30
40
0.0 1.5 3.1 4.6 6.1
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 4 pixels or 5degrees
Figure 7. Inconsistency when the network transmission delay is 800 ms and the lag is 300 ms.
0
10
20
30
40
0.0 1.5 3.1 4.6 6.1
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 10 pixels or 15degrees
0
10
20
30
40
0.0 1.5 3.1 4.6 6.1
Time (seconds)
Inconsistency(pixels)
GS-DR-LL GS-DR
The threshold is 4 pixels or 5degrees
Figure 8. Inconsistency when the network transmission delay is 800 ms and the lag is 500 ms.
6. CONCLUSIONS
Compared with traditional DR, GS-DR can eliminate after
inconsistency through the synchronization of physical clocks, but
it cannot tackle before inconsistency, which would significantly
influence the usability and fairness of a game. In this paper, we
proposed a method named GS-DR-LL, which combines local lag
and GS-DR, to decrease before inconsistency through delaying
updating the execution results of local operations to local scene.
Performance evaluation indicates that GS-DR-LL can effectively
decrease before inconsistency, and the effects increase with the
lag.
GS-DR-LL has significant implications to consistency
maintenance approaches. First, GS-DR-LL shows that improved
DR can not only eliminate after inconsistency but also decrease
6 The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006
before inconsistency, with proper lag and threshold, it would even
eliminate before inconsistency. As a result, the application of DR
can be greatly broadened and it could be used in the systems
which request high consistency (e.g. highly interactive games).
Second, GS-DR-LL shows that by combining local lag and 
GSDR, the constraint on selecting lag value is removed and a lag,
which is smaller than typical network transmission delay, could
be used. As a result, the application of local lag can be greatly
broadened and it could be used in the systems which have large
typical network transmission delay (e.g. Internet based games).
7. REFERENCES
[1] Mauve, M., Vogel, J., Hilt, V., and Effelsberg, W. Local-Lag
and Timewarp: Providing Consistency for Replicated
Continuous Applications. IEEE Transactions on Multimedia,
Vol. 6, No.1, 2004, 47-57.
[2] Li, F.W., Li, L.W., and Lau, R.W. Supporting Continuous
Consistency in Multiplayer Online Games. In Proc. of ACM
Multimedia, 2004, 388-391.
[3] Pantel, L. and Wolf, L. On the Suitability of Dead
Reckoning Schemes for Games. In Proc. of NetGames, 2002,
79-84.
[4] Alhalabi, M.O., Horiguchi, S., and Kunifuji, S. An
Experimental Study on the Effects of Network Delay in
Cooperative Shared Haptic Virtual Environment. Computers
and Graphics, Vol. 27, No. 2, 2003, 205-213.
[5] Pantel, L. and Wolf, L.C. On the Impact of Delay on 
RealTime Multiplayer Games. In Proc. of NOSSDAV, 2002, 
2329.
[6] Meehan, M., Razzaque, S., Whitton, M.C., and Brooks, F.P.
Effect of Latency on Presence in Stressful Virtual
Environments. In Proc. of IEEE VR, 2003, 141-148.
[7] Bernier, Y.W. Latency Compensation Methods in
Client/Server In-Game Protocol Design and Optimization. In
Proc. of Game Developers Conference, 2001.
[8] Aggarwal, S., Banavar, H., and Khandelwal, A. Accuracy in
Dead-Reckoning based Distributed Multi-Player Games. In
Proc. of NetGames, 2004, 161-165.
[9] Raynal, M. and Schiper, A. From Causal Consistency to
Sequential Consistency in Shared Memory Systems. In Proc.
of Conference on Foundations of Software Technology and
Theoretical Computer Science, 1995, 180-194.
[10] Ahamad, M., Burns, J.E., Hutto, P.W., and Neiger, G. Causal
Memory. In Proc. of International Workshop on Distributed
Algorithms, 1991, 9-30.
[11] Herlihy, M. and Wing, J. Linearizability: a Correctness
Condition for Concurrent Objects. ACM Transactions on
Programming Languages and Systems, Vol. 12, No. 3, 1990,
463-492.
[12] Misra, J. Axioms for Memory Access in Asynchronous
Hardware Systems. ACM Transactions on Programming
Languages and Systems, Vol. 8, No. 1, 1986, 142-153.
[13] Dabrowski, J.R. and Munson, E.V. Is 100 Milliseconds too
Fast. In Proc. of SIGCHI Conference on Human Factors in
Computing Systems, 2001, 317-318.
[14] Chen, H., Chen, L., and Chen, G.C. Effects of Local-Lag
Mechanism on Cooperation Performance in a Desktop CVE
System. Journal of Computer Science and Technology, Vol.
20, No. 3, 2005, 396-401.
[15] Chen, L., Chen, H., and Chen, G.C. Echo: a Method to
Improve the Interaction Quality of CVEs. In Proc. of IEEE
VR, 2005, 269-270.
The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 7
GUESS: Gossiping Updates for Efficient Spectrum Sensing
Nabeel Ahmed
University of Waterloo
David R. Cheriton School of
Computer Science
n3ahmed@uwaterloo.ca
David Hadaller
University of Waterloo
David R. Cheriton School of
Computer Science
dthadaller@uwaterloo.ca
Srinivasan Keshav
University of Waterloo
David R. Cheriton School of
Computer Science
keshav@uwaterloo.ca
ABSTRACT
Wireless radios of the future will likely be frequency-agile,
that is, supporting opportunistic and adaptive use of the RF
spectrum. Such radios must coordinate with each other to
build an accurate and consistent map of spectral 
utilization in their surroundings. We focus on the problem of
sharing RF spectrum data among a collection of wireless
devices. The inherent requirements of such data and the
time-granularity at which it must be collected makes this
problem both interesting and technically challenging. We
propose GUESS, a novel incremental gossiping approach to
coordinated spectral sensing. It (1) reduces protocol 
overhead by limiting the amount of information exchanged 
between participating nodes, (2) is resilient to network 
alterations, due to node movement or node failures, and (3) 
allows exponentially-fast information convergence. We outline
an initial solution incorporating these ideas and also show
how our approach reduces network overhead by up to a 
factor of 2.4 and results in up to 2.7 times faster information
convergence than alternative approaches.
Categories and Subject Descriptors
C.2.4 [Distributed Systems]: Distributed applications
General Terms
Algorithms, Performance, Experimentation
1. INTRODUCTION
There has recently been a huge surge in the growth of
wireless technology, driven primarily by the availability of
unlicensed spectrum. However, this has come at the cost
of increased RF interference, which has caused the Federal
Communications Commission (FCC) in the United States to
re-evaluate its strategy on spectrum allocation. Currently,
the FCC has licensed RF spectrum to a variety of public and
private institutions, termed primary users. New spectrum
allocation regimes implemented by the FCC use dynamic
spectrum access schemes to either negotiate or 
opportunistically allocate RF spectrum to unlicensed secondary users
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
D1
D2
D5
D3
D4
Primary User
Shadowed
Secondary Users
Secondary Users detect
Primary's Signal
Shadowed
Secondary User
Figure 1: Without cooperation, shadowed users are not
able to detect the presence of the primary user.
that can use it when the primary user is absent. The second
type of allocation scheme is termed opportunistic spectrum
sharing. The FCC has already legislated this access method
for the 5 GHz band and is also considering the same for
TV broadcast bands [1]. As a result, a new wave of 
intelligent radios, termed cognitive radios (or software defined
radios), is emerging that can dynamically re-tune their 
radio parameters based on interactions with their surrounding
environment.
Under the new opportunistic allocation strategy, 
secondary users are obligated not to interfere with primary
users (senders or receivers). This can be done by sensing
the environment to detect the presence of primary users.
However, local sensing is not always adequate, especially in
cases where a secondary user is shadowed from a primary
user, as illustrated in Figure 1. Here, coordination between
secondary users is the only way for shadowed users to 
detect the primary. In general, cooperation improves sensing
accuracy by an order of magnitude when compared to not
cooperating at all [5].
To realize this vision of dynamic spectrum access, two 
fundamental problems must be solved: (1) Efficient and 
coordinated spectrum sensing and (2) Distributed spectrum 
allocation. In this paper, we propose strategies for coordinated
spectrum sensing that are low cost, operate on timescales
comparable to the agility of the RF environment, and are
resilient to network failures and alterations. We defer the
problem of spectrum allocation to future work.
Spectrum sensing techniques for cognitive radio networks
[4, 17] are broadly classified into three regimes; (1) 
centralized coordinated techniques, (2) decentralized coordinated
techniques, and (3) decentralized uncoordinated techniques.
We advocate a decentralized coordinated approach, similar
in spirit to OSPF link-state routing used in the Internet.
This is more effective than uncoordinated approaches 
because making decisions based only on local information is
fallible (as shown in Figure 1). Moreover, compared to 
cen12
tralized approaches, decentralized techniques are more 
scalable, robust, and resistant to network failures and security
attacks (e.g. jamming).
Coordinating sensory data between cognitive radio devices
is technically challenging because accurately assessing 
spectrum usage requires exchanging potentially large amounts of
data with many radios at very short time scales. Data size
grows rapidly due to the large number (i.e. thousands) of
spectrum bands that must be scanned. This data must also
be exchanged between potentially hundreds of neighboring
secondary users at short time scales, to account for rapid
changes in the RF environment.
This paper presents GUESS, a novel approach to 
coordinated spectrum sensing for cognitive radio networks. Our
approach is motivated by the following key observations:
1. Low-cost sensors collect approximate data: Most 
devices have limited sensing resolution because they are
low-cost and low duty-cycle devices and thus cannot
perform complex RF signal processing (e.g. matched
filtering). Many are typically equipped with simple
energy detectors that gather only approximate 
information.
2. Approximate summaries are sufficient for coordination:
Approximate statistical summaries of sensed data are
sufficient for correlating sensed information between
radios, as relative usage information is more 
important than absolute usage data. Thus, exchanging 
exact RF information may not be necessary, and more
importantly, too costly for the purposes of spectrum
sensing.
3. RF spectrum changes incrementally: On most bands,
RF spectrum utilization changes infrequently. 
Moreover, utilization of a specific RF band affects only that
band and not the entire spectrum. Therefore, if the
usage pattern of a particular band changes 
substantially, nodes detecting that change can initiate an 
update protocol to update the information for that band
alone, leaving in place information already collected
for other bands. This allows rapid detection of change
while saving the overhead of exchanging unnecessary
information.
Based on these observations, GUESS makes the following
contributions:
1. A novel approach that applies randomized gossiping 
algorithms to the problem of coordinated spectrum 
sensing. These algorithms are well suited to coordinated
spectrum sensing due to the unique characteristics of
the problem: i.e. radios are power-limited, mobile and
have limited bandwidth to support spectrum sensing
capabilities.
2. An application of in-network aggregation for 
dissemination of spectrum summaries. We argue that 
approximate summaries are adequate for performing accurate
radio parameter tuning.
3. An extension of in-network aggregation and 
randomized gossiping to support incremental maintenance of
spectrum summaries. Compared to standard 
gossiping approaches, incremental techniques can further 
reduce overhead and protocol execution time by 
requiring fewer radio resources.
The rest of the paper is organized as follows. Section 2
motivates the need for a low cost and efficient approach to
coordinated spectrum sensing. Section 3 discusses related
work in the area, while Section 4 provides a background on
in-network aggregation and randomized gossiping. Sections
5 and 6 discuss extensions and protocol details of these 
techniques for coordinated spectrum sensing. Section 7 presents
simulation results showcasing the benefits of GUESS, and
Section 8 presents a discussion and some directions for 
future work.
2. MOTIVATION
To estimate the scale of the problem, In-stat predicts that
the number of WiFi-enabled devices sold annually alone will
grow to 430 million by 2009 [2]. Therefore, it would be 
reasonable to assume that a typical dense urban environment
will contain several thousand cognitive radio devices in range
of each other. As a result, distributed spectrum sensing and
allocation would become both important and fundamental.
Coordinated sensing among secondary radios is essential
due to limited device sensing resolution and physical RF 
effects such as shadowing. Cabric et al. [5] illustrate the gains
from cooperation and show an order of magnitude reduction
in the probability of interference with the primary user when
only a small fraction of secondary users cooperate.
However, such coordination is non-trivial due to: (1) the
limited bandwidth available for coordination, (2) the need to
communicate this information on short timescales, and (3)
the large amount of sensory data that needs to be exchanged.
Limited Bandwidth: Due to restrictions of cost and
power, most devices will likely not have dedicated hardware
for supporting coordination. This implies that both data
and sensory traffic will need to be time-multiplexed onto a
single radio interface. Therefore, any time spent 
communicating sensory information takes away from the device"s
ability to perform its intended function. Thus, any such
coordination must incur minimal network overhead.
Short Timescales: Further compounding the problem
is the need to immediately propagate updated RF sensory
data, in order to allow devices to react to it in a timely 
fashion. This is especially true due to mobility, as rapid changes
of the RF environment can occur due to device and obstacle
movements. Here, fading and multi-path interference 
heavily impact sensing abilities. Signal level can drop to a deep
null with just a λ/4 movement in receiver position (3.7 cm
at 2 GHz), where λ is the wavelength [14]. Coordination
which does not support rapid dissemination of information
will not be able to account for such RF variations.
Large Sensory Data: Because cognitive radios can 
potentially use any part of the RF spectrum, there will be 
numerous channels that they need to scan. Suppose we wish to
compute the average signal energy in each of 100 discretized
frequency bands, and each signal can have up to 128 discrete
energy levels. Exchanging complete sensory information 
between nodes would require 700 bits per transmission (for
100 channels, each requiring seven bits of information). 
Exchanging this information among even a small group of 50
devices each second would require (50 time-steps × 50 
devices × 700 bits per transmission) = 1.67 Mbps of aggregate
network bandwidth.
Contrast this to the use of a randomized gossip protocol to
disseminate such information, and the use of FM bit vectors
to perform in-network aggregation. By applying gossip and
FM aggregation, aggregate bandwidth requirements drop to
(c·logN time-steps × 50 devices × 700 bits per transmission)
= 0.40 Mbps, since 12 time-steps are needed to propagate
the data (with c = 2, for illustrative purpoes1
). This is
explained further in Section 4.
Based on these insights, we propose GUESS, a low-overhead
approach which uses incremental extensions to FM 
aggregation and randomized gossiping for efficient coordination
within a cognitive radio network. As we show in Section 7,
1
Convergence time is correlated with the connectivity topology
of the devices, which in turn depends on the environment.
13
X
A
A
X
B
B
X
Figure 2: Using FM aggregation to compute average signal level measured by a group of devices.
these incremental extensions can further reduce bandwidth
requirements by up to a factor of 2.4 over the standard 
approaches discussed above.
3. RELATED WORK
Research in cognitive radio has increased rapidly [4, 17]
over the years, and it is being projected as one of the leading
enabling technologies for wireless networks of the future [9].
As mentioned earlier, the FCC has already identified new
regimes for spectrum sharing between primary users and
secondary users and a variety of systems have been proposed
in the literature to support such sharing [4, 17].
Detecting the presence of a primary user is non-trivial,
especially a legacy primary user that is not cognitive 
radio aware. Secondary users must be able to detect the 
primary even if they cannot properly decode its signals. This
has been shown by Sahai et al. [16] to be extremely 
difficult even if the modulation scheme is known. Sophisticated
and costly hardware, beyond a simple energy detector, is 
required to improve signal detection accuracy [16]. Moreover,
a shadowed secondary user may not even be able to detect
signals from the primary. As a result, simple local 
sensing approaches have not gained much momentum. This has
motivated the need for cooperation among cognitive radios
[16].
More recently, some researchers have proposed approaches
for radio coordination. Liu et al. [11] consider a centralized
access point (or base station) architecture in which 
sensing information is forwarded to APs for spectrum allocation
purposes. APs direct mobile clients to collect such 
sensing information on their behalf. However, due to the need
of a fixed AP infrastructure, such a centralized approach is
clearly not scalable.
In other work, Zhao et al. [17] propose a distributed 
coordination approach for spectrum sensing and allocation.
Cognitive radios organize into clusters and coordination 
occurs within clusters. The CORVUS [4] architecture proposes
a similar clustering method that can use either a centralized
or decentralized approach to manage clusters. Although an
improvement over purely centralized approaches, these 
techniques still require a setup phase to generate the clusters,
which not only adds additional delay, but also requires many
of the secondary users to be static or quasi-static. In 
contrast, GUESS does not place such restrictions on secondary
users, and can even function in highly mobile environments.
4. BACKGROUND
This section provides the background for our approach.
We present the FM aggregation scheme that we use to 
generate spectrum summaries and perform in-network 
aggregation. We also discuss randomized gossiping techniques for
disseminating aggregates in a cognitive radio network.
4.1 FM Aggregation
Aggregation is the process where nodes in a distributed
network combine data received from neighboring nodes with
their local value to generate a combined aggregate. This 
aggregate is then communicated to other nodes in the 
network and this process repeats until the aggregate at all
nodes has converged to the same value, i.e. the global 
aggregate. Double-counting is a well known problem in this
process, where nodes may contribute more than once to the
aggregate, causing inaccuracy in the final result. Intuitively,
nodes can tag the aggregate value they transmit with 
information about which nodes have contributed to it. However,
this approach is not scalable. Order and Duplicate 
Insensitive (ODI) techniques have been proposed in the literature
[10, 15]. We adopt the ODI approach pioneered by Flajolet
and Martin (FM) for the purposes of aggregation. Next we
outline the FM approach; for full details, see [7].
Suppose we want to compute the number of nodes in the
network, i.e. the COUNT query. To do so, each node 
performs a coin toss experiment as follows: toss an unbiased
coin, stopping after the first head is seen. The node then
sets the ith bit in a bit vector (initially filled with zeros),
where i is the number of coin tosses it performed. The 
intuition is that as the number of nodes doing coin toss 
experiments increases, the probability of a more significant bit
being set in one of the nodes" bit vectors increases.
These bit vectors are then exchanged among nodes. When
a node receives a bit vector, it updates its local bit vector
by bitwise OR-ing it with the received vector (as shown in
Figure 2 which computes AVERAGE). At the end of the
aggregation process, every node, with high probability, has
the same bit vector. The actual value of the count aggregate
is then computed using the following formula, AGGF M =
2j−1
/0.77351, where j represents the bit position of the least
significant zero in the aggregate bit vector [7].
Although such aggregates are very compact in nature, 
requiring only O(logN) state space (where N is the number
of nodes), they may not be very accurate as they can only
approximate values to the closest power of 2, potentially
causing errors of up to 50%. More accurate aggregates can
be computed by maintaining multiple bit vectors at each
node, as explained in [7]. This decreases the error to within
O(1/
√
m), where m is the number of such bit vectors.
Queries other than count can also be computed using 
variants of this basic counting algorithm, as discussed in [3] (and
shown in Figure 2). Transmitting FM bit vectors between
nodes is done using randomized gossiping, discussed next.
4.2 Gossip Protocols
Gossip-based protocols operate in discrete time-steps; a
time-step is the required amount of time for all 
transmissions in that time-step to complete. At every time-step, each
node having something to send randomly selects one or more
neighboring nodes and transmits its data to them. The 
randomized propagation of information provides fault-tolerance
and resilience to network failures and outages. We 
emphasize that this characteristic of the protocol also allows it to
operate without relying on any underlying network
structure. Gossip protocols have been shown to provide
exponentially fast convergence2
, on the order of O(log N)
[10], where N is the number of nodes (or radios). These
protocols can therefore easily scale to very dense 
environments.
2
Convergence refers to the state in which all nodes have the most
up-to-date view of the network.
14
Two types of gossip protocols are:
• Uniform Gossip: In uniform gossip, at each 
timestep, each node chooses a random neighbor and sends
its data to it. This process repeats for O(log(N)) steps
(where N is the number of nodes in the network). 
Uniform gossip provides exponentially fast convergence,
with low network overhead [10].
• Random Walk: In random walk, only a subset of
the nodes (termed designated nodes) communicate in a
particular time-step. At startup, k nodes are randomly
elected as designated nodes. In each time-step, each
designated node sends its data to a random neighbor,
which becomes designated for the subsequent 
timestep (much like passing a token). This process repeats
until the aggregate has converged in the network. 
Random walk has been shown to provide similar 
convergence bounds as uniform gossip in problems of similar
context [8, 12].
5. INCREMENTAL PROTOCOLS
5.1 Incremental FM Aggregates
One limitation of FM aggregation is that it does not 
support updates. Due to the probabilistic nature of FM, once
bit vectors have been ORed together, information cannot
simply be removed from them as each node"s contribution
has not been recorded. We propose the use of delete vectors,
an extension of FM to support updates. We maintain a 
separate aggregate delete vector whose value is subtracted from
the original aggregate vector"s value to obtain the resulting
value as follows.
AGGINC = (2a−1
/0.77351) − (2b−1
/0.77351) (1)
Here, a and b represent the bit positions of the least 
significant zero in the original and delete bit vectors respectively.
Suppose we wish to compute the average signal level 
detected in a particular frequency. To compute this, we 
compute the SUM of all signal level measurements and divide
that by the COUNT of the number of measurements. A
SUM aggregate is computed similar to COUNT (explained
in Section 4.1), except that each node performs s coin toss
experiments, where s is the locally measured signal level.
Figure 2 illustrates the sequence by which the average signal
energy is computed in a particular band using FM 
aggregation.
Now suppose that the measured signal at a node changes
from s to s . The vectors are updated as follows.
• s > s: We simply perform (s − s) more coin toss 
experiments and bitwise OR the result with the original
bit vector.
• s < s: We increase the value of the delete vector by
performing (s − s ) coin toss experiments and bitwise
OR the result with the current delete vector.
Using delete vectors, we can now support updates to the
measured signal level. With the original implementation of
FM, the aggregate would need to be discarded and a new one
recomputed every time an update occurred. Thus, delete
vectors provide a low overhead alternative for applications
whose data changes incrementally, such as signal level 
measurements in a coordinated spectrum sensing environment.
Next we discuss how these aggregates can be communicated
between devices using incremental routing protocols.
5.2 Incremental Routing Protocol
We use the following incremental variants of the routing
protocols presented in Section 4.2 to support incremental
updates to previously computed aggregates.
Update Received OR
Local Update Occurs
Recovered
Susceptible
Time-stamp Expires
Initial State
Additional
Update
Received
Infectious
Clean Up
Figure 3: State diagram each device passes through as
updates proceed in the system
• Incremental Gossip Protocol (IGP): When an 
update occurs, the updated node initiates the gossiping
procedure. Other nodes only begin gossiping once they
receive the update. Therefore, nodes receiving the 
update become active and continue communicating with
their neighbors until the update protocol terminates,
after O(log(N)) time steps.
• Incremental Random Walk Protocol (IRWP):
When an update (or updates) occur in the system, 
instead of starting random walks at k random nodes in
the network, all k random walks are initiated from the
updated node(s). The rest of the protocol proceeds in
the same fashion as the standard random walk 
protocol. The allocation of walks to updates is discussed
in more detail in [3], where the authors show that the
number of walks has an almost negligible impact on
network overhead.
6. PROTOCOL DETAILS
Using incremental routing protocols to disseminate 
incremental FM aggregates is a natural fit for the problem of
coordinated spectrum sensing. Here we outline the 
implementation of such techniques for a cognitive radio network.
We continue with the example from Section 5.1, where we
wish to perform coordination between a group of wireless
devices to compute the average signal level in a particular
frequency band.
Using either incremental random walk or incremental 
gossip, each device proceeds through three phases, in order to
determine the global average signal level for a particular
frequency band. Figure 3 shows a state diagram of these
phases.
Susceptible: Each device starts in the susceptible state
and becomes infectious only when its locally measured signal
level changes, or if it receives an update message from a
neighboring device. If a local change is observed, the device
updates either the original or delete bit vector, as described
in Section 5.1, and moves into the infectious state. If it
receives an update message, it ORs the received original
and delete bit vectors with its local bit vectors and moves
into the infectious state.
Note, because signal level measurements may change 
sporadically over time, a smoothing function, such as an 
exponentially weighted moving average, should be applied to
these measurements.
Infectious: Once a device is infectious it continues to
send its up-to-date bit vectors, using either incremental 
random walk or incremental gossip, to neighboring nodes. Due
to FM"s order and duplicate insensitive (ODI) properties, 
simultaneously occurring updates are handled seamlessly by
the protocol.
Update messages contain a time stamp indicating when
the update was generated, and each device maintains a 
lo15
0
200
400
600
800
1000
1 10 100
Number of Measured Signal Changes
Executiontime(ms)
Incremental Gossip Uniform Gossip
(a) Incremental Gossip and Uniform 
Gossip on Clique
0
200
400
600
800
1000
1 10 100
Number of Measured Signal Changes
ExecutionTime(ms).
Incremental Random Walk Random Walk
(b) Incremental Random Walk and 
Random Walk on Clique
0
400
800
1200
1600
2000
1 10 100
Number of Measured Signal Changes
ExecutionTime(ms).
Random Walk Incremental Random Walk
(c) Incremental Random Walk and 
Random Walk on Power-Law Random Graph
Figure 4: Execution times of Incremental Protocols
0.9
1.4
1.9
2.4
2.9
1 10 100
Number of Measured Signal Changes
OverheadImprovementRatio.
(NormalizedtoUniformGossip)
Incremental Gossip Uniform Gossip
(a) Incremental Gossip and Uniform 
Gossip on Clique
0.9
1.4
1.9
2.4
2.9
1 10 100
Number of Measured Signal Changes
OverheadImprovementRatio.
(NormalizedtoRandomWalk) Incremental Random Walk Random Walk
(b) Incremental Random Walk and 
Random Walk on Clique
0.9
1.1
1.3
1.5
1.7
1.9
1 10 100
Number of Measured Signal Changes
OverheadImprovementRatio.
(NormalizedtoRandomWalk)
Random Walk Incremental Random Walk
(c) Incremental Random Walk and 
Random Walk on Power-Law Random Graph
Figure 5: Network overhead of Incremental Protocols
cal time stamp of when it received the most recent update.
Using this information, a device moves into the recovered
state once enough time has passed for the most recent 
update to have converged. As discussed in Section 4.2, this
happens after O(log(N)) time steps.
Recovered: A recovered device ceases to propagate any
update information. At this point, it performs clean-up and
prepares for the next infection by entering the susceptible
state. Once all devices have entered the recovered state, the
system will have converged, and with high probability, all
devices will have the up-to-date average signal level. Due
to the cumulative nature of FM, even if all devices have not
converged, the next update will include all previous updates.
Nevertheless, the probability that gossip fails to converge is
small, and has been shown to be O(1/N) [10].
For coordinated spectrum sensing, non-incremental 
routing protocols can be implemented in a similar fashion. 
Random walk would operate by having devices periodically drop
the aggregate and re-run the protocol. Each device would
perform a coin toss (biased on the number of walks) to 
determine whether or not it is a designated node. This is
different from the protocol discussed above where only 
updated nodes initiate random walks. Similar techniques can
be used to implement standard gossip.
7. EVALUATION
We now provide a preliminary evaluation of GUESS in
simulation. A more detailed evaluation of this approach can
be found in [3]. Here we focus on how incremental 
extensions to gossip protocols can lead to further improvements
over standard gossiping techniques, for the problem of 
coordinated spectrum sensing.
Simulation Setup: We implemented a custom 
simulator in C++. We study the improvements of our 
incremental gossip protocols over standard gossiping in two 
dimensions: execution time and network overhead. We use two
topologies to represent device connectivity: a clique, to 
eliminate the effects of the underlying topology on protocol 
performance, and a BRITE-generated [13] power-law random
graph (PLRG), to illustrate how our results extend to more
realistic scenarios. We simulate a large deployment of 1,000
devices to analyze protocol scalability.
In our simulations, we compute the average signal level in
a particular band by disseminating FM bit vectors. In each
run of the simulation, we induce a change in the measured
signal at one or more devices. A run ends when the new
average signal level has converged in the network.
For each data point, we ran 100 simulations and 95% 
confidence intervals (error bars) are shown.
Simulation Parameters: Each transmission involves
sending 70 bits of information to a neighboring node. To
compute the AVERAGE aggregate, four bit vectors need to
be transmitted: the original SUM vector, the SUM delete
vector, the original COUNT vector, and the COUNT delete
vector. Non-incremental protocols do not transmit the delete
vectors. Each transmission also includes a time stamp of
when the update was generated.
We assume nodes communicate on a common control 
channel at 2 Mbps. Therefore, one time-step of protocol 
execution corresponds to the time required for 1,000 nodes to
sequentially send 70 bits at 2 Mbps. Sequential use of the
control channel is a worst case for our protocols; in practice,
multiple control channels could be used in parallel to reduce
execution time. We also assume nodes are loosely time 
synchronized, the implications of which are discussed further in
[3]. Finally, in order to isolate the effect of protocol 
operation on performance, we do not model the complexities of
the wireless channel in our simulations.
Incremental Protocols Reduce Execution Time:
Figure 4(a) compares the performance of incremental gossip
(IGP) with uniform gossip on a clique topology. We observe
that both protocols have almost identical execution times.
This is expected as IGP operates in a similar fashion to
16
uniform gossip, taking O(log(N)) time-steps to converge.
Figure 4(b) compares the execution times of 
incremental random walk (IRWP) and standard random walk on a
clique. IRWP reduces execution time by a factor of 2.7 for a
small number of measured signal changes. Although random
walk and IRWP both use k random walks (in our simulations
k = number of nodes), IRWP initiates walks only from 
updated nodes (as explained in Section 5.2), resulting in faster
information convergence. These improvements carry over to
a PLRG topology as well (as shown in Figure 4(c)), where
IRWP is 1.33 times faster than random walk.
Incremental Protocols Reduce Network Overhead:
Figure 5(a) shows the ratio of data transmitted using 
uniform gossip relative to incremental gossip on a clique. For
a small number of signal changes, incremental gossip incurs
2.4 times less overhead than uniform gossip. This is because
in the early steps of protocol execution, only devices which
detect signal changes communicate. As more signal changes
are introduced into the system, gossip and incremental 
gossip incur approximately the same overhead.
Similarly, incremental random walk (IRWP) incurs much
less overhead than standard random walk. Figure 5(b) shows
a 2.7 fold reduction in overhead for small numbers of 
signal changes on a clique. Although each protocol uses the
same number of random walks, IRWP uses fewer network
resources than random walk because it takes less time to
converge. This improvement also holds true on more 
complex PLRG topologies (as shown in Figure 5(c)), where we
observe a 33% reduction in network overhead.
From these results it is clear that incremental techniques
yield significant improvements over standard approaches to
gossip, even on complex topologies. Because spectrum 
utilization is characterized by incremental changes to usage,
incremental protocols are ideally suited to solve this 
problem in an efficient and cost effective manner.
8. DISCUSSION AND FUTURE WORK
We have only just scratched the surface in addressing the
problem of coordinated spectrum sensing using incremental
gossiping. Next, we outline some open areas of research.
Spatial Decay: Devices performing coordinated sensing
are primarily interested in the spectrum usage of their local
neighborhood. Therefore, we recommend the use of 
spatially decaying aggregates [6], which limits the impact of an
update on more distant nodes. Spatially decaying 
aggregates work by successively reducing (by means of a decay
function) the value of the update as it propagates further
from its origin. One challenge with this approach is that
propagation distance cannot be determined ahead of time
and more importantly, exhibits spatio-temporal variations.
Therefore, finding the optimal decay function is non-trivial,
and an interesting subject of future work.
Significance Threshold: RF spectrum bands 
continually experience small-scale changes which may not 
necessarily be significant. Deciding if a change is significant can be
done using a significance threshold β, below which any 
observed change is not propagated by the node. Choosing an
appropriate operating value for β is application dependent,
and explored further in [3].
Weighted Readings: Although we argued that most
devices will likely be equipped with low-cost sensing 
equipment, there may be situations where there are some special
infrastructure nodes that have better sensing abilities than
others. Weighting their measurements more heavily could
be used to maintain a higher degree of accuracy. 
Determining how to assign such weights is an open area of research.
Implementation Specifics: Finally, implementing 
gossip for coordinated spectrum sensing is also open. If 
implemented at the MAC layer, it may be feasible to piggy-back
gossip messages over existing management frames (e.g. 
networking advertisement messages). As well, we also require
the use of a control channel to disseminate sensing 
information. There are a variety of alternatives for 
implementing such a channel, some of which are outlined in [4]. The
trade-offs of different approaches to implementing GUESS
is a subject of future work.
9. CONCLUSION
Spectrum sensing is a key requirement for dynamic 
spectrum allocation in cognitive radio networks. The nature of
the RF environment necessitates coordination between 
cognitive radio devices. We propose GUESS, an approximate
yet low overhead approach to perform efficient coordination
between cognitive radios. The fundamental contributions of
GUESS are: (1) an FM aggregation scheme for efficient 
innetwork aggregation, (2) a randomized gossiping approach
which provides exponentially fast convergence and 
robustness to network alterations, and (3) incremental variations
of FM and gossip which we show can reduce the 
communication time by up to a factor of 2.7 and reduce network
overhead by up to a factor of 2.4. Our preliminary 
simulation results showcase the benefits of this approach and we
also outline a set of open problems that make this a new
and exciting area of research.
10. REFERENCES
[1] Unlicensed Operation in the TV Broadcast Bands and
Additional Spectrum for Unlicensed Devices Below 900 MHz in
the 3 GHz band, May 2004. Notice of Proposed Rule-Making
04-186, Federal Communications Commission.
[2] In-Stat: Covering the Full Spectrum of Digital Communications
Market Research, from Vendor to End-user, December 2005.
http://www.in-stat.com/catalog/scatalogue.asp?id=28.
[3] N. Ahmed, D. Hadaller, and S. Keshav. Incremental
Maintenance of Global Aggregates. UW. Technical Report
CS-2006-19, University of Waterloo, ON, Canada, 2006.
[4] R. W. Brodersen, A. Wolisz, D. Cabric, S. M. Mishra, and
D. Willkomm. CORVUS: A Cognitive Radio Approach for
Usage of Virtual Unlicensed Spectrum. Technical report, July
2004.
[5] D. Cabric, S. M. Mishra, and R. W. Brodersen. Implementation
Issues in Spectrum Sensing for Cognitive Radios. In Asilomar
Conference, 2004.
[6] E. Cohen and H. Kaplan. Spatially-Decaying Aggregation Over
a Network: Model and Algorithms. In Proceedings of SIGMOD
2004, pages 707-718, New York, NY, USA, 2004. ACM Press.
[7] P. Flajolet and G. N. Martin. Probabilistic Counting
Algorithms for Data Base Applications. J. Comput. Syst. Sci.,
31(2):182-209, 1985.
[8] C. Gkantsidis, M. Mihail, and A. Saberi. Random Walks in
Peer-to-Peer Networks. In Proceedings of INFOCOM 2004,
pages 1229-1240, 2004.
[9] E. Griffith. Previewing Intel"s Cognitive Radio Chip, June 2005.
http://www.internetnews.com/wireless/article.php/3513721.
[10] D. Kempe, A. Dobra, and J. Gehrke. Gossip-Based
Computation of Aggregate Information. In FOCS 2003, page
482, Washington, DC, USA, 2003. IEEE Computer Society.
[11] X. Liu and S. Shankar. Sensing-based Opportunistic Channel
Access. In ACM Mobile Networks and Applications
(MONET) Journal, March 2005.
[12] Q. Lv, P. Cao, E. Cohen, K. Li, and S. Shenker. Search and
Replication in Unstructured Peer-to-Peer Networks. In
Proceedings of ICS, 2002.
[13] A. Medina, A. Lakhina, I. Matta, and J. Byers. BRITE: an
Approach to Universal Topology Generation. In Proceedings of
MASCOTS conference, Aug. 2001.
[14] S. M. Mishra, A. Sahai, and R. W. Brodersen. Cooperative
Sensing among Cognitive Radios. In ICC 2006, June 2006.
[15] S. Nath, P. B. Gibbons, S. Seshan, and Z. R. Anderson.
Synopsis Diffusion for Robust Aggregation in Sensor Networks.
In Proceedings of SenSys 2004, pages 250-262, 2004.
[16] A. Sahai, N. Hoven, S. M. Mishra, and R. Tandra. Fundamental
Tradeoffs in Robust Spectrum Sensing for Opportunistic
Frequency Reuse. Technical Report UC Berkeley, 2006.
[17] J. Zhao, H. Zheng, and G.-H. Yang. Distributed Coordination
in Dynamic Spectrum Allocation Networks. In Proceedings of
DySPAN 2005, Baltimore (MD), Nov. 2005.
17
Heuristics-Based Scheduling of
Composite Web Service Workloads
Thomas Phan Wen-Syan Li
IBM Almaden Research Center
650 Harry Rd.
San Jose, CA 95120
{phantom,wsl}@us.ibm.com
ABSTRACT
Web services can be aggregated to create composite workflows that
provide streamlined functionality for human users or other systems.
Although industry standards and recent research have sought to 
define best practices and to improve end-to-end workflow 
composition, one area that has not fully been explored is the scheduling
of a workflow"s web service requests to actual service 
provisioning in a multi-tiered, multi-organisation environment. This issue
is relevant to modern business scenarios where business processes
within a workflow must complete within QoS-defined limits. 
Because these business processes are web service consumers, service
requests must be mapped and scheduled across multiple web 
service providers, each with its own negotiated service level 
agreement. In this paper we provide heuristics for scheduling service
requests from multiple business process workflows to web service
providers such that a business value metric across all workflows is
maximised. We show that a genetic search algorithm is appropriate
to perform this scheduling, and through experimentation we show
that our algorithm scales well up to a thousand workflows and 
produces better mappings than traditional approaches.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed 
Systems-distributed applications; D.2.8 [Software Engineering]:
Metrics-complexity measures, performance measures
1. INTRODUCTION
Web services can be composed into workflows to provide 
streamlined end-to-end functionality for human users or other systems.
Although previous research efforts have looked at ways to 
intelligently automate the composition of web services into workflows
(e.g. [1, 9]), an important remaining problem is the assignment of
web service requests to the underlying web service providers in a
multi-tiered runtime scenario within constraints. In this paper we
address this scheduling problem and examine means to manage a
large number of business process workflows in a scalable manner.
The problem of scheduling web service requests to providers is
relevant to modern business domains that depend on multi-tiered
service provisioning. Consider the example shown in Figure 1
that illustrates our problem space. Workflows comprise multiple
related business processes that are web service consumers; here we
assume that the workflows represent requested service from 
customers or automated systems and that the workflow has already
been composed with an existing choreography toolkit. These 
workflows are then submitted to a portal (not shown) that acts as a
scheduling agent between the web service consumers and the web
service providers.
In this example, a workflow could represent the actions needed to
instantiate a vacation itinerary, where one business process requests
booking an airline ticket, another business process requests a hotel
room, and so forth. Each of these requests target a particular service
type (e.g. airline reservations, hotel reservations, car reservations,
etc.), and for each service type, there are multiple instances of 
service providers that publish a web service interface. An important
challenge is that the workflows must meet some quality-of-service
(QoS) metric, such as end-to-end completion time of all its business
processes, and that meeting or failing this goal results in the 
assignment of a quantitative business value metric for the workflow; 
intuitively, it is desired that all workflows meet their respective QoS
goals. We further leverage the notion that QoS service agreements
are generally agreed-upon between the web service providers and
the scheduling agent such that the providers advertise some level
of guaranteed QoS to the scheduler based upon runtime conditions
such as turnaround time and maximum available concurrency. The
resulting problem is then to schedule and assign the business 
processes" requests for service types to one of the service providers
for that type. The scheduling must be done such that the aggregate
business value across all the workflows is maximised.
In Section 3 we state the scenario as a combinatorial problem and
utilise a genetic search algorithm [5] to find the best assignment
of web service requests to providers. This approach converges 
towards an assignment that maximises the overall business value for
all the workflows.
In Section 4 we show through experimentation that this search
heuristic finds better assignments than other algorithms (greedy,
round-robin, and proportional). Further, this approach allows us to
scale the number of simultaneous workflows (up to one thousand
workflows in our experiments) and yet still find effective schedules.
2. RELATED WORK
In the context of service assignment and scheduling, [11] maps
web service calls to potential servers using linear programming, but
their work is concerned with mapping only single workflows; our
principal focus is on scalably scheduling multiple workflows (up
30
Service Type
SuperHotels.com
Business
Process
Business
Process
Workflow
...
Business
Process
Business
Process
...
HostileHostels.com
IncredibleInns.com
Business
Process
Business
Process
Business
Process
...
Business
Process
Service
Provider
SkyHighAirlines.com
SuperCrazyFlights.com
Business
Process
.
.
.
.
.
.
Advertised QoS
Service Agreement
CarRentalService.com
Figure 1: An example scenario demonstrating the interaction between business processes in workflows and web service providers.
Each business process accesses a service type and is then mapped to a service provider for that type.
to one thousand as we show later) using different business 
metrics and a search heuristic. [10] presents a dynamic 
provisioning approach that uses both predictive and reactive techniques for
multi-tiered Internet application delivery. However, the 
provisioning techniques do not consider the challenges faced when there are
alternative query execution plans and replicated data sources. [8]
presents a feedback-based scheduling mechanism for multi-tiered
systems with back-end databases, but unlike our work, it assumes
a tighter coupling between the various components of the system.
Our work also builds upon prior scheduling research. The classic
job-shop scheduling problem, shown to be NP-complete [4] [3], is
similar to ours in that tasks within a job must be scheduled onto 
machinery (c.f. our scenario is that business processes within a 
workflow must be scheduled onto web service providers). The salient
differences are that the machines can process only one job at a time
(we assume servers can multi-task but with degraded performance
and a maximum concurrency level), tasks within a job cannot 
simultaneously run on different machines (we assume business 
processes can be assigned to any available server), and the principal
metric of performance is the makespan, which is the time for the
last task among all the jobs to complete (and as we show later, 
optimising on the makespan is insufficient for scheduling the business
processes, necessitating different metrics).
3. DESIGN
In this section we describe our model and discuss how we can
find scheduling assignments using a genetic search algorithm.
3.1 Model
We base our model on the simplified scenario shown in Figure
1. Specifically, we assume that users or automated systems request
the execution of a workflow. The workflows comprise business 
processes, each of which makes one web service invocation to a 
service type. Further, business processes have an ordering in the 
workflow. The arrangement and execution of the business processes and
the data flow between them are all managed by a composition or
choreography tool (e.g. [1, 9]). Although composition languages
can use sophisticated flow-control mechanisms such as conditional
branches, for simplicity we assume the processes execute 
sequentially in a given order.
This scenario can be naturally extended to more complex 
relationships that can be expressed in BPEL [7], which defines how
business processes interact, messages are exchanged, activities are
ordered, and exceptions are handled. Due to space constraints,
we focus on the problem space presented here and will extend our
model to more advanced deployment scenarios in the future.
Each workflow has a QoS requirement to complete within a 
specified number of time units (e.g. on the order of seconds, as 
detailed in the Experiments section). Upon completion (or failure),
the workflow is assigned a business value. We extended this 
approach further and considered different types of workflow 
completion in order to model differentiated QoS levels that can be applied
by businesses (for example, to provide tiered customer service).
We say that a workflow is successful if it completes within its QoS
requirement, acceptable if it completes within a constant factor κ
31
of its QoS bound (in our experiments we chose κ=3), or failing
if it finishes beyond κ times its QoS bound. For each category,
a business value score is assigned to the workflow, with the 
successful category assigned the highest positive score, followed by
acceptable and then failing. The business value point 
distribution is non-uniform across workflows, further modelling cases
where some workflows are of higher priority than others.
Each service type is implemented by a number of different 
service providers. We assume that the providers make service level
agreements (SLAs) to guarantee a level of performance defined by
the completion time for completing a web service invocation. 
Although SLAs can be complex, in this paper we assume for 
simplicity that the guarantees can take the form of a linear performance
degradation under load. This guarantee is defined by several 
parameters: α is the expected completion time (for example, on the
order of seconds) if the assigned workload of web service requests
is less than or equal to β, the maximum concurrency, and if the
workload is higher than β, the expected completion for a workload
of size ω is α+ γ(ω − β) where γ is a fractional coefficient. In our
experiments we vary α, β, and γ with different distributions.
Ideally, all workflows would be able to finish within their QoS
limits and thus maximise the aggregate business value across all
workflows. However, because we model service providers with
degrading performance under load, not all workflows will achieve
their QoS limit: it may easily be the case that business processes
are assigned to providers who are overloaded and cannot complete
within the respective workflow"s QoS limit. The key research 
problem, then, is to assign the business processes to the web service
providers with the goal of optimising on the aggregate business
value of all workflows.
Given that the scope of the optimisation is the entire set of 
workflows, it may be that the best scheduling assignments may result in
some workflows having to fail in order for more workflows to 
succeed. This intuitive observation suggests that traditional scheduling
approaches such as round-robin or proportional assignments will
not fare well, which is what we observe and discuss in Section 4.
On the other hand, an exhaustive search of all the possible 
assignments will find the best schedule, but the computational complexity
is prohibitively high. Suppose there are W workflows with an 
average of B business processes per workflow. Further, in the worst
case each business process requests one service type, for which
there are P providers. There are thus W · PB
combinations to
explore to find the optimal assignments of business processes to
providers. Even for small configurations (e.g. W =10, B=5, P=10),
the computational time for exhaustive search is significant, and in
our work we look to scale these parameters. In the next subsection,
discuss how a genetic search algorithm can be used to converge
toward the optimum scheduling assignments.
3.2 Genetic algorithm
Given an exponential search space of business process 
assignments to web service providers, the problem is to find the optimal
assignment that produces the overall highest aggregate business
value across all workflows. To explore the solution space, we use
a genetic algorithm (GA) search heuristic that simulates Darwinian
natural selection by having members of a population compete to
survive in order to pass their genetic chromosomes onto the next
generation; after successive generations, there is a tendency for the
chromosomes to converge toward the best combination [5] [6].
Although other search heuristics exist that can solve 
optimization problems (e.g. simulated annealing or steepest-ascent 
hillclimbing), the business process scheduling problem fits well with a
GA because potential solutions can be represented in a matrix form
and allows us to use prior research in effective GA chromosome
recombination to form new members of the population (e.g. [2]).
0 1 2 3 4
0 1 2 0 2 1
1 0 1 0 1 0
2 1 2 0 0 1
Figure 2: An example chromosome representing a scheduling
assignment of (workflow,service type) → service provider. Each
row represents a workflow, and each column represents a 
service type. For example, here there are 3 workflows (0 to 2) and
5 service types (0 to 4). In workflow 0, any request for service
type 3 goes to provider 2. Note that the service provider 
identifier is within a range limited to its service type (i.e. its column),
so the 2 listed for service type 3 is a different server from
server 2 in other columns.
Chromosome representation of a solution. In Figure 2 we
show an example chromosome that encodes one scheduling 
assignment. The representation is a 2-dimensional matrix that maps
{workflow, service type} to a service provider. For a business 
process in workflow i and utilising service type j, the (i, j)th
entry in
the table is the identifier for the service provider to which the 
business process is assigned. Note that the service provider identifier is
within a range limited to its service type.
GA execution. A GA proceeds as follows. Initially a random
set of chromosomes is created for the population. The 
chromosomes are evaluated (hashed) to some metric, and the best ones
are chosen to be parents. In our problem, the evaluation produces
the net business value across all workflows after executing all 
business processes once they are assigned to their respective service
providers according to the mapping in the chromosome. The 
parents recombine to produce children, simulating sexual crossover,
and occasionally a mutation may arise which produces new 
characteristics that were not available in either parent. The principal idea
is that we would like the children to be different from the parents
(in order to explore more of the solution space) yet not too 
different (in order to contain the portions of the chromosome that result
in good scheduling assignments). Note that finding the global 
optimum is not guaranteed because the recombination and mutation
are stochastic.
GA recombination and mutation. As mentioned, the 
chromosomes are 2-dimensional matrices that represent scheduling 
assignments. To simulate sexual recombination of two chromosomes to
produce a new child chromosome, we applied a one-point crossover
scheme twice (once along each dimension). The crossover is best
explained by analogy to Cartesian space as follows. A random
point is chosen in the matrix to be coordinate (0, 0). Matrix 
elements from quadrants II and IV from the first parent and elements
from quadrants I and III from the second parent are used to create
the new child. This approach follows GA best practices by keeping
contiguous chromosome segments together as they are transmitted
from parent to child.
The uni-chromosome mutation scheme randomly changes one
of the service provider assignments to another provider within the
available range. Other recombination and mutation schemes are an
area of research in the GA community, and we look to explore new
operators in future work.
GA evaluation function. An important GA component is the
evaluation function. Given a particular chromosome representing
one scheduling mapping, the function deterministically calculates
the net business value across all workloads. The business 
processes in each workload are assigned to service providers, and each
provider"s completion time is calculated based on the service 
agreement guarantee using the parameters mentioned in Section 3.1,
namely the unloaded completion time α, the maximum 
concur32
rency β, and a coefficient γ that controls the linear performance
degradation under heavy load. Note that the evaluation function
can be easily replaced if desired; for example, other evaluation
functions can model different service provider guarantees or 
parallel workflows.
4. EXPERIMENTS AND RESULTS
In this section we show the benefit of using our GA-based 
scheduler. Because we wanted to scale the scenarios up to a large number
of workflows (up to 1000 in our experiments), we implemented a
simulation program that allowed us to vary parameters and to 
measure the results with different metrics. The simulator was written
in standard C++ and was run on a Linux (Fedora Core) desktop
computer running at 2.8 GHz with 1GB of RAM.
We compared our algorithm against alternative candidates:
• A well-known round-robin algorithm that assigns each 
business process in circular fashion to the service providers for a
particular service type. This approach provides the simplest
scheme for load-balancing.
• A random-proportional algorithm that proportionally assigns
business processes to the service providers; that is, for a
given service type, the service providers are ranked by their
guaranteed completion time, and business processes are 
assigned proportionally to the providers based on their 
completion time. (We also tried a proportionality scheme based
on both the completion times and maximum concurrency but
attained the same results, so only the former scheme"s results
are shown here.)
• A strawman greedy algorithm that always assigns business
processes to the service provider that has the fastest 
guaranteed completion time. This algorithm represents a naive 
approach based on greedy, local observations of each workflow
without taking into consideration all workflows.
In the experiments that follow, all results were averaged across
20 trials, and to help normalise the effects of randomisation used
during the GA, each trial started by reading in pre-initialised data
from disk. In Table 1 we list our experimental parameters.
In Figure 3 we show the results of running our GA against the
three candidate alternatives. The x-axis shows the number for 
workflows scaled up to 1000, and the y-axis shows the aggregate 
business value for all workflows. As can be seen, the GA consistently
produces the highest business value even as the number of 
workflows grows; at 1000 workflows, the GA produces a 115% 
improvement over the next-best alternative. (Note that although we
are optimising against the business value metric we defined earlier,
genetic algorithms are able to converge towards the optimal value
of any metric, as long as the evaluation function can consistently
measure a chromosome"s value with that metric.)
As expected, the greedy algorithm performs very poorly because
it does the worst job at balancing load: all business processes for
a given service type are assigned to only one server (the one 
advertised to have the fastest completion time), and as more 
business processes arrive, the provider"s performance degrades linearly.
The round-robin scheme is initially outperformed by the 
randomproportional scheme up to around 120 workflows (as shown in the
magnified graph of Figure 4), but as the number of workflows 
increases, the round-robin scheme consistently wins over 
randomproportional. The reason is that although the random-proportional
scheme assigns business processes to providers proportionally 
according to the advertised completion times (which is a measure of
the power of the service provider), even the best providers will
eventually reach a real-world maximum concurrency for the large
-2000
-1000
0
1000
2000
3000
4000
5000
6000
7000
0 200 400 600 800 1000
Aggregatebusinessvalueacrossallworkflows
Total number of workflows
Business value scores of scheduling algorithms
Genetic algorithm
Round robin
Random proportional
Greedy
Figure 3: Net business value scores of different scheduling algorithms.
-500
0
500
1000
1500
2000
2500
3000
3500
4000
0 50 100 150 200Aggregatebusinessvalueacrossallworkflows
Total number of workflows
Business value scores of scheduling algorithms
Genetic algorithm
Round robin
Random proportional
Greedy
Figure 4: Magnification of the left-most region in Figure 3.
number of workflows that we are considering. For a very large
number of workflows, the round-robin scheme is able to better 
balance the load across all service providers.
To better understand the behaviour resulting from the scheduling
assignments, we show the workflow completion results in Figures
5, 6, and 7 for 100, 500, and 900 workflows, respectively. These
figures show the percentage of workflows that are successful (can
complete with their QoS limit), acceptable (can complete within
κ=3 times their QoS limit), and failed (cannot complete within κ=3
times their QoS limit). The GA consistently produces the highest
percentage of successful workflows (resulting in higher business
values for the aggregate set of workflows). Further, the round-robin
scheme produces better results than the random-proportional for a
large number of workflows but does not perform as well as the GA.
In Figure 8 we graph the makespan resulting from the same
experiments above. Makespan is a traditional metric from the job
scheduling community measuring elapsed time for the last job to
complete. While useful, it does not capture the high-level business
value metric that we are optimising against. Indeed, the makespan
is oblivious to the fact that we provide multiple levels of 
completion (successful, acceptable, and failed) and assign business value
scores accordingly. For completeness, we note that the GA 
provides the fastest makespan, but it is matched by the round robin
algorithm. The GA produces better business values (as shown in
Figure 3) because it is able to search the solution space to find 
better mappings that produce more successful workflows (as shown in
Figures 5 to 7).
We also looked at the effect of the scheduling algorithms on
balancing the load. Figure 9 shows the percentage of services
providers that were accessed while the workflows ran. As expected,
the greedy algorithm always hits one service provider; on the other
hand, the round-robin algorithm is the fastest to spread the business
33
Experimental parameter Comment
Workflows 5 to 1000
Business processes per workflow uniform random: 1 - 10
Service types 10
Service providers per service type uniform random: 1 - 10
Workflow QoS goal uniform random: 10-30 seconds
Service provider completion time (α) uniform random: 1 - 12 seconds
Service provider maximum concurrency (β) uniform random: 1 - 12
Service provider degradation coefficient (γ) uniform random: 0.1 - 0.9
Business value for successful workflows uniform random: 10 - 50 points
Business value for acceptable workflows uniform random: 0 - 10 points
Business value for failed workflows uniform random: -10 - 0 points
GA: number of parents 20
GA: number of children 80
GA: number of generations 1000
Table 1: Experimental parameters
Failed
Acceptable (completed but not within QoS)
Successful (completed within QoS)
0%
20%
40%
60%
80%
100%
RoundRobinRandProportionalGreedyGeneticAlg
Percentageofallworkflows
Workflow behaviour, 100 workflows
Figure 5: Workflow behaviour for 100 workflows.
Failed
Acceptable (completed but not within QoS)
Successful (completed within QoS)
0%
20%
40%
60%
80%
100%
RoundRobinRandProportionalGreedyGeneticAlg
Percentageofallworkflows
Workflow behaviour, 500 workflows
Figure 6: Workflow behaviour for 500 workflows.
Failed
Acceptable (completed but not within QoS)
Successful (completed within QoS)
0%
20%
40%
60%
80%
100%
RoundRobinRandProportionalGreedyGeneticAlg
Percentageofallworkflows
Workflow behaviour, 500 workflows
Figure 7: Workflow behaviour for 900 workflows.
0
50
100
150
200
250
300
0 200 400 600 800 1000
Makespan[seconds]
Number of workflows
Maximum completion time for all workflows
Genetic algorithm
Round robin
Random proportional
Greedy
Figure 8: Maximum completion time for all workflows. This value
is the makespan metric used in traditional scheduling research. 
Although useful, the makespan does not take into consideration the 
business value scoring in our problem domain.
processes. Figure 10 is the percentage of accessed service providers
(that is, the percentage of service providers represented in Figure
9) that had more assigned business processes than their advertised
maximum concurrency. For example, in the greedy algorithm only
one service provider is utilised, and this one provider quickly 
becomes saturated. On the other hand, the random-proportional 
algorithm uses many service providers, but because business processes
are proportionally assigned with more assignments going to the
better providers, there is a tendency for a smaller percentage of
providers to become saturated.
For completeness, we show the performance of the genetic 
algorithm itself in Figure 11. The algorithm scales linearly with an
increasing number of workflows. We note that the round-robin,
random-proportional, and greedy algorithms all finished within 1
second even for the largest workflow configuration. However, we
feel that the benefit of finding much higher business value scores
justifies the running time of the GA; further we would expect that
the running time will improve with both software tuning as well as
with a computer faster than our off-the-shelf PC.
5. CONCLUSION
Business processes within workflows can be orchestrated to 
access web services. In this paper we looked at multi-tiered service
provisioning where web service requests to service types can be
mapped to different service providers. The resulting problem is
that in order to support a very large number of workflows, the
assignment of business process to web service provider must be
intelligent. We used a business value metric to measure the 
be34
0
0.2
0.4
0.6
0.8
1
0 200 400 600 800 1000
Percentageofallserviceproviders
Number of workflows
Service providers utilised
Genetic algorithm
Round robin
Random proportional
Greedy
Figure 9: The percentage of service providers utilized during 
workload executions. The Greedy algorithm always hits the one service
provider, while the Round Robin algorithm spreads requests evenly
across the providers.
0
0.2
0.4
0.6
0.8
1
0 200 400 600 800 1000
Percentageofallserviceproviders
Number of workflows
Service providers saturated
Genetic algorithm
Round robin
Random proportional
Greedy
Figure 10: The percentage of service providers that are saturated
among those providers who were utilized (that is, percentage of the 
service providers represented in Figure 9). A saturated service provider
is one whose workload is greater that its advertised maximum 
concurrency.
0
5
10
15
20
25
0 200 400 600 800 1000
Runningtimeinseconds
Total number of workflows
Running time of genetic algorithm
GA running time
Figure 11: Running time of the genetic algorithm.
haviour of workflows meeting or failing QoS values, and we 
optimised our scheduling to maximise the aggregate business value
across all workflows. Since the solution space of scheduler 
mappings is exponential, we used a genetic search algorithm to search
the space and converge toward the best schedule. With a default
configuration for all parameters and using our business value 
scoring, the GA produced up to 115% business value improvement over
the next best algorithm. Finally, because a genetic algorithm will
converge towards the optimal value using any metric (even other
than the business value metric we used), we believe our approach
has strong potential for continuing work.
In future work, we look to acquire real-world traces of web 
service instances in order to get better estimates of service agreement
guarantees, although we expect that such guarantees between the
providers and their consumers are not generally available to the
public. We will also look at other QoS metrics such as CPU and
I/O usage. For example, we can analyse transfer costs with 
varying bandwidth, latency, data size, and data distribution. Further,
we hope to improve our genetic algorithm and compare it to more
scheduler alternatives. Finally, since our work is complementary
to existing work in web services choreography (because we rely on
pre-configured workflows), we look to integrate our approach with
available web service workflow systems expressed in BPEL.
6. REFERENCES
[1] A. Ankolekar, et al. DAML-S: Semantic Markup For Web
Services, In Proc. of the Int"l Semantic Web Working
Symposium, 2001.
[2] L. Davis. Job Shop Scheduling with Genetic Algorithms,
In Proc. of the Int"l Conference on Genetic Algorithms, 1985.
[3] H.-L. Fang, P. Ross, and D. Corne. A Promising Genetic
Algorithm Approach to Job-Shop Scheduling, Rescheduling,
and Open-Shop Scheduling Problems , In Proc. on the 5th
Int"l Conference on Genetic Algorithms, 1993.
[4] M. Gary and D. Johnson. Computers and Intractability: A
Guide to the Theory of NP-Completeness, Freeman, 1979.
[5] J. Holland. Adaptation in Natural and Artificial Systems:
An Introductory Analysis with Applications to Biology,
Control, and Artificial Intelligence, MIT Press, 1992.
[6] D. Goldberg. Genetic Algorithms in Search, Optimization
and Machine Learning, Kluwer Academic Publishers, 1989.
[7] Business Processes in a Web Services World,
www-128.ibm.com/developerworks/
webservices/library/ws-bpelwp/.
[8] G. Soundararajan, K. Manassiev, J. Chen, A. Goel, and C.
Amza. Back-end Databases in Shared Dynamic Content
Server Clusters, In Proc. of the IEEE Int"l Conference on
Autonomic Computing, 2005.
[9] B. Srivastava and J. Koehler. Web Service Composition
Current Solutions and Open Problems, ICAP, 2003.
[10] B. Urgaonkar, P. Shenoy, A. Chandra, and P. Goyal.
Dynamic Provisioning of Multi-Tier Internet Applications,
In Proc. of the IEEE Int"l Conference on Autonomic
Computing, 2005.
[11] L. Zeng, B. Benatallah, M. Dumas, J. Kalagnanam, and Q.
Sheng. Quality Driven Web Services Composition, In
Proc. of the WWW Conference, 2003.
35
Concept and Architecture of a Pervasive Document Editing
and Managing System
Stefania Leone Thomas B. Hodel Harald Gall
University of Zurich, Switzerland University of Zurich, Switzerland University of Zurich, Switzerland
Department of Informatics Department of Informatics Department of Informatics
leone@ifi.unizh.ch hodel@ifi.unizh.ch gall@ifi.unizh.ch
ABSTRACT
Collaborative document processing has been addressed by many
approaches so far, most of which focus on document versioning
and collaborative editing. We address this issue from a different
angle and describe the concept and architecture of a pervasive
document editing and managing system. It exploits database
techniques and real-time updating for sophisticated collaboration
scenarios on multiple devices. Each user is always served with 
upto-date documents and can organize his work based on document
meta data. For this, we present our conceptual architecture for
such a system and discuss it with an example.
Categories and Subject Descriptors
C.2.4 Distributed Systems [Computer-Communication
Networks]: Computer System Organization, Distributed Systems,
Distributed Applications
General Terms
Management, Measurement, Documentation, Economics, Human
Factors
1. INTRODUCTION
Text documents are a valuable resource for virtually any enterprise
and organization. Documents like papers, reports and general
business documentations contain a large part of today"s (business)
knowledge. Documents are mostly stored in a hierarchical folder
structure on file servers and it is difficult to organize them in regard
to classification, versioning etc., although it is of utmost importance
that users can find, retrieve and edit up-to-date versions of
documents whenever they want and, in a user-friendly way.
1.1 Problem Description
With most of the commonly used word-processing applications
documents can be manipulated by only one user at a time: tools for
pervasive collaborative document editing and management, are
rarely deployed in today"s world. Despite the fact, that people strive
for location- and time- independence, the importance of pervasive
collaborative work, i.e. collaborative document editing and
management is totally neglected. Documents could therefore be
seen as a vulnerable source in today"s world, which demands for an
appropriate solution: The need to store, retrieve and edit these
documents collaboratively anytime, everywhere and with almost
every suitable device and with guaranteed mechanisms for security,
consistency, availability and access control, is obvious.
In addition, word processing systems ignore the fact that the history
of a text document contains crucial information for its management.
Such meta data includes creation date, creator, authors, version,
location-based information such as time and place when/where a
user reads/edits a document and so on. Such meta data can be
gathered during the documents creation process and can be used
versatilely. Especially in the field of pervasive document
management, meta data is of crucial importance since it offers
totally new ways of organizing and classifying documents: On the
one hand, the user"s actual situation influences the user"s objectives.
Meta data could be used to give the user the best possible view on
the documents, dependent of his actual information. On the other
hand, as soon as the user starts to work, i.e. reads or edits a
document, new meta data can be gathered in order to make the
system more adaptable and in a sense to the users situation and, to
offer future users a better view on the documents.
As far as we know, no system exists, that satisfies the
aforementioned requirements. A very good overview about 
realtime communication and collaboration system is described in [7].
We therefore strive for a pervasive document editing and
management system, which enables pervasive (and collaborative)
document editing and management: users should be able to read and
edit documents whenever, wherever, with whomever and with
whatever device.
In this paper, we present collaborative database-based real-time
word processing, which provides pervasive document editing and
management functionality. It enables the user to work on
documents collaboratively and offers sophisticated document
management facility: the user is always served with up-to-date
documents and can organize and manage documents on the base of
meta data. Additionally document data is treated as ‘first class
citizen" of the database as demanded in [1].
1.2 Underlying Concepts
The concept of our pervasive document editing and management
system requires an appropriate architectural foundation. Our
concept and implementation are based on the TeNDaX [3]
collaborative database-based document editing and management
system, which enables pervasive document editing and managing.
TeNDaX is a Text Native Database eXtension. It enables the
storage of text in databases in a native form so that editing text is
finally represented as real-time transactions. Under the term ‘text
editing" we understand the following: writing and deleting text
(characters), copying & pasting text, defining text layout &
structure, inserting notes, setting access rights, defining business
processes, inserting tables, pictures, and so on i.e. all the actions
regularly carried out by word processing users. With ‘real-time
transaction" we mean that editing text (e.g. writing a
character/word) invokes one or several database transactions so that
everything, which is typed appears within the editor as soon as these
objects are stored persistently. Instead of creating files and storing
them in a file system, the content and all of the meta data belonging
to the documents is stored in a special way in the database, which
enables very fast real-time transactions for all editing tasks [2].
The database schema and the above-mentioned transactions are
created in such a way that everything can be done within a 
multiuser environment, as is usual done by database technology. As a
consequence, many of the achievements (with respect to data
organization and querying, recovery, integrity and security
enforcement, multi-user operation, distribution management,
uniform tool access, etc.) are now, by means of this approach, also
available for word processing.
2. APPROACH
Our pervasive editing and management system is based on the
above-mentioned database-based TeNDaX approach, where
document data is stored natively in the database and supports
pervasive collaborative text editing and document management.
We define the pervasive document editing and management system,
as a system, where documents can easily be accessed and
manipulated everywhere (within the network), anytime
(independently of the number of users working on the same
document) and with any device (desktop, notebook, PDA, mobile
phone etc.).
DB 3
RTSC 4
RTSC 1
RTSC 2
RTSC 3
AS 1
AS 3
DB 1
DB 2
AS 2
AS 4
DB 4
A
B
C
D
E
F
G
Figure 1. TeNDaX Application Architecture
In contrast to documents stored locally on the hard drive or on a file
server, our system automatically serves the user with the up-to-date
version of a document and changes done on the document are stored
persistently in the database and immediately propagated to all
clients who are working on the same document. Additionally, meta
data gathered during the whole document creation process enables
sophisticated document management. With the TeXt SQL API as
abstract interface, this approach can be used by any tool and for any
device.
The system is built on the following components (see Figure 1): An
editor in Java implements the presentation layer (A-G in Figure 1).
The aim of this layer is the integration in a well-known 
wordprocessing application such as OpenOffice.
The business logic layer represents the interface between the
database and the word-processing application. It consists of the
following three components: The application server (marked as AS
1-4 in Figure 1) enables text editing within the database
environment and takes care of awareness, security, document
management etc., all within a collaborative, real-time and multi-user
environment. The real-time server component (marked as RTSC 
14 in Figure 1) is responsible for the propagation of information, i.e.
updates between all of the connected editors.
The storage engine (data layer) primarily stores the content of
documents as well as all related meta data within the database
Databases can be distributed in a peer-to-peer network (DB 1-4 in
Figure 1)..
In the following, we will briefly present the database schema, the
editor and the real-time server component as well as the concept of
dynamic folders, which enables sophisticated document
management on the basis of meta data.
2.1 Application Architecture
A database-based real-time collaborative editor allows the same
document to be opened and edited simultaneously on the same
computer or over a network of several computers and mobile
devices. All concurrency issues, as well as message propagation, are
solved within this approach, while multiple instances of the same
document are being opened [3]. Each insert or delete action is a
database transaction and as such, is immediately stored persistently
in the database and propagated to all clients working on the same
document.
2.1.1 Database Schema
As it was mentioned earlier that text is stored in a native way. Each
character of a text document is stored as a single object in the
database [3]. When storing text in such a native form, the
performance of the employed database system is of crucial
importance. The concept and performance issues of such a text
database are described in [3], collaborative layouting in [2],
dynamic collaborative business processes within documents in [5],
the text editing creation time meta data model in [6] and the relation
to XML databases in [7].
Figure 2 depicts the core database schema. By connecting a client to
the database, a Session instance is created. One important attribute
of the Session is the DocumentSession. This attribute refers to
DocumentSession instances, which administrates all opened
documents. For each opened document, a DocumentSession
instance is created. The DocumentSession is important for the 
realtime server component, which, in case of a
42
is beforeis after
Char
(ID)
has
TextElement
(ID)
starts
with
is used
by
InternalFile
(ID)
is in includes
created
at
has
inserted
by
inserted
is active
ir
ir
CharacterValue
(Unicode)
has
List
(ID)
starts
starts
with
ends ends with
FileSize
has
User
(ID)
last read by
last written by
created
at
created by
Style
DTD
(ID)
is used
by
uses
uses
is used
by
Authors
arehas
Description
Password
Picture
UserColors
UserListSecurity
has
has
has
has
has
has
FileNode
(ID)
references/isreferencedby
is dynamic DynStructure
NodeDetails
has
has is NodeType
is parent
of
has
parent
has
Role
(ID)
created
at
created
created
by
Name
has
Description
is user
Name
has
has
main role
FileNodeAccessMatrix
(ID)
has
is
AccessMatrix
read option
grand option
write option
contains
has
access
Times
opened … times with … by
contains/ispartof
ir
ir
is...andincludes
Lineage
(ID)
references
is after
is before
CopyPaste
(ID)
references
is in
is copy
of
is a copy
from
hasCopyPaste
(ID)
is activeLength has
Str (Stream)
has
inserted by / inserted
RegularChar
StartChar EndChar
File
ExternalFile
is from
URL
Type
(extension)
is of
Title
has
DocumentSession
(ID)
is opened
by
has
opened
has
opened
Session
(ID)
isconnectedwith
launched by
VersionNumber
uses
has
read option
grand option
write option
ends with
is used
by
is in has
is unique
DTD (Stream)
has
has
Name
Column
(ID)
has set on
On/off
isvisible…for
false
LanguageProfile
(ID)
has
contains
Name
Profile
Marking
(ID)
has
parent
internal
is copy
from
hasRank
is onPosition
starts
with
ends with
is logical style
is itemized
is italic
is enumerated
is underline
is
is part of
Alignment
Size has
Font has
hasColor
is bold
has
uses
ElementName
StylesheetName
isused
by
Process
(ID)
is running by OS
is web session
MainRoles
Roles has
has
Timestamp
(Date, Time)
created
at
Timestamp
(Date, Time)
Timestamp
(Date, Time)
Timestamp
(Date, Time)
Timestamp
(Date, Time)created
at
Type
has
Port
IP
has
has
MessagePropagator
(ID)
Picture
(Stream)
Name
Picture
(ID)
has
contains
LayoutBlock WorkflowBlockLogicalBlock
contains
BlockDataType
has
property
BlockData is of
WorkflowInstance
(ID)
isin
TaskInstance
(ID)
has
parent
Timestamp
(Date, Time)
Timestamp
(Date, Time)
Timestamp
(Date, Time)
Timestamp
(Date, Time)
last modified at
completed at
started at
created
at
is on
has
Name
created by
has
attached
Comment
Typeis of
Timestamp
(Date, Time)
Timestamp
(Date, Time)
Timestamp
(Date, Time)
created
at
started at
<< last modified at
is
Category
Editors
has
Status
has
Timestamp
(Date, Time)
<< status last modified
Timestamp
(Date, Time)
is due at
DueType
has
Timezone
has
Notes
has
SecurityLevel
hasset
Timestamp
(Date, Time)
<< is completed at
isfollowedby
Task
(Code)
Description
has
Indent
references
hasbeenopenedat...by
Timestamp
RedoHistory
is before
is after
references
hasCharCounter
is inhas
has
Offset
ActionID
(Code)
Timestamp
(Date, Time)
invoked
at
invoked
by
Version
(ID)
isbuild
from
has
created
byarchived
has
Comment
Timestamp
(Date, Time)
<<createdat
UndoHistory
(ID)
starts
ends
has
Name
created
by
Name
has
is before
is after
<< references
CharCounter
has
is in
created
at
Timestamp
is active
created
by
is used
by
Offset
has
created
at
Timestamp
Index
(ID)
lastmodifiedby
Lexicon
(ID)
isof
Frequency
is
occurring
is stop word
Term
is
is in
ends with
starts
with
<< original starts with
WordNumber
SentenceNumber
ParagraphNumber
Citatons
has
is in
is
is in
istemporary
is in
has
Structure
has
ElementPath
createdat
Timestamp
<< describes
SpiderBuild
(ID)
is updated
is deleted
Timestamp
(Date, Time)
<<lastupdatedat
has validated structure
<<neededtoindex
Time
(ms)
IndexUpdate
nextupdatein
hasindexed
isrunningbyOS
lastupdate
enabled
Timestamp
Time
(s)
Documents
StopCharacter
Description
Character
Value
(ASCII)
is sentence stop
is paragraph stop
Name
has
is
is
OptionsSettings
show information show warningsshow exceptions
do lineage recording
do internal lineage recording
ask for unknown source
show intra document
lineage information
are set
for
X
X
X
VirtualBorder
(ID)
isonhas
{1, 2}
{1, 2}
ir
ir
UserMode
(Code)
UserMode
(Code)
Figure 2. TeNDaX Database Schema (Object Role Modeling Diagram)
change on a document done by a client, is responsible for sending
update information to all the clients working on the same
document. The DocumentId in the class DocumentSession points
to a FileNode instance, and corresponds to the ID of the opened
document. Instances of the class FileNode either represent a
folder node or a document node. The folder node corresponds to a
folder of a file system and the document node to that of a file.
Instances of the class Char represent the characters of a
document. The value of a character is stored in the attribute
CharacterValue. The sequence is defined by the attributes After
and Before of the class Char. Particular instances of Char mark
the beginning and the end of a document. The methods
InsertChars and RemoveChars are used to add and delete
characters.
2.1.2 Editor
As seen above, each document is natively stored in the database.
Our editor does not have a replica of one part of the native text
database in the sense of database replicas. Instead, it has a so-called
image as its replica. Even if several authors edit the same text at the
same time, they work on one unique document at all times. The
system guarantees this unique view.
Editing a document involves a number of steps: first, getting the
required information out of the image, secondly, invoking the
corresponding methods within the database, thirdly, changing the
image, and fourthly, informing all other clients about the changes.
2.1.3 Real-Time Server Component
The real-time server component is responsible for the real-time
propagation of any changes on a document done within an editor to
all the editors who are working or have opened the same document.
When an editor connects to the application server, which in turn
connects to the database, the database also establishes a connection
to the real-time server component (if there isn"t already a
connection). The database system informs the real-time server
component about each new editor session (session), which the 
realtime server component administrates in his SessionManager. Then,
the editor as well connects to the real-time server component. The
real-time server component adds the editor socket to the client"s
data structure in the SessionManager and is then ready to
communicate.
Each time a change on a document from an editor is persistently
stored in the database, the database sends a message to the real-time
server component, which in turns, sends the changes to all the
43
editors working on the same document. Therefore, a special
communication protocol is used: the update protocol.
Update Protocol
The real-time server component uses the update protocol to
communicate with the database and the editors. Messages are sent
from the database to the real-time server component, which sends
the messages to the affected editors. The update protocol consists of
different message types. Messages consist of two packages:
package one contains information for the real-time server
component whereas package two is passed to the editors and
contains the update information, as depicted in Figure 3.
|| RTSC || Parameter | … | Parameter|| || Editor Data ||
Protocol between database system and
real-time server component
Protocol between real -time server
component and editors
Figure 3. Update Protocol
In the following, two message types are presented:
||u|sessionId,...,sessionId||||editor data||
u: update message, sessionId: Id of the client session
With this message type the real-time server component sends the
editor data package to all editors specified in the sessionId list.
||ud|fileId||||editor data||
ud: update document message, fileId: Id of the file
With this message type, the real-time server component sends the
editor data to all editors who have opened the document with the
indicated file-Id.
Class Model
Figure 4 depicts the class model as well as the environment of the
real-time server component. The environment consists mainly of the
editor and the database, but any other client application that could
make use of the real-time server component can connect.
ConnectionListener: This class is responsible for the connection to
the clients, i.e. to the database and the editors. Depending on the
connection type (database or editor) the connection is passed to an
EditorWorker instance or DatabaseMessageWorker instance
respectively.
EditorWorker: This class manages the connections of type ‘editor".
The connection (a socket and its input and output stream) is stored
in the SessionManager.
SessionManager: This class is similar to an ‘in-memory database":
all editor session information, e.g. the editor sockets, which editor
has opened which document etc. are stored within this data
structure.
DatabaseMessageWorker: This class is responsible for the
connections of type ‘database". At run-time, only one connection
exists for each database. Update messages from the database are
sent to the DatabaseMessageWorker and, with the help of
additional information from the SessionManager, sent to the
corresponding clients.
ServiceClass: This class offers a set of methods for reading, writing
and logging messages.
tdb.mp.editor tdb.mp.database
tdb.mp.mgmt
EditorWorker
DatabaseMessageWorker
SessionManager
MessageHandler
ConnectionListener
ServiceClass
MessageQueue
tdb.mp.listener tdb.mp.service
junit.tests
1
*
1
*
1 *
1
*
1*
1
*
Editors Datenbanksystem 1
2
1
*
1
*
1
*
TCP/IP
Figure 4. Real-Time Server Component Class Diagram
2.1.4 Dynamic Folders
As mentioned above, every editing action invoked by a user is
immediately transferred to the database. At the same time, more
information about the current transaction is gathered.
As all information is stored in the database, one character can hold a
multitude of information, which can later be used for the retrieval of
documents. Meta data is collected at character level, from document
structure (layout, workflow, template, semantics, security,
workflow and notes), on the level of a document section and on the
level of the whole document [6].
All of the above-mentioned meta data is crucial information for
creating content and knowledge out of word processing documents.
This meta data can be used to create an alternative storage system
for documents. In any case, it is not an easy task to change users"
familiarity to the well known hierarchical file system. This is also
the main reason why we do not completely disregard the classical
file system, but rather enhance it. Folders which correspond to the
classical hierarchical file system will be called static folders.
Folders where the documents are organized according to meta data,
will be called dynamic folders. As all information is stored in the
database, the file system, too, is based on the database.
The dynamic folders build up sub-trees, which are guided by the
meta data selected by the user. Thus, the first step in using a
dynamic folder is the definition of how it should be built. For each
level of a dynamic folder, exactly one meta data item is used to. The
following example illustrates the steps which have to be taken in
order to define a dynamic folder, and the meta data which should be
used.
As a first step, the meta data which will be used for the dynamic
folder must be chosen (see Table 1): The sequence of the meta data
influences the structure of the folder. Furthermore, for each meta
data used, restrictions and granularity must be defined by the user;
if no restrictions are defined, all accessible documents are listed.
The granularity therefore influences the number of sub-folders
which will be created for the partitioning of the documents.
44
As the user enters the tree structure of the dynamic folder, he can
navigate through the branches to arrive at the document(s) he is
looking for. The directory names indicate which meta data
determines the content of the sub-folder in question. At each level,
the documents, which have so far been found to match the meta
data, can be inspected.
Table 1. Defining dynamic folders (example)
Level Meta data Restrictions Granularity
1 Creator Only show documents
which have been created
by the users Leone or
Hodel or Gall
One folder per
creator
2 Current
location
Only show documents
which where read at my
current location
One folder per
task status
3 Authors Only show documents
where at least 40% was
written by user ‘Leone"
Each 20% one
folder
ad-hoc changes of granularity and restrictions are possible in order
to maximize search comfort for the user. It is possible to predefine
dynamic folders for frequent use, e.g. a location-based folder, as
well as to create and modify dynamic folders on an ad-hoc basis.
Furthermore, the content of such dynamic folders can change from
one second to another, depending on the changes made by other
users at that moment.
3. VALIDATION
The proposed architecture is validated on the example of a character
insertion. Insert operations are the mostly used operations in a
(collaborative) editing system. The character insertion is based on
the TeNDaX Insert Algorithm which is formally described in the
following. The algorithm is simplified for this purpose.
3.1 Insert Characters Algorithm
The symbol c stands for the object character, p stands for the
previous character, n stands for the next character of a character
object c and the symbol l stands for a list of character objects.
c = character
p=previous character
n = next character
l = list of characters
The symbol c1 stands for the first character in the list l, ci stands
for a character in the list l at the position i, whereas i is a value
between 1 and the length of the list l, and cn stands for the last
character in the list l.
c1 = first character in list l
ci = character at position i in list l
cn = last character in list l
The symbol β stands for the special character that marks the
beginning of a document and ε stands for the special character
that marks the end of a document.
β=beginning of document
ε=end of document
The function startTA starts a transaction.
startTA = start transaction
The function commitTA commits a transaction that was started.
commitTA = commit transaction
The function checkWriteAccess checks if the write access for a
document session s is granted.
checkWriteAccess(s) = check if write access for document session
s is granted
The function lock acquires an exclusive lock for a character c and
returns 1 for a success and 0 for no success.
lock(c) = acquire the lock for character c
success : return 1, no success : return 0
The function releaseLocks releases all locks that a transaction has
acquired so far.
releaseLocks = release all locks
The function getPrevious returns the previous character and
getNext returns the next character of a character c.
getPrevious(c) = return previous character of character c
getNext(c) = return next character of character c
The function linkBefore links a preceding character p with a
succeeding character x and the function linkAfter links a
succeeding character n with a preceding character y.
linkBefore(p,x) = link character p to character x
linkAfter(n,y) = link character n to character y
The function updateString links a character p with the first
character c1 of a character list l and a character n with the last
character cn of a character list l
updateString(l, p, n) = linkBefore(p cl)∧ linkAfter(n, cn )
The function insertChar inserts a character c in the table Char
with the fields After set to a character p and Before set to a
character n.
insertChar(c, p, n) = linkAfter(c,p) ∧ linkBefore(c,n) ∧
linkBefore(p,c) ∧ linkAfter(n,c)
The function checkPreceding determines the previous character's
CharacterValue of a character c and if the previous character's
status is active.
checkPreceding(c) = return status and CharacterValue of the
previous character
The function checkSucceeding determines the next character's
CharacterValue of a character c and if the next character's status is
active.
45
checkSucceeding(c) = return status and CharacterValue of the
next character
The function checkCharValue determines the CharacterValue of a
character c.
checkCharValue(c) = return CharacterValue of character c
The function sendUpdate sends an update message
(UpdateMessage) from the database to the real-time server
component.
sendUpdate(UpdateMessage)
The function Read is used in the real-time server component to
read the UpdateMessage.
Read(UpdateInformationMessage)
The function AllocatEditors checks on the base of the
UpdateMessage and the SessionManager, which editors have to
be informed.
AllocateEditors(UpdateInformationMessage, SessionManager) =
returns the affected editors
The function SendMessage(EditorData) sends the editor part of
the UpdateMessage to the editors
SendMessage(EditorData)
In TeNDaX, the Insert Algorithm is implemented in the class
method InsertChars of the class Char which is depicted in Figure
2. The relevant parameters for the definitions beneath, are
introduced in the following list:
- nextCharacterOID: OID of the character situated next to the
string to be inserted
- previousCharacterOID: OID of the character situated
previously to the string to be inserted
- characterOIDs (List): List of character which have to be
inserted
Thus, the insertion of characters can be defined stepwise as
follows:
Start a transaction.
startTA
Select the character that is situated before the character that
follows the string to be inserted.
getPrevious(nextCharacterOID) = PrevChar(prevCharOID) ⇐
Π After ϑOID = nextCharacterOID(Char))
Acquire the lock for the character that is situated in the document
before the character that follows the string which shall be inserted.
lock(prevCharId)
At this time the list characterOIDs contains the characters c1 to cn
that shall be inserted.
characterOIDs={ c1, …, cn }
Each character of the string is inserted at the appropriate position
by linking the preceding and the succeeding character to it.
For each character ci of characterOIDs:
insertChar(ci, p, n)
Whereas ci ∈ { c1,…, cn }
Check if the preceding and succeeding characters are active or if it
is the beginning or the end of the document.
checkPreceding(prevCharOID) = IsOK(IsActive,
CharacterValue) ⇐ Π IsActive, CharacterValue (ϑ OID =
nextCharacterOID(Char))
checkSucceeding(nextCharacterOID) = IsOK(IsActive,
CharacterValue)⇐ Π IsActive, CharacterValue (ϑ OID =
nextCharacterOID(Char))
Update characters before and after the string to be inserted.
updateString(characterOIDs, prevCharOID, nextCharacterOID)
Release all locks and commit Transaction.
releaseLocks
commitTA
Send update information to the real-time server component
sendUpdate(UpdatenMessage)
Read update message and inform affected editors of the change
Read(UpdateMessage)
Allocate Editors(UpdateMessage, SessionManager)
SendMessage(EditorData)
3.2 Insert Characters Example
Figure 1 gives a snapshot the system, i.e. of its architecture: four
databases are distributed over a peer-to-peer network. Each
database is connected to an application server (AS) and each
application server is connected to a real-time server component
(RTSC). Editors are connected to one or more real-time server
components and to the corresponding databases.
Considering that editor A (connected to database 1 and 4) and
editor B (connected to database 1 and 2) are working on the same
document stored in database 1. Editor B now inserts a character
into this document. The insert operation is passed to application
server 1, which in turns, passes it to the database 1, where an
insert operation is invoked; the characters are inserted according
to the algorithm discussed in the previous section. After the
insertion, database 1 sends an update message (according to the
update protocol discussed before) to real-time server component 1
(via AS 1). RTCS 1 combines the received update information
with the information in his SessionManager and sends the editor
data to the affected editors, in this case to editor A and B, where
the changes are immediately shown.
Occurring collaboration conflicts are solved and described in [3].
4. SUMMARY
With the approach presented in this paper and the implemented
prototype, we offer real-time collaborative editing and management
of documents stored in a special way in a database. With this
approach we provide security, consistency and availability of
documents and consequently offer pervasive document editing and
management. Pervasive document editing and management is
enabled due to the proposed architecture with the embedded 
real46
time server component, which propagates changes to a document
immediately and consequently offers up-to-date documents.
Document editing and managing is consequently enabled anywhere,
anytime and with any device.
The above-descried system is implemented in a running prototype.
The system will be tested soon in line with a student workshop next
autumn.
REFERENCES
[1] Abiteboul, S., Agrawal, R., et al.: The Lowell Database
Research Self Assessment. Massachusetts, USA, 2003.
[2] Hodel, T. B., Businger, D., and Dittrich, K. R.: Supporting
Collaborative Layouting in Word Processing. IEEE
International Conference on Cooperative Information
Systems (CoopIS), Larnaca, Cyprus, IEEE, 2004.
[3] Hodel, T. B. and Dittrich, K. R.: "Concept and prototype of a
collaborative business process environment for document
processing." Data & Knowledge Engineering 52, Special
Issue: Collaborative Business Process Technologies(1): 
61120, 2005.
[4] Hodel, T. B., Dubacher, M., and Dittrich, K. R.: Using
Database Management Systems for Collaborative Text
Editing. ACM European Conference of 
Computersupported Cooperative Work (ECSCW CEW 2003),
Helsinki, Finland, 2003.
[5] Hodel, T. B., Gall, H., and Dittrich, K. R.: Dynamic
Collaborative Business Processes within Documents. ACM
Special Interest Group on Design of Communication
(SIGDOC) , Memphis, USA, 2004.
[6] Hodel, T. B., R. Hacmac, and Dittrich, K. R.: Using Text
Editing Creation Time Meta Data for Document
Management. Conference on Advanced Information
Systems Engineering (CAiSE'05), Porto, Portugal, Springer
Lecture Notes, 2005.
[7] Hodel, T. B., Specker, F. and Dittrich, K. R.: Embedded
SOAP Server on the Operating System Level for ad-hoc
Automatic Real-Time Bidirectional Communication.
Information Resources Management Association (IRMA),
San Diego, USA, 2005.
[8] O"Kelly, P.: Revolution in Real-Time Communication and
Collaboration: For Real This Time. Application Strategies:
In-Depth Research Report. Burton Group, 2005.
47
Authority Assignment in Distributed Multi-Player
Proxy-based Games
Sudhir Aggarwal Justin Christofoli
Department of Computer Science
Florida State University, Tallahassee, FL
{sudhir, christof}@cs.fsu.edu
Sarit Mukherjee Sampath Rangarajan
Center for Networking Research
Bell Laboratories, Holmdel, NJ
{sarit, sampath}@bell-labs.com
ABSTRACT
We present a proxy-based gaming architecture and 
authority assignment within this architecture that can lead to 
better game playing experience in Massively Multi-player 
Online games. The proposed game architecture consists of 
distributed game clients that connect to game proxies (referred
to as communication proxies) which forward game related
messages from the clients to one or more game servers. 
Unlike proxy-based architectures that have been proposed in
the literature where the proxies replicate all of the game
state, the communication proxies in the proposed 
architecture support clients that are in proximity to it in the 
physical network and maintain information about selected 
portions of the game space that are relevant only to the clients
that they support. Using this architecture, we propose an
authority assignment mechanism that divides the 
authority for deciding the outcome of different actions/events that
occur within the game between client and servers on a per
action/event basis. We show that such division of 
authority leads to a smoother game playing experience by 
implementing this mechanism in a massively multi-player online
game called RPGQuest. In addition, we argue that cheat
detection techniques can be easily implemented at the 
communication proxies if they are made aware of the game-play
mechanics.
Categories and Subject Descriptors
C.2.4 [Computer-Communication networks]: Distributed
Systems-Distributed Applications
General Terms
Games, Performance
1. INTRODUCTION
In Massively Multi-player On-line Games (MMOG), game
clients who are positioned across the Internet connect to
a game server to interact with other clients in order to be
part of the game. In current architectures, these 
interactions are direct in that the game clients and the servers 
exchange game messages with each other. In addition, current
MMOGs delegate all authority to the game server to make
decisions about the results pertaining to the actions that
game clients take and also to decide upon the result of other
game related events. Such centralized authority has been
implemented with the claim that this improves the security
and consistency required in a gaming environment.
A number of works have shown the effect of network latency
on distributed multi-player games [1, 2, 3, 4]. It has been
shown that network latency has real impact on practical
game playing experience [3, 5]. Some types of games can
function quite well even in the presence of large delays. For
example, [4] shows that in a modern RPG called Everquest
2, the breakpoint of the game when adding artificial 
latency was 1250ms. This is accounted to the fact that the
combat system used in Everquest 2 is queueing based and
has very low interaction. For example, a player queues up
4 or 5 spells they wish to cast, each of these spells take 1-2
seconds to actually perform, giving the server plenty of time
to validate these actions. But there are other games such as
FPS games that break even in the presence of moderate 
network latencies [3, 5]. Latency compensation techniques have
been proposed to alleviate the effect of latency [1, 6, 7] but
it is obvious that if MMOGs are to increase in 
interactivity and speed, more architectures will have to be developed
that address responsiveness, accuracy and consistency of the
gamestate.
In this paper, we propose two important features that would
make game playing within MMOGs more responsive for
movement and scalable. First, we propose that centralized
server-based architectures be made hierarchical through the
introduction of communication proxies so that game updates
made by clients that are time sensitive, such as movement,
can be more efficiently distributed to other players within
their game-space. Second, we propose that assignment of
authority in terms of who makes the decision on client 
actions such as object pickups and hits, and collisions between
players, be distributed between the clients and the servers in
order to distribute the computing load away from the central
server. In order to move towards more complex real-time
networked games, we believe that definitions of authority
must be refined.
Most currently implemented MMOGs have game servers
that have almost absolute authority. We argue that there is
no single consistent view of the virtual game space that can
be maintained on any one component within a network that
has significant latency, such as the one that many MMOG
players would experience. We believe that in most cases, the
client with the most accurate view of an entity is the best
suited to make decisions for that entity when the causality
of that action will not immediately affect any other 
players. In this paper we define what it means to have authority
within the context of events and objects in a virtual game
space. We then show the benefits of delegating authority
for different actions and game events between the clients
and server.
In our model, the game space consists of game clients 
(representing the players) and objects that they control. We
divide the client actions and game events (we will 
collectively refer to these as events) such as collisions, hits etc.
into three different categories, a) events for which the game
client has absolute authority, b) events for which the game
server has absolute authority, and c) events for which the
authority changes dynamically from client to the server and
vice-versa. Depending on who has the authority, that 
entity will make decisions on the events that happen within a
game space. We propose that authority for all decisions that
pertain to a single player or object in the game that neither
affects the other players or objects, nor are affected by the
actions of other players be delegated to that player"s game
client. These type of decisions would include collision 
detection with static objects within the virtual game space and
hit detection with linear path bullets (whose trajectory is
fixed and does not change with time) fired by other players.
Authority for decisions that could be affected by two or more
players should be delegated to the impartial central server,
in some cases, to ensure that no conflicts occur and in other
cases can be delegated to the clients responsible for those
players. For example, collision detection of two players that
collide with each other and hit detection of non-linear 
bullets (that changes trajectory with time) should be delegated
to the server. Decision on events such as item pickup (for
example, picking up items in a game to accumulate points)
should be delegated to a server if there are multiple 
players within close proximity of an item and any one of the
players could succeed in picking the item; for item pick-up
contention where the client realizes that no other player, 
except its own player, is within a certain range of the item,
the client could be delegated the responsibility to claim the
item. The client"s decision can always be accurately verified
by the server.
In summary, we argue that while current authority models
that only delegate responsibility to the server to make 
authoritative decisions on events is more secure than allowing
the clients to make the decisions, these types of models add
undesirable delays to events that could very well be decided
by the clients without any inconsistency being introduced
into the game. As networked games become more complex,
our architecture will become more applicable. This 
architecture is applicable for massively multiplayer games where
the speed and accuracy of game-play are a major concern
while consistency between player game-states is still desired.
We propose that a mixed authority assignment mechanism
such as the one outlined above be implemented in high 
interaction MMOGs.
Our paper has the following contributions. First we propose
an architecture that uses communication proxies to enable
clients to connect to the game server. A communication
proxy in the proposed architecture maintains information
only about portions of the game space that are relevant to
clients connected to it and is able to process the movement
information of objects and players within these portions.
In addition, it is capable of multicasting this information
only to a relevant subset of other communication proxies.
These functionalities of a communication proxy leads to a
decrease in latency of event update and subsequently, better
game playing experience. Second, we propose a mixed 
authority assignment mechanism as described above that 
improves game playing experience. Third, we implement the
proposed mixed authority assignment mechanism within a
MMOG called RPGQuest [8] to validate its viability within
MMOGs.
In Section 2, we describe the proxy-based game 
architecture in more detail and illustrate its advantages. In 
Section 3, we provide a generic description of the mixed 
authority assignment mechanism and discuss how it improves
game playing experience. In Section 4, we show the 
feasibility of implementing the proposed mixed authority 
assignment mechanism within existing MMOGs by describing a
proof-of-concept implementation within an existing MMOG
called RPGQuest. Section 5 discusses related work. In 
Section 6, we present our conclusions and discuss future work.
2. PROXY-BASED GAME ARCHITECTURE
Massively Multi-player Online Games (MMOGs) usually 
consist of a large game space in which the players and 
different game objects reside and move around and interact with
each-other. State information about the whole game space
could be kept in a single central server which we would 
refer to as a Central-Server Architecture. But to alleviate
the heavy demand on the processing for handling the large
player population and the objects in the game in real-time, a
MMOG is normally implemented using a distributed server
architecture where the game space is further sub-divided
into regions so that each region has relatively smaller 
number of players and objects that can be handled by a single
server. In other words, the different game regions are hosted
by different servers in a distributed fashion. When a player
moves out of one game region to another adjacent one, the
player must communicate with a different server (than it was
currently communicating with) hosting the new region. The
servers communicate with one another to hand off a player
or an object from one region to another. In this model, the
player on the client machine has to establish multiple 
gaming sessions with different servers so that it can roam in the
entire game space.
We propose a communication proxy based architecture where
a player connects to a (geographically) nearby proxy instead
of connecting to a central server in the case of a 
centralserver architecture or to one of the servers in case of 
dis2 The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006
tributed server architecture. In the proposed architecture,
players who are close by geographically join a particular
proxy. The proxy then connects to one or more game servers,
as needed by the set of players that connect to it and 
maintains persistent transport sessions with these server. This
alleviates the problem of each player having to connect 
directly to multiple game servers, which can add extra 
connection setup delay. Introduction of communication proxies
also mitigates the overhead of a large number of transport
sessions that must be managed and reduces required network
bandwidth [9] and processing at the game servers both with
central server and distributed server architectures. With
central server architectures, communication proxies reduce
the overhead at the server by not requiring the server to 
terminate persistent transport sessions from every one of the
clients. With distributed-server architectures, additionally,
communication proxies eliminate the need for the clients to
maintain persistent transport sessions to every one of the
servers. Figure 1 shows the proposed architecture.
Figure 1: Architecture of the gaming environment.
Note that the communication proxies need not be cognizant
of the game. They host a number of players and inform the
servers which players are hosted by the proxy in question.
Also note that the players hosted by a proxy may not be in
the same game space. That is, a proxy hosts players that
are geographically close to it, but the players themselves
can reside in different parts of the game space. The proxy
communicates with the servers responsible for maintaining
the game spaces subscribed by the different players. The
proxies communicate with one another in a peer-to-peer to
fashion. The responsiveness of the game can be improved
for updates that do not need to wait on processing at a
central authority. In this way, information about players can
be disseminated faster before even the game server gets to
know about it. This definitely improves the responsiveness
of the game. However, it ignores consistency that is critical
in MMORPGs. The notion that an architecture such as this
one can still maintain temporal consistency will be discussed
in detail in Section 3.
Figure 2 shows and example of the working principle of the
proposed architecture. Assume that the game space is 
divided into 9 regions and there are three servers responsible
for managing the regions. Server S1 owns regions 1 and 2,
S2 manages 4, 5, 7, and 8, and S3 is responsible for 3, 6 and
9.
Figure 2: An example.
There are four communication proxies placed in 
geographically distant locations. Players a, b, c join proxy P1, proxy P2
hosts players d, e, f, players g, h are with proxy P3, whereas
players i, j, k, l are with proxy P4. Underneath each player,
the figure shows which game region the player is located
currently. For example, players a, b, c are in regions 1, 2, 6,
respectively. Therefore, proxy P1 must communicate with
servers S1 and S3. The reader can verify the rest of the links
between the proxies and the servers.
Players can move within the region and between regions.
Player movement within a region will be tracked by the
proxy hosting the player and this movement information
(for example, the player"s new coordinates) will be 
multicast to a subset of other relevant communication proxies
directly. At the same time, this information will be sent
to the server responsible for that region with the indication
that this movement has already been communicated to all
the other relevant communication proxies (so that the server
does not have to relay this information to all the proxies).
For example, if player a moves within region 1, this 
information will be communicated by proxy P1 to server S1 and
multicast to proxies P3 and P4. Note that proxies that do
not keep state information about this region at this point
in time (because they do not have any clients within that
region) such as P2 do not have to receive this movement
information.
If a player is at the boundary of a region and moves into
a new region, there are two possibilities. The first 
possibility is that the proxy hosting the player can identify the
region into which the player is moving (based on the 
trajectory information) because it is also maintaining state 
information about the new region at that point in time. In
this case, the proxy can update movement information 
directly at the other relevant communication proxies and also
send information to the appropriate server informing of the
movement (this may require handoff between servers as we
will describe). Consider the scenario where player a is at
the boundary of region 1 and proxy P1 can identify that the
player is moving into region 2. Because proxy P1 is currently
keeping state information about region 2, it can inform all
The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 3
the other relevant communication proxies (in this example,
no other proxy maintains information about region 2 at this
point and so no update needs to be sent to any of the other
proxies) about this movement and then inform the server 
independently. In this particular case, server S1 is responsible
for region 2 as well and so no handoff between servers would
be needed. Now consider another scenario where player j
moves from region 9 to region 8 and that proxy P4 is able
to identify this movement. Again, because proxy P4 
maintains state information about region 8, it can inform any
other relevant communication proxies (again, none in this
example) about this movement. But now, regions 9 and 8
are managed by different servers (servers S3 and S2 
respectively) and thus a hand-off between these servers is needed.
We propose that in this particular scenario, the handoff be
managed by the proxy P4 itself. When the proxy sends
movement update to server S3 (informing the server that
the player is moving out of its region), it would also send
a message to server S2 informing the server of the presence
and location of the player in one of its region.
In the intra-region and inter-region scenarios described above,
the proxy is able to manage movement related information,
update only the relevant communication proxies about the
movement, update the servers with the movement and 
enable handoff of a player between the servers if needed. In
this way, the proxy performs movement updates without 
involving the servers in any way in this time-critical function
thereby speeding up the game and improving game 
playing experience for the players. We consider this the fast
path for movement update. We envision the proxies to be
just communication proxies in that they do not know about
the workings of specific games. They merely process 
movement information of players and objects and communicate
this information to the other proxies and the servers. If the
proxies are made more intelligent in that they understand
more of the game logic, it is possible for them to quickly
check on claims made by the clients and mitigate cheating.
The servers could perform the same functionality but with
more delay. Even without being aware of game logic, the
proxies can provide additional functionalities such as 
timestamping messages to make the game playing experience
more accurate [10] and fair [11].
The second possibility that should be considered is when
players move between regions. It is possible that a player
moves from one region to another but the proxy that is
hosting the player is not able to determine the region into
which the player is moving, a) the proxy does not 
maintain state information about all the regions into which the
player could potentially move, or b) the proxy is not able
to determine which region the player may move into (even if
maintains state information about all these regions). In this
case, we propose that the proxy be not responsible for 
making the movement decision, but instead communicate the
movement indication to the server responsible for the region
within which the player is currently located. The server will
then make the movement decision and then a) inform all
the proxies including the proxy hosting the player, and b)
initiate handoff with another server if the player moves into
a region managed by another server. We consider this the
slow path for movement update in that the servers need
to be involved in determining the new position of the player.
In the example, assume that player a moves from region 1
to region 4. Proxy P1 does not maintain state information
about region 4 and thus would pass the movement 
information to server S1. The server will identify that the player
has moved into region 4 and would inform proxy P1 as well
as proxy P2 (which is the only other proxy that maintains
information about region 4 at this point in time). Server S1
will also initiate a handoff of player a with server S2. Proxy
P1 will now start maintaining state information about 
region 4 because one of its hosted players, player a has moved
into this region. It will do so by requesting and receiving
the current state information about region 4 from server S2
which is responsible for this region.
Thus, a proxy architecture allows us to make use of faster
movement updates through the fast path through a proxy if
and when possible as opposed to conventional server-based
architectures that always have to use the slow path through
the server for movement updates. By selectively maintaining
relevant regional game state information at the proxies, we
are able to achieve this capability in our architecture without
the need for maintaining the complete game state at every
proxy.
3. ASSIGNMENT OF AUTHORITY
As a MMOG is played, the players and the game objects that
are part of the game, continually change their state. For 
example, consider a player who owns a tank in a battlefield
game. Based on action of the player, the tank changes its
position in the game space, the amount of ammunition the
tank contains changes as it fires at other tanks, the tank 
collects bonus firing power based on successful hits, etc. 
Similarly objects in the battlefield, such as flags, buildings etc.
change their state when a flag is picked up by a player (i.e.
tank) or a building is destroyed by firing at it. That is,
some decision has to be made on the state of each player
and object as the game progresses. Note that the state of
a player and/or object can contain several parameters (e.g.,
position, amount of ammunition, fuel storage, points 
collected, etc), and if any of the parameters changes, the state
of the player/object changes.
In a client-server based game, the server controls all the
players and the objects. When a player at a client machine
makes a move, the move is transmitted to the server over
the network. The server then analyzes the move, and if
the move is a valid one, changes the state of the player at
the server and informs the client of the change. The client
subsequently updates the state of the player and renders
the player at the new location. In this case the authority to
change the state of the player resides with the server entirely
and the client simply follows what the server instructs it to
do.
Most of the current first person shooter (FPS) games and
role playing games (RPG) fall under this category. In 
current FPS games, much like in RPG games, the client is not
trusted. All moves and actions that it makes are validated.
If a client detects that it has hit another player with a bullet,
it proceeds assuming that it is a hit. Meanwhile, an update
is sent to the server and the server will send back a message
either affirming or denying that the player was hit. If the
remote player was not hit, then the client will know that it
4 The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006
did not actually make the shot. If it did make the hit, an
update will also be sent from the server to the other clients
informing them that the other player was hit. A difference
that occurs in some RPGs is that they use very dumb client
programs. Some RPGs do not maintain state information
at the client and therefore, cannot predict anything such as
hits at the client. State information is not maintained 
because the client is not trusted with it. In RPGs, a cheating
player with a hacked game client can use state information
stored at the client to gain an advantage and find things
such as hidden treasure or monsters lurking around the 
corner. This is a reason why most MMORPGs do not send a
lot of state information to the client and causes the game
to be less responsive and have lower interaction game-play
than FPS games.
In a peer-to-peer game, each peer controls the player and
object that it owns. When a player makes a move, the
peer machine analyzes the move and if it is a valid one,
changes the state of the player and places the player in new
position. Afterwards, the owner peer informs all other peers
about the new state of the player and the rest of the peers
update the state of the player. In this scenario, the authority
to change the state of the player is given to the owning peer
and all other peers simply follow the owner.
For example, Battle Zone Flag (BzFlag) [12] is a 
multiplayer client-server game where the client has all authority
for making decisions. It was built primarily with LAN play
in mind and cheating as an afterthought. Clients in BzFlag
are completely authoritative and when they detect that they
were hit by a bullet, they send an update to the server which
simply forwards the message along to all other players. The
server does no sort of validation.
Each of the above two traditional approaches has its own set
of advantages and disadvantages. The first approach, which
we will refer to as server authoritative henceforth, uses a
centralized method to assign authority. While a centralized
approach can keep the state of the game (i.e., state of all the
players and objects) consistent across any number of client
machines, it suffers from delayed response in game-play as
any move that a player at the client machine makes must go
through one round-trip delay to the server before it can take
effect on the client"s screen. In addition to the round-trip 
delay, there is also queuing delay in processing the state change
request at the server. This can result in additional 
processing delay, and can also bring in severe scalability problems
if there are large number of clients playing the game. One
definite advantage of the server authoritative approach is
that it can easily detect if a client is cheating and can take
appropriate action to prevent cheating.
The peer-to-peer approach, henceforth referred to as client
authoritative, can make games very responsive. However,
it can make the game state inconsistent for a few players
and tie break (or roll back) has to be performed to bring the
game back to a consistent state. Neither tie break nor roll
back is a desirable feature of online gaming. For example,
assume that for a game, the goal of each player is to collect
as many flags as possible from the game space (e.g. BzFlag).
When two players in proximity try to collect the same flag
at the same time, depending on the algorithm used at the
client-side, both clients may determine that it is the winner,
although in reality only one player can pick the flag up. Both
players will see on their screen that it is the winner. This
makes the state of the game inconsistent. Ways to recover
from this inconsistency are to give the flag to only one player
(using some tie break rule) or roll the game back so that the
players can try again. Neither of these two approaches is
a pleasing experience for online gaming. Another problem
with client authoritative approach is that of cheating by
clients as there is no cross checking of the validation of the
state changes authorized by the owner client.
We propose to use a hybrid approach to assign the authority
dynamically between the client and the server. That is, we
assign the authority to the client to make the game 
responsive, and use the server"s authority only when the client"s
individual authoritative decisions can make the game state
inconsistent. By moving the authority of time critical 
updates to the client, we avoid the added delay caused by 
requiring the server to validate these updates. For example,
in the flag pickup game, the clients will be given the 
authority to pickup flags only when other players are not within
a range that they could imminently pickup a flag. Only
when two or more players are close by so that more than
one player may claim to have picked up a flag, the authority
for movement and flag pickup would go to the central server
so that the game state does not become inconsistent. We
believe that in a large game-space where a player is often
in a very wide open and sparsely populated area such as
those often seen in the game Second Life [13], this hybrid
architecture would be very beneficial because of the long 
periods that the client would have authority to send movement
updates for itself. This has two advantages over the 
centralauthority approach, it distributes the processing load down
to the clients for the majority of events and it allows for a
more responsive game that does not need to wait on a server
for validation.
We believe that our notion of authority can be used to 
develop a globally consistent state model of the evolution of
a game. Fundamentally, the consistent state of the system
is the one that is defined by the server. However, if local
authority is delegated to the client, in this case, the client"s
state is superimposed on the server"s state to determine the
correct global state. For example, if the client is 
authoritative with respect to movement of a player, then the 
trajectory of the player is the true trajectory and must 
replace the server"s view of the player"s trajectory. Note that
this could be problematic and lead to temporal 
inconsistency only if, for example, two or more entities are moving
in the same region and can interact with each other. In
this situation, the client authority must revert to the server
and the sever would then make decisions. Thus, the client
is only authoritative in situations where there is no 
potential to imminently interact with other players. We believe
that in complex MMOGs, when allowing more rapid 
movement, it will still be the case that local authority is possible
for significant spans of game time. Note that it might also
be possible to minimize the occurrences of the Dead Man
Shooting problem described in [14]. This could be done by
allowing the client to be authoritative for more actions such
as its player"s own death and disallowing other players from
making preemptive decisions based on a remote player.
The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 5
One reason why the client-server based architecture has gained
popularity is due to belief that the fastest route to the other
clients is through the server. While this may be true, we aim
to create a new architecture where decisions do not always
have to be made at the game server and the fastest route to
a client is actually through a communication proxy located
close to the client. That is, the shortest distance in our 
architecture is not through the game server but through the
communication proxy. After a client makes an action such
as movement, it will simultaneously distribute it directly to
the clients and the game server by way of the 
communications proxy. We note that our architecture however is not
practical for a game where game players setup their own
servers in an ad-hoc fashion and do not have access to 
proxies at the various ISPs. This proxy and distributed authority
architecture can be used to its full potential only when the
proxies can be placed at strategic places within the main
ISPs and evenly distributed geographically.
Our game architecture does not assume that the client is
not to be trusted. We are designing our architecture on the
fact that there will be sufficient cheat deterring and 
detection mechanisms present so that it will be both undesirable
and very difficult to cheat [15]. In our proposed approach,
we can make the games cheat resilient by using the 
proxybased architecture when client authoritative decisions take
place. In order to achieve this, the proxies have to be game
cognizant so that decisions made by a client can be cross
checked by a proxy that the client connects to. For 
example, assume that in a game a plane controlled by a client
moves in the game space. It is not possible for the plane to
go through a building unharmed. In a client authoritative
mode, it is possible for the client to cheat by maneuvering
the plane through a building and claiming the plane to be
unharmed. However, when such move is published by the
client, the proxy, being aware of the game space that the
plane is in, can quickly check that the client has misused
the authority and then can block such move. This allows us
to distribute authority to make decisions about the clients.
In the following section we use a multiplayer game called
RPGQuest to implement different authoritative schemes and
discuss our experience with the implementation. Our 
implementation shows the viability of our proposed solution.
4. IMPLEMENTATION EXPERIENCE
We have experimented with the authority assignment 
mechanism described in the last section by implementing the
mechanisms in a game called RPGQuest. A screen shot from
this game is shown in Figure 3. The purpose of the 
implementation is to test its feasibility in a real game. RPGQuest
is a basic first person game where the player can move
around a three dimensional environment. Objects are placed
within the game world and players gain points for each 
object that is collected. The game clients connect to a game
server which allows many players to coexist in the same
game world. The basic functionality of this game is 
representative of current online first person shooter and role playing
games. The game uses the DirectX 8 graphics API and 
DirectPlay networking API. In this section we will discuss the
three different versions of the game that we experimented
with.
Figure 3: The RPGQuest Game.
The first version of the game, which is the original 
implementation of RPGQuest, was created with a completely 
authoritative server and a non-authoritative client. Authority
given to the server includes decisions of when a player 
collides with static objects and other players and when a player
picks up an object. This version of the game performs well
up to 100ms round-trip latency between the client and the
server. There is little lag between the time player hits a
wall and the time the server corrects the player"s position.
However, as more latency is induced between the client and
server, the game becomes increasingly difficult to play. With
the increased latency, the messages coming from the server
correcting the player when it runs into a wall are not 
received fast enough. This causes the player to pass through
the wall for the period that it is waiting for the server to
resolve the collision.
When studying the source code of the original version of
the RPGQuest game, there is a substantial delay that is
unavoidable each time an action must be validated by the
server. Whenever a movement update is sent to the server,
the client must then wait whatever the round trip delay is,
plus some processing time at the server in order to receive
its validated or corrected position. This is obviously 
unacceptable in any game where movement or any other rapidly
changing state information must be validated and 
disseminated to the other clients rapidly.
In order to get around this problem, we developed a second
version of the game, which gives all authority to the client.
The client was delegated the authority to validate its own
movement and the authority to pick up objects without 
validation from the server. In this version of the game when
a player moves around the game space, the client validates
that the player"s new position does not intersect with any
walls or static objects. A position update is then sent to the
server which then immediately forwards the update to the
other clients within the region. The update does not have
to go through any extra processing or validation.
This game model of complete authority given to the client
is beneficial with respect to movement. When latencies of
6 The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006
100ms and up are induced into the link between the client
and server, the game is still playable since time critical 
aspects of the game like movement do not have to wait on a
reply from the server. When a player hits a wall, the 
collision is processed locally and does not have to wait on the
server to resolve the collision.
Although game playing experience with respect to 
responsiveness is improved when the authority for movement is
given to the client, there are still aspects of games that do
not benefit from this approach. The most important of these
is consistency. Although actions such as movement are time
critical, other actions are not as time critical, but instead
require consistency among the player states. An example of
a game aspect that requires consistency is picking up objects
that should only be possessed by a single player.
In our client authoritative version of RPGQuest clients send
their own updates to all other players whenever they pick up
an object. From our tests we have realized this is a problem
because when there is a realistic amount of latency between
the client and server, it is possible for two players to pick
up the same object at the same time. When two players
attempt to pick up an object at physical times which are
close to each other, the update sent by the player who picked
up the object first will not reach the second player in time
for it to see that the object has already been claimed. The
two players will now both think that they own the object.
This is why a server is still needed to be authoritative in this
situation and maintain consistency throughout the players.
These two versions of the RPGQuest game has showed us
why it is necessary to mix the two absolute models of 
authority. It is better to place authority on the client for quickly
changing actions such as movement. It is not desirable to
have to wait for server validation on a movement that could
change before the reply is even received. It is also sometimes
necessary to place consistency over efficiency in aspects of
the game that cannot tolerate any inconsistencies such as
object ownership. We believe that as the interactivity of
games increases, our architecture of mixed authority that
does not rely on server validation will be necessary.
To test the benefits and show the feasibility of our 
architecture of mixed authority, we developed a third version of
the RPGQuest game that distributed authority for 
different actions between the client and server. In this version,
in the interest of consistency, the server remained 
authoritative for deciding who picked up an object. The client
was given full authority to send positional updates to other
clients and verify its own position without the need to 
verify its updates with the server. When the player tries to
move their avatar, the client verifies that the move will not
cause it to move through a wall. A positional update is then
sent to the server which then simply forwards it to the other
clients within the region. This eliminates any extra 
processing delay that would occur at the server and is also a more
accurate means of verification since the client has a more
accurate view of its own state than the server.
This version of the RPGQuest game where authority is 
distributed between the client and server is an improvement
from the server authoritative version. The client has no 
delay in waiting for an update for its own position and other
clients do not have to wait on the server to verify the update.
The inconsistencies where two clients can pick up the same
object in the client authoritative architecture are not present
in this version of the client. However, the benefits of mixed
authority will not truly be seen until an implementation of
our communication proxy is integrated into the game. With
the addition of the communication proxy, after the client
verifies its own positional updates it will be able to send the
update to all clients within its region through a low latency
link instead of having to first go through the game server
which could possibly be in a very remote location.
The coding of the different versions of the game was very
simple. The complexity of the client increased very slightly
in the client authoritative and hybrid models. The 
original dumb clients of RPGQuest know the position of other
players; it is not just sent a screen snapshot from the server.
The server updates each client with the position of all nearby
clients. The dumb clients use client side prediction to fill
in the gaps between the updates they receive. The only 
extra processing the client has to do in the hybrid architecture
is to compare its current position to the positions of all 
objects (walls, boxes, etc.) in its area. This obviously means
that each client will have to already have downloaded the
locations of all static objects within its current region.
5. RELATED WORK
It has been noted that in addition to latency, bandwidth
requirements also dictate the type of gaming architecture to
be used. In [16], different types of architectures are 
studied with respect to bandwidth efficiencies and latency. It is
pointed out that Central Server architectures are not 
scalable because of bandwidth requirements at the server but
the overhead for consistency checks are limited as they are
performed at the server. A Peer-to-Peer architecture, on the
other hand, is scalable but there is a significant overhead
for consistency checks as this is required at every player.
The paper proposes a hybrid architecture which is 
Peer-toPeer in terms of message exchange (and thereby is scalable)
where a Central Server is used for off-line consistency checks
(thereby mitigating consistency check overhead). The paper
provides an implementation example of BZFlag which is a
peer-to-peer game which is modified to transfer all 
authority to a central server. In essence, this paper advocates an
authority architecture which is server based even for 
peerto-peer games, but does not consider division of authority
between a client and a server to minimize latency which
could affect game playing experience even with the type of
latency found in server based games (where all authority is
with the server).
There is also previous work that has suggested that proxy
based architectures be used to alleviate the latency 
problem and in addition use proxies to provide congestion 
control and cheat-proof mechanisms in distributed multi-player
games [17]. In [18], a proxy server-network architecture is
presented that is aimed at improving scalability of 
multiplayer games and lowering latency in server-client data 
transmission. The main goal of this work is to improve scalability
of First-Person Shooter (FPS) and RPG games. The further
objective is to improve the responsiveness MMOGs by 
providing low latency communications between the client and
The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 7
server. The architecture uses interconnected proxy servers
that each have a full view of the global game state. Proxy
servers are located at various different ISPs. It is mentioned
in this work that dividing the game space among multiple
games servers such as the federated model presented in [19]
is inefficient for a relatively fast game flow and that the
proposed architecture alleviates this problem because users
do not have to connect to a different server whenever they
cross the server boundary. This architecture still requires all
proxies to be aware of the overall game state over the whole
game space unlike our work where we require the proxies
to maintain only partial state information about the game
space.
Fidelity based agent architectures have been proposed in [20,
21]. These works propose a distributed client-server 
architecture for distributed interactive simulations where 
different servers are responsible for different portions of the game
space. When an object moves from one portion to another,
there is a handoff from one server to another. Although
these works propose an architecture where different portions
of the simulation space are managed by different servers,
they do not address the issue of decreasing the bandwidth
required through the use of communication proxies.
Our work differs from the above discussed previous works by
proposing a) a distributed proxy-based architecture to 
decrease bandwidth requirements at the clients and the servers
without requiring the proxies to keep state information about
the whole game space, b) a dynamic authority assignment
technique to reduce latency (by performing consistency checks
locally at the client whenever possible) by splitting the 
authority between the clients and servers on a per object basis,
and c) proposing that cheat detection can be built into the
proxies if they are provided more information about the 
specific game instead of using them purely as communication
proxies (although this idea has not been implemented yet
and is part of our future work).
6. CONCLUSIONS AND FUTURE WORK
In this paper, we first proposed a proxy-based 
architecture for MMOGs that enables MMOGs to scale to a large
number of users by mitigating the need for a large 
number of transport sessions to be maintained and decreasing
both bandwidth overhead and latency of event update. 
Second, we proposed a mixed authority assignment mechanism
that divides authority for making decisions on actions and
events within the game between the clients and server and
argued how such an authority assignment leads to better
game playing experience without sacrificing the consistency
of the game. Third, to validate the viability of the mixed
authority assignment mechanism, we implemented it within
a MMOG called RPGQuest and described our 
implementation experience.
In future work, we propose to implement the 
communications proxy architecture described in this paper and 
integrate the mixed authority mechanism within this 
architecture. We propose to evaluate the benefits of the proxy-based
architecture in terms of scalability, accuracy and 
responsiveness. We also plan to implement a version of the RPGQuest
game with dynamic assignment of authority to allow players
the authority to pickup objects when no other players are
near. As discussed earlier, this will allow for a more efficient
and responsive game in certain situations and alleviate some
of the processing load from the server.
Also, since so much trust is put into the clients of our 
architecture, it will be necessary to integrate into the 
architecture many of the cheat detection schemes that have been
proposed in the literature. Software such as Punkbuster [22]
and a reputation system like those proposed by [23] and [15]
would be integral to the operation of an architecture such as
ours which has a lot of trust placed on the client. We further
propose to make the proxies in our architecture more game
cognizant so that cheat detection mechanisms can be built
into the proxies themselves.
7. REFERENCES
[1] Y. W. Bernier. Latency Compensation Methods in
Client/Server In-game Protocol Design and
Optimization. In Proc. of Game Developers
Conference"01, 2001.
[2] Lothar Pantel and Lars C. Wolf. On the impact of
delay on real-time multiplayer games. In NOSSDAV
"02: Proceedings of the 12th international workshop on
Network and operating systems support for digital
audio and video, pages 23-29, New York, NY, USA,
2002. ACM Press.
[3] G. Armitage. Sensitivity of Quake3 Players to Network
Latency. In Proc. of IMW2001, Workshop Poster
Session, November 2001. http://www.geocities.com/
gj armitage/q3/quake-results.html.
[4] Tobias Fritsch, Hartmut Ritter, and Jochen Schiller.
The effect of latency and network limitations on
mmorpgs: a field study of everquest2. In NetGames
"05: Proceedings of 4th ACM SIGCOMM workshop on
Network and system support for games, pages 1-9,
New York, NY, USA, 2005. ACM Press.
[5] Tom Beigbeder, Rory Coughlan, Corey Lusher, John
Plunkett, Emmanuel Agu, and Mark Claypool. The
effects of loss and latency on user performance in
unreal tournament 2003. In NetGames "04:
Proceedings of 3rd ACM SIGCOMM workshop on
Network and system support for games, pages
144-151, New York, NY, USA, 2004. ACM Press.
[6] Y. Lin, K. Guo, and S. Paul. Sync-MS: Synchronized
Messaging Service for Real-Time Multi-Player
Distributed Games. In Proc. of 10th IEEE
International Conference on Network Protocols
(ICNP), Nov 2002.
[7] Katherine Guo, Sarit Mukherjee, Sampath
Rangarajan, and Sanjoy Paul. A fair message
exchange framework for distributed multi-player
games. In NetGames "03: Proceedings of the 2nd
workshop on Network and system support for games,
pages 29-41, New York, NY, USA, 2003. ACM Press.
[8] T. Barron. Multiplayer Game Programming, chapter
16-17, pages 672-731. Prima Tech"s Game
Development Series. Prima Publishing, 2001.
8 The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006
[9] Carsten Griwodz and P˚al Halvorsen. The fun of using
tcp for an mmorpg. In NOSSDAV "06: Proceedings of
the International Workshop on Network and Operating
Systems Support for Digital Audio and VIdeo, New
York, NY, USA, 2006. ACM Press.
[10] Sudhir Aggarwal, Hemant Banavar, Amit Khandelwal,
Sarit Mukherjee, and Sampath Rangarajan. Accuracy
in dead-reckoning based distributed multi-player
games. In NetGames "04: Proceedings of 3rd ACM
SIGCOMM workshop on Network and system support
for games, pages 161-165, New York, NY, USA, 2004.
ACM Press.
[11] Sudhir Aggarwal, Hemant Banavar, Sarit Mukherjee,
and Sampath Rangarajan. Fairness in dead-reckoning
based distributed multi-player games. In NetGames
"05: Proceedings of 4th ACM SIGCOMM workshop on
Network and system support for games, pages 1-10,
New York, NY, USA, 2005. ACM Press.
[12] Riker, T. et al. Bzflag. http://www.bzflag.org,
2000-2006.
[13] Linden Lab. Second life. http://secondlife.com,
2003.
[14] Martin Mauve. How to keep a dead man from
shooting. In IDMS "00: Proceedings of the 7th
International Workshop on Interactive Distributed
Multimedia Systems and Telecommunication Services,
pages 199-204, London, UK, 2000. Springer-Verlag.
[15] Max Skibinsky. Massively Multiplayer Game
Development 2, chapter The Quest for Holy 
ScalePart 2: P2P Continuum, pages 355-373. Charles River
Media, 2005.
[16] Joseph D. Pellegrino and Constantinos Dovrolis.
Bandwidth requirement and state consistency in three
multiplayer game architectures. In NetGames "03:
Proceedings of the 2nd workshop on Network and
system support for games, pages 52-59, New York,
NY, USA, 2003. ACM Press.
[17] M. Mauve J. Widmer and S. Fischer. A Generic Proxy
Systems for Networked Computer Games. In Proc. of
the Workshop on Network Games, Netgames 2002,
April 2002.
[18] S. Gorlatch J. Muller, S. Fischer and M.Mauve. A
Proxy Server Network Architecture for Real-Time
Computer Games. In Euor-Par 2004 Parallel
Processing: 10th International EURO-PAR
Conference, August-September 2004.
[19] H. Hazeyama T. Limura and Y. Kadobayashi. Zoned
Federation of Game Servers: A Peer-to-Peer Approach
to Scalable Multiplayer On-line Games. In Proc. of
ACM Workshop on Network Games, Netgames 2004,
August-September 2004.
[20] B. Kelly and S. Aggarwal. A Framework for a Fidelity
Based Agent Architecture for Distributed Interactive
Simulation. In Proc. 14th Workshop on Standards for
Distributed Interactive Simulation, pages 541-546,
March 1996.
[21] S. Aggarwal and B. Kelly. Hierarchical Structuring for
Distributed Interactive Simulation. In Proc. 13th
Workshop on Standards for Distributed Interactive
Simulation, pages 125-132, Sept 1995.
[22] Even Balance, Inc. Punkbuster.
http://www.evenbalance.com/, 2001-2006.
[23] Y. Wang and J. Vassileva. Trust and Reputation
Model in Peer-to-Peer Networks. In Third
International Conference on Peer-to-Peer Computing,
2003.
The 5th Workshop on Network & System Support for Games 2006 - NETGAMES 2006 9
Composition of a DIDS by Integrating Heterogeneous IDSs
on Grids
Paulo F. Silva and Carlos B. Westphall and Carla M. Westphall
Network and Management Laboratory
Department of Computer Science and Statistics
Federal University of Santa Catarina, Florianópolis, Brazil
Marcos D. Assunção
Grid Computing and Distributed Systems Laboratory and NICTA Victoria Laboratory
Department of Computer Science and Software Engineering
The University of Melbourne, Victoria, 3053, Australia
{paulo,westphal,assuncao,carla}@lrg.ufsc.br
ABSTRACT
This paper considers the composition of a DIDS (Distributed
Intrusion Detection System) by integrating heterogeneous IDSs
(Intrusion Detection Systems). A Grid middleware is used for this
integration. In addition, an architecture for this integration is
proposed and validated through simulation.
Categories and Subject Descriptors
C.2.4 [Distributed Systes]: Client/Server, Distributed
Applications.
1. INTRODUCTION
Solutions for integrating heterogeneous IDSs (Intrusion Detection
Systems) have been proposed by several groups [6],[7],[11],[2].
Some reasons for integrating IDSs are described by the IDWG
(Intrusion Detection Working Group) from the IETF (Internet
Engineering Task Force) [12] as follows:
• Many IDSs available in the market have strong and weak
points, which generally make necessary the deployment of
more than one IDS to provided an adequate solution.
• Attacks and intrusions generally originate from multiple
networks spanning several administrative domains; these
domains usually utilize different IDSs. The integration of
IDSs is then needed to correlate information from multiple
networks to allow the identification of distributed attacks and
or intrusions.
• The interoperability/integration of different IDS components
would benefit the research on intrusion detection and speed
up the deployment of IDSs as commercial products.
DIDSs (Distributed Intrusion Detection Systems) therefore started
to emerge in early 90s [9] to allow the correlation of intrusion
information from multiple hosts, networks or domains to detect
distributed attacks. Research on DIDSs has then received much
interest, mainly because centralised IDSs are not able to provide
the information needed to prevent such attacks [13].
However, the realization of a DIDS requires a high degree of
coordination. Computational Grids are appealing as they enable
the development of distributed application and coordination in a
distributed environment. Grid computing aims to enable
coordinate resource sharing in dynamic groups of individuals
and/or organizations. Moreover, Grid middleware provides means
for secure access, management and allocation of remote resources;
resource information services; and protocols and mechanisms for
transfer of data [4].
According to Foster et al. [4], Grids can be viewed as a set of
aggregate services defined by the resources that they share. OGSA
(Open Grid Service Architecture) provides the foundation for this
service orientation in computational Grids. The services in OGSA
are specified through well-defined, open, extensible and 
platformindependent interfaces, which enable the development of
interoperable applications.
This article proposes a model for integration of IDSs by using
computational Grids. The proposed model enables heterogeneous
IDSs to work in a cooperative way; this integration is termed
DIDSoG (Distributed Intrusion Detection System on Grid). Each
of the integrated IDSs is viewed by others as a resource accessed
through the services that it exposes. A Grid middleware provides
several features for the realization of a DIDSoG, including [3]:
decentralized coordination of resources; use of standard protocols
and interfaces; and the delivery of optimized QoS (Quality of
Service).
The service oriented architecture followed by Grids (OGSA)
allows the definition of interfaces that are adaptable to different
platforms. Different implementations can be encapsulated by a
service interface; this virtualisation allows the consistent access to
resources in heterogeneous environments [3]. The virtualisation of
the environment through service interfaces allows the use of
services without the knowledge of how they are actually
implemented. This characteristic is important for the integration
of IDSs as the same service interfaces can be exposed by different
IDSs.
Grid middleware can thus be used to implement a great variety of
services. Some functions provided by Grid middleware are [3]: (i)
data management services, including access services, replication,
and localisation; (ii) workflow services that implement coordinate
execution of multiple applications on multiple resources; (iii)
auditing services that perform the detection of frauds or
intrusions; (iv) monitoring services which implement the
discovery of sensors in a distributed environment and generate
alerts under determined conditions; (v) services for identification
of problems in a distributed environment, which implement the
correlation of information from disparate and distributed logs.
These services are important for the implementation of a DIDSoG.
A DIDS needs services for the location of and access to
distributed data from different IDSs. Auditing and monitoring
services take care of the proper needs of the DIDSs such as:
secure storage, data analysis to detect intrusions, discovery of
distributed sensors, and sending of alerts. The correlation of
distributed logs is also relevant because the detection of
distributed attacks depends on the correlation of the alert
information generated by the different IDSs that compose the
DIDSoG.
The next sections of this article are organized as follows. Section
2 presents related work. The proposed model is presented in
Section 3. Section 4 describes the development and a case study.
Results and discussion are presented in Section 5. Conclusions
and future work are discussed in Section 6.
2. RELATED WORK
DIDMA [5] is a flexible, scalable, reliable, and 
platformindependent DIDS. DIDMA architecture allows distributed
analysis of events and can be easily extended by developing new
agents. However, the integration with existing IDSs and the
development of security components are presented as future work
[5]. The extensibility of DIDS DIDMA and the integration with
other IDSs are goals pursued by DIDSoG. The flexibility,
scalability, platform independence, reliability and security
components discussed in [5] are achieved in DIDSoG by using a
Grid platform.
More efficient techniques for analysis of great amounts of data in
wide scale networks based on clustering and applicable to DIDSs
are presented in [13]. The integration of heterogeneous IDSs to
increase the variety of intrusion detection techniques in the
environment is mentioned as future work [13] DIDSoG thus aims
at integrating heterogeneous IDSs [13].
Ref. [10] presents a hierarchical architecture for a DIDS;
information is collected, aggregated, correlated and analysed as it
is sent up in the hierarchy. The architecture comprises of several
components for: monitoring, correlation, intrusion detection by
statistics, detection by signatures and answers. Components in the
same level of the hierarchy cooperate with one another. The
integration proposed by DIDSoG also follows a hierarchical
architecture. Each IDS integrated to the DIDSoG offers
functionalities at a given level of the hierarchy and requests
functionalities from IDSs from another level. The hierarchy
presented in [10] integrates homogeneous IDSs whereas the
hierarchical architecture of DIDSoG integrates heterogeneous
IDSs.
There are proposals on integrating computational Grids and IDSs
[6],[7],[11],[2]. Ref. [6] and [7] propose the use of Globus
Toolkit for intrusion detection, especially for DoS (Denial of
Service) and DDoS (Distributed Denial of Service) attacks;
Globus is used due to the need to process great amounts of data to
detect these kinds of attack. A two-phase processing architecture
is presented. The first phase aims at the detection of momentary
attacks, while the second phase is concerned with chronic or
perennial attacks.
Traditional IDSs or DIDSs are generally coordinated by a central
point; a characteristic that leaves them prone to attacks. Leu et al.
[6] point out that IDSs developed upon Grids platforms are less
vulnerable to attacks because of the distribution provided for such
platforms. Leu et al. [6],[7] have used tools to generate several
types of attacks - including TCP, ICMP and UDP flooding - and
have demonstrated through experimental results the advantages of
applying computational Grids to IDSs.
This work proposes the development of a DIDS upon a Grid
platform. However, the resulting DIDS integrates heterogeneous
IDSs whereas the DIDSs upon Grids presented by Leu et al.
[6][7] do not consider the integration of heterogeneous IDSs. The
processing in phases [6][7] is also contemplated by DIDSoG,
which is enabled by the specification of several levels of
processing allowed by the integration of heterogeneous IDSs.
The DIDS GIDA (Grid Intrusion Detection Architecture) targets
at the detection of intrusions in a Grid environment [11]. GridSim
Grid simulator was used for the validation of DIDS GIDA.
Homogeneous resources were used to simplify the development
[11]. However, the possibility of applying heterogeneous
detection systems is left for future work
Another DIDS for Grids is presented by Choon and Samsudim
[2]. Scenarios demonstrating how a DIDS can execute on a Grid
environment are presented.
DIDSoG does not aim at detecting intrusions in a Grid
environment. In contrast, DIDSoG uses the Grid to compose a
DIDS by integrating specific IDSs; the resulting DIDS could
however be used to identify attacks in a Grid environment. Local
and distributed attacks can be detected through the integration of
traditional IDSs while attacks particular to Grids can be detected
through the integration of Grid IDSs.
3. THE PROPOSED MODEL
DIDSoG presents a hierarchy of intrusion detection services; this
hierarchy is organized through a two-dimensional vector defined
by Scope:Complexity. The IDSs composing DIDSoG can be
organized in different levels of scope or complexity, depending on
its functionalities, the topology of the target environment and
expected results.
Figure 1 presents a DIDSoG composed by different intrusion
detection services (i.e. data gathering, data aggregation, data
correlation, analysis, intrusion response and management)
provided by different IDSs. The information flow and the
relationship between the levels of scope and complexity are
presented in this figure.
Information about the environment (host, network or application)
is collected by Sensors located both in user 1"s and user 2"s
computers in domain 1. The information is sent to both simple
Analysers that act on the information from a single host (level
1:1), and to aggregation and correlation services that act on
information from multiple hosts from the same domain (level 2:1).
Simple Analysers in the first scope level send the information to
more complex Analysers in the next levels of complexity (level 1:
N). When an Analyser detects an intrusion, it communicates with
Countermeasure and Monitoring services registered to its scope.
An Analyser can invoke a Countermeasure service that replies to a
detected attack, or informs a Monitoring service about the
ongoing attack, so the administrator can act accordingly.
Aggregation and correlation resources in the second scope receive
information from Sensors from different users" computers (user
1"s and user 2"s) in the domain 1. These resources process the
received information and send it to the analysis resources
registered to the first level of complexity in the second scope
(level 2:1). The information is also sent to the aggregation and
correlation resources registered in the first level of complexity in
the next scope (level 3:1).
User 1
Domain 1
Analysers
Level 1:1
Local
Sensors
Analysers
Level 1:N
Aggreg.
Correlation
Level 2:1
User 2
Domain 1
Local
Sensors
Analysers
Level 2:1
Analysers
Level 2:N
Aggreg.
Correlation
Level 3:1
Domain 2
Monitor
Level 1
Monitor
Level 2
Analysers
Level 3:1
Analysers
Level 3:N
Monitor
Level 3
Response
Level 1
Response
Level 2
Response
Level 3
Fig. 1. How DIDSoG works.
The analysis resources in the second scope act like the analysis
resources in the first scope, directing the information to a more
complex analysis resource and putting the Countermeasure and
Monitoring resources in action in case of detected attacks.
Aggregation and correlation resources in the third scope receive
information from domains 1 and 2. These resources then carry out
the aggregation and correlation of the information from different
domains and send it to the analysis resources in the first level of
complexity in the third scope (level 3:1). The information could
also be sent to the aggregate service in the next scope in case of
any resources registered to such level.
The analysis resources in the third scope act similar to the analysis
resources in the first and second scopes, except that the analysis
resources in the third scope act on information from multiple
domains.
The functionalities of the registered resources in each of the
scopes and complexity level can vary from one environment to
another. The model allows the development of N levels of scope
and complexity.
Figure 2 presents the architecture of a resource participating in the
DIDSoG. Initially, the resource registers itself to GIS (Grid
Information Service) so other participating resources can query
the services provided. After registering itself, the resource
requests information about other intrusion detection resources
registered to the GIS.
A given resource of DIDSoG interacts with other resources, by
receiving data from the Source Resources, processing it, and
sending the results to the Destination Resources, therefore
forming a grid of intrusion detection resources.
Grid Resource
BaseNative
IDS
Grid Origin Resources
Grid Destination Resources
Grid Information Service
Descri
ptor
Connec
tor
Fig. 2. Architecture of a resource participating of the DIDSoG.
A resource is made up of four components: Base, Connector,
Descriptor and Native IDS. Native IDS corresponds to the IDS
being integrated to the DIDSoG. This component process the data
received from the Origin Resources and generates new data to be
sent to the Destination Resources. A Native IDS component can
be any tool processes information related to intrusion detection,
including analysis, data gathering, data aggregation, data
correlation, intrusion response or management.
The Descriptor is responsible for the information that identifies a
resource and its respective Destination Resources in the DIDSoG.
Figure 3 presents the class diagram of the stored information by
the Descriptor. The ResourceDescriptor class has Feature, Level,
DataType and Target Resources type members. Feature class
represents the functionalities that a resource has. Type, name and
version attributes refer to the functions offered by the Native IDS
component, its name and version, respectively. Level class
identifies the level of target and complexity in which the resource
acts. DataType class represents the data format that the resource
accepts to receive. DataType class is specialized by classes Text,
XML and Binary. Class XML contains the DTDFile attribute to
specify the DTD file that validates the received XML.
-ident
-version
-description
ResourceDescriptor
-featureType
-name
-version
Feature
1
1
-type
-version
DataType
-escope
-complex
Level
1
1
Text Binary
-DTDFile
XML
1
1
TargetResources
1
1 10..*
-featureType
Resource11
1
1
Fig. 3. Class Diagram of the Descriptor component.
TargetResources class represents the features of the Destination
Resources of a determined resource. This class aggregates
Resource. The Resource class identifies the characteristics of a
Destination Resource. This identification is made through the
featureType attribute and the Level and DataType classes.
A given resource analyses the information from Descriptors from
other resources, and compares this information with the
information specified in TargetResources to know to which
resources to send the results of its processing.
The Base component is responsible for the communication of a
resource with other resources of the DIDSoG and with the Grid
Information Service. It is this component that registers the
resource and the queries other resources in the GIS.
The Connector component is the link between Base and Native
IDS. The information that Base receives from Origin Resources is
passed to Connector component. The Connector component
performs the necessary changes in the data so that it is understood
by Native IDS, and sends this data to Native IDS for processing.
The Connector component also has the responsibility of collecting
the information processed by Native IDS, and making the
necessary changes so the information can pass through the
DIDSoG again. After these changes, Connector sends the
information to the Base, which in turn sends it to the Destination
Resources in accordance with the specifications of the Descriptor
component.
4. IMPLEMENTATION
We have used GridSim Toolkit 3 [1] for development and
evaluation of the proposed model. We have used and extended
GridSim features to model and simulate the resources and
components of DIDSoG.
Figure 4 presents the Class diagram of the simulated DIDSoG.
The Simulation_DIDSoG class starts the simulation components.
The Simulation_User class represents a user of DIDSoG. This
class" function is to initiate the processing of a resource Sensor,
from where the gathered information will be sent to other
resources. DIDSoG_GIS keeps a registry of the DIDSoG
resources.The DIDSoG_BaseResource class implements the Base
component (see Figure 2). DIDSoG_BaseResource interacts with
DIDSoG_Descriptor class, which represents the Descriptor
component. The DIDSoG_Descriptor class is created from an
XML file that specifies a resource descriptor (see Figure 3).
DIDSoG_BaseResource
DIDSoG_Descriptor
11
DIDSoG_GIS
Simulation_User
Simulation_DIDSoG
1
*1*
1
1
GridInformationService
GridSim GridResource
Fig. 4. Class Diagram of the simulatated DIDSoG.
A Connector component must be developed for each Native IDS
integrated to DIDSoG. The Connector component is implemented
by creating a class derived from DIDSoG_BaseResource. The new
class will implement new functionalities in accordance with the
needs of the corresponding Native IDS.
In the simulation environment, data collection resources, analysis,
aggregation/correlation and generation of answers were
integrated. Classes were developed to simulate the processing of
each Native IDS components associated to the resources. For each
simulated Native IDS a class derived from
DIDSoG_BaseResource was developed. This class corresponds to
the Connector component of the Native IDS and aims at the
integrating the IDS to DIDSoG.
A XML file describing each of the integrated resources is chosen
by using the Connector component. The resulting relationship
between the resources integrated to the DIDSoG, in accordance
with the specification of its respective descriptors, is presented in
Figure 5.
The Sensor_1 and Sensor_2 resources generate simulated data in
the TCPDump [8] format. The generated data is directed to
Analyser_1 and Aggreg_Corr_1 resources, in the case of
Sensor_1, and to Aggreg_Corr_1 in the case of Sensor_2,
according to the specification of their descriptors.
User_1
Analyser_
1
Level 1:1
Sensor_1
Aggreg_
Corr_1
Level 2:1
User_2
Sensor_2
Analyser_2
Level 2:1
Analyser_3
Level 2:2
TCPDump
TCPDump
TCPDumpAg
TCPDumpAg
IDMEF
IDMEF
IDMEF
TCPDump

Countermeasure_1
Level 1

Countermeasure_2
Level 2
Fig. 5. Flow of the execution of the simulation.
The Native IDS of Analyser_1 generates alerts for any attempt of
connection to port 23. The data received from Analyser_1 had
presented such features, generating an IDMEF (Intrusion
Detection Message Exchange Format) alert [14]. The generated
alert was sent to Countermeasure_1 resource, where a warning
was dispatched to the administrator informing him of the alert
received.
The Aggreg_Corr_1 resource received the information generated
by sensors 1 and 2. Its processing activities consist in correlating
the source IP addresses with the received data. The resultant
information of the processing of Aggreg_Corr_1 was directed to
the Analyser_2 resource.
The Native IDS component of the Analyser_2 generates alerts
when a source tries to connect to the same port number of
multiple destinations. This situation is identified by the
Analyser_2 in the data received from Aggreg_Corr_1 and an alert
in IDMEF format is then sent to the Countermeasures_2 resource.
In addition to generating alerts in IDMEF format, Analyser_2 also
directs the received data to the Analyser_3, in the level of
complexity 2. The Native IDS component of Analyser_3
generates alerts when the transmission of ICMP messages from a
given source to multiple destinations is detected. This situation is
detected in the data received from Analyser_2, and an IDMEF
alert is then sent to the Countermeasure_2 resource.
The Countermeasure_2 resource receives the alerts generated by
analysers 3 and 2, in accordance with the implementation of its
Native IDS component. Warnings on alerts received are
dispatched to the administrator.
The simulation carried out demonstrates how DIDSoG works.
Simulated data was generated to be the input for a grid of
intrusion detection systems composed by several distinct
resources. The resources carry out tasks such as data collection,
aggregation and analysis, and generation of alerts and warnings in
an integrated manner.
5. EXPERIMENT RESULTS
The hierarchic organization of scope and complexity provides a
high degree of flexibility to the model. The DIDSoG can be
modelled in accordance with the needs of each environment. The
descriptors define data flow desired for the resulting DIDS.
Each Native IDS is integrated to the DIDSoG through a
Connector component. The Connector component is also flexible
in the DIDSoG. Adaptations, conversions of data types and
auxiliary processes that Native IDSs need are provided by the
Connector. Filters and generation of Specific logs for each Native
IDS or environment can also be incorporated to the Connector.
If the integration of a new IDS to an environment already
configured is desired, it is enough to develop the Connector for
the desired IDS and to specify the resource Descriptor. After the
specification of the Connector and the Descriptor the new IDS is
integrated to the DIDSoG.
Through the definition of scopes, resources can act on data of
different source groups. For example, scope 1 can be related to a
given set of hosts, scope 2 to another set of hosts, while scope 3
can be related to hosts from scopes 1 and 2. Scopes can be defined
according to the needs of each environment.
The complexity levels allow the distribution of the processing
between several resources inside the same scope. In an analysis
task, for example, the search for simple attacks can be made by
resources of complexity 1, whereas the search for more complex
attacks, that demands more time, can be performed by resources
of complexity 2. With this, the analysis of the data is made by two
resources.
The distinction between complexity levels can also be organized
in order to integrate different techniques of intrusion detection.
The complexity level 1 could be defined for analyses based on
signatures, which are simpler techniques; the complexity level 2
for techniques based on behaviour, that require greater
computational power; and the complexity level 3 for intrusion
detection in applications, where the techniques are more specific
and depend on more data.
The division of scopes and the complexity levels make the
processing of the data to be carried out in phases. No resource has
full knowledge about the complete data processing flow. Each
resource only knows the results of its processing and the
destination to which it sends the results. Resources of higher
complexity must be linked to resources of lower complexity.
Therefore, the hierarchic structure of the DIDSoG is maintained,
facilitating its extension and integration with other domains of
intrusion detection.
By carrying out a hierarchic relationship between the several
chosen analysers for an environment, the sensor resource is not
overloaded with the task to send the data to all the analysers. An
initial analyser will exist (complexity level 1) to which the sensor
will send its data, and this analyser will then direct the data to the
next step of the processing flow. Another feature of the
hierarchical organization is the easy extension and integration
with other domains. If it is necessary to add a new host (sensor) to
the DIDSoG, it is enough to plug it to the first hierarchy of
resources. If it is necessary to add a new analyser, it will be in the
scope of several domains, it is enough to relate it to another
resource of same scope.
The DIDSoG allows different levels to be managed by different
entities. For example, the first scope can be managed by the local
user of a host. The second scope, comprising several hosts of a
domain can be managed by the administrator of the domain. A
third entity can be responsible for managing the security of
several domains in a joint way. This entity can act in the scope 3
independently from others.
With the proposed model for integration of IDSs in Grids, the
different IDSs of an environment (or multiple IDSs integrated) act
in a cooperative manner improving the intrusion detection
services, mainly in two aspects. First, the information from
multiple sources are analysed in an integrated way to search for
distributed attacks. This integration can be made under several
scopes. Second, there is a great diversity of data aggregation
techniques, data correlation and analysis, and intrusion response
that can be applied to the same environment; these techniques can
be organized under several levels of complexity.
6. CONCLUSION
The integration of heterogeneous IDSs is important. However, the
incompatibility and diversity of IDS solutions make such
integration extremely difficult. This work thus proposed a model
for composition of DIDS by integrating existing IDSs on a
computational Grid platform (DIDSoG). IDSs in DIDSoG are
encapsulated as Grid services for intrusion detection. A
computational Grid platform is used for the integration by
providing the basic requirements for communication, localization,
resource sharing and security mechanisms.
The components of the architecture of the DIDSoG were
developed and evaluated using the GridSim Grid simulator.
Services for communication and localization were used to carry
out the integration between components of different resources.
Based on the components of the architecture, several resources
were modelled forming a grid of intrusion detection. The
simulation demonstrated the usefulness of the proposed model.
Data from the sensor resources was read and this data was used to
feed other resources of DIDSoG.
The integration of distinct IDSs could be observed through the
simulated environment. Resources providing different intrusion
detection services were integrated (e.g. analysis, correlation,
aggregation and alert). The communication and localization
services provided by GridSim were used to integrate components
of different resources. Various resources were modelled following
the architecture components forming a grid of intrusion detection.
The components of DIDSoG architecture have served as base for
the integration of the resources presented in the simulation.
During the simulation, the different IDSs cooperated with one
another in a distributed manner; however, in a coordinated way
with an integrated view of the events, having, thus, the capability
to detect distributed attacks. This capability demonstrates that the
IDSs integrated have resulted in a DIDS.
Related work presents cooperation between components of a
specific DIDS. Some work focus on either the development of
DIDSs on computational Grids or the application of IDSs to
computational Grids. However, none deals with the integration of
heterogeneous IDSs. In contrast, the proposed model developed
and simulated in this work, can shed some light into the question
of integration of heterogeneous IDSs.
DIDSoG presents new research opportunities that we would like
to pursue, including: deployment of the model in a more realistic
environment such as a Grid; incorporation of new security
services; parallel analysis of data by Native IDSs in multiple
hosts.
In addition to the integration of IDSs enabled by a grid
middleware, the cooperation of heterogeneous IDSs can be
viewed as an economic problem. IDSs from different
organizations or administrative domains need incentives for
joining a grid of intrusion detection services and for collaborating
with other IDSs. The development of distributed strategy proof
mechanisms for integration of IDSs is a challenge that we would
like to tackle.
7. REFERENCES
[1] Sulistio, A.; Poduvaly, G.; Buyya, R; and Tham, CK,
Constructing A Grid Simulation with Differentiated Network
Service Using GridSim, Proc. of the 6th International
Conference on Internet Computing (ICOMP'05), June 27-30,
2005, Las Vegas, USA.
[2] Choon, O. T.; Samsudim, A. Grid-based Intrusion Detection
System. The 9th
IEEE Asia-Pacific Conference
Communications, September 2003.
[3] Foster, I.; Kesselman, C.; Tuecke, S. The Physiology of the
Grid: An Open Grid Service Architecture for Distributed
System Integration. Draft June 2002. Available at
http://www.globus.org/research/papers/ogsa.pdf. Access Feb.
2006.
[4] Foster, Ian; Kesselman, Carl; Tuecke, Steven. The anatomy
of the Grid: enabling scalable virtual organizations.
International Journal of Supercomputer Applications, 2001.
[5] Kannadiga, P.; Zulkernine, M. DIDMA: A Distributed
Intrusion Detection System Using Mobile Agents.
Proceedings of the IEEE Sixth International Conference on
Software Engineering, Artificial Intelligence, Networking
and Parallel/Distributed Computing, May 2005.
[6] Leu, Fang-Yie, et al. Integrating Grid with Intrusion
Detection. Proceedings of 19th
IEEE AINA"05, March 2005.
[7] Leu, Fang-Yie, et al. A Performance-Based Grid Intrusion
Detection System. Proceedings of the 29th
IEEE
COMPSAC"05, July 2005.
[8] McCanne, S.; Leres, C.; Jacobson, V.; TCPdump/Libpcap,
http://www.tcpdump.org/, 1994.
[9] Snapp, S. R. et al. DIDS (Distributed Intrusion Detection
System) - Motivation, Architecture and An Early Prototype.
Proceeding of the Fifteenth IEEE National Computer
Security Conference. Baltimore, MD, October 1992.
[10] Sterne, D.; et al. A General Cooperative Intrusion Detection
Architecture for MANETs. Proceedings of the Third IEEE
IWIA"05, March 2005.
[11] Tolba, M. F. ; et al. GIDA: Toward Enabling Grid Intrusion
Detection Systems. 5th IEEE International Symposium on
Cluster Computing and the Grid, May 2005.
[12] Wood, M. Intrusion Detection message exchange
requirements. Draft-ietf-idwg-requirements-10, October
2002. Available at 
http://www.ietf.org/internet-drafts/draftietf-idwg-requirements-10.txt. Access March 2006.
[13] Zhang, Yu-Fang; Xiong, Z.; Wang, X. Distributed Intrusion
Detection Based on Clustering. Proceedings of IEEE
International Conference Machine Learning and Cybernetics,
August 2005.
[14] Curry, D.; Debar, H. Intrusion Detection Message exchange
format data model and Extensible Markup Language (XML)
Document Type Definition. Draft-ietf-idwg-idmef-xml-10,
March 2006. Available at 
http://www.ietf.org/internetdrafts/draft-ietf-idwg-idmef-xml-16.txt.
Remote Access to Large Spatial Databases ∗
Egemen Tanin
Frantiˇsek Brabec
Hanan Samet
Computer Science Department
Center for Automation Research
Institute for Advanced Computer Studies
University of Maryland, College Park, MD 20742
{egemen,brabec,hjs}@umiacs.umd.edu
www.cs.umd.edu/{~egemen,~brabec,~hjs}
ABSTRACT
Enterprises in the public and private sectors have been 
making their large spatial data archives available over the 
Internet. However, interactive work with such large volumes
of online spatial data is a challenging task. We propose
two efficient approaches to remote access to large spatial
data. First, we introduce a client-server architecture where
the work is distributed between the server and the 
individual clients for spatial query evaluation, data visualization,
and data management. We enable the minimization of the
requirements for system resources on the client side while
maximizing system responsiveness as well as the number of
connections one server can handle concurrently. Second, for
prolonged periods of access to large online data, we 
introduce APPOINT (an Approach for Peer-to-Peer Oﬄoading
the INTernet). This is a centralized peer-to-peer approach
that helps Internet users transfer large volumes of online
data efficiently. In APPOINT, active clients of the 
clientserver architecture act on the server"s behalf and 
communicate with each other to decrease network latency, improve
service bandwidth, and resolve server congestions.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: 
Distributed Systems-Client/server, Distributed applications,
Distributed databases; H.2.8 [Database Management]:
Database Applications-Spatial databases and GIS
General Terms
Performance, Management
1. INTRODUCTION
In recent years, enterprises in the public and private 
sectors have provided access to large volumes of spatial data
over the Internet. Interactive work with such large volumes
of online spatial data is a challenging task. We have been 
developing an interactive browser for accessing spatial online
databases: the SAND (Spatial and Non-spatial Data) 
Internet Browser. Users of this browser can interactively and
visually manipulate spatial data remotely. Unfortunately,
interactive remote access to spatial data slows to a crawl
without proper data access mechanisms. We developed two
separate methods for improving the system performance, 
together, form a dynamic network infrastructure that is highly
scalable and provides a satisfactory user experience for 
interactions with large volumes of online spatial data.
The core functionality responsible for the actual database
operations is performed by the server-based SAND system.
SAND is a spatial database system developed at the 
University of Maryland [12]. The client-side SAND Internet
Browser provides a graphical user interface to the facilities
of SAND over the Internet. Users specify queries by 
choosing the desired selection conditions from a variety of menus
and dialog boxes.
SAND Internet Browser is Java-based, which makes it 
deployable across many platforms. In addition, since Java has
often been installed on target computers beforehand, our
clients can be deployed on these systems with little or no
need for any additional software installation or 
customization. The system can start being utilized immediately 
without any prior setup which can be extremely beneficial in
time-sensitive usage scenarios such as emergencies.
There are two ways to deploy SAND. First, any standard
Web browser can be used to retrieve and run the client piece
(SAND Internet Browser) as a Java application or an applet.
This way, users across various platforms can continuously
access large spatial data on a remote location with little or
15
no need for any preceding software installation. The second
option is to use a stand-alone SAND Internet Browser along
with a locally-installed Internet-enabled database 
management system (server piece). In this case, the SAND Internet
Browser can still be utilized to view data from remote 
locations. However, frequently accessed data can be downloaded
to the local database on demand, and subsequently accessed
locally. Power users can also upload large volumes of spatial
data back to the remote server using this enhanced client.
We focused our efforts in two directions. We first aimed at
developing a client-server architecture with efficient caching
methods to balance local resources on one side and the 
significant latency of the network connection on the other. The
low bandwidth of this connection is the primary concern in
both cases. The outcome of this research primarily addresses
the issues of our first type of usage (i.e., as a remote browser
application or an applet) for our browser and other similar
applications. The second direction aims at helping users
that wish to manipulate large volumes of online data for
prolonged periods. We have developed a centralized 
peerto-peer approach to provide the users with the ability to
transfer large volumes of data (i.e., whole data sets to the
local database) more efficiently by better utilizing the 
distributed network resources among active clients of a 
clientserver architecture. We call this architecture 
APPOINTApproach for Peer-to-Peer Oﬄoading the INTernet. The
results of this research addresses primarily the issues of the
second type of usage for our SAND Internet Browser (i.e.,
as a stand-alone application).
The rest of this paper is organized as follows. Section 2 
describes our client-server approach in more detail. Section 3
focuses on APPOINT, our peer-to-peer approach. Section 4
discusses our work in relation to existing work. Section 5
outlines a sample SAND Internet Browser scenario for both
of our remote access approaches. Section 6 contains 
concluding remarks as well as future research directions.
2. THE CLIENT-SERVER APPROACH
Traditionally, Geographic Information Systems (GIS)
such as ArcInfo from ESRI [2] and many spatial databases
are designed to be stand-alone products. The spatial
database is kept on the same computer or local area network
from where it is visualized and queried. This architecture
allows for instantaneous transfer of large amounts of data
between the spatial database and the visualization module
so that it is perfectly reasonable to use large-bandwidth 
protocols for communication between them. There are however
many applications where a more distributed approach is 
desirable. In these cases, the database is maintained in one 
location while users need to work with it from possibly distant
sites over the network (e.g., the Internet). These connections
can be far slower and less reliable than local area networks
and thus it is desirable to limit the data flow between the
database (server) and the visualization unit (client) in order
to get a timely response from the system.
Our client-server approach (Figure 1) allows the actual
database engine to be run in a central location maintained
by spatial database experts, while end users acquire a 
Javabased client component that provides them with a gateway
into the SAND spatial database engine.
Our client is more than a simple image viewer. Instead, it
operates on vector data allowing the client to execute many
operations such as zooming or locational queries locally. In
Figure 1: SAND Internet Browser - Client-Server
architecture.
essence, a simple spatial database engine is run on the client.
This database keeps a copy of a subset of the whole database
whose full version is maintained on the server. This is a
concept similar to ‘caching". In our case, the client acts as
a lightweight server in that given data, it evaluates queries
and provides the visualization module with objects to be
displayed. It initiates communication with the server only
in cases where it does not have enough data stored locally.
Since the locally run database is only updated when 
additional or newer data is needed, our architecture allows the
system to minimize the network traffic between the client
and the server when executing the most common user-side
operations such as zooming and panning. In fact, as long
as the user explores one region at a time (i.e., he or she is
not panning all over the database), no additional data needs
to be retrieved after the initial population of the client-side
database. This makes the system much more responsive
than the Web mapping services. Due to the complexity of
evaluating arbitrary queries (i.e., more complex queries than
window queries that are needed for database visualization),
we do not perform user-specified queries on the client. All
user queries are still evaluated on the server side and the
results are downloaded onto the client for display. However,
assuming that the queries are selective enough (i.e., there are
far fewer elements returned from the query than the number
of elements in the database), the response delay is usually
within reasonable limits.
2.1 Client-Server Communication
As mentioned above, the SAND Internet Browser is a
client piece of the remotely accessible spatial database server
built around the SAND kernel. In order to communicate
with the server, whose application programming interface
(API) is a Tcl-based scripting language, a servlet specifically
designed to interface the SAND Internet Browser with the
SAND kernel is required on the server side. This servlet 
listens on a given port of the server for incoming requests from
the client. It translates these requests into the SAND-Tcl
language. Next, it transmits these SAND-Tcl commands or
scripts to the SAND kernel. After results are provided by
the kernel, the servlet fetches and processes them, and then
sends those results back to the originating client.
Once the Java servlet is launched, it waits for a client to
initiate a connection. It handles both requests for the actual
client Java code (needed when the client is run as an applet)
and the SAND traffic. When the client piece is launched,
it connects back to the SAND servlet, the communication
is driven by the client piece; the server only responds to
the client"s queries. The client initiates a transaction by
6
sending a query. The Java servlet parses the query and
creates a corresponding SAND-Tcl expression or script in
the SAND kernel"s native format. It is then sent to the
kernel for evaluation or execution. The kernel"s response
naturally depends on the query and can be a boolean value,
a number or a string representing a value (e.g., a default
color) or, a whole tuple (e.g., in response to a nearest tuple
query). If a script was sent to the kernel (e.g., requesting
all the tuples matching some criteria), then an arbitrary
amount of data can be returned by the SAND server. In this
case, the data is first compressed before it is sent over the
network to the client. The data stream gets decompressed
at the client before the results are parsed.
Notice, that if another spatial database was to be used
instead of the SAND kernel, then only a simple 
modification to the servlet would need to be made in order for the
SAND Internet Browser to function properly. In 
particular, the queries sent by the client would need to be recoded
into another query language which is native to this different
spatial database. The format of the protocol used for 
communication between the servlet and the client is unaffected.
3. THE PEER-TO-PEER APPROACH
Many users may want to work on a complete spatial data
set for a prolonged period of time. In this case, making an
initial investment of downloading the whole data set may be
needed to guarantee a satisfactory session. Unfortunately,
spatial data tends to be large. A few download requests
to a large data set from a set of idle clients waiting to be
served can slow the server to a crawl. This is due to the fact
that the common client-server approach to transferring data
between the two ends of a connection assumes a designated
role for each one of the ends (i.e, some clients and a server).
We built APPOINT as a centralized peer-to-peer system
to demonstrate our approach for improving the common
client-server systems. A server still exists. There is a 
central source for the data and a decision mechanism for the
service. The environment still functions as a client-server
environment under many circumstances. Yet, unlike many
common client-server environments, APPOINT maintains
more information about the clients. This includes, 
inventories of what each client downloads, their availabilities, etc.
When the client-server service starts to perform poorly or
a request for a data item comes from a client with a poor
connection to the server, APPOINT can start appointing
appropriate active clients of the system to serve on behalf
of the server, i.e., clients who have already volunteered their
services and can take on the role of peers (hence, moving
from a client-server scheme to a peer-to-peer scheme). The
directory service for the active clients is still performed by
the server but the server no longer serves all of the requests.
In this scheme, clients are used mainly for the purpose of
sharing their networking resources rather than introducing
new content and hence they help oﬄoad the server and scale
up the service. The existence of a server is simpler in terms
of management of dynamic peers in comparison to pure 
peerto-peer approaches where a flood of messages to discover
who is still active in the system should be used by each peer
that needs to make a decision. The server is also the main
source of data and under regular circumstances it may not
forward the service.
Data is assumed to be formed of files. A single file forms
the atomic means of communication. APPOINT optimizes
requests with respect to these atomic requests. Frequently
accessed data sets are replicated as a byproduct of having
been requested by a large number of users. This opens up
the potential for bypassing the server in future downloads for
the data by other users as there are now many new points of
access to it. Bypassing the server is useful when the server"s
bandwidth is limited. Existence of a server assures that
unpopular data is also available at all times. The service
depends on the availability of the server. The server is now
more resilient to congestion as the service is more scalable.
Backups and other maintenance activities are already 
being performed on the server and hence no extra 
administrative effort is needed for the dynamic peers. If a peer goes
down, no extra precautions are taken. In fact, APPOINT
does not require any additional resources from an already
existing client-server environment but, instead, expands its
capability. The peers simply get on to or get off from a table
on the server.
Uploading data is achieved in a similar manner as 
downloading data. For uploads, the active clients can again be
utilized. Users can upload their data to a set of peers other
than the server if the server is busy or resides in a distant
location. Eventually the data is propagated to the server.
All of the operations are performed in a transparent 
fashion to the clients. Upon initial connection to the server,
they can be queried as to whether or not they want to share
their idle networking time and disk space. The rest of the
operations follow transparently after the initial contact. 
APPOINT works on the application layer but not on lower 
layers. This achieves platform independence and easy 
deployment of the system. APPOINT is not a replacement but
an addition to the current client-server architectures. We
developed a library of function calls that when placed in a
client-server architecture starts the service. We are 
developing advanced peer selection schemes that incorporate the
location of active clients, bandwidth among active clients,
data-size to be transferred, load on active clients, and 
availability of active clients to form a complete means of selecting
the best clients that can become efficient alternatives to the
server.
With APPOINT we are defining a very simple API that
could be used within an existing client-server system easily.
Instead of denial of service or a slow connection, this API
can be utilized to forward the service appropriately. The
API for the server side is:
start(serverPortNo)
makeFileAvailable(file,location,boolean)
callback receivedFile(file,location)
callback errorReceivingFile(file,location,error)
stop()
Similarly the API for the client side is:
start(clientPortNo,serverPortNo,serverAddress)
makeFileAvailable(file,location,boolean)
receiveFile(file,location)
sendFile(file,location)
stop()
The server, after starting the APPOINT service, can make
all of the data files available to the clients by using the
makeFileAvailable method. This will enable APPOINT
to treat the server as one of the peers.
The two callback methods of the server are invoked when
a file is received from a client, or when an error is 
encountered while receiving a file from a client. APPOINT 
guar7
Figure 2: The localization operation in APPOINT.
antees that at least one of the callbacks will be called so
that the user (who may not be online anymore) can always
be notified (i.e., via email). Clients localizing large data
files can make these files available to the public by using the
makeFileAvailable method on the client side.
For example, in our SAND Internet Browser, we have the
localization of spatial data as a function that can be chosen
from our menus. This functionality enables users to 
download data sets completely to their local disks before starting
their queries or analysis. In our implementation, we have
calls to the APPOINT service both on the client and the
server sides as mentioned above. Hence, when a localization
request comes to the SAND Internet Browser, the browser
leaves the decisions to optimally find and localize a data set
to the APPOINT service. Our server also makes its data
files available over APPOINT. The mechanism for the 
localization operation is shown with more details from the
APPOINT protocols in Figure 2. The upload operation is
performed in a similar fashion.
4. RELATED WORK
There has been a substantial amount of research on 
remote access to spatial data. One specific approach has
been adopted by numerous Web-based mapping services
(MapQuest [5], MapsOnUs [6], etc.). The goal in this 
approach is to enable remote users, typically only equipped
with standard Web browsers, to access the company"s 
spatial database server and retrieve information in the form of
pictorial maps from them. The solution presented by most
of these vendors is based on performing all the calculations
on the server side and transferring only bitmaps that 
represent results of user queries and commands. Although the
advantage of this solution is the minimization of both 
hardware and software resources on the client site, the resulting
product has severe limitations in terms of available 
functionality and response time (each user action results in a new
bitmap being transferred to the client).
Work described in [9] examines a client-server 
architecture for viewing large images that operates over a 
lowbandwidth network connection. It presents a technique
based on wavelet transformations that allows the 
minimization of the amount of data needed to be transferred over
the network between the server and the client. In this case,
while the server holds the full representation of the large 
image, only a limited amount of data needs to be transferred
to the client to enable it to display a currently requested
view into the image. On the client side, the image is 
reconstructed into a pyramid representation to speed up zooming
and panning operations. Both the client and the server keep
a common mask that indicates what parts of the image are
available on the client and what needs to be requested. This
also allows dropping unnecessary parts of the image from the
main memory on the server.
Other related work has been reported in [16] where a
client-server architecture is described that is designed to 
provide end users with access to a server. It is assumed that
this data server manages vast databases that are impractical
to be stored on individual clients. This work blends raster
data management (stored in pyramids [22]) with vector data
stored in quadtrees [19, 20].
For our peer-to-peer transfer approach (APPOINT), 
Napster is the forefather where a directory service is centralized
on a server and users exchange music files that they have
stored on their local disks. Our application domain, where
the data is already freely available to the public, forms a
prime candidate for such a peer-to-peer approach. Gnutella
is a pure (decentralized) peer-to-peer file exchange system.
Unfortunately, it suffers from scalability issues, i.e., floods of
messages between peers in order to map connectivity in the
system are required. Other systems followed these popular
systems, each addressing a different flavor of sharing over
the Internet. Many peer-to-peer storage systems have also
recently emerged. PAST [18], Eternity Service [7], CFS [10],
and OceanStore [15] are some peer-to-peer storage systems.
Some of these systems have focused on anonymity while 
others have focused on persistence of storage. Also, other 
approaches, like SETI@Home [21], made other resources, such
as idle CPUs, work together over the Internet to solve large
scale computational problems. Our goal is different than
these approaches. With APPOINT, we want to improve 
existing client-server systems in terms of performance by using
idle networking resources among active clients. Hence, other
issues like anonymity, decentralization, and persistence of
storage were less important in our decisions. Confirming
the authenticity of the indirectly delivered data sets is not
yet addressed with APPOINT. We want to expand our 
research, in the future, to address this issue.
From our perspective, although APPOINT employs some
of the techniques used in peer-to-peer systems, it is also
closely related to current Web caching architectures. 
Squirrel [13] forms the middle ground. It creates a pure 
peer-topeer collaborative Web cache among the Web browser caches
of the machines in a local-area network. Except for this 
recent peer-to-peer approach, Web caching is mostly a 
wellstudied topic in the realm of server/proxy level caching [8,
11, 14, 17]. Collaborative Web caching systems, the most
relevant of these for our research, focus on creating 
either a hierarchical, hash-based, central directory-based, or
multicast-based caching schemes. We do not compete with
these approaches. In fact, APPOINT can work in 
tandem with collaborative Web caching if they are deployed
together. We try to address the situation where a request
arrives at a server, meaning all the caches report a miss.
Hence, the point where the server is reached can be used to
take a central decision but then the actual service request
can be forwarded to a set of active clients, i.e., the 
down8
load and upload operations. Cache misses are especially
common in the type of large data-based services on which
we are working. Most of the Web caching schemes that are
in use today employ a replacement policy that gives a 
priority to replacing the largest sized items over smaller-sized
ones. Hence, these policies would lead to the immediate 
replacement of our relatively large data files even though they
may be used frequently. In addition, in our case, the user
community that accesses a certain data file may also be very
dispersed from a network point of view and thus cannot take
advantage of any of the caching schemes. Finally, none of
the Web caching methods address the symmetric issue of
large data uploads.
5. A SAMPLE APPLICATION
FedStats [1] is an online source that enables ordinary 
citizens access to official statistics of numerous federal agencies
without knowing in advance which agency produced them.
We are using a FedStats data set as a testbed for our work.
Our goal is to provide more power to the users of FedStats
by utilizing the SAND Internet Browser. As an example,
we looked at two data files corresponding to 
Environmental Protection Agency (EPA)-regulated facilities that have
chlorine and arsenic, respectively. For each file, we had the
following information available: EPA-ID, name, street, city,
state, zip code, latitude, longitude, followed by flags to 
indicate if that facility is in the following EPA programs: 
Hazardous Waste, Wastewater Discharge, Air Emissions, 
Abandoned Toxic Waste Dump, and Active Toxic Release.
We put this data into a SAND relation where the spatial
attribute ‘location" corresponds to the latitude and 
longitude. Some queries that can be handled with our system on
this data include:
1. Find all EPA-regulated facilities that have arsenic and
participate in the Air Emissions program, and:
(a) Lie in Georgia to Illinois, alphabetically.
(b) Lie within Arkansas or 30 miles within its border.
(c) Lie within 30 miles of the border of Arkansas (i.e.,
both sides of the border).
2. For each EPA-regulated facility that has arsenic, find
all EPA-regulated facilities that have chlorine and:
(a) That are closer to it than to any other 
EPAregulated facility that has arsenic.
(b) That participate in the Air Emissions program
and are closer to it than to any other 
EPAregulated facility which has arsenic. In order to
avoid reporting a particular facility more than
once, we use our ‘group by EPA-ID" mechanism.
Figure 3 illustrates the output of an example query that
finds all arsenic sites within a given distance of the border of
Arkansas. The sites are obtained in an incremental manner
with respect to a given point. This ordering is shown by
using different color shades.
With this example data, it is possible to work with the
SAND Internet Browser online as an applet (connecting to
a remote server) or after localizing the data and then 
opening it locally. In the first case, for each action taken, the
client-server architecture will decide what to ask for from
the server. In the latter case, the browser will use the 
peerto-peer APPOINT architecture for first localizing the data.
6. CONCLUDING REMARKS
An overview of our efforts in providing remote access to
large spatial data has been given. We have outlined our
approaches and introduced their individual elements. Our
client-server approach improves the system performance by
using efficient caching methods when a remote server is 
accessed from thin-clients. APPOINT forms an alternative 
approach that improves performance under an existing 
clientserver system by using idle client resources when individual
users want work on a data set for longer periods of time
using their client computers.
For the future, we envision development of new efficient 
algorithms that will support large online data transfers within
our peer-to-peer approach using multiple peers 
simultaneously. We assume that a peer (client) can become 
unavailable at any anytime and hence provisions need to be in place
to handle such a situation. To address this, we will augment
our methods to include efficient dynamic updates. Upon
completion of this step of our work, we also plan to run
comprehensive performance studies on our methods.
Another issue is how to access data from different sources
in different formats. In order to access multiple data sources
in real time, it is desirable to look for a mechanism that
would support data exchange by design. The XML 
protocol [3] has emerged to become virtually a standard for
describing and communicating arbitrary data. GML [4] is
an XML variant that is becoming increasingly popular for
exchange of geographical data. We are currently working
on making SAND XML-compatible so that the user can 
instantly retrieve spatial data provided by various agencies in
the GML format via their Web services and then explore,
query, or process this data further within the SAND 
framework. This will turn the SAND system into a universal tool
for accessing any spatial data set as it will be deployable on
most platforms, work efficiently given large amounts of data,
be able to tap any GML-enabled data source, and provide
an easy to use graphical user interface. This will also 
convert the SAND system from a research-oriented prototype
into a product that could be used by end users for 
accessing, viewing, and analyzing their data efficiently and with
minimum effort.
7. REFERENCES
[1] Fedstats: The gateway to statistics from over 100 U.S.
federal agencies. http://www.fedstats.gov/, 2001.
[2] Arcinfo: Scalable system of software for geographic
data creation, management, integration, analysis, and
dissemination. http://www.esri.com/software/
arcgis/arcinfo/index.html, 2002.
[3] Extensible markup language (xml).
http://www.w3.org/XML/, 2002.
[4] Geography markup language (gml) 2.0.
http://opengis.net/gml/01-029/GML2.html, 2002.
[5] Mapquest: Consumer-focused interactive mapping site
on the web. http://www.mapquest.com, 2002.
[6] Mapsonus: Suite of online geographic services.
http://www.mapsonus.com, 2002.
[7] R. Anderson. The Eternity Service. In Proceedings of
the PRAGOCRYPT"96, pages 242-252, Prague, Czech
Republic, September 1996.
[8] L. Breslau, P. Cao, L. Fan, G. Phillips, and
S. Shenker. Web caching and Zipf-like distributions:
9
Figure 3: Sample output from the SAND Internet Browser - Large dark dots indicate the result of a query
that looks for all arsenic sites within a given distance from Arkansas. Different color shades are used to
indicate ranking order by the distance from a given point.
Evidence and implications. In Proceedings of the IEEE
Infocom"99, pages 126-134, New York, NY, March
1999.
[9] E. Chang, C. Yap, and T. Yen. Realtime visualization
of large images over a thinwire. In R. Yagel and
H. Hagen, editors, Proceedings IEEE Visualization"97
(Late Breaking Hot Topics), pages 45-48, Phoenix,
AZ, October 1997.
[10] F. Dabek, M. F. Kaashoek, D. Karger, R. Morris, and
I. Stoica. Wide-area cooperative storage with CFS. In
Proceedings of the ACM SOSP"01, pages 202-215,
Banff, AL, October 2001.
[11] A. Dingle and T. Partl. Web cache coherence.
Computer Networks and ISDN Systems,
28(7-11):907-920, May 1996.
[12] C. Esperan¸ca and H. Samet. Experience with
SAND/Tcl: a scripting tool for spatial databases.
Journal of Visual Languages and Computing,
13(2):229-255, April 2002.
[13] S. Iyer, A. Rowstron, and P. Druschel. Squirrel: A
decentralized peer-to-peer Web cache. Rice
University/Microsoft Research, submitted for
publication, 2002.
[14] D. Karger, A. Sherman, A. Berkheimer, B. Bogstad,
R. Dhanidina, K. Iwamoto, B. Kim, L. Matkins, and
Y. Yerushalmi. Web caching with consistent hashing.
Computer Networks, 31(11-16):1203-1213, May 1999.
[15] J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski,
P. Eaton, D. Geels, R. Gummadi, S. Rhea,
H. Weatherspoon, W. Weimer, C. Wells, and B. Zhao.
OceanStore: An architecture for global-scale persistent
store. In Proceedings of the ACM ASPLOS"00, pages
190-201, Cambridge, MA, November 2000.
[16] M. Potmesil. Maps alive: viewing geospatial
information on the WWW. Computer Networks and
ISDN Systems, 29(8-13):1327-1342, September 1997.
Also Hyper Proceedings of the 6th International World
Wide Web Conference, Santa Clara, CA, April 1997.
[17] M. Rabinovich, J. Chase, and S. Gadde. Not all hits
are created equal: Cooperative proxy caching over a
wide-area network. Computer Networks and ISDN
Systems, 30(22-23):2253-2259, November 1998.
[18] A. Rowstron and P. Druschel. Storage management
and caching in PAST, a large-scale, persistent
peer-to-peer storage utility. In Proceedings of the ACM
SOSP"01, pages 160-173, Banff, AL, October 2001.
[19] H. Samet. Applications of Spatial Data Structures:
Computer Graphics, Image Processing, and GIS.
Addison-Wesley, Reading, MA, 1990.
[20] H. Samet. The Design and Analysis of Spatial Data
Structures. Addison-Wesley, Reading, MA, 1990.
[21] SETI@Home. http://setiathome.ssl.berkeley.edu/,
2001.
[22] L. J. Williams. Pyramidal parametrics. Computer
Graphics, 17(3):1-11, July 1983. Also Proceedings of
the SIGGRAPH"83 Conference, Detroit, July 1983.
10
An Architectural Framework and a Middleware for
Cooperating Smart Components
∗
Ant´onio Casimiro
U.Lisboa
casim@di.fc.ul.pt
J¨org Kaiser
U.Ulm

kaiser@informatik.uniulm.de
Paulo Ver´ıssimo
U.Lisboa
pjv@di.fc.ul.pt
ABSTRACT
In a future networked physical world, a myriad of smart
sensors and actuators assess and control aspects of their 
environments and autonomously act in response to it. 
Examples range in telematics, traffic management, team robotics
or home automation to name a few. To a large extent, such
systems operate proactively and independently of direct 
human control driven by the perception of the environment and
the ability to organize respective computations dynamically.
The challenging characteristics of these applications include
sentience and autonomy of components, issues of 
responsiveness and safety criticality, geographical dispersion, mobility
and evolution. A crucial design decision is the choice of
the appropriate abstractions and interaction mechanisms.
Looking to the basic building blocks of such systems we
may find components which comprise mechanical 
components, hardware and software and a network interface, thus
these components have different characteristics compared to
pure software components. They are able to spontaneously
disseminate information in response to events observed in
the physical environment or to events received from other
component via the network interface. Larger autonomous
components may be composed recursively from these 
building blocks.
The paper describes an architectural framework and a
middleware supporting a component-based system and an
integrated view on events-based communication comprising
the real world events and the events generated in the 
system. It starts by an outline of the component-based system
construction. The generic event architecture GEAR is 
introduced which describes the event-based interaction between
the components via a generic event layer. The generic event
layer hides the different communication channels including
∗This work was partially supported by the EC, through
project IST-2000-26031 (CORTEX), and by the FCT,
through the Large-Scale Informatic Systems 
Laboratory (LaSIGE) and project POSI/1999/CHS/33996 
(DEFEATS).
the interactions through the environment. An appropriate
middleware is presented which reflects these needs and 
allows to specify events which have quality attributes to 
express temporal constraints. This is complemented by the
notion of event channels which are abstractions of the 
underlying network and allow to enforce quality attributes. They
are established prior to interaction to reserve the needed
computational and network resources for highly predictable
event dissemination.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems-Distributed applications; C.3 [Special-Purpose
and Application-Based Systems]: Real-Time and 
embedded systems
General Terms
Design
1. INTRODUCTION
In recent years we have seen the continuous improvement
of technologies that are relevant for the construction of 
distributed embedded systems, including trustworthy visual,
auditory, and location sensing [11], communication and 
processing. We believe that in a future networked physical
world a new class of applications will emerge, composed of
a myriad of smart sensors and actuators to assess and 
control aspects of their environments and autonomously act in
response to it. The anticipated challenging characteristics
of these applications include autonomy, responsiveness and
safety criticality, large scale, geographical dispersion, 
mobility and evolution.
In order to deal with these challenges, it is of 
fundamental importance to use adequate high-level models, 
abstractions and interaction paradigms. Unfortunately, when 
facing the specific characteristics of the target systems, the
shortcomings of current architectures and middleware 
interaction paradigms become apparent. Looking to the basic
building blocks of such systems we may find components
which comprise mechanical parts, hardware, software and
a network interface. However, classical event/object 
models are usually software oriented and, as such, when 
trans28
ported to a real-time, embedded systems setting, their 
harmony is cluttered by the conflict between, on the one side,
send/receive of software events (message-based), and on
the other side, input/output of hardware or real-world
events, register-based. In terms of interaction paradigms,
and although the use of event-based models appears to be
a convenient solution [10, 22], these often lack the 
appropriate support for non-functional requirements like reliability,
timeliness or security.
This paper describes an architectural framework and a
middleware, supporting a component-based system and an
integrated view on event-based communication comprising
the real world events and the events generated in the system.
When choosing the appropriate interaction paradigm it
is of fundamental importance to address the challenging 
issues of the envisaged sentient applications. Unlike classical
approaches that confine the possible interactions to the 
application boundaries, i.e. to its components, we consider
that the environment surrounding the application also plays
a relevant role in this respect. Therefore, the paper starts by
clarifying several issues concerning our view of the system,
about the interactions that may take place and about the
information flows. This view is complemented by 
providing an outline of the component-based system construction
and, in particular, by showing that it is possible to 
compose larger applications from basic components, following
an hierarchical composition approach.
This provides the necessary background to introduce the
Generic-Events Architecture (GEAR), which describes
the event-based interaction between the components via a
generic event layer while allowing the seamless integration
of physical and computer information flows. In fact, the
generic event layer hides the different communication 
channels, including the interactions through the environment.
Additionally, the event layer abstraction is also adequate
for the proper handling of the non-functional requirements,
namely reliability and timeliness, which are particularly 
stringent in real-time settings. The paper devotes particular 
attention to this issue by discussing the temporal aspects of
interactions and the needs for predictability.
An appropriate middleware is presented which reflects
these needs and allows to specify events which have quality
attributes to express temporal constraints. This is 
complemented by the notion of Event Channels (EC), which are 
abstractions of the underlying network while being abstracted
by the event layer. In fact, event channels play a 
fundamental role in securing the functional and non-functional
(e.g. reliability and timeliness) properties of the envisaged
applications, that is, in allowing the enforcement of quality
attributes. They are established prior to interaction to 
reserve the needed computational and network resources for
highly predictable event dissemination.
The paper is organized as follows. In Section 3 we 
introduce the fundamental notions and abstractions that we
adopt in this work to describe the interactions taking place
in the system. Then, in Section 4, we describe the 
componentbased approach that allows composition of objects. GEAR
is then described in Section 5 and Section 6 focuses on 
temporal aspects of the interactions. Section 7 describes the
COSMIC middleware, which may be used to specify the 
interaction between sentient objects. A simple example to
highlight the ideas presented in the paper appears in 
Section 8 and Section 9 concludes the paper.
2. RELATED WORK
Our work considers a wired physical world in which a
very large number of autonomous components cooperate.
It is inspired by many research efforts in very different 
areas. Event-based systems in general have been introduced to
meet the requirements of applications in which entities 
spontaneously generate information and disseminate it [1, 25,
22]. Intended for large systems and requiring quite complex
infrastructures, these event systems do not consider 
stringent quality aspects like timeliness and dependability issues.
Secondly, they are not created to support inter-operability
between tiny smart devices with substantial resource 
constraints.
In [10] a real-time event system for CORBA has been
introduced. The events are routed via a central event server
which provides scheduling functions to support the real-time
requirements. Such a central component is not available
in an infrastructure envisaged in our system architecture
and the developed middleware TAO (The Ace Orb) is quite
complex and unsuitable to be directly integrated in smart
devices.
There are efforts to implement CORBA for control 
networks, tailored to connect sensor and actuator components [15,
19]. They are targeted for the CAN-Bus [9], a popular 
network developed for the automotive industry. However, in
these approaches the support for timeliness or 
dependability issues does not exist or is only very limited.
A new scheme to integrate smart devices in a CORBA 
environment is proposed in [17] and has lead to the proposal of
a standard by the Object Management Group (OMG) [26].
Smart transducers are organized in clusters that are 
connected to a CORBA system by a gateway.
The clusters form isolated subnetworks. A special master
node enforces the temporal properties in the cluster subnet.
A CORBA gateway allows to access sensor data and write
actuator data by means of an interface file system (IFS).
The basic structure is similar to the WAN-of-CANs 
structure which has been introduced in the CORTEX project [4].
Islands of tight control may be realized by a control network
and cooperate via wired or wireless networks covering a large
number of these subnetworks. However, in contrast to the
event channel model introduced in this paper, all 
communication inside a cluster relies on a single technical solution of
a synchronous communication channel. Secondly, although
the temporal behaviour of a single cluster is rigorously 
defined, no model to specify temporal properties for 
clusterto-CORBA or cluster-to-cluster interactions is provided.
3. INFORMATION FLOW AND
INTERACTION MODEL
In this paper we consider a component-based system model
that incorporates previous work developed in the context of
the IST CORTEX project [5]. As mentioned above, a 
fundamental idea underlying the approach is that applications can
be composed of a large number of smart components that
are able to sense their surrounding environment and 
interact with it. These components are referred to as sentient
objects, a metaphor elaborated in CORTEX and inspired
on the generic concept of sentient computing introduced in
[12]. Sentient objects accept input events from a variety of
different sources (including sensors, but not constrained to
that), process them, and produce output events, whereby
29
they actuate on the environment and/or interact with other
objects. Therefore, the following kinds of interactions can
take place in the system:
Environment-to-object interactions: correspond to a
flow of information from the environment to 
application objects, reporting about the state of the former,
and/or notifying about events taking place therein.
Object-to-object interactions: correspond to a flow of
information among sentient objects, serving two 
purposes. The first is related with complementing the
assessment of each individual object about the state
of the surrounding space. The second is related to 
collaboration, in which the object tries to influence other
objects into contributing to a common goal, or into
reacting to an unexpected situation.
Object-to-environment interactions: correspond to a
flow of information from an object to the environment,
with the purpose of forcing a change in the state of the
latter.
Before continuing, we need to clarify a few issues with
respect to these possible forms of interaction. We consider
that the environment can be a producer or consumer of 
information while interacting with sentient objects. The 
environment is the real (physical) world surrounding an 
object, not necessarily close to the object or limited to certain
boundaries. Quite clearly, the information produced by the
environment corresponds to the physical representation of
real-time entities, of which typical examples include 
temperature, distance or the state of a door. On the other hand,
actuation on the environment implies the manipulation of
these real-time entities, like increasing the temperature 
(applying more heat), changing the distance (applying some
movement) or changing the state of the door (closing or
opening it). The required transformations between system
representations of these real-time entities and their physical
representations is accomplished, generically, by sensors and
actuators. We further consider that there may exist dumb
sensors and actuators, which interact with the objects by
disseminating or capturing raw transducer information, and
smart sensors and actuators, with enhanced processing 
capabilities, capable of speaking some more elaborate event
dialect (see Sections 5 and 6.1). Interaction with the 
environment is therefore done through sensors and actuators,
which may, or may not be part of sentient objects, as 
discussed in Section 4.2.
State or state changes in the environment are considered
as events, captured by sensors (in the environment or within
sentient objects) and further disseminated to other 
potentially interested sentient objects in the system. In 
consequence, it is quite natural to base the communication and 
interaction among sentient objects and with the environment
on an event-based communication model. Moreover, typical
properties of event-based models, such as anonymous and
non-blocking communication, are highly desirable in systems
where sentient objects can be mobile and where interactions
are naturally very dynamic.
A distinguishing aspect of our work from many of the 
existing approaches, is that we consider that sentient objects
may indirectly communicate with each other through the
environment, when they act on it. Thus the environment
constitutes an interaction and communication channel and
is in the control and awareness loop of the objects. In other
words, when a sentient object actuates on the environment it
will be able to observe the state changes in the environment
by means of events captured by the sensors. Clearly, other
objects might as well capture the same events, thus 
establishing the above-mentioned indirect communication path.
In systems that involve interactions with the environment
it is very important to consider the possibility of 
communication through the environment. It has been shown that
the hidden channels developing through the latter (e.g., 
feedback loops) may hinder software-based algorithms ignoring
them [30]. Therefore, any solution to the problem requires
the definition of convenient abstractions and appropriate 
architectural constructs.
On the other hand, in order to deal with the information
flow through the whole computer system and environment in
a seamless way, handling software and hardware events
uniformly, it is also necessary to find adequate abstractions.
As discussed in Section 5, the Generic-Events Architecture
introduces the concept of Generic Event and an Event Layer
abstraction which aim at dealing, among others, with these
issues.
4. SENTIENT OBJECT COMPOSITION
In this section we analyze the most relevant issues related
with the sentient object paradigm and the construction of
systems composed of sentient objects.
4.1 Component-based System Construction
Sentient objects can take several different forms: they
can simply be software-based components, but they can also
comprise mechanical and/or hardware parts, amongst which
the very sensorial apparatus that substantiates sentience,
mixed with software components to accomplish their task.
We refine this notion by considering a sentient object as an
encapsulating entity, a component with internal logic and
active processing elements, able to receive, transform and
produce new events. This interface hides the internal 
hardware/software structure of the object, which may be 
complex, and shields the system from the low-level functional
and temporal details of controlling a specific sensor or 
actuator.
Furthermore, given the inherent complexity of the 
envisaged applications, the number of simultaneous input events
and the internal size of sentient objects may become too
large and difficult to handle. Therefore, it should be 
possible to consider the hierarchical composition of sentient 
objects so that the application logic can be separated across as
few or as many of these objects as necessary. On the other
hand, composition of sentient objects should normally be
constrained by the actual hardware component"s structure,
preventing the possibility of arbitrarily composing sentient
objects. This is illustrated in Figure 1, where a sentient
object is internally composed of a few other sentient 
objects, each of them consuming and producing events, some
of which only internally propagated.
Observing the figure, and recalling our previous discussion
about the possible interactions, we identify all of them here:
an object-to-environment interaction occurs between the 
object controlling a WLAN transmitter and some WLAN 
receiver in the environment; an environment-to-object 
interaction takes place when the object responsible for the GPS
30
G P S
r e c e p t i o n
W i r e l e s s
t r a n s m i s s i o n
D o p p l e r
r a d a r
P h y s i c a l f e e d b a c k
O b j e c t ' s b o d y
I n t e r n a l N e t w o r k
Figure 1: Component-aware sentient object 
composition.
signal reception uses the information transmitted by the
satellites; finally, explicit object-to-object interactions occur
internally to the container object, through an internal 
communication network. Additionally, it is interesting to 
observe that implicit communication can also occur, whether
the physical feedback develops through the environment 
internal to the container object (as depicted) or through the
environment external to this object. However, there is a 
subtle difference between both cases. While in the former the
feedback can only be perceived by objects internal to the
container, bounding the extent to which consistency must
be ensured, such bounds do not exist in the latter. In fact,
the notion of sentient object as an encapsulating entity may
serve other purposes (e.g., the confinement of feedback and
of the propagation of events), beyond the mere hierarchical
composition of objects.
To give a more concrete example of such component-aware
object composition we consider a scenario of cooperating
robots. Each robot is made of several components, 
corresponding, for instance, to axis and manipulator controllers.
Together with the control software, each of these controllers
may be a sentient object. On the other hand, a robot itself
is a sentient object, composed of the objects materialized
by the controllers, and the environment internal to its own
structure, or body.
This means that it should be possible to define 
cooperation activities using the events produced by robot sentient
objects, without the need to know the internal structure of
robots, or the events produced by body objects or by smart
sensors within the body. From an engineering point of view,
however, this also means that robot sentient object may
have to generate new events that reflect its internal state,
which requires the definition of a gateway to make the bridge
between the internal and external environments.
4.2 Encapsulation and Scoping
Now an important question is about how to represent and
disseminate events in a large scale networked world. As we
have seen above, any event generated by a sentient object
could, in principle, be visible anywhere in the system and
thus received by any other sentient object. However, there
are substantial obstacles to such universal interactions, 
originating from the components heterogeneity in such a 
largescale setting.
Firstly, the components may have severe performance 
constraints, particularly because we want to integrate smart
sensors and actuators in such an architecture. Secondly, the
bandwidth of the participating networks may vary largely.
Such networks may be low power, low bandwidth fieldbuses,
or more powerful wireless networks as well as high speed
backbones. Thirdly, the networks may have widely different
reliability and timeliness characteristics. Consider a 
platoon of cooperating vehicles. Inside a vehicle there may be
a field-bus like CAN [8, 9], TTP/A [17] or LIN [20], with a
comparatively low bandwidth. On the other hand, the 
vehicles are communicating with others in the platoon via a
direct wireless link. Finally, there may be multiple platoons
of vehicles which are coordinated by an additional wireless
network layer.
At the abstraction level of sentient objects, such 
heterogeneity is reflected by the notion of body-vs-environment.
At the network level, we assume the WAN-of-CANs 
structure [27] to model the different networks. The notion of
body and environment is derived from the recursively 
defined component-based object model. A body is similar to
a cell membrane and represents a quality of service 
container for the sentient objects inside. On the network level,
it may be associated with the components coupled by a 
certain CAN. A CAN defines the dissemination quality which
can be expected by the cooperating objects.
In the above example, a vehicle may be a sentient object,
whose body is composed of the respective lower level objects
(sensors and actuators) which are connected by the internal
network (see Figure 1). Correspondingly, the platoon can be
seen itself as an object composed of a collection of 
cooperating vehicles, its body being the environment encapsulated by
the platoon zone. At the network level, the wireless network
represents the respective CAN. However, several platoons
united by their CANs may interact with each other and 
objects further away, through some wider-range, possible fixed
networking substrate, hence the concept of WAN-of-CANs.
The notions of body-environment and WAN-of-CANs are
very useful when defining interaction properties across such
boundaries. Their introduction obeyed to our belief that
a single mechanism to provide quality measures for 
interactions is not appropriate. Instead, a high level construct
for interaction across boundaries is needed which allows to
specify the quality of dissemination and exploits the 
knowledge about body and environment to assess the feasibility of
quality constraints. As we will see in the following section,
the notion of an event channel represents this construct in
our architecture. It disseminates events and allows the 
network independent specification of quality attributes. These
attributes must be mapped to the respective properties of
the underlying network structure.
5. A GENERIC EVENTS ARCHITECTURE
In order to successfully apply event-based object-oriented
models, addressing the challenges enumerated in the 
introduction of this paper, it is necessary to use adequate 
architectural constructs, which allow the enforcement of 
fundamental properties such as timeliness or reliability.
We propose the Generic-Events Architecture (GEAR),
depicted in Figure 2, which we briefly describe in what 
follows (for a more detailed description please refer to [29]).
The L-shaped structure is crucial to ensure some of the 
properties described.
Environment: The physical surroundings, remote and close,
solid and etherial, of sentient objects.
31
C o m m ' sC o m m ' sC o m m ' s
T r a n s l a t i o n
L a y e r
T r a n s l a t i o n
L a y e r
B o d y
E n v i r o n m e n t
B o d y
E n v i r o n m e n t
B o d y
E n v i r o n m e n t
( i n c l u d i n g o p e r a t i o n a l n e t w o r k )
( o f o b j e c t o r o b j e c t c o m p o u n d )
T r a n s l a t i o n
L a y e r
T r a n s l a t i o n
S e n t i e n t
O b j e c t
S e n t i e n t
O b j e c t
S e n t i e n t
O b j e c t
R e g u l a r N e t w o r k
c o n s u m ep r o d u c e
E v e n t
L a y e r
E v e n t
L a y e r
E v e n t
L a y e r
S e n t i e n t
O b j e c t
Figure 2: Generic-Events architecture.
Body: The physical embodiment of a sentient object (e.g.,
the hardware where a mechatronic controller resides,
the physical structure of a car). Note that due to the
compositional approach taken in our model, part of
what is environment to a smaller object seen 
individually, becomes body for a larger, containing object.
In fact, the body is the internal environment of the
object. This architecture layering allows composition
to take place seamlessly, in what concerns information
flow.
Inside a body there may also be implicit knowledge,
which can be exploited to make interaction more 
efficient, like the knowledge about the number of 
cooperating entities, the existence of a specific 
communication network or the simple fact that all components are
co-located and thus the respective events do not need
to specify location in their context attributes. Such 
intrinsic information is not available outside a body and,
therefore, more explicit information has to be carried
by an event.
Translation Layer: The layer responsible for physical event
transformation from/to their native form to event 
channel dialect, between environment/body and an event
channel. Essentially one doing observation and 
actuation operations on the lower side, and doing 
transactions of event descriptions on the other. On the lower
side this layer may also interact with dumb sensors or
actuators, therefore talking the language of the 
specific device. These interactions are done through 
operational networks (hence the antenna symbol in the
figure).
Event Layer: The layer responsible for event propagation
in the whole system, through several Event Channels
(EC):. In concrete terms, this layer is a kind of 
middleware that provides important event-processing services
which are crucial for any realistic event-based system.
For example, some of the services that imply the 
processing of events may include publishing, subscribing,
discrimination (zoning, filtering, fusion, tracing), and
queuing.
Communication Layer: The layer responsible for 
wrapping events (as a matter of fact, event descriptions
in EC dialect) into carrier event-messages, to be
transported to remote places. For example, a 
sensing event generated by a smart sensor is wrapped in
an event-message and disseminated, to be caught by
whoever is concerned. The same holds for an 
actuation event produced by a sentient object, to be 
delivered to a remote smart actuator. Likewise, this may
apply to an event-message from one sentient object
to another. Dumb sensors and actuators do not send
event-messages, since they are unable to understand
the EC dialect (they do not have an event layer 
neither a communication layer- they communicate, if
needed, through operational networks).
Regular Network: This is represented in the horizontal
axis of the block diagram by the communication layer,
which encompasses the usual LAN, TCP/IP, and 
realtime protocols, desirably augmented with reliable and/or
ordered broadcast and other protocols.
The GEAR introduces some innovative ideas in distributed
systems architecture. While serving an object model based
on production and consumption of generic events, it treats
events produced by several sources (environment, body, 
objects) in a homogeneous way. This is possible due to the use
of a common basic dialect for talking about events and due
to the existence of the translation layer, which performs the
necessary translation between the physical representation of
a real-time entity and the EC compliant format. Crucial to
the architecture is the event layer, which uses event channels
to propagate events through regular network infrastructures.
The event layer is realized by the COSMIC middleware, as
described in Section 7.
5.1 Information Flow in GEAR
The flow of information (external environment and 
computational part) is seamlessly supported by the L-shaped
architecture. It occurs in a number of different ways, which
demonstrates the expressiveness of the model with regard to
the necessary forms of information encountered in real-time
cooperative and embedded systems.
Smart sensors produce events which report on the 
environment. Body sensors produce events which report on
the body. They are disseminated by the local event layer
module, on an event channel (EC) propagated through the
regular network, to any relevant remote event layer 
modules where entities showed an interest on them, normally,
sentient objects attached to the respective local event layer
modules.
Sentient objects consume events they are interested in,
process them, and produce other events. Some of these
events are destined to other sentient objects. They are 
published on an EC using the same EC dialect that serves, e.g.,
sensor originated events. However, these events are 
semantically of a kind such that they are to be subscribed by
the relevant sentient objects, for example, the sentient 
objects composing a robot controller system, or, at a higher
level, the sentient objects composing the actual robots in
32
a cooperative application. Smart actuators, on the other
hand, merely consume events produced by sentient objects,
whereby they accept and execute actuation commands. 
Alternatively to talking to other sentient objects, sentient
objects can produce events of a lower level, for example,
actuation commands on the body or environment. They
publish these exactly the same way: on an event channel
through the local event layer representative. Now, if these
commands are of concern to local actuator units (e.g., body,
including internal operational networks), they are passed on
to the local translation layer. If they are of concern to a
remote smart actuator, they are disseminated through the
distributed event layer, to reach the former. In any case,
if they are also of interest to other entities, such as other
sentient objects that wish to be informed of the actuation
command, then they are also disseminated through the EC
to these sentient objects.
A key advantage of this architecture is that event-messages
and physical events can be globally ordered, if necessary,
since they all pass through the event layer. The model also
offers opportunities to solve a long lasting problem in 
realtime, computer control, and embedded systems: the 
inconsistency between message passing and the feedback loop 
information flow subsystems.
6. TEMPORAL ASPECTS OF THE
INTERACTIONS
Any interaction needs some form of predictability. If safety
critical scenarios are considered as it is done in CORTEX,
temporal aspects become crucial and have to be made 
explicit. The problem is how to define temporal constraints
and how to enforce them by appropriate resource usage in a
dynamic ad-hoc environment. In an system where 
interactions are spontaneous, it may be also necessary to determine
temporal properties dynamically. To do this, the respective
temporal information must be stated explicitly and available
during run-time. Secondly, it is not always ensured that
temporal properties can be fulfilled. In these cases, 
adaptations and timing failure notification must be provided [2,
28]. In most real-time systems, the notion of a deadline
is the prevailing scheme to express and enforce timeliness.
However, a deadline only weakly reflect the temporal 
characteristics of the information which is handled. Secondly, a
deadline often includes implicit knowledge about the system
and the relations between activities. In a rather well defined,
closed environment, it is possible to make such implicit 
assumptions and map these to execution times and deadlines.
E.g. the engineer knows how long a vehicle position can be
used before the vehicle movement outdates this information.
Thus he maps this dependency between speed and position
on a deadline which then assures that the position error
can be assumed to be bounded. In a open environment, this
implicit mapping is not possible any more because, as an 
obvious reason, the relation between speed and position, and
thus the error bound, cannot easily be reverse engineered
from a deadline. Therefore, our event model includes 
explicit quality attributes which allow to specify the temporal
attributes for every individual event. This is of course an
overhead compared to the use of implicit knowledge, but in
a dynamic environment such information is needed.
To illustrate the problem, consider the example of the
position of a vehicle. A position is a typical example for
time, value entity [30]. Thus, the position is useful if we
can determine an error bound which is related to time, e.g. if
we want a position error below 10 meters to establish a safety
property between cooperating cars moving with 5 m/sec,
the position has a validity time of 2 seconds. In a time,
value entity entity we can trade time against the precision
of the value. This is known as value over time and time over
value [18]. Once having established the time-value relation
and captured in event attributes, subscribers of this event
can locally decide about the usefulness of an information. In
the GEAR architecture temporal validity is used to reason
about safety properties in a event-based system [29]. We
will briefly review the respective notions and see how they
are exploited in our COSMIC event middleware.
Consider the timeline of generating an event representing
some real-time entity [18] from its occurrence to the 
notification of a certain sentient object (Figure 3). The real-time
entity is captured at the sensor interface of the system and
has to be transformed in a form which can be treated by a
computer. During the time interval t0 the sensor reads the
real-time entity and a time stamp is associated with the 
respective value. The derived time, value entity represents
an observation. It may be necessary to perform substantial
local computations to derive application relevant 
information from the raw sensor data. However, it should be noted
that the time stamp of the observation is associated with
the capture time and thus independent from further signal
processing and event generation. This close relationship 
between capture time and the associated value is supported by
smart sensors described above.
The processed sensor information is assembled in an event
data structure after ts to be published to an event channel.
As is described later, the event includes the time stamp of
generation and the temporal validity as attributes.
The temporal validity is an application defined measure
for the expiration of a time, value . As we explained in
the example of a position above, it may vary dependent on
application parameters. Temporal validity is a more general
concept than that of a deadline. It is independent of a 
certain technical implementation of a system. While deadlines
may be used to schedule the respective steps in an event
generation and dissemination, a temporal validity is an 
intrinsic property of a time, value entity carried in an event.
A temporal validity allows to reason about the usefulness
of information and is beneficial even in systems in which
timely dissemination of events cannot be enforced because
it enables timing failure detection at the event consumer. It
is obvious that deadlines or periods can be derived from the
temporal validity of an event. To set a deadline, knowledge
of an implementation, worst case execution times or 
message dissemination latencies is necessary. Thus, in the 
timeline of Figure 3 every interval may have a deadline. Event
dissemination through soft real-time channels in COSMIC
exploits the temporal validity to define dissemination 
deadlines. Quality attributes can be defined, for instance, in
terms of validity interval, omission degree pairs. These 
allow to characterize the usefulness of the event for a certain
application, in a certain context. Because of that, quality
attributes of an event clearly depend on higher level issues,
such as the nature of the sentient object or of the smart
sensor that produced the event. For instance, an event 
containing an indication of some vehicle speed must have 
different quality attributes depending on the kind of vehicle
33
real-world
event
observation:
<time stamp, value>
event generated
ready to be transmitted
event
received
notification
, to
t
event
producer communication network
event
consumer
event channel
push <event>
, ts , tm , tt , tn
, t o : t i m e t o o b t a i n a n o b s e r v a t i o n
, t s : t i m e t o p r o c e s s s e n s o r r e a d i n g
, t m : t i m e t o a s s e m b l e a n e v e n t m e s s a g e
, t t : t i m e t o t r a n s f e r t h e e v e n t o n t h e r e g u l a r n e t w o r k
, t n : t i m e f o r n o t i f i c a t i o n o n t h e c o n s u m e r s i t e
Figure 3: Event processing and dissemination.
from which it originated, or depending on its current speed.
The same happens with the position event of the car 
example above, whose validity depends on the current speed
and on a predefined required precision. However, since 
quality attributes are strictly related with the semantics of the
application or, at least, with some high level knowledge of
the purpose of the system (from which the validity of the
information can be derived), the definition of these quality
attributes may be done by exploiting the information 
provided at the programming interface. Therefore, it is 
important to understand how the system programmer can 
specify non-functional requirements at the API, and how these
requirements translate into quality attributes assigned to
events. While temporal validity is identified as an intrinsic
event property, which is exploited to decide on the 
usefulness of data at a certain point in time, it is still necessary
to provide a communication facility which can disseminate
the event before the validity is expired.
In a WAN-of-CANs network structure we have to cope
with very different network characteristics and quality of
service properties. Therefore, when crossing the network
boundaries the quality of service guarantees available in a
certain network will be lost and it will be very hard, costly
and perhaps impossible to achieve these properties in the
next larger area of the WAN-of CANs structure. CORTEX
has a couple of abstractions to cope with this situation 
(network zones, body/environment) which have been discussed
above. From the temporal point of view we need a high
level abstraction like the temporal validity for the 
individual event now to express our quality requirements of the 
dissemination over the network. The bound, coverage pair,
introduced in relation with the TCB [28] seems to be an
appropriate approach. It considers the inherent uncertainty
of networks and allows to trade the quality of dissemination
against the resources which are needed. In relation with
the event channel model discussed later, the bound, 
coverage pair allows to specify the quality properties of an event
channel independently of specific technical issues. Given
the typical environments in which sentient applications will
operate, where it is difficult or even impossible to provide
timeliness or reliability guarantees, we proposed an 
alternative way to handle non-functional application requirements,
in relation with the TCB approach [28]. The proposed 
approach exploits intrinsic characteristics of applications, such
as fail-safety, or time-elasticity, in order to secure QoS 
specifications of the form bound, coverage . Instead of 
constructing systems that rely on guaranteed bounds, the idea
is to use (possibly changing) bounds that are secured with a
constant probability all over the execution. This obviously
requires an application to be able to adapt to changing 
conditions (and/or changing bounds) or, if this is not possible,
to be able to perform some safety procedures when the 
operational conditions degrade to an unbearable level. The
bounds we mentioned above refer essentially to timeliness
bounds associated to the execution of local or distributed
activities, or combinations thereof. From these bounds it is
then possible to derive the quality attributes, in particular
validity intervals, that characterize the events published in
the event channel.
6.1 The Role of Smart Sensors and Actuators
Smart devices encapsulate hardware, software and 
mechanical components and provide information and a set of
well specified functions and which are closely related to
the interaction with the environment. The built-in 
computational components and the network interface enable the
implementation of a well-defined high level interface that
does not just provide raw transducer data, but a processed,
application-related set of events. Moreover, they exhibit an
autonomous spontaneous behaviour. They differ from 
general purpose nodes because they are dedicated to a certain
functionality which complies to their sensing and 
actuating capabilities while general purpose node may execute any
program.
Concerning the sentient object model, smart sensors and
actuators may be basic sentient objects themselves, 
consuming events from the real-world environment and producing
the respective generic events for the system"s event layer or,
34
vice versa consuming a generic event and converting it to a
real-world event by an actuation. Smart components 
therefore constitute the periphery, i.e. the real-world interface of
a more complex sentient object. The model of sentient 
objects also constitutes the framework to built more complex
virtual sensors by relating multiple (primary, i.e. sensors
which directly sense a physical entity) sensors.
Smart components translate events of the environment
to an appropriate form available at the event layer or, vice
versa, transform a system event into an actuation. For smart
components we can assume that:
• Smart components have dedicated resources to 
perform a specific function.
• These resources are not used for other purposes during
normal real-time operation.
• No local temporal conflicts occur that will change the
observable temporal behaviour.
• The functions of a component can usually only be
changed during a configuration procedure which is not
performed when the component is involved in critical
operations.
• An observation of the environment as a time,value
pair can be obtained with a bounded jitter in time.
Many predictability and scheduling problems arise from
the fact, that very low level timing behaviours have to be
handled on a single processor. Here, temporal 
encapsulation of activities is difficult because of the possible side 
effects when sharing a single processor resource. Consider the
control of a simple IR-range detector which is used for 
obstacle avoidance. Dependent on its range and the speed of
a vehicle, it has to be polled to prevent the vehicle from
crashing into an obstacle. On a single central processor,
this critical activity has to be coordinated with many 
similar, possibly less critical functions. It means that a very
fine grained schedule has to be derived based purely on the
artifacts of the low level device control. In a smart 
sensor component, all this low level timing behaviour can be
optimized and encapsulated. Thus we can assume temporal
encapsulation similar to information hiding in the functional
domain. Of course, there is still the problem to guarantee
that an event will be disseminated and recognized in due
time by the respective system components, but this relates
to application related events rather than the low artifacts of
a device timing. The main responsibility to provide 
timeliness guarantees is shifted to the event layer where these
events are disseminated. Smart sensors thus lead to network
centric system model. The network constitute the shared 
resource which has to be scheduled in a predictable way. The
COSMIC middleware introduced in the next section is an
approach to provide predictable event dissemination for a
network of smart sensors and actuators.
7. AN EVENT MODEL ANDMIDDLEWARE
FOR COOPERATING SMART DEVICES
An event model and a middleware suitable for smart 
components must support timely and reliable communication
and also must be resource efficient. COSMIC 
(COoperating Smart devices) is aimed at supporting the 
interaction between those components according to the concepts
introduced so far. Based on the model of a WAN-of-CANs,
we assume that the components are connected to some form
of CAN as a fieldbus or a special wireless sensor network
which provides specific network properties. E.g. a fieldbus
developed for control applications usually includes 
mechanisms for predictable communication while other networks
only support a best effort dissemination. A gateway 
connects these CANs to the next level in the network hierarchy.
The event system should allow the dynamic interaction over
a hierarchy of such networks and comply with the overall
CORTEX generic event model. Events are typed 
information carriers and are disseminated in a publisher/ subscriber
style [24, 7], which is particularly suitable because it 
supports generative, anonymous communication [3] and does
not create any artificial control dependencies between 
producers of information and the consumers. This decoupling
in space (no references or names of senders or receivers are
needed for communication) and the flow decoupling (no 
control transfer occurs with a data transfer) are well known [24,
7, 14] and crucial properties to maintain autonomy of 
components and dynamic interactions.
It is obvious that not all networks can provide the same
QoS guarantees and secondly, applications may have widely
differing requirements for event dissemination. 
Additionally, when striving for predictability, resources have to be
reserved and data structures must be set up before 
communication takes place. Thus, these things can not predictably
be made on the fly while disseminating an event. Therefore,
we introduced the notion of an event channel to cope with
differing properties and requirements and have an object to
which we can assign resources and reservations. The 
concept of an event channel is not new [10, 25], however, it has
not yet been used to reflect the properties of the underlying
heterogeneous communication networks and mechanisms as
described by the GEAR architecture. Rather, existing event
middleware allows to specify the priorities or deadlines of
events handled in an event server. Event channels allow
to specify the communication properties on the level of the
event system in a fine grained way. An event channel is
defined by:
event channel := subject, quality attributeList,
handlers
The subject determines the types of events event which
may be issued to the channel. The quality attributes model
the properties of the underlying communication network
and dissemination scheme. These attributes include latency
specifications, dissemination constraints and reliability 
parameters. The notion of zones which represent a guaranteed
quality of service in a subnetwork support this approach.
Our goal is to handle the temporal specifications as bound,
coverage pairs [28] orthogonal to the more technical 
questions of how to achieve a certain synchrony property of the
dissemination infrastructure. Currently, we support 
quality attributes of event channels in a CAN-Bus environment
represented by explicit synchrony classes.
The COSMIC middleware maps the channel properties to
lower level protocols of the regular network. Based on our
previous work on predictable protocols for the CAN-Bus,
COSMIC defines an abstract network which provides hard,
soft and non real-time message classes [21].
Correspondingly, we distinguish three event channel classes
according to their synchrony properties: hard real-time 
channels, soft real-time channels and non-real-time channels.
Hard real-time channels (HRTC) guarantee event 
propagation within the defined time constraints in the presence
35
of a specified number of omission faults. HRTECs are 
supported by a reservation scheme which is similar to the scheme
used in time-triggered protocols like TTP [16][31], TTP/A [17],
and TTCAN [8]. However, a substantial advantage over a
TDMA scheme is that due to CAN-Bus properties, 
bandwidth which was reserved but is not needed by a HRTEC
can be used by less critical traffic [21].
Soft real-time channels (SRTC) exploit the temporal 
validity interval of events to derive deadlines for scheduling.
The validity interval defines the point in time after which
an event becomes temporally inconsistent. Therefore, in a
real-time system an event is useless after this point and may
me discarded. The transmission deadline (DL) is defined as
the latest point in time when a message has to be 
transmitted and is specified in a time interval which is derived from
the expiration time:
tevent ready < DL < texpiration − ∆notification
texpiration defines the point in time when the temporal
validity expires. ∆notification is the expected end-to-end 
latency which includes the transfer time over the network and
the time the event may be delayed by the local event 
handling in the nodes. As said before, event deadlines are used
to schedule the dissemination by SRTECs. However, 
deadlines may be missed in transient overload situations or due
to arbitrary arrival times of events. On the publisher side
the application"s exception handler is called whenever the
event deadline expires before event transmission. At this
point in time the event is also not expected to arrive at the
subscriber side before the validity expires. Therefore, the
event is removed from the sending queue. On the subscriber
side the expiration time is used to schedule the delivery of
the event. If the event cannot be delivered until its 
expiration time it is removed from the respective queues allocated
by the COSMIC middleware. This prevents the 
communication system to be loaded by outdated messages.
Non-real-time channels do not assume any temporal 
specification and disseminate events in a best effort manner. An
instance of an event channel is created locally, whenever a
publisher makes an announcement for publication or a 
subscriber subscribes for an event notification. When a 
publisher announces publication, the respective data structures
of an event channel are created by the middleware. When
a subscriber subscribes to an event channel, it may specify
context attributes of an event which are used to filter events
locally. E.g. a subscriber may only be interested in events
generated at a certain location. Additionally the subscriber
specifies quality properties of the event channel. A more 
detailed description of the event channels can be found in [13].
Currently, COSMIC handles all event channels which 
disseminate events beyond the CAN network boundary as non
real-time event channels. This is mainly because we use the
TCP/IP protocol to disseminate events over wireless links
or to the standard Ethernet. However, there are a 
number of possible improvements which can easily be integrated
in the event channel model. The Timely Computing Base
(TCB) [28] can be exploited for timing failure detection and
thus would provide awareness for event dissemination in 
environments where timely delivery of events cannot be 
enforced. Additionally, there are wireless protocols which can
provide timely and reliable message delivery [6, 23] which
may be exploited for the respective event channel classes.
Events are the information carriers which are exchanged
between sentient objects through event channels. To cope
with the requirements of an ad-hoc environment, an event
includes the description of the context in which it has been
generated and quality attributes defining requirements for
dissemination. This is particularly important in an open,
dynamic environment where an event may travel over 
multiple networks. An event instance is specified as:
event := subject, context attributeList,
quality attributeList, contents
A subject defines the type of the event and is related
to the event contents. It supports anonymous 
communication and is used to route an event. The subject has to
match to the subject of the event channel through which
the event is disseminated. Attributes are complementary
to the event contents. They describe individual functional
and non-functional properties of the event. The context 
attributes describe the environment in which the event has
been generated, e.g. a location, an operational mode or a
time of occurrence. The quality attributes specify 
timeliness and dependability aspects in terms of validity 
interval, omission degree pairs. The validity interval defines the
point in time after which an event becomes temporally 
inconsistent [18]. As described above, the temporal validity
can be mapped to a deadline. However, usually a 
deadline is an engineering artefact which is used for scheduling
while the temporal validity is a general property of a time,
value entity. In a environment where a deadline cannot
be enforced, a consumer of an event eventually must decide
whether the event still is temporally consistent, i.e. 
represents a valid time, value entity.
7.1 The Architecture of the COSMIC
Middleware
On the architectural level, COSMIC distinguish three 
layers roughly depicted in Figure 4. Two of them, the event
layer and the abstract network layer are implemented by the
COSMIC middleware. The event layer provides the API for
the application and realizes the abstraction of event and
event channels.
The abstract network implements real-time message classes
and adapts the quality requirements to the underlying real
network. An event channel handler resides in every node. It
supports the programming interface and provides the 
necessary data structures for event-based communication. 
Whenever an object subscribes to a channel or a publisher 
announces a channel, the event channel handler is involved. It
initiates the binding of the channel"s subject, which is 
represented by a network independent unique identifier to an 
address of the underlying abstract network to enable 
communication [14]. The event channel handler then tightly 
cooperates with the respective handlers of the abstract network
layer to disseminate events or receive event notifications. It
should be noted that the QoS properties of the event layer
in general depend on what the abstract network layer can
provide. Thus, it may not always be possible to e.g. support
hard real-time event channels because the abstract network
layer cannot provide the respective guarantees. In [13], we
describe the protocols and services of the abstract network
layer particularly for the CAN-Bus.
As can be seen in Figure 4, the hard real-time (HRT)
message class is supported by a dedicated handler which is
able to provide the time triggered message dissemination.
36

event
notifications
HRT-msg
list
SRT-msg
queue
NRT-msg
queue
HRT-msg
calendar
HRTC
Handler
S/NRTC
Handler
Abstract Network
Layer
CAN Layer
RX Buffer TX Buffer
RX, TX, error
interrupts
Event Channel
Specs.
Event Layer
send
messages
exception
notification
exceptions,
notifications
ECH:
Event Channel
Handler
p u b l i s h a n n o u n c e s u b s c r i b e
b i n d i n g
p r o t o c o l
c o n f i g .
p r o t o c o l
Global
Time
Service
event
notifications
HRT-msg
list
SRT-msg
queue
NRT-msg
queue
HRT-msg
calendar
HRTC
Handler
S/NRTC
Handler
Abstract Network
Layer
CAN Layer
RX Buffer TX Buffer
RX, TX, error
interrupts
Event Channel
Specs.
Event Layer
send
messages
exception
notification
exceptions,
notifications
ECH:
Event Channel
Handler
p u b l i s h a n n o u n c e s u b s c r i b e
b i n d i n g
p r o t o c o l
c o n f i g .
p r o t o c o l
Global
Time
Service
Figure 4: Architecture layers of COSMIC.
The HRT handler maintains the HRT message list, which
contains an entry for each local HRT message to be sent.
The entry holds the parameters for the message, the 
activation status and the binding information. Messages are
scheduled on the bus according to the HRT message 
calendar which comprises the precise start time for each time slot
allocated for a message. Soft real-time message queues order
outgoing messages according to their transmission deadlines
derived from the temporal validity interval. If the 
transmission deadline is exceeded, the event message is purged out of
the queue. The respective application is notified via the 
exception notification interface and can take actions like trying
to publish the event again or publish it to a channel of 
another class. Incoming event messages are ordered according
to their temporal validity. If an event message arrive, the
respective applications are notified. At the moment, an 
outdated message is deleted from the queue and if the queue
runs out of space, the oldest message is discarded. 
However, there are other policies possible depending on event
attributes and available memory space. Non real-time 
messages are FIFO ordered in a fixed size circular buffer.
7.2 Status of COSMIC
The goal for developing COSMIC was to provide a 
platform to seamlessly integrate smart tiny components in a
large system. Therefore, COSMIC should run also on the
small, resource constraint devices which are built around 
16Bit or even 8-Bit micro-controllers. The distributed 
COSMIC middleware has been implemented and tested on 
various platforms. Under RT-Linux, we support the real-time
channels over the CAN Bus as described above. The 
RTLinux version runs on Pentium processors and is currently
evaluated before we intent to port it to a smart sensor or
actuator. For the interoperability in a WAN-of-CANs 
environment, we only provide non real-time channels at the 
moment. This version includes a gateway between the 
CANbus and a TCP/IP network. It allows us to use a 
standard wireless 802.11 network. The non real-time version of
COSMIC is available on Linux, RT-Linux and on the 
microcontroller families C167 (Infineon) and 68HC908 (Motorola).
Both micro-controllers have an on-board CAN controller
and thus do not require additional hardware components for
the network. The memory footprint of COSMIC is about 13
Kbyte on a C167 and slightly more on the 68HC908 where it
fits into the on-board flash memory without problems. 
Because only a few channels are required on such a smart sensor
or actuator component, the requirement of RAM (which is
a scarce resource on many single chip systems) to hold the
dynamic data structures of a channel is low. The COSMIC
middleware makes it very easy to include new smart sensors
in an existing system. Particularly, the application running
on a smart sensor to condition and process the raw physical
data must not be aware of any low level network specific 
details. It seamlessly interacts with other components of the
system exclusively via event channels.
The demo example, briefly described in the next chapter,
is using a distributed infrastructure of tiny smart sensors
and actuators directly cooperating via event channels over
heterogeneous networks.
8. AN ILLUSTRATIVE EXAMPLE
A simple example for many important properties of the
proposed system showing the coordination through the 
environment and events disseminated over the network is the
demo of two cooperating robots depicted in Figure 5.
Each robot is equipped with smart distance sensors, speed
sensors, acceleration sensors and one of the robots (the guide
(KURT2) in front (Figure 5)) has a tracking camera 
allowing to follow a white line. The robots form a WAN-of-CANs
system in which their local CANs are interconnected via a
wireless 802.11 network. COSMIC provides the event layer
for seamless interaction. The blind robot (N.N.) is 
searching the guide randomly. Whenever the blind robot detects
(by its front distance sensors) an obstacle, it checks whether
this may be the guide. For this purpose, it dynamically 
subscribes to the event channel disseminating distance events
from rear distance sensors of the guide(s) and compares
these with the distance events from its local front sensors.
If the distance is approximately the same it infers that it
is really behind a guide. Now N.N. also subscribes to the
event channels of the tracking camera and the speed sensors
37
Figure 5: Cooperating robots.
to follow the guide. The demo application highlights the
following properties of the system:
1. Dynamic interaction of robots which is not known in
advance. In principle, any two a priori unknown robots
can cooperate. All what publishers and subscribers
have to know to dynamically interact in this 
environment is the subject of the respective event class. A
problem will be to receive only the events of the robot
which is closest. A robot identity does not help much
to solve this problem. Rather, the position of the event
generation entity which is captured in the respective
attributes can be evaluated to filter the relevant event
out of the event stream. A suitable wireless protocol
which uses proximity to filter events has been proposed
by Meier and Cahill [22] in the CORTEX project.
2. Interaction through the environment. The 
cooperation between the robots is controlled by sensing the
distance between the robots. If the guide detects that
the distance grows, it slows down. Respectively, if the
blind robot comes too close it reduces its speed. The
local distance sensors produce events which are 
disseminated through a low latency, highly predictable event
channel. The respective reaction time can be 
calculated as function of the speed and the distance of the
robots and define a dynamic dissemination deadline
for events. Thus, the interaction through the 
environment will secure the safety properties of the 
application, i.e. the follower may not crash into the guide and
the guide may not loose the follower. Additionally, the
robots have remote subscriptions to the respective 
distance events which are used to check it with the local
sensor readings to validate that they really follow the
guide which they detect with their local sensors. 
Because there may be longer latencies and omissions, this
check occasionally will not be possible. The 
unavailability of the remote events will decrease the quality
of interaction and probably and slow down the robots,
but will not affect safety properties.
3. Cooperative sensing. The blind robot subscribes to the
events of the line tracking camera. Thus it can see
through the eye of the guide. Because it knows the 
distance to the guide and the speed as well, it can foresee
the necessary movements. The proposed system 
provides the architectural framework for such a 
cooperation. The respective sentient object controlling the
actuation of the robot receives as input the position
and orientation of the white line to be tracked. In the
case of the guide robot, this information is directly 
delivered as a body event with a low latency and a high
reliability over the internal network. For the follower
robot, the information comes also via an event channel
but with different quality attributes. These quality 
attributes are reflected in the event channel description.
The sentient object controlling the actuation of the
follower is aware of the increased latency and higher
probability of omission.
9. CONCLUSION AND FUTURE WORK
The paper addresses problems of building large distributed
systems interacting with the physical environment and 
being composed from a huge number of smart components.
We cannot assume that the network architecture in such a
system is homogeneous. Rather multiple edge- networks
are fused to a hierarchical, heterogeneous wide area 
network. They connect the tiny sensors and actuators 
perceiving the environment and providing sentience to the 
application. Additionally, mobility and dynamic deployment of
components require the dynamic interaction without fixed,
a priori known addressing and routing schemes. The work
presented in the paper is a contribution towards the 
seamless interaction in such an environment which should not be
restricted by technical obstacles. Rather it should be 
possible to control the flow of information by explicitly specifying
functional and temporal dissemination constraints.
The paper presented the general model of a sentient 
object to describe composition, encapsulation and interaction
in such an environment and developed the Generic Event 
Architecture GEAR which integrates the interaction through
the environment and the network. While appropriate 
abstractions and interaction models can hide the functional
heterogeneity of the networks, it is impossible to hide the
quality differences. Therefore, one of the main concerns is
to define temporal properties in such an open 
infrastructure. The notion of an event channel has been introduced
which allows to specify quality aspects explicitly. They can
be verified at subscription and define a boundary for event
dissemination. The COSMIC middleware is a first attempt
to put these concepts into operation. COSMIC allows the
interoperability of tiny components over multiple network
boundaries and supports the definition of different real-time
event channel classes.
There are many open questions that emerged from our
work. One direction of future research will be the inclusion
of real-world communication channels established between
sensors and actuators in the temporal analysis and the 
ordering of such events in a cause-effect chain. Additionally,
the provision of timing failure detection for the adaptation
of interactions will be in the focus of our research. To reduce
network traffic and only disseminate those events to the 
subscribers which they are really interested in and which have
a chance to arrive timely, the encapsulation and scoping
schemes have to be transformed into respective multi-level
filtering rules. The event attributes which describe aspects
of the context and temporal constraints for the 
dissemination will be exploited for this purpose. Finally, it is intended
to integrate the results in the COSMIC middleware to 
enable experimental assessment.
38
10. REFERENCES
[1] J. Bacon, K. Moody, J. Bates, R. Hayton, C. Ma,
A. McNeil, O. Seidel, and M. Spiteri. Generic support
for distributed applications. IEEE Computer,
33(3):68-76, 2000.
[2] L. B. Becker, M. Gergeleit, S. Schemmer, and E. Nett.
Using a flexible real-time scheduling strategy in a
distributed embedded application. In Proc. of the 9th
IEEE International Conference on Emerging
Technologies and Factory Automation (ETFA), Lisbon,
Portugal, Sept. 2003.
[3] N. Carriero and D. Gelernter. Linda in context.
Communications of the ACM, 32(4):444-458, apr 1989.
[4] A. Casimiro (Ed.). Preliminary definition of cortex
system architecture. CORTEX project,
IST-2000-26031, Deliverable D4, Apr. 2002.
[5] CORTEX project Annex 1, Description of Work.
Technical report, CORTEX project, IST-2000-26031,
Oct. 2000. http://cortex.di.fc.ul.pt.
[6] R. Cunningham and V. Cahill. Time bounded medium
access control for ad-hoc networks. In Proceedings of
the Second ACM International Workshop on Principles
of Mobile Computing (POMC"02), pages 1-8, Toulouse,
France, Oct. 2002. ACM Press.
[7] P. T. Eugster, P. Felber, R. Guerraoui, and A.-M.
Kermarrec. The many faces of publish/subscribe.
Technical Report DSC ID:200104, EPFL, Lausanne,
Switzerland, 2001.
[8] T. F¨uhrer, B. M¨uller, W. Dieterle, F. Hartwich,
R. Hugel, and M.Walther. Time triggered
communication on CAN, 2000.
http://www.can-cia.org/can/ttcan/fuehrer.pdf.
[9] R. B. GmbH. CAN Specification Version 2.0. Technical
report, Sept. 1991.
[10] T. Harrison, D. Levine, and D. Schmidt. The design
and performance of a real-time corba event service. In
Proceedings of the 1997 Conference on Object Oriented
Programming Systems, Languages and Applications
(OOPSLA), pages 184-200, Atlanta, Georgia, USA,
1997. ACM Press.
[11] J. Hightower and G. Borriello. Location systems for
ubiquitous computing. IEEE Computer, 34(8):57-66,
aug 2001.
[12] A. Hopper. The Clifford Paterson Lecture, 1999
Sentient Computing. Philosophical Transactions of the
Royal Society London, 358(1773):2349-2358, Aug. 2000.
[13] J. Kaiser, C. Mitidieri, C. Brudna, and C. Pereira.
COSMIC: A Middleware for Event-Based Interaction
on CAN. In Proc. 2003 IEEE Conference on Emerging
Technologies and Factory Automation, Lisbon,
Portugal, Sept. 2003.
[14] J. Kaiser and M. Mock. Implementing the real-time
publisher/subscriber model on the controller area
network (CAN). In Proceedings of the 2nd International
Symposium on Object-oriented Real-time distributed
Computing (ISORC99), Saint-Malo, France, May 1999.
[15] K. Kim, G. Jeon, S. Hong, T. Kim, and S. Kim.
Integrating subscription-based and connection-oriented
communications into the embedded CORBA for
the CAN Bus. In Proceedings of the IEEE Real-time
Technology and Application Symposium, May 2000.
[16] H. Kopetz and G. Gr¨unsteidl. TTP - A
Time-Triggered Protocol for Fault-Tolerant Real-Time
Systems. Technical Report rr-12-92, Institut f¨ur
Technische Informatik, Technische Universit¨at Wien,
Treilstr. 3/182/1, A-1040 Vienna, Austria, 1992.
[17] H. Kopetz, M. Holzmann, and W. Elmenreich. A
Universal Smart Transducer Interface: TTP/A.
International Journal of Computer System, Science
Engineering, 16(2), Mar. 2001.
[18] H. Kopetz and P. Ver´ıssimo. Real-time and
Dependability Concepts. In S. J. Mullender, editor,
Distributed Systems, 2nd Edition, ACM-Press,
chapter 16, pages 411-446. Addison-Wesley, 1993.
[19] S. Lankes, A. Jabs, and T. Bemmerl. Integration of a
CAN-based connection-oriented communication model
into Real-Time CORBA. In Workshop on Parallel and
Distributed Real-Time Systems, Nice, France, Apr.
2003.
[20] Local Interconnect Network: LIN Specification
Package Revision 1.2. Technical report, Nov. 2000.
[21] M. Livani, J. Kaiser, and W. Jia. Scheduling hard and
soft real-time communication in the controller area
network. Control Engineering, 7(12):1515-1523, 1999.
[22] R. Meier and V. Cahill. Steam: Event-based
middleware for wireless ad-hoc networks. In Proceedings
of the International Workshop on Distributed
Event-Based Systems (ICDCS/DEBS"02), pages
639-644, Vienna, Austria, 2002.
[23] E. Nett and S. Schemmer. Reliable real-time
communication in cooperative mobile applications.
IEEE Transactions on Computers, 52(2):166-180, Feb.
2003.
[24] B. Oki, M. Pfluegl, A. Seigel, and D. Skeen. The
information bus - an architecture for extensible
distributed systems. Operating Systems Review,
27(5):58-68, 1993.
[25] O. M. G. (OMG). CORBAservices: Common Object
Services Specification - Notification Service
Specification, Version 1.0, 2000.
[26] O. M. G. (OMG). Smart transducer interface, initial
submission, June 2001.
[27] P. Ver´ıssimo, V. Cahill, A. Casimiro, K. Cheverst,
A. Friday, and J. Kaiser. Cortex: Towards supporting
autonomous and cooperating sentient entities. In
Proceedings of European Wireless 2002, Florence, Italy,
Feb. 2002.
[28] P. Ver´ıssimo and A. Casimiro. The Timely Computing
Base model and architecture. Transactions on
Computers - Special Issue on Asynchronous Real-Time
Systems, 51(8):916-930, Aug. 2002.
[29] P. Ver´ıssimo and A. Casimiro. Event-driven support of
real-time sentient objects. In Proceedings of the 8th
IEEE International Workshop on Object-oriented
Real-time Dependable Systems, Guadalajara, Mexico,
Jan. 2003.
[30] P. Ver´ıssimo and L. Rodrigues. Distributed Systems for
System Architects. Kluwer Academic Publishers, 2001.
39
Selfish Caching in Distributed Systems:
A Game-Theoretic Analysis
Byung-Gon Chun
∗
bgchun@cs.berkeley.edu
Kamalika Chaudhuri
†
kamalika@cs.berkeley.edu
Hoeteck Wee
‡
hoeteck@cs.berkeley.edu
Marco Barreno
§
barreno@cs.berkeley.edu
Christos H. Papadimitriou
†
christos@cs.berkeley.edu
John Kubiatowicz
∗
kubitron@cs.berkeley.edu
Computer Science Division
University of California, Berkeley
ABSTRACT
We analyze replication of resources by server nodes that act 
selfishly, using a game-theoretic approach. We refer to this as the 
selfish caching problem. In our model, nodes incur either cost for 
replicating resources or cost for access to a remote replica. We show the
existence of pure strategy Nash equilibria and investigate the price
of anarchy, which is the relative cost of the lack of coordination.
The price of anarchy can be high due to undersupply problems, but
with certain network topologies it has better bounds. With a 
payment scheme the game can always implement the social optimum
in the best case by giving servers incentive to replicate.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed 
Systems
General Terms
Algorithms, Economics, Theory, Performance
1. INTRODUCTION
Wide-area peer-to-peer file systems [2,5,22,32,33], peer-to-peer
caches [15, 16], and web caches [6, 10] have become popular over
the last few years. Caching1
of files in selected servers is widely
used to enhance the performance, availability, and reliability of
these systems. However, most such systems assume that servers
cooperate with one another by following protocols optimized for
overall system performance, regardless of the costs incurred by
each server.
In reality, servers may behave selfishly - seeking to maximize
their own benefit. For example, parties in different 
administrative domains utilize their local resources (servers) to better 
support clients in their own domains. They have obvious incentives to
cache objects2
that maximize the benefit in their domains, possibly
at the expense of globally optimum behavior. It has been an open
question whether these caching scenarios and protocols maintain
their desirable global properties (low total social cost, for example)
in the face of selfish behavior.
In this paper, we take a game-theoretic approach to analyzing
the problem of caching in networks of selfish servers through 
theoretical analysis and simulations. We model selfish caching as a
non-cooperative game. In the basic model, the servers have two
possible actions for each object. If a replica of a requested object
is located at a nearby node, the server may be better off accessing
the remote replica. On the other hand, if all replicas are located too
far away, the server is better off caching the object itself. Decisions
about caching the replicas locally are arrived at locally, taking into
account only local costs. We also define a more elaborate payment
model, in which each server bids for having an object replicated at
another site. Each site now has the option of replicating an object
and collecting the related bids. Once all servers have chosen a 
strategy, each game specifies a configuration, that is, the set of servers
that replicate the object, and the corresponding costs for all servers.
Game theory predicts that such a situation will end up in a Nash
equilibrium, that is, a set of (possibly randomized) strategies with
the property that no player can benefit by changing its strategy
while the other players keep their strategies unchanged [28]. 
Foundational considerations notwithstanding, it is not easy to accept
randomized strategies as the behavior of rational agents in a 
distributed system (see [28] for an extensive discussion) - but this
is what classical game theory can guarantee. In certain very 
fortunate situations, however (see [9]), the existence of pure (that is,
deterministic) Nash equilibria can be predicted.
With or without randomization, however, the lack of 
coordination inherent in selfish decision-making may incur costs well 
beyond what would be globally optimum. This loss of efficiency is
1
We will use caching and replication interchangeably.
2
We use the term object as an abstract entity that represents files
and other data objects.
21
quantified by the price of anarchy [21]. The price of anarchy is
the ratio of the social (total) cost of the worst possible Nash 
equilibrium to the cost of the social optimum. The price of anarchy
bounds the worst possible behavior of a selfish system, when left
completely on its own. However, in reality there are ways whereby
the system can be guided, through seeding or incentives, to a 
preselected Nash equilibrium. This optimistic version of the price of
anarchy [3] is captured by the smallest ratio between a Nash 
equilibrium and the social optimum.
In this paper we address the following questions :
• Do pure strategy Nash equilibria exist in the caching game?
• If pure strategy Nash equilibria do exist, how efficient are
they (in terms of the price of anarchy, or its optimistic 
counterpart) under different placement costs, network topologies,
and demand distributions?
• What is the effect of adopting payments? Will the Nash 
equilibria be improved?
We show that pure strategy Nash equilibria always exist in the
caching game. The price of anarchy of the basic game model can
be O(n), where n is the number of servers; the intuitive reason is
undersupply. Under certain topologies, the price of anarchy does
have tighter bounds. For complete graphs and stars, it is O(1). For
D-dimensional grids, it is O(n
D
D+1 ). Even the optimistic price of
anarchy can be O(n). In the payment model, however, the game
can always implement a Nash equilibrium that is same as the social
optimum, so the optimistic price of anarchy is one.
Our simulation results show several interesting phases. As the
placement cost increases from zero, the price of anarchy increases.
When the placement cost first exceeds the maximum distance 
between servers, the price of anarchy is at its highest due to 
undersupply problems. As the placement cost further increases, the price
of anarchy decreases, and the effect of replica misplacement 
dominates the price of anarchy.
The rest of the paper is organized as follows. In Section 2 we 
discuss related work. Section 3 discusses details of the basic game and
analyzes the bounds of the price of anarchy. In Section 4 we discuss
the payment game and analyze its price of anarchy. In Section 5 we
describe our simulation methodology and study the properties of
Nash equilibria observed. We discuss extensions of the game and
directions for future work in Section 6.
2. RELATED WORK
There has been considerable research on wide-area peer-to-peer
file systems such as OceanStore [22], CFS [5], PAST [32], 
FARSITE [2], and Pangaea [33], web caches such as NetCache [6] and
SummaryCache [10], and peer-to-peer caches such as Squirrel [16].
Most of these systems use caching for performance, availability,
and reliability. The caching protocols assume obedience to the 
protocol and ignore participants" incentives. Our work starts from the
assumption that servers are selfish and quantifies the cost of the
lack of coordination when servers behave selfishly.
The placement of replicas in the caching problem is the most 
important issue. There is much work on the placement of web 
replicas, instrumentation servers, and replicated resources. All 
protocols assume obedience and ignore participants" incentives. In [14],
Gribble et al. discuss the data placement problem in peer-to-peer
systems. Ko and Rubenstein propose a self-stabilizing, distributed
graph coloring algorithm for the replicated resource placement [20].
Chen, Katz, and Kubiatowicz propose a dynamic replica 
placement algorithm exploiting underlying distributed hash tables [4].
Douceur and Wattenhofer describe a hill-climbing algorithm to 
exchange replicas for reliability in FARSITE [8]. RaDar is a 
system that replicates and migrates objects for an Internet hosting 
service [31]. Tang and Chanson propose a coordinated en-route web
caching that caches objects along the routing path [34]. 
Centralized algorithms for the placement of objects, web proxies, mirrors,
and instrumentation servers in the Internet have been studied 
extensively [18,19,23,30].
The facility location problem has been widely studied as a 
centralized optimization problem in theoretical computer science and
operations research [27]. Since the problem is NP-hard, 
approximation algorithms based on primal-dual techniques, greedy 
algorithms, and local search have been explored [17, 24, 26]. Our
caching game is different from all of these in that the optimization
process is performed among distributed selfish servers.
There is little research in non-cooperative facility location games,
as far as we know. Vetta [35] considers a class of problems where
the social utility is submodular (submodularity means decreasing
marginal utility). In the case of competitive facility location among
corporations he proves that any Nash equilibrium gives an expected
social utility within a factor of 2 of optimal plus an additive term
that depends on the facility opening cost. Their results are not 
directly applicable to our problem, however, because we consider
each server to be tied to a particular location, while in their model
an agent is able to open facilities in multiple locations. Note that in
that paper the increase of the price of anarchy comes from 
oversupply problems due to the fact that competing corporations can open
facilities at the same location. On the other hand, the significant
problems in our game are undersupply and misplacement.
In a recent paper, Goemans et al. analyze content distribution on
ad-hoc wireless networks using a game-theoretic approach [12]. As
in our work, they provide monetary incentives to mobile users for
caching data items, and provide tight bounds on the price of 
anarchy and speed of convergence to (approximate) Nash equilibria.
However, their results are incomparable to ours because their 
payoff functions neglect network latencies between users, they 
consider multiple data items (markets), and each node has a limited
budget to cache items.
Cost sharing in the facility location problem has been studied
using cooperative game theory [7, 13, 29]. Goemans and Skutella
show strong connections between fair cost allocations and linear
programming relaxations for facility location problems [13]. P´al
and Tardos develop a method for cost-sharing that is approximately
budget-balanced and group strategyproof and show that the method
recovers 1/3 of the total cost for the facility location game [29].
Devanur, Mihail, and Vazirani give a strategyproof cost allocation
for the facility location problem, but cannot achieve group 
strategyproofness [7].
3. BASIC GAME
The caching problem we study is to find a configuration that
meets certain objectives (e.g., minimum total cost). Figure 1 shows
examples of caching among four servers. In network (a), A stores
an object. Suppose B wants to access the object. If it is cheaper
to access the remote replica than to cache it, B accesses the remote
replica as shown in network (b). In network (c), C wants to access
the object. If C is far from A, C caches the object instead of 
accessing the object from A. It is possible that in an optimal configuration
it would be better to place replicas in A and B. Understanding the
placement of replicas by selfish servers is the focus of our study.
The caching problem is abstracted as follows. There is a set N of
n servers and a set M of m objects. The distance between servers
can be represented as a distance matrix D (i.e., dij is the distance
22
Server
Server
Server
Server
A
B
C
D
(a)
Server
Server
Server
Server
A
B
C
D
(b)
Server
Server
Server
Server
A
B
C
D
(c)
Figure 1: Caching. There are four servers labeled A, B, C, and D. The rectangles are object replicas. In (a), A stores an object. If B incurs less cost
accessing A"s replica than it would caching the object itself, it accesses the object from A as in (b). If the distance cost is too high, the server caches
the object itself, as C does in (c). This figure is an example of our caching game model.
from server i to server j). D models an underlying network 
topology. For our analysis we assume that the distances are symmetric
and the triangle inequality holds on the distances (for all servers
i, j, k: dij + djk ≥ dik). Each server has demand from clients
that is represented by a demand matrix W (i.e., wij is the demand
of server i for object j). When a server caches objects, the server
incurs some placement cost that is represented by a matrix α (i.e.,
αij is a placement cost of server i for object j).
In this study, we assume that servers have no capacity limit. As
we discuss in the next section, this fact means that the caching
behavior with respect to each object can be examined separately.
Consequently, we can talk about configurations of the system with
respect to a given object:
DEFINITION 1. A configuration X for some object O is the set
of servers replicating this object.
The goal of the basic game is to find configurations that are achieved
when servers optimize their cost functions locally.
3.1 Game Model
We take a game-theoretic approach to analyzing the 
uncapacitated caching problem among networked selfish servers. We model
the selfish caching problem as a non-cooperative game with n 
players (servers/nodes) whose strategies are sets of objects to cache. In
the game, each server chooses a pure strategy that minimizes its
cost. Our focus is to investigate the resulting configuration, which
is the Nash equilibrium of the game. It should be emphasized that
we consider only pure strategy Nash equilibria in this paper.
The cost model is an important part of the game. Let Ai be the
set of feasible strategies for server i, and let Si ∈ Ai be the strategy
chosen by server i. Given a strategy profile S = (S1, S2, ..., Sn),
the cost incurred by server i is defined as:
Ci(S) =
j∈Si
αij +
j /∈Si
wij di (i,j). (1)
where αij is the placement cost of object j, wij is the demand that
server i has for object j, (i, j) is the closest server to i that caches
object j, and dik is the distance between i and k. When no server
caches the object, we define distance cost di (i,j) to be dM -large
enough that at least one server will choose to cache the object.
The placement cost can be further divided into first-time 
installation cost and maintenance cost:
αij = k1i + k2i
UpdateSizej
ObjectSizej
1
T
Pj
k
wkj , (2)
where k1i is the installation cost, k2i is the relative weight 
between the maintenance cost and the installation cost, Pj is the 
ratio of the number of writes over the number of reads and writes,
UpdateSizej is the size of an update, ObjectSizej is the size of
the object, and T is the update period. We see tradeoffs between
different parameters in this equation. For example, placing replicas
becomes more expensive as UpdateSizej increases, Pj increases,
or T decreases. However, note that by varying αij itself we can
capture the full range of behaviors in the game. For our analysis,
we use only αij .
Since there is no capacity limit on servers, we can look at each
single object as a separate game and combine the pure strategy
equilibria of these games to obtain a pure strategy equilibrium of
the multi-object game. Fabrikant, Papadimitriou, and Talwar 
discuss this existence argument: if two games are known to have pure
equilibria, and their cost functions are cross-monotonic, then their
union is also guaranteed to have pure Nash equilibria, by a 
continuity argument [9]. A Nash equilibrium for the multi-object game is
the cross product of Nash equilibria for single-object games. 
Therefore, we can focus on the single object game in the rest of this paper.
For single object selfish caching, each server i has two strategies
- to cache or not to cache. The object under consideration is j.
We define Si to be 1 when server i caches j and 0 otherwise. The
cost incurred by server i is
Ci(S) = αij Si + wij di (i,j)(1 − Si). (3)
We refer to this game as the basic game. The extent to which Ci(S)
represents actual cost incurred by server i is beyond the scope of
this paper; we will assume that an appropriate cost function of the
form of Equation 3 can be defined.
3.2 Nash Equilibrium Solutions
In principle, we can start with a random configuration and let
this configuration evolve as each server alters its strategy and 
attempts to minimize its cost. Game theory is interested in stable
solutions called Nash equilibria. A pure strategy Nash equilibrium
is reached when no server can benefit by unilaterally changing its
strategy. A Nash equilibrium3
(S∗
i , S∗
−i) for the basic game 
specifies a configuration X such that ∀i ∈ N, i ∈ X ⇔ S∗
i = 1.
Thus, we can consider a set E of all pure strategy Nash equilibrium
configurations:
X ∈ E ⇔ ∀i ∈ N,
∀Si ∈ Ai, Ci(S∗
i , S∗
−i) ≤ Ci(Si, S∗
−i)
(4)
By this definition, no server has incentive to deviate in the 
configurations since it cannot reduce its cost.
For the basic game, we can easily see that:
X ∈ E ⇔ ∀i ∈ N, ∃j ∈ X s.t. dji ≤ α
and ∀j ∈ X, ¬∃k ∈ X s.t. dkj < α
(5)
The first condition guarantees that there is a server that places the
replica within distance α of each server i. If the replica is not placed
3
The notation for strategy profile (S∗
i , S∗
−i) separates node i s
strategy (S∗
i ) from the strategies of other nodes (S∗
−i).
23
A B1−α
0
0
0
0
0 0
0
0
0
0
2
n
nodes
2
n
nodes
(a)
A B1−α
0
0
0
0
0 0
0
0
0
0
2
n
nodes
2
n
nodes
(b)
A B1−α
2
n
nodes
2
n
nodes
n2
n2
n2
n2
n2 n2
n2
n2
n2
n2
(c)
Figure 2: Potential inefficiency of Nash equilibria illustrated by two clusters of n
2
servers. The intra-cluster distances are all zero and the distance
between clusters is α − 1, where α is the placement cost. The dark nodes replicate the object. Network (a) shows a Nash equilibrium in the basic
game, where one server in a cluster caches the object. Network (b) shows the social optimum where two replicas, one for each cluster, are placed. The
price of anarchy is O(n) and even the optimistic price of anarchy is O(n). This high price of anarchy comes from the undersupply of replicas due to
the selfish nature of servers. Network (c) shows a Nash equilibrium in the payment game, where two replicas, one for each cluster, are placed. Each
light node in each cluster pays 2/n to the dark node, and the dark node replicates the object. Here, the optimistic price of anarchy is one.
at i, then it is placed at another server within distance α of i, so i has
no incentive to cache. If the replica is placed at i, then the second
condition ensures there is no incentive to drop the replica because
no two servers separated by distance less than α both place replicas.
3.3 Social Optimum
The social cost of a given strategy profile is defined as the total
cost incurred by all servers, namely:
C(S) =
n−1
i=0
Ci(S) (6)
where Ci(S) is the cost incurred by server i given by Equation 1.
The social optimum cost, referred to as C(SO) for the remainder
of the paper, is the minimum social cost. The social optimum cost
will serve as an important base case against which to measure the
cost of selfish caching. We define C(SO) as:
C(SO) = min
S
C(S) (7)
where S varies over all possible strategy profiles. Note that in the
basic game, this means varying configuration X over all possible
configurations. In some sense, C(SO) represents the best possible
caching behavior - if only nodes could be convinced to cooperate
with one another.
The social optimum configuration is a solution of a mini-sum
facility location problem, which is NP-hard [11]. To find such 
configurations, we formulate an integer programming problem:
minimize
Èi
Èj
¢αij xij +
Èk wij dikyijk
£
subject to
∀i, j
Èk yijk = I(wij)
∀i, j, k xij − ykji ≥ 0
∀i, j xij ∈ {0, 1}
∀i, j, k yijk ∈ {0, 1}
(8)
Here, xij is 1 if server i replicates object j and 0 otherwise; yijk
is 1 if server i accesses object j from server k and 0 otherwise;
I(w) returns 1 if w is nonzero and 0 otherwise. The first constraint
specifies that if server i has demand for object j, then it must access
j from exactly one server. The second constraint ensures that server
i replicates object j if any other server accesses j from i.
3.4 Analysis
To analyze the basic game, we first give a proof of the existence
of pure strategy Nash equilibria. We discuss the price of anarchy in
general and then on specific underlying topologies. In this analysis
we use simply α in place of αij , since we deal with a single object
and we assume placement cost is the same for all servers. In 
addition, when we compute the price of anarchy, we assume that all
nodes have the same demand (i.e., ∀i ∈ N wij = 1).
THEOREM 1. Pure strategy Nash equilibria exist in the basic
game.
PROOF. We show a constructive proof. First, initialize the set
V to N. Then, remove all nodes with zero demand from V . Each
node x defines βx, where βx = α
wxj
. Furthermore, let Z(y) =
{z : dzy ≤ βz, z ∈ V }; Z(y) represents all nodes z for which y
lies within βz from z.
Pick a node y ∈ V such that βy ≤ βx for all x ∈ V . Place a
replica at y and then remove y and all z ∈ Z(y) from V . No such z
can have incentive to replicate the object because it can access y"s
replica at lower (or equal) cost. Iterate this process of placing 
replicas until V is empty. Because at each iteration y is the remaining
node with minimum β, no replica will be placed within distance
βy of any such y by this process. The resulting configuration is a
pure-strategy Nash equilibrium of the basic game.
The Price of Anarchy (POA): To quantify the cost of lack of
coordination, we use the price of anarchy [21] and the optimistic
price of anarchy [3]. The price of anarchy is the ratio of the social
costs of the worst-case Nash equilibrium and the social optimum,
and the optimistic price of anarchy is the ratio of the social costs of
the best-case Nash equilibrium and the social optimum.
We show general bounds on the price of anarchy. Throughout
our discussion, we use C(SW ) to represent the cost of worst case
Nash equilibrium, C(SO) to represent the cost of social optimum,
and PoA to represent the price of anarchy, which is C(SW )
C(SO)
.
The worst case Nash equilibrium maximizes the total cost 
under the constraint that the configuration meets the Nash condition.
Formally, we can define C(SW ) as follows.
C(SW ) = max
X∈E
(α|X| +
i
min
j∈X
dij) (9)
where minj∈X dij is the distance to the closest replica (including i
itself) from node i and X varies through Nash equilibrium 
configurations.
Bounds on the Price of Anarchy: We show bounds of the price
of anarchy varying α. Let dmin = min(i,j)∈N×N,i=j dij and
dmax = max(i,j)∈N×N dij . We see that if α ≤ dmin, PoA = 1
24
Topology PoA
Complete graph 1
Star ≤ 2
Line O(
√
n)
D-dimensional grid O(n
D
D+1 )
Table 1: PoA in the basic game for specific topologies
trivially, since every server caches the object for both Nash 
equilibrium and social optimum. When α > dmax, there is a transition
in Nash equilibria: since the placement cost is greater than any 
distance cost, only one server caches the object and other servers 
access it remotely. However, the social optimum may still place 
multiple replicas. Since α ≤ C(SO) ≤ α+minj∈N
Èi dij when α >
dmax, we obtain
α+maxj∈N
Èi dij
α+minj∈N
Èi dij
≤ PoA ≤
α+maxj∈N
Èi dij
α
.
Note that depending on the underlying topology, even the lower
bound of PoA can be O(n). Finally, there is a transition when
α > maxj∈N
Èi dij. In this case, PoA =
α+maxj∈N
Èi dij
α+minj∈N
Èi dij
and
it is upper bounded by 2.
Figure 2 shows an example of the inefficiency of a Nash 
equilibrium. In the network there are two clusters of servers whose
size is n
2
. The distance between two clusters is α − 1 where α is
the placement cost. Figure 2(a) shows a Nash equilibrium where
one server in a cluster caches the object. In this case, C(SW ) =
α + (α − 1)n
2
, since all servers in the other cluster accesses the 
remote replica. However, the social optimum places two replicas, one
for each cluster, as shown in Figure 2(b). Therefore, C(SO) = 2α.
PoA =
α+(α−1) n
2
2α
, which is O(n). This bad price of anarchy
comes from an undersupply of replicas due to the selfish nature of
the servers. Note that all Nash equilibria have the same cost; thus
even the optimistic price of anarchy is O(n).
In Appendix A, we analyze the price of anarchy with specific
underlying topologies and show that PoA can have tighter bounds
than O(n) for the complete graph, star, line, and D-dimensional
grid. In these topologies, we set the distance between directly 
connected nodes to one. We describe the case where α > 1, since
PoA = 1 trivially when α ≤ 1. A summary of the results is
shown in Table 1.
4. PAYMENT GAME
In this section, we present an extension to the basic game with
payments and analyze the price of anarchy and the optimistic price
of anarchy of the game.
4.1 Game Model
The new game, which we refer to as the payment game, allows
each player to offer a payment to another player to give the latter
incentive to replicate the object. The cost of replication is shared
among the nodes paying the server that replicates the object.
The strategy for each player i is specified by a triplet (vi, bi, ti) ∈
{N, Ê+, Ê+}. vi specifies the player to whom i makes a bid,
bi ≥ 0 is the value of the bid, and ti ≥ 0 denotes a threshold
for payments beyond which i will replicate the object. In addition,
we use Ri to denote the total amount of bids received by a node i
(Ri =
Èj:vj =i bj).
A node i replicates the object if and only if Ri ≥ ti, that is, the
amount of bids it receives is greater than or equal to its threshold.
Let Ii denote the corresponding indicator variable, that is, Ii equals
1 if i replicates the object, and 0 otherwise. We make the rule that
if a node i makes a bid to another node j and j replicates the object,
then i must pay j the amount bi. If j does not replicate the object,
i does not pay j.
Given a strategy profile, the outcome of the game is the set of
tuples {(Ii, vi, bi, Ri)}. Ii tells us whether player i replicates the
object or not, bi is the payment player i makes to player vi, and
Ri is the total amount of bids received by player i. To compute
the payoffs given the outcome, we must now take into account the
payments a node makes, in addition to the placement costs and
access costs of the basic game.
By our rules, a server node i pays bi to node vi if vi replicates
the object, and receives a payment of Ri if it replicates the object
itself. Its net payment is biIvi − RiIi. The total cost incurred by
each node is the sum of its placement cost, access cost, and net
payment. It is defined as
Ci(S) = αij Ii + wij di (i,j)(1 − Ii) + biIvi − RiIi. (10)
The cost of social optimum for the payment game is same as that
for the basic game, since the net payments made cancel out.
4.2 Analysis
In analyzing the payment model, we first show that a Nash 
equilibrium in the basic game is also a Nash equilibrium in the payment
game. We then present an important positive result - in the 
payment game the socially optimal configuration can always be 
implemented by a Nash equilibrium. We know from the counterexample
in Figure 2 that this is not guaranteed in the the basic game. In this
analysis we use α to represent αij .
THEOREM 2. Any configuration that is a pure strategy Nash
equilibrium in the basic game is also a pure strategy Nash 
equilibrium in the payment game. Therefore, the price of anarchy of the
payment game is at least that of the basic game.
PROOF. Consider any Nash equilibrium configuration in the 
basic game. For each node i replicating the object, set its threshold ti
to 0; everyone else has threshold α. Also, for all i, bi = 0.
A node that replicates the object does not have incentive to change
its strategy: changing the threshold does not decrease its cost, and
it would have to pay at least α to access a remote replica or 
incentivize a nearby node to cache. Therefore it is better off keeping its
threshold and bid at 0 and replicating the object.
A node that is not replicating the object can access the object 
remotely at a cost less than or equal to α. Lowering its threshold does
not decrease its cost, since all bi are zero. The payment necessary
for another server to place a replica is at least α.
No player has incentive to deviate, so the current configuration
is a Nash equilibrium.
In fact, Appendix B shows that the PoA of the payment game
can be more than that of the basic game in a given topology.
Now let us look at what happens to the example shown in 
Figure 2 in the best case. Suppose node B"s neighbors each decide
to pay node B an amount 2/n. B does not have an incentive to
deviate, since accessing the remote replica does not decrease its
cost. The same argument holds for A because of symmetry in the
graph. Since no one has an incentive to deviate, the configuration is
a Nash equilibrium. Its total cost is 2α, the same as in the socially
optimal configuration shown in Figure 2(b). Next we prove that
indeed the payment game always has a strategy profile that 
implements the socially optimal configuration as a Nash equilibrium. We
first present the following observation, which is used in the proof,
about thresholds in the payment game.
OBSERVATION 1. If node i replicates the object, j is the 
nearest node to i among the other nodes that replicate the object, and
dij < α in a Nash equilibrium, then i should have a threshold at
25
least (α − dij). Otherwise, it cannot collect enough payment to
compensate for the cost of replicating the object and is better off
accessing the replica at j.
THEOREM 3. In the payment game, there is always a pure 
strategy Nash equilibrium that implements the social optimum 
configuration. The optimistic price of anarchy in the payment game is
therefore always one.
PROOF. Consider the socially optimal configuration φopt. Let
No be the set of nodes that replicate the object and Nc = N − No
be the rest of the nodes. Also, for each i in No, let Qi denote the
set of nodes that access the object from i, not including i itself. In
the socially optimal configuration, dij ≤ α for all j in Qi.
We want to find a set of payments and thresholds that makes this
configuration implementable. The idea is to look at each node i in
No and distribute the minimum payment needed to make i replicate
the object among the nodes that access the object from i. For each
i in No, and for each j in Qi, we define
δj = min{α, min
k∈No−{i}
djk} − dji (11)
Note that δj is the difference between j"s cost for accessing the
replica at i and j"s next best option among replicating the object
and accessing some replica other than i. It is clear that δj ≥ 0.
CLAIM 1. For each i ∈ No, let be the nearest node to i in
No. Then,
Èj∈Qi
δj ≥ α − di .
PROOF. (of claim) Assume the contrary, that is,
Èj∈Qi
δj <
α − di . Consider the new configuration φnew wherein i does not
replicate and each node in Qi chooses its next best strategy (either
replicating or accessing the replica at some node in No − {i}). In
addition, we still place replicas at each node in No − {i}. It is easy
to see that cost of φopt minus cost of φnew is at least:
(α +
j∈Qi
dij) − (di +
j∈Qi
min{α, min
k∈No−{i}
dik})
= α − di −
j∈Qi
δj > 0,
which contradicts the optimality of φopt.
We set bids as follows. For each i in No, bi = 0 and for each j
in Qi, j bids to i (i.e., vj = i) the amount:
bj = max{0, δj − i/(|Qi| + 1)}, j ∈ Qi (12)
where i =
Èj∈Qi
δj − α + di ≥ 0 and |Qi| is the cardinality of
Qi. For the thresholds, we have:
ti =
α if i ∈ Nc;Èj∈Qi
bj if i ∈ No.
(13)
This fully specifies the strategy profile of the nodes, and it is easy
to see that the outcome is indeed the socially optimal configuration.
Next, we verify that the strategies stipulated constitute a Nash
equilibrium. Having set ti to α for i in Nc means that any node
in N is at least as well off lowering its threshold and replicating
as bidding α to some node in Nc to make it replicate, so we may
disregard the latter as a profitable strategy. By observation 1, to
ensure that each i in No does not deviate, we require that if is the
nearest node to i in No, then
Èj∈Qi
bj is at least (α − di ). 
Otherwise, i will raise ti above
Èj∈Qi
bj so that it does not replicate
and instead accesses the replica at . We can easily check that
j∈Qi
bj ≥
j∈Qi
δj −
|Qi| i
|Qi| + 1
= α − di +
i
|Qi| + 1
≥ α − di .
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
0 20 40 60 80 100 120 140 160 180 200
1
10
100
C(NE)/C(SO)
AverageNumberofReplicas
alpha
PoA
Ratio
OPoA
Replica (SO)
Replica (NE)
Figure 3: We present P oA, Ratio, and OP oA results for the basic
game, varying α on a 100-node line topology, and we show number
of replicas placed by the Nash equilibria and by the optimal solution.
We see large peaks in P oA and OP oA at α = 100, where a phase
transition causes an abrupt transition in the lines.
Therefore, each node i ∈ No does not have incentive to change
ti since i loses its payments received or there is no change, and i
does not have incentive to bi since it replicates the object. Each
node j in Nc has no incentive to change tj since changing tj does
not reduce its cost. It also does not have incentive to reduce bj
since the node where j accesses does not replicate and j has to
replicate the object or to access the next closest replica, which costs
at least the same from the definition of bj . No player has incentive
to deviate, so this strategy profile is a Nash equilibrium.
5. SIMULATION
We run simulations to compare Nash equilibria for the 
singleobject caching game with the social optimum computed by solving
the integer linear program described in Equation 8 using Mosek [1].
We examine price of anarchy (PoA), optimistic price of anarchy
(OPoA), and the average ratio of the costs of Nash equilibria and
social optima (Ratio), and when relevant we also show the average
numbers of replicas placed by the Nash equilibrium (Replica(NE))
and the social optimum (Replica(SO)). The PoA and OPoA are
taken from the worst and best Nash equilibria, respectively, that we
observe over the runs. Each data point in our figures is based on
1000 runs, randomly varying the initial strategy profile and player
order. The details of the simulations including protocols and a 
discussion of convergence are presented in Appendix C.
In our evaluation, we study the effects of variation in four 
categories: placement cost, underlying topology, demand distribution,
and payments. As we vary the placement cost α, we directly 
influence the tradeoff between caching and not caching. In order to get
a clear picture of the dependency of PoA on α in a simple case, we
first analyze the basic game with a 100-node line topology whose
edge distance is one.
We also explore transit-stub topologies generated using the 
GTITM library [36] and power-law topologies (Router-level 
BarabasiAlbert model) generated using the BRITE topology generator [25].
For these topologies, we generate an underlying physical graph of
3050 physical nodes. Both topologies have similar minimum, 
average, and maximum physical node distances. The average distance
is 0.42. We create an overlay of 100 server nodes and use the same
overlay for all experiments with the given topology.
In the game, each server has a demand whose distribution is
Bernoulli(p), where p is the probability of having demand for the
object; the default unless otherwise specified is p = 1.0.
26
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
1
10
100
C(NE)/C(SO)
AverageNumberofReplicas
alpha
PoA
Ratio
OPoA
Replica (SO)
Replica (NE)
(a)
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
1
10
100
C(NE)/C(SO)
AverageNumberofReplicas
alpha
PoA
Ratio
OPoA
Replica (SO)
Replica (NE)
(b)
Figure 4: Transit-stub topology: (a) basic game, (b) payment game. We show the P oA, Ratio, OP oA, and the number of replicas placed while
varying α between 0 and 2 with 100 servers on a 3050-physical-node transit-stub topology.
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
1
10
100
C(NE)/C(SO)
AverageNumberofReplicas
alpha
PoA
Ratio
OPoA
Replica (SO)
Replica (NE)
(a)
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
1
10
100
C(NE)/C(SO)
AverageNumberofReplicas
alpha
PoA
Ratio
OPoA
Replica (SO)
Replica (NE)
(b)
Figure 5: Power-law topology: (a) basic game, (b) payment game. We show the P oA, Ratio, OP oA, and the number of replicas placed while
varying α between 0 and 2 with 100 servers on a 3050-physical-node power-law topology.
5.1 Varying Placement Cost
Figure 3 shows PoA, OPoA, and Ratio, as well as number
of replicas placed, for the line topology as α varies. We observe
two phases. As α increases the PoA rises quickly to a peak at
100. After 100, there is a gradual decline. OPoA and Ratio show
behavior similar to PoA.
These behaviors can be explained by examining the number of
replicas placed by Nash equilibria and by optimal solutions. We see
that when α is above one, Nash equilibrium solutions place fewer
replicas than optimal on average. For example, when α is 100,
the social optimum places four replicas, but the Nash equilibrium
places only one. The peak in PoA at α = 100 occurs at the point
for a 100-node line where the worst-case cost of accessing a remote
replica is slightly less than the cost of placing a new replica, so 
selfish servers will never place a second replica. The optimal solution,
however, places multiple replicas to decrease the high global cost
of access. As α continues to increase, the undersupply problem
lessens as the optimal solution places fewer replicas.
5.2 Different Underlying Topologies
In Figure 4(a) we examine an overlay graph on the more realistic
transit-stub topology. The trends for the PoA, OPoA, and Ratio
are similar to the results for the line topology, with a peak in PoA
at α = 0.8 due to maximal undersupply.
In Figure 5(a) we examine an overlay graph on the power-law
topology. We observe several interesting differences between the
power-law and transit-stub results. First, the PoA peaks at a lower
level in the power-law graph, around 2.3 (at α = 0.9) while the
peak PoA in the transit-stub topology is almost 3.0 (at α = 0.8).
After the peak, PoA and Ratio decrease more slowly as α 
increases. OPoA is close to one for the whole range of α values.
This can be explained by the observation in Figure 5(a) that there
is no significant undersupply problem here like there was in the
transit-stub graph. Indeed the high PoA is due mostly to 
misplacement problems when α is from 0.7 to 2.0, since there is little
decrease in PoA when the number of replicas in social optimum
changes from two to one. The OPoA is equal to one in the figure
when the same number of replicas are placed.
5.3 Varying Demand Distribution
Now we examine the effects of varying the demand distribution.
The set of servers with demand is random for p < 1, so we 
calculate the expected PoA by averaging over 5 trials (each data point
is based on 5000 runs). We run simulations for demand levels of
p ∈ {0.2, 0.6, 1.0} as α is varied on the 100 servers on top of
the transit-stub graph. We observe that as demand falls, so does
expected PoA. As p decreases, the number of replicas placed in
the social optimum decreases, but the number in Nash equilibria
changes little. Furthermore, when α exceeds the overlay diameter,
the number in Nash equilibria stays constant when p varies. 
Therefore, lower p leads to a lesser undersupply problem, agreeing with
intuition. We do not present the graph due to space limitations and
redundancy; the PoA for p = 1.0 is identical to PoA in Figure 4(a),
and the lines for p = 0.6 and p = 0.2 are similar but lower and flatter.
27
5.4 Effects of Payment
Finally, we discuss the effects of payments on the efficiency of
Nash equilibria. The results are presented in Figure 4(b) and 
Figure 5(b). As shown in the analysis, the simulations achieve OPoA
close to one (it is not exactly one because of randomness in the
simulations). The Ratio for the payment game is much lower than
the Ratio for the basic game, since the protocol for the payment
game tends to explore good regions in the space of Nash 
equilibria. We observe in Figure 4 that for α ≥ 0.4, the average number
of replicas of Nash equilibria gets closer with payments to that of
the social optimum than it does without. We observe in Figure 5
that more replicas are placed with payments than without when α
is between 0.7 and 1.3, the only range of significant undersupply in
the power-law case. The results confirm that payments give servers
incentive to replicate the object and this leads to better equilibria.
6. DISCUSSION AND FUTURE WORK
We suggest several interesting extensions and directions. One
extension is to consider multiple objects in the capacitated caching
game, in which servers have capacity limits when placing objects.
Since caching one object affects the ability to cache another, there
is no separability of a multi-object game into multiple single object
games. As studied in [12], one way to formulate this problem is to
find the best response of a server by solving a knapsack problem
and to compute Nash equilibria.
In our analyses, we assume that all nodes have the same demand.
However, nodes could have different demand depending on objects.
We intend to examine the effects of heterogeneous demands (or
heterogeneous placement costs) analytically. We also want to look
at the following aggregation effect. Suppose there are n − 1
clustered nodes with distance of α−1 from a node hosting a replica.
All nodes have demands of one. In that case, the price of anarchy
is O(n). However, if we aggregate n − 1 nodes into one node with
demand n − 1, the price of anarchy becomes O(1), since α should
be greater than (n − 1)(α − 1) to replicate only one object. Such
aggregation can reduce the inefficiency of Nash equilibria.
We intend to compute the bounds of the price of anarchy under
different underlying topologies such as random graphs or 
growthrestricted metrics. We want to investigate whether there are certain
distance constraints that guarantee O(1) price of anarchy. In 
addition, we want to run large-scale simulations to observe the change
in the price of anarchy as the network size increases.
Another extension is to consider server congestion. Suppose the
distance is the network distance plus γ × (number of accesses)
where γ is an extra delay when an additional server accesses the
replica. Then, when α > γ, it can be shown that PoA is bounded
by α
γ
. As γ increases, the price of anarchy bound decreases, since
the load of accesses is balanced across servers.
While exploring the caching problem, we made several 
observations that seem counterintuitive. First, the PoA in the payment
game can be worse than the PoA in the basic game. Another 
observation we made was that the number of replicas in a Nash 
equilibrium can be more than the number of replicas in the social 
optimum even without payments. For example, a graph with diameter
slightly more than α may have a Nash equilibrium configuration
with two replicas at the two ends. However, the social optimum
may place one replica at the center. We leave the investigation of
more examples as an open issue.
7. CONCLUSIONS
In this work we introduce a novel non-cooperative game model
to characterize the caching problem among selfish servers without
any central coordination. We show that pure strategy Nash 
equilibria exist in the game and that the price of anarchy can be O(n) in
general, where n is the number of servers, due to undersupply 
problems. With specific topologies, we show that the price of anarchy
can have tighter bounds. More importantly, with payments, servers
are incentivized to replicate and the optimistic price of anarchy is
always one. Non-cooperative caching is a more realistic model than
cooperative caching in the competitive Internet, hence this work is
an important step toward viable federated caching systems.
8. ACKNOWLEDGMENTS
We thank Kunal Talwar for enlightening discussions regarding
this work.
9. REFERENCES
[1] http://www.mosek.com.
[2] A. Adya et al. FARSITE: Federated, Available, and Reliable
Storage for an Incompletely Trusted Environment. In Proc.
of USENIX OSDI, 2002.
[3] E. Anshelevich, A. Dasgupta, E. Tardos, and T. Wexler.
Near-optimal Network Design with Selfish Agents. In Proc.
of ACM STOC, 2003.
[4] Y. Chen, R. H. Katz, and J. D. Kubiatowicz. SCAN: A
Dynamic, Scalable, and Efficient Content Distribution
Network. In Proc. of Intl. Conf. on Pervasive Computing,
2002.
[5] F. Dabek et al. Wide-area Cooperative Storage with CFS. In
Proc. of ACM SOSP, Oct. 2001.
[6] P. B. Danzig. NetCache Architecture and Deploment. In
Computer Networks and ISDN Systems, 1998.
[7] N. Devanur, M. Mihail, and V. Vazirani. Strategyproof
cost-sharing Mechanisms for Set Cover and Facility
Location Games. In Proc. of ACM EC, 2003.
[8] J. R. Douceur and R. P. Wattenhofer. Large-Scale Simulation
of Replica Placement Algorithms for a Serverless Distributed
File System. In Proc. of MASCOTS, 2001.
[9] A. Fabrikant, C. H. Papadimitriou, and K. Talwar. The
Complexity of Pure Nash Equilibria. In Proc. of ACM STOC,
2004.
[10] L. Fan, P. Cao, J. Almeida, and A. Z. Broder. Summary
Cache: A Scalable Wide-area Web Cache Sharing Protocol.
IEEE/ACM Trans. on Networking, 8(3):281-293, 2000.
[11] M. R. Garey and D. S. Johnson. Computers and
Intractability: A Guide to the Theory of NP-Completeness.
W. H. Freeman and Co., 1979.
[12] M. X. Goemans, L. Li, V. S. Mirrokni, and M. Thottan.
Market Sharing Games Applied to Content Distribution in
ad-hoc Networks. In Proc. of ACM MOBIHOC, 2004.
[13] M. X. Goemans and M. Skutella. Cooperative Facility
Location Games. In Proc. of ACM-SIAM SODA, 2000.
[14] S. Gribble et al. What Can Databases Do for Peer-to-Peer? In
WebDB Workshop on Databases and the Web, June 2001.
[15] K. P. Gummadi et al. Measurement, Modeling, and Analysis
of a Peer-to-Peer File-Sharing Workload. In Proc. of ACM
SOSP, October 2003.
[16] S. Iyer, A. Rowstron, and P. Druschel. Squirrel: A
Decentralized Peer-to-Peer Web Cache. In Proc. of ACM
PODC, 2002.
[17] K. Jain and V. V. Vazirani. Primal-Dual Approximation
Algorithms for Metric Facility Location and k-Median
Problems. In Proc. of IEEE FOCS, 1999.
28
[18] S. Jamin et al. On the Placement of Internet Instrumentation.
In Proc. of IEEE INFOCOM, pages 295-304, 2000.
[19] S. Jamin et al. Constrained Mirror Placement on the Internet.
In Proc. of IEEE INFOCOM, pages 31-40, 2001.
[20] B.-J. Ko and D. Rubenstein. A Distributed, Self-stabilizing
Protocol for Placement of Replicated Resources in Emerging
Networks. In Proc. of IEEE ICNP, 2003.
[21] E. Koutsoupias and C. Papadimitriou. Worst-Case Equilibria.
In STACS, 1999.
[22] J. Kubiatowicz et al. OceanStore: An Architecture for
Global-scale Persistent Storage. In Proc. of ACM ASPLOS.
ACM, November 2000.
[23] B. Li, M. J. Golin, G. F. Italiano, and X. Deng. On the
Optimal Placement of Web Proxies in the Internet. In Proc.
of IEEE INFOCOM, 1999.
[24] M. Mahdian, Y. Ye, and J. Zhang. Improved Approximation
Algorithms for Metric Facility Location Problems. In Proc.
of Intl. Workshop on Approximation Algorithms for
Combinatorial Optimization Problems, 2002.
[25] A. Medina, A. Lakhina, I. Matta, and J. Byers. BRITE:
Universal Topology Generation from a User"s Perspective.
Technical Report 2001-003, 1 2001.
[26] R. R. Mettu and C. G. Plaxton. The Online Median Problem.
In Proc. of IEEE FOCS, 2000.
[27] P. B. Mirchandani and R. L. Francis. Discrete Location
Theory. Wiley-Interscience Series in Discrete Mathematics
and Optimization, 1990.
[28] M. J. Osborne and A. Rubinstein. A Course in Game Theory.
MIT Press, 1994.
[29] M. Pal and E. Tardos. Group Strategyproof Mechanisms via
Primal-Dual Algorithms. In Proc. of IEEE FOCS, 2003.
[30] L. Qiu, V. N. Padmanabhan, and G. M. Voelker. On the
Placement of Web Server Replicas. In Proc. of IEEE
INFOCOM, 2001.
[31] M. Rabinovich, I. Rabinovich, R. Rajaraman, and
A. Aggarwal. A Dynamic Object Replication and Migration
Protocol for an Internet Hosting Service. In Proc. of IEEE
ICDCS, 1999.
[32] A. Rowstron and P. Druschel. Storage Management and
Caching in PAST, A Large-scale, Persistent Peer-to-peer
Storage Utility. In Proc. of ACM SOSP, October 2001.
[33] Y. Saito, C. Karamanolis, M. Karlsson, and M. Mahalingam.
Taming Aggressive Replication in the Pangaea Wide-Area
File System. In Proc. of USENIX OSDI, 2002.
[34] X. Tang and S. T. Chanson. Coordinated En-route Web
Caching. In IEEE Trans. Computers, 2002.
[35] A. Vetta. Nash Equilibria in Competitive Societies, with
Applications to Facility Location, Traffic Routing, and
Auctions. In Proc. of IEEE FOCS, 2002.
[36] E. W. Zegura, K. L. Calvert, and S. Bhattacharjee. How to
Model an Internetwork. In Proc. of IEEE INFOCOM, 1996.
APPENDIX
A. ANALYZING SPECIFIC TOPOLOGIES
We now analyze the price of anarchy (PoA) for the basic game
with specific underlying topologies and show that PoA can have
better bounds. We look at complete graph, star, line, and 
Ddimensional grid. In all these topologies, we set the distance 
between two directly connected nodes to one. We describe the case
where α > 1, since PoA = 1 trivially when α ≤ 1.
A BC D3
α
3
α
4
3α
4
α
4
α
Figure 6: Example where the payment game has a Nash equilibrium
which is worse than any Nash equilibrium in the basic game. The 
unlabeled distances between the nodes in the cluster are all 1. The 
thresholds of white nodes are all α and the thresholds of dark nodes are all
α/4. The two dark nodes replicate the object in this payment game
Nash equilibrium.
For a complete graph, PoA = 1, and for a star, PoA ≤ 2.
For a complete graph, when α > 1, both Nash equilibria and 
social optima place one replica at one server, so PoA = 1. For
star, when 1 < α < 2, the worst case Nash equilibrium places
replicas at all leaf nodes. However, the social optimum places
one replica at the center node. Therefore, PoA = (n−1)α+1
α+(n−1)
≤
2(n−1)+1
1+(n−1)
≤ 2. When α > 2, the worst case Nash equilibrium
places one replica at a leaf node and the other nodes access the
remote replica, and the social optimum places one replica at the
center. PoA = α+1+2(n−2)
α+(n−1)
= 1 + n
α+(n−1)
≤ 2.
For a line, the price of anarchy is O(
√
n). When 1 < α < n,
the worst case Nash equilibrium places replicas every 2α so that
there is no overlap between areas covered by two adjacent servers
that cache the object. The social optimum places replicas at least
every
√
2α. The placement of replicas for the social optimum is
as follows. Suppose there are two replicas separated by distance
d. By placing an additional replica in the middle, we want to have
the reduction of distance to be at least α. The distance reduction
is d/2 + 2{((d/2 − 1) − 1) + ((d/2 − 2) − 2) + ... + ((d/2 −
d/4) − d/4)} ≥ d2
/8. d should be at most 2
√
2α. Therefore, the
distance between replicas in the social optimum is at most
√
2α.
C(SW ) = α(n−1)
2α
+ α(α+1)
2
(n−1)
2α
= Θ(αn). C(SO) ≥ α n−1√
2α
+
2
√
2α/2(
√
2α/2+1)
2
n−1√
2α
. C(SO) = Ω(
√
αn). Therefore, PoA =
O(
√
α). When α > n − 1, the worst case Nash equilibrium places
one replica at a leaf node and C(SW ) = α + (n−1)n
2
. However,
the social optimum still places replicas every
√
2α. If we view
PoA as a continuous function of α and compute a derivative of
PoA, the derivative becomes 0 when α is Θ(n2
), which means
the function decreases as α increases from n. Therefore, PoA is
maximum when α is n, and PoA = Θ(n2
)
Ω(
√
nn)
= O(
√
n). When
α > (n−1)n
2
, the social optimum also places only one replica, and
PoA is trivially bounded by 2. This result holds for the ring and
it can be generalized to the D-dimensional grid. As the dimension
in the grid increases, the distance reduction of additional replica
placement becomes Ω(dD+1
) where d is the distance between two
adjacent replicas. Therefore, PoA = Θ(n2)
Ω(n
1
D+1 n)
= O(n
D
D+1 ).
B. PAYMENT CAN DO WORSE
Consider the network in Figure 6 where α > 1+α/3. Any Nash
equilibrium in the basic game model would have exactly two 
replicas - one in the left cluster, and one in the right. It is easy to verify
that the worst placement (in terms of social cost) of two replicas
occurs when they are placed at nodes A and B. This placement can
be achieved as a Nash equilibrium in the payment game, but not in
the basic game since A and B are a distance 3α/4 apart.
29
Algorithm 1 Initialization for the Basic Game
L1 = a random subset of servers
for each node i in N do
if i ∈ L1 then
Si = 1 ; replicate the object
else
Si = 0
Algorithm 2 Move Selection of i for the Basic Game
Cost1 = α
Cost2 = minj∈X−{i} dij ; X is the current configuration
Costmin = min{Cost1, Cost2}
if Costnow > Costmin then
if Costmin == Cost1 then
Si = 1
else
Si = 0
C. NASH DYNAMICS PROTOCOLS
The simulator initializes the game according to the given 
parameters and a random initial strategy profile and then iterates through
rounds. Initially the order of player actions is chosen randomly. In
each round, each server performs the Nash dynamics protocol that
adjusts its strategies greedily in the chosen order. When a round
passes without any server changing its strategy, the simulation ends
and a Nash equilibrium is reached.
In the basic game, we pick a random initial subset of servers to
replicate the object as shown in Algorithm 1. After the 
initialization, each player runs the move selection procedure described in
Algorithm 2 (in algorithms 2 and 4, Costnow represents the 
current cost for node i). This procedure chooses greedily between
replication and non-replication. It is not hard to see that this Nash
dynamics protocol converges in two rounds.
In the payment game, we pick a random initial subset of servers
to replicate the object by setting their thresholds to 0. In addition,
we initialize a second random subset of servers to replicate the 
object with payments from other servers. The details are shown in 
Algorithm 3. After the initialization, each player runs the move 
selection procedure described in Algorithm 4. This procedure chooses
greedily between replication and accessing a remote replica, with
the possibilities of receiving and making payments, respectively.
In the protocol, each node increases its threshold value by incr if it
does not replicate the object. By this ramp up procedure, the cost of
replicating an object is shared fairly among the nodes that access a
replica from a server that does cache. If incr is small, cost is shared
more fairly, and the game tends to reach equilibria that encourages
more servers to store replicas, though the convergence takes longer.
If incr is large, the protocol converges quickly, but it may miss 
efficient equilibria. In the simulations we set incr to 0.1. Most of our
A
B C
a
b
c
α/3+1
2α/3−1
2α/3
Figure 7: An example where the Nash dynamics protocol does not
converge in the payment game.
Algorithm 3 Initialization for the Payment Game
L1 = a random subset of servers
for each node i in N do
bi = 0
if i ∈ L1 then
ti = 0 ; replicate the object
else
ti = α
L2 = {}
for each node i in N do
if coin toss == head then
Mi = {j : d(j, i) < mink∈L1∪L2 d(j, k)}
if Mi != ∅ then
for each node j ∈ Mi do
bj = max{
α+
Èk∈Mi
d(i,k)
|Mi|
− d(i, j), 0}
L2 = L2 ∪ {i}
Algorithm 4 Move Selection of i for the Payment Game
Cost1 = α − Ri
Cost2 = minj∈N−{i}{tj − Rj + dij }
Costmin = min{Cost1, Cost2}
if Costnow > Costmin then
if Costmin == Cost1 then
ti = Ri
else
ti = Ri + incr
vi = argminj{tj − Rj + dij}
bi = tvi − Rvi
simulation runs converged, but there were a very few cases where
the simulation did not converge due to the cycles of dynamics. The
protocol does not guarantee convergence within a certain number
of rounds like the protocol for the basic game.
We provide an example graph and an initial condition such that
the Nash dynamics protocol does not converge in the payment game
if started from this initial condition. The graph is represented by
a shortest path metric on the network shown in Figure 7. In the
starting configuration, only A replicates the object, and a pays it
an amount α/3 to do so. The thresholds for A, B and C are α/3
each, and the thresholds for a, b and c are 2α/3. It is not hard to
verify that the Nash dynamics protocol will never converge if we
start with this condition.
The Nash dynamics protocol for the payment game needs 
further investigation. The dynamics protocol for the payment game
should avoid cycles of actions to achieve stabilization of the 
protocol. Finding a self-stabilizing dynamics protocol is an interesting
problem. In addition, a fixed value of incr cannot adapt to changing
environments. A small value of incr can lead to efficient equilibria,
but it can take long time to converge. An important area for future
research is looking at adaptively changing incr.
30
Fairness in Dead-Reckoning based Distributed
Multi-Player Games
Sudhir Aggarwal Hemant Banavar
Department of Computer Science
Florida State University, Tallahassee, FL
Email: {sudhir, banavar}@cs.fsu.edu
Sarit Mukherjee Sampath Rangarajan
Center for Networking Research
Bell Laboratories, Holmdel, NJ
Email: {sarit, sampath}@bell-labs.com
ABSTRACT
In a distributed multi-player game that uses dead-reckoning vectors
to exchange movement information among players, there is 
inaccuracy in rendering the objects at the receiver due to network delay
between the sender and the receiver. The object is placed at the 
receiver at the position indicated by the dead-reckoning vector, but by
that time, the real position could have changed considerably at the
sender. This inaccuracy would be tolerable if it is consistent among
all players; that is, at the same physical time, all players see 
inaccurate (with respect to the real position of the object) but the same
position and trajectory for an object. But due to varying network
delays between the sender and different receivers, the inaccuracy
is different at different players as well. This leads to unfairness
in game playing. In this paper, we first introduce an error 
measure for estimating this inaccuracy. Then we develop an algorithm
for scheduling the sending of dead-reckoning vectors at a sender
that strives to make this error equal at different receivers over time.
This algorithm makes the game very fair at the expense of 
increasing the overall mean error of all players. To mitigate this effect, we
propose a budget based algorithm that provides improved fairness
without increasing the mean error thereby maintaining the accuracy
of game playing. We have implemented both the scheduling 
algorithm and the budget based algorithm as part of BZFlag, a popular
distributed multi-player game. We show through experiments that
these algorithms provide fairness among players in spite of widely
varying network delays. An additional property of the proposed 
algorithms is that they require less number of DRs to be exchanged
(compared to the current implementation of BZflag) to achieve the
same level of accuracy in game playing.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed 
Systems-Distributed applications
General Terms
Algorithms, Design, Experimentation, Performance
1. INTRODUCTION
In a distributed multi-player game, players are normally 
distributed across the Internet and have varying delays to each other
or to a central game server. Usually, in such games, the players are
part of the game and in addition they may control entities that make
up the game. During the course of the game, the players and the
entities move within the game space. A player sends information
about her movement as well as the movement of the entities she
controls to the other players using a Dead-Reckoning (DR) vector.
A DR vector contains information about the current position of the
player/entity in terms of x, y and z coordinates (at the time the DR
vector was sent) as well as the trajectory of the entity in terms of
the velocity component in each of the dimensions. Each of the 
participating players receives such DR vectors from one another and
renders the other players/entities on the local consoles until a new
DR vector is received for that player/entity. In a peer-to-peer game,
players send DR vectors directly to each other; in a client-server
game, these DR vectors may be forwarded through a game server.
The idea of DR is used because it is almost impossible for 
players/entities to exchange their current positions at every time unit.
DR vectors are quantization of the real trajectory (which we refer
to as real path) at a player. Normally, a new DR vector is computed
and sent whenever the real path deviates from the path extrapolated
using the previous DR vector (say, in terms of distance in the x, y,
z plane) by some amount specified by a threshold. We refer to the
trajectory that can be computed using the sequence of DR vectors
as the exported path. Therefore, at the sending player, there is a 
deviation between the real path and the exported path. The error due
to this deviation can be removed if each movement of player/entity
is communicated to the other players at every time unit; that is a
DR vector is generated at every time unit thereby making the real
and exported paths the same. Given that it is not feasible to 
satisfy this due to bandwidth limitations, this error is not of practical
interest. Therefore, the receiving players can, at best, follow the
exported path. Because of the network delay between the sending
and receiving players, when a DR vector is received and rendered
at a player, the original trajectory of the player/entity may have 
already changed. Thus, in physical time, there is a deviation at the
receiving player between the exported path and the rendered 
trajectory (which we refer to as placed path). We refer to this error
as the export error. Note that the export error, in turn, results in a
deviation between the real and the placed paths.
The export error manifests itself due to the deviation between the
exported path at the sender and the placed path at the receiver (i)
1
before the DR vector is received at the receiver (referred to as the
before export error, and (ii) after the DR vector is received at the 
receiver (referred to as the after export error). In an earlier paper [1],
we showed that by synchronizing the clocks at all the players and
by using a technique based on time-stamping messages that carry
the DR vectors, we can guarantee that the after export error is made
zero. That is, the placed and the exported paths match after the DR
vector is received. We also showed that the before export error can
never be eliminated since there is always a non-zero network delay,
but can be significantly reduced using our technique [1]. 
Henceforth we assume that the players use such a technique which results
in unavoidable but small overall export error.
In this paper we consider the problem of different and varying
network delays between each sender-receiver pair of a DR vector,
and consequently, the different and varying export errors at the 
receivers. Due to the difference in the export errors among the 
receivers, the same entity is rendered at different physical time at
different receivers. This brings in unfairness in game playing. For
instance a player with a large delay would always see an entity
late in physical time compared to the other players and, 
therefore, her action on the entity would be delayed (in physical time)
even if she reacted instantaneously after the entity was rendered.
Our goal in this paper is to improve the fairness of these games in
spite of the varying network delays by equalizing the export error
at the players. We explore whether the time-average of the export
errors (which is the cumulative export error over a period of time
averaged over the time period) at all the players can be made the
same by scheduling the sending of the DR vectors appropriately at
the sender. We propose two algorithms to achieve this.
Both the algorithms are based on delaying (or dropping) the
sending of DR vectors to some players on a continuous basis to
try and make the export error the same at all the players. At an
abstract level, the algorithm delays sending DR vectors to players
whose accumulated error so far in the game is smaller than others;
this would mean that the export error due to this DR vector at these
players will be larger than that of the other players, thereby making
them the same. The goal is to make this error at least approximately
equal at every DR vector with the deviation in the error becoming
smaller as time progresses.
The first algorithm (which we refer to as the scheduling 
algorithm) is based on estimating the delay between players and 
refining the sending of DR vectors by scheduling them to be sent
to different players at different times at every DR generation point.
Through an implementation of this algorithm using the open source
game BZflag, we show that this algorithm makes the game very fair
(we measure fairness in terms of the standard deviation of the 
error). The drawback of this algorithm is that it tends to push the
error of all the players towards that of the player with the worst
error (which is the error at the farthest player, in terms of delay,
from the sender of the DR). To alleviate this effect, we propose
a budget based algorithm which budgets how the DRs are sent to
different players. At a high level, the algorithm is based on the
idea of sending more DRs to players who are farther away from
the sender compared to those who are closer. Experimental results
from BZflag illustrates that the budget based algorithm follows a
more balanced approach. It improves the fairness of the game but
at the same time does so without pushing up the mean error of the
players thereby maintaining the accuracy of the game. In addition,
the budget based algorithm is shown to achieve the same level of
accuracy of game playing as the current implementation of BZflag
using much less number of DR vectors.
2. PREVIOUS WORK
Earlier work on network games to deal with network latency has
mostly focussed on compensation techniques for packet delay and
loss [2, 3, 4]. These methods are aimed at making large delays and
message loss tolerable for players but does not consider the 
problems that may be introduced by varying delays from the server to
different players or from the players to one another. For example,
the concept of local lag has been used in [3] where each player
delays every local operation for a certain amount of time so that 
remote players can receive information about the local operation and
execute the same operation at the about same time, thus reducing
state inconsistencies. The online multi-player game MiMaze [2, 5,
6], for example, takes a static bucket synchronization approach to
compensate for variable network delays. In MiMaze, each player
delays all events by 100 ms regardless of whether they are 
generated locally or remotely. Players with a network delay larger
than 100 ms simply cannot participate in the game. In general,
techniques based on bucket synchronization depend on imposing a
worst case delay on all the players.
There have been a few papers which have studied the problem of
fairness in a distributed game by more sophisticated message 
delivery mechanisms. But these works [7, 8] assume the existence of
a global view of the game where a game server maintains a view
(or state) of the game. Players can introduce objects into the game
or delete objects that are already part of the game (for example, in
a first-person shooter game, by shooting down the object). These
additions and deletions are communicated to the game server 
using action messages. Based on these action messages, the state
of the game is changed at the game server and these changes are
communicated to the players using update messages. Fairness is
achieved by ordering the delivery of action and update messages at
the game server and players respectively based on the notion of a
fair-order which takes into account the delays between the game
server and the different players. Objects that are part of the game
may move but how this information is communicated to the players
seems to be beyond the scope of these works. In this sense, these
works are very limited in scope and may be applicable only to 
firstperson shooter games and that too to only games where players are
not part of the game.
DR vectors can be exchanged directly among the players 
(peerto-peer model) or using a central server as a relay (client-server
model). It has been shown in [9] that multi-player games that
use DR vectors together with bucket synchronization are not 
cheatproof unless additional mechanisms are put in place. Both the
scheduling algorithm and the budget-based algorithm described in
our paper use DR vectors and hence are not cheat-proof. For 
example, a receiver could skew the delay estimate at the sender to
make the sender believe that the delay between the sender and the
receiver is high thereby gaining undue advantage. We emphasize
that the focus of this paper is on fairness without addressing the
issue of cheating.
In the next section, we describe the game model that we use
and illustrate how senders and receivers exchange DR vectors and
how entities are rendered at the receivers based on the time-stamp
augmented DR vector exchange as described in [1]. In Section 4,
we describe the DR vector scheduling algorithm that aims to make
the export error equal across the players with varying delays from
the sender of a DR vector, followed by experimental results 
obtained from instrumentation of the scheduling algorithm on the
open source game BZFlag. Section 5, describes the budget based
algorithm that achieves improved fairness but without reducing the
level accuracy of game playing. Conclusions are presented in 
Section 6.
2
3. GAME MODEL
The game architecture is based on players distributed across the
Internet and exchanging DR vectors to each other. The DR 
vectors could either be sent directly from one player to another 
(peerto-peer model) or could be sent through a game server which 
receives the DR vector from a player and forwards it to other players
(client-server model). As mentioned before, we assume 
synchronized clocks among the participating players.
Each DR vector sent from one player to another specifies the 
trajectory of exactly one player/entity. We assume a linear DR vector
in that the information contained in the DR vector is only enough at
the receiving player to compute the trajectory and render the entity
in a straight line path. Such a DR vector contains information about
the starting position and velocity of the player/entity where the 
velocity is constant1
. Thus, the DR vectors sent by a player specifies
the current time at the player when the DR vector is computed (not
the time at which this DR vector is sent to the other players as we
will explain later), the current position of the player/entity in terms
of the x, y, z coordinates and the velocity vector in the direction
of x, y and z coordinates. Specifically, the ith
DR vector sent by
player j about the kth
entity is denoted by DRj
ik and is represented
by the following tuple (Tj
ik, xj
ik, yj
ik, zj
ik, vxj
ik, vyj
ik, vzj
ik).
Without loss of generality, in the rest of the discussion, we 
consider a sequence of DR vectors sent by only one player and for
only one entity. For simplicity, we consider a two dimensional
game space rather than a three dimensional one. Hence we use
DRi to denote the ith
such DR vector represented as the tuple
(Ti, xi, yi, vxi, vyi). The receiving player computes the starting
position for the entity based on xi, yi and the time difference 
between when the DR vector is received and the time Ti at which it
was computed. Note that the computation of time difference is 
feasible since all the clocks are synchronized. The receiving player
then uses the velocity components to project and render the 
trajectory of the entity. This trajectory is followed until a new DR vector
is received which changes the position and/or velocity of the entity.
timeT1
Real
Exported
Placed
dt1
A
B
C
D
DR1
= (T1, x1, y1, vx1, vy1)
computed at time T1 and
sent to the receiver
DR0
= (T0, x0, y0, vx0, vy0)
computed at time T0 and
sent to the receiver
T0
dt0
Placed
E
Figure 1: Trajectories and deviations.
Based on this model, Figure 1 illustrates the sending and 
receiv1
Other type of DR vectors include quadratic DR vectors which
specify the acceleration of the entity and cubic spline DR vectors
that consider the starting position and velocity and the ending 
position and velocity of the entity.
ing of DR vectors and the different errors that are encountered. The
figure shows the reception of DR vectors at a player (henceforth
called the receiver). The horizontal axis shows the time which is
synchronized among all the players. The vertical axis tries to 
conceptually capture the two-dimensional position of an entity. 
Assume that at time T0 a DR vector DR0 is computed by the sender
and immediately sent to the receiver. Assume that DR0 is received
at the receiver after a delay of dt0 time units. The receiver 
computes the initial position of the entity as (x0 + vx0 × dt0, y0 +
vy0 × dt0) (shown as point E). The thick line EBD represents the
projected and rendered trajectory at the receiver based on the 
velocity components vx0 and vy0 (placed path). At time T1 a DR vector
DR1 is computed for the same entity and immediately sent to the
receiver2
. Assume that DR1 is received at the receiver after a delay
of dt1 time units. When this DR vector is received, assume that the
entity is at point D. A new position for the entity is computed as
(x1 + vx1 × dt1, y1 + vy0 × dt1) and the entity is moved to this
position (point C). The velocity components vx1 and vy1 are used
to project and render this entity further.
Let us now consider the error due to network delay. Although
DR1 was computed at time T1 and sent to the receiver, it did not
reach the receiver until time T1 + dt1. This means, although the
exported path based on DR1 at the sender at time T1 is the 
trajectory AC, until time T1 + dt1, at the receiver, this entity was being
rendered at trajectory BD based on DR0. Only at time T1 + dt1
did the entity get moved to point C from which point onwards the
exported and the placed paths are the same. The deviation between
the exported and placed paths creates an error component which we
refer to as the export error. A way to represent the export error is
to compute the integral of the distance between the two trajectories
over the time when they are out of sync. We represent the integral
of the distances between the placed and exported paths due to some
DR DRi over a time interval [t1, t2] as Err(DRi, t1, t2). In the
figure, the export error due to DR1 is computed as the integral of
the distance between the trajectories AC and BD over the time
interval [T1, T1 + dt1]. Note that there could be other ways of 
representing this error as well, but in this paper, we use the integral of
the distance between the two trajectories as a measure of the export
error. Note that there would have been an export error created due
to the reception of DR0 at which time the placed path would have
been based on a previous DR vector. This is not shown in the figure
but it serves to remind the reader that the export error is cumulative
when a sequence of DR vectors are received. Starting from time
T1 onwards, there is a deviation between the real and the exported
paths. As we discussed earlier, this export error is unavoidable.
The above figure and example illustrates one receiver only. But
in reality, DR vectors DR0 and DR1 are sent by the sender to all
the participating players. Each of these players receives DR0 and
DR1 after varying delays thereby creating different export error
values at different players. The goal of the DR vector scheduling
algorithm to be described in the next section is to make this 
(cumulative) export error equal at every player independently for each of
the entities that make up the game.
4. SCHEDULING ALGORITHM 
FORSENDING DR VECTORS
In Section 3 we showed how delay from the sender of a new DR
2
Normally, DR vectors are not computed on a periodic basis but
on an on-demand basis where the decision to compute a new DR
vector is based on some threshold being exceeded on the deviation
between the real path and the path exported by the previous DR
vector.
3
vector to the receiver of the DR vector could lead to export error
because of the deviation of the placed path from the exported path
at the receiver until this new DR vector is received. We also 
mentioned that the goal of the DR vector scheduling algorithm is to
make the export error equal at all receivers over a period of time.
Since the game is played in a distributed environment, it makes
sense for the sender of an entity to keep track of all the errors at the
receivers and try to make them equal. However, the sender cannot
know the actual error at a receiver till it gets some information 
regarding the error back from the receiver. Our algorithm estimates
the error to compute a schedule to send DR vectors to the receivers
and corrects the error when it gets feedbacks from the receivers. In
this section we provide motivations for the algorithm and describe
the steps it goes through. Throughout this section, we will use the
following example to illustrate the algorithm.
timeT1
Exported path
Placed path
at receiver 2
dt1
A
B
C
D
E
F
T0
G2
G1
dt2
DR1 sent
to receiver 1
DR1 sent
to receiver 2
T1
1 T1
2
da1
da2
G
H
I
J
K
L
N
M
DR1 estimated
to be received
by receiver 2
DR1 estimated
to be received
by receiver 1
DR1 actually
received by
receiver 1
DR1 actually
received by
receiver 2
DR0 sent to
both receivers
DR1 computed
by sender
Placed path
at receiver 1
Figure 2: DR vector flow between a sender and two receivers
and the evolution of estimated and actual placed paths at the
receivers. DR0 = (T0, T0, x0, y0, vx0, vy0), sent at time T0 to
both receivers. DR1 = (T1, T1
1 , x1, y1, vx1, vy1) sent at time
T1
1 = T1+δ1 to receiver 1 and DR1 = (T1, T2
1 , x1, y1, vx1, vy1)
sent at time T2
1 = T1 + δ2 to receiver 2.
Consider the example in Figure 2. The figure shows a single
sender sending DR vectors for an entity to two different receivers
1 and 2. DR0 computed at T0 is sent and received by the receivers
sometime between T0 and T1 at which time they move the location
of the entity to match the exported path. Thus, the path of the
entity is shown only from the point where the placed path matches
the exported path for DR0. Now consider DR1. At time T1, DR1
is computed by the sender but assume that it is not immediately
sent to the receivers and is only sent after time δ1 to receiver 1
(at time T1
1 = T1 + δ1) and after time δ2 to receiver 2 (at time
T2
1 = T1 + δ2). Note that the sender includes the sending 
timestamp with the DR vector as shown in the figure. Assume that
the sender estimates (it will be clear shortly why the sender has to
estimate the delay) that after a delay of dt1, receiver 1 will receive
it, will use the coordinate and velocity parameters to compute the
entity"s current location and move it there (point C) and from this
time onwards, the exported and the placed paths will become the
same. However, in reality, receiver 1 receives DR1 after a delay
of da1 (which is less than sender"s estimates of dt1), and moves
the corresponding entity to point H. Similarly, the sender estimates
that after a delay of dt2, receiver 2 will receive DR1, will compute
the current location of the entity and move it to that point (point
E), while in reality it receives DR1 after a delay of da2 > dt2 and
moves the entity to point N. The other points shown on the placed
and exported paths will be used later in the discussion to describe
different error components.
4.1 Computation of Relative Export Error
Referring back to the discussion from Section 3, from the sender"s
perspective, the export error at receiver 1 due to DR1 is given
by Err(DR1, T1, T1 + δ1 + dt1) (the integral of the distance 
between the trajectories AC and DB over the time interval [T1, T1 +
δ1 + dt1]) of Figure 2. This is due to the fact that the sender uses
the estimated delay dt1 to compute this error. Similarly, the 
export error from the sender"s perspective at received 2 due to DR1
is given by Err(DR1, T1, T1 + δ2 + dt2) (the integral of the 
distance between the trajectories AE and DF over the time interval
[T1, T1 + δ2 + dt2]). Note that the above errors from the sender"s
perspective are only estimates. In reality, the export error will be
either smaller or larger than the estimated value, based on whether
the delay estimate was larger or smaller than the actual delay that
DR1 experienced. This difference between the estimated and the
actual export error is the relative export error (which could either
be positive or negative) which occurs for every DR vector that is
sent and is accumulated at the sender.
The concept of relative export error is illustrated in Figure 2.
Since the actual delay to receiver 1 is da1, the export error 
induced by DR1 at receiver 1 is Err(DR1, T1, T1 + δ1 + da1).
This means, there is an error in the estimated export error and the
sender can compute this error only after it gets a feedback from the
receiver about the actual delay for the delivery of DR1, i.e., the
value of da1. We propose that once receiver 1 receives DR1, it
sends the value of da1 back to the sender. The receiver can 
compute this information as it knows the time at which DR1 was sent
(T1
1 = T1 + δ1, which is appended to the DR vector as shown in
Figure 2) and the local receiving time (which is synchronized with
the sender"s clock). Therefore, the sender computes the relative
export error for receiver 1, represented using R1 as
R1 = Err(DR1, T1, T1 + δ1 + dt1)
− Err(DR1, T1, T1 + δ1 + da1)
= Err(DR1, T1 + δ1 + dt1, T1 + δ1 + da1)
Similarly the relative export error for receiver 2 is computed as
R2 = Err(DR1, T1, T1 + δ2 + dt2)
− Err(DR1, T1, T1 + δ2 + da2)
= Err(DR1, T1 + δ2 + dt2, T1 + δ2 + da2)
Note that R1 > 0 as da1 < dt1, and R2 < 0 as da2 > dt2.
Relative export errors are computed by the sender as and when it
receives the feedback from the receivers. This example shows the
4
relative export error values after DR1 is sent and the corresponding
feedbacks are received.
4.2 Equalization of Error Among Receivers
We now explain what we mean by making the errors equal
at all the receivers and how this can be achieved. As stated 
before the sender keeps estimates of the delays to the receivers, dt1
and dt2 in the example of Figure 2. This says that at time T1
when DR1 is computed, the sender already knows how long it may
take messages carrying this DR vector to reach the receivers. The
sender uses this information to compute the export errors, which are
Err(DR1, T1, T1 + δ1 + dt1) and Err(DR1, T1, T1 + δ2 + dt2)
for receivers 1 and 2, respectively. Note that the areas of these error
components are a function of δ1 and δ2 as well as the network 
delays dt1 and dt2. If we are to make the exports errors due to DR1
the same at both receivers, the sender needs to choose δ1 and δ2
such that
Err(DR1, T1, T1 + δ1 + dt1) = Err(DR1, T1, T1 + δ2 + dt2).
But when T1 was computed there could already have been 
accumulated relative export errors due to previous DR vectors (DR0 and
the ones before). Let us represent the accumulated relative error up
to DRi for receiver j as Ri
j. To accommodate these accumulated
relative errors, the sender should now choose δ1 and δ2 such that
R0
1 + Err(DR1, T1, T1 + δ1 + dt1) =
R0
2 + Err(DR1, T1, T1 + δ2 + dt2)
The δi determines the scheduling instant of the DR vector at the
sender for receiver i. This method of computation of δ"s ensures
that the accumulated export error (i.e., total actual error) for each
receiver equalizes at the transmission of each DR vector.
In order to establish this, assume that the feedback for DR vector
Di from a receiver comes to the sender before schedule for Di+1 is
computed. Let Si
m and Ai
m denote the estimated error for receiver
m used for computing schedule for Di and accumulated error for
receiver m computed after receiving feedback for Di, respectively.
Then Ri
m = Ai
m −Si
m. In order to compute the schedule instances
(i.e., δ"s) for Di, for any pair of receivers m and n, we do Ri−1
m +
Si
m = Ri−1
n + Si
n. The following theorem establishes the fact
that the accumulated export error is equalized at every scheduling
instant.
THEOREM 4.1. When the schedule instances for sending Di
are computed for any pair of receivers m and n, the following 
condition is satisfied:
i−1
k=1
Ak
m + Si
m =
i−1
k=1
Ak
n + Si
n.
Proof: By induction. Assume that the premise holds for some i.
We show that it holds for i+1. The base case for i = 1 holds since
initially R0
m = R0
n = 0, and the S1
m = S1
n is used to compute the
scheduling instances.
In order to compute the schedule for Di+1, the we first compute
the relative errors as
Ri
m = Ai
m − Si
m, and Ri
n = Ai
n − Si
n.
Then to compute δ"s we execute
Ri
m + Si+1
m = Ri
n + Si+1
n
Ai
m − Si
m + Si+1
m = Ai
n − Si
n + Si+1
n .
Adding the condition of the premise on both sides we get,
i
k=1
Ak
m + Si+1
m =
i
k=1
Ak
n + Si+1
n .
4.3 Computation of the Export Error
Let us now consider how the export errors can be computed.
From the previous section, to find δ1 and δ2 we need to find
Err(DR1, T1, T1 +δ1 +dt1) and Err(DR1, T1, T1 +δ2 +dt2).
Note that the values of R0
1 and R0
2 are already known at the sender.
Consider the computation of Err(DR1, T1, T1 +δ1 +dt1). This is
the integral of the distance between the trajectories AC due to DR1
and BD due to DR0. From DR0 and DR1, point A is (X1, Y1) =
(x1, y1) and point B is (X0, Y0) = (x0 + (T1 − T0) × vx0, y0 +
(T1 − T0) × vy0). The trajectory AC can be represented as a
function of time as (X1(t), Y1(t) = (X1 + vx1 × t, Y1 + vy1 × t)
and the trajectory of BD can be represented as (X0(t), Y0(t) =
(X0 + vx0 × t, Y0 + vy0 × t).
The distance between the two trajectories as a function of time
then becomes,
dist(t) = (X1(t) − X0(t))2 + (Y1(t) − Y0(t))2
= ((X1 − X0) + (vx1 − vx0)t)2
+((Y1 − Y0) + (vy1 − vy0)t)2
= ((vx1 − vx0)2 + (vy1 − vy0)2)t2
+2((X1 − X0)(vx1 − vx0)
+(Y1 − Y0)(vy1 − vy0))t
+(X1 − X0)2 + (Y1 − Y0)2
Let
a = (vx1 − vx0)2
+ (vy1 − vy0)2
b = 2((X1 − X0)(vx1 − vx0)
+(Y1 − Y0)(vy1 − vy0))
c = (X1 − X0)2
+ (Y1 − Y0)2
Then dist(t) can be written as
dist(t) = a × t2 + b × t + c.
Then Err(DR1, t1, t2) for some time interval [t1, t2] becomes
t2
t1
dist(t) dt =
t2
t1
a × t2 + b × t + c dt.
A closed form solution for the indefinite integral
a × t2 + b × t + c dt =
(2at + b)
√
at2 + bt + c
4a
+
1
2
ln
1
2b
+ at
√
a
+ at2 + bt + c c
1
√
a
−
1
8
ln
1
2b
+ at
√
a
+ at2 + bt + c b2
a− 3
2
Err(DR1, T1, T1 +δ1 +dt1) and Err(DR1, T1, T1 +δ2 +dt2)
can then be calculated by applying the appropriate limits to the
above solution. In the next section, we consider the computation
of the δ"s for N receivers.
5
4.4 Computation of Scheduling Instants
We again look at the computation of δ"s by referring to Figure 2.
The sender chooses δ1 and δ2 such that R0
1 + Err(DR1, T1, T1 +
δ1 +dt1) = R0
2 +Err(DR1, T1, T1 +δ2 +dt2). If R0
1 and R0
2 both
are zero, then δ1 and δ2 should be chosen such that Err(DR1, T1, T1+
δ1 +dt1) = Err(DR1, T1, T1 +δ2 +dt2). This equality will hold
if δ1 + dt1 = δ2 + dt2. Thus, if there is no accumulated relative
export error, all that the sender needs to do is to choose the δ"s in
such a way that they counteract the difference in the delay to the
two receivers, so that they receive the DR vector at the same time.
As discussed earlier, because the sender is not able to a priori learn
the delay, there will always be an accumulated relative export error
from a previous DR vector that does have to be taken into account.
To delve deeper into this, consider the computation of the 
export error as illustrated in the previous section. To compute the
δ"s we require that R0
1 + Err(DR1, T1, T1 + δ1 + dt1) = R0
2 +
Err(DR1, T1, T1 + δ2 + dt2). That is,
R0
1 +
T1+δ1+dt1
T1
dist(t) dt = R0
2 +
T1+δ2+dt2
T1
dist(t) dt.
That is
R0
1 +
T1+dt1
T1
dist(t) dt +
T1+dt1+δ1
T1+dt1
dist(t) dt =
R0
2 +
T1+dt2
T1
dist(t) dt +
T1+dt2+δ2
T1+dt2
dist(t) dt.
The components R0
1, R0
2, are already known to (or estimated by)
the sender. Further, the error components
T1+dt1
T1
dist(t) dt and
T1+dt2
T1
dist(t) dt can be a priori computed by the sender using
estimated values of dt1 and dt2. Let us use E1 to denote R0
1 +
T1+dt1
T1
dist(t) dt and E2 to denote R0
2 +
T1+dt2
T1
dist(t) dt.
Then, we require that
E1 +
T1+dt1+δ1
T1+dt1
dist(t) dt = E2 +
T1+dt2+δ2
T1+dt2
dist(t) dt.
Assume that E1 > E2. Then, for the above equation to hold, we
require that
T1+dt1+δ1
T1+dt1
dist(t) dt <
T1+dt2+δ2
T1+dt2
dist(t) dt.
To make the game as fast as possible within this framework, the δ
values should be made as small as possible so that DR vectors are
sent to the receivers as soon as possible subject to the fairness 
requirement. Given this, we would choose δ1 to be zero and compute
δ2 from the equation
E1 = E2 +
T1+dt2+δ2
T1+dt2
dist(t) dt.
In general, if there are N receivers 1, . . . , N, when a sender 
generates a DR vector and decides to schedule them to be sent, it first
computes the Ei values for all of them from the accumulated 
relative export errors and estimates of delays. Then, it finds the smallest
of these values. Let Ek be the smallest value. The sender makes δk
to be zero and computes the rest of the δ"s from the equality
Ei +
T1+dti+δi
T1+dti
dist(t) dt = Ek,
∀i 1 ≤ i ≤ N, i = k. (1)
The δ"s thus obtained gives the scheduling instants of the DR 
vector for the receivers.
4.5 Steps of the Scheduling Algorithm
For the purpose of the discussion below, as before let us denote
the accumulated relative export error at a sender for receiver k up
until DRi to be Ri
k. Let us denote the scheduled delay at the sender
before DRi is sent to receiver k as δi
k. Given the above discussion,
the algorithm steps are as follows:
1. The sender computes DRi at (say) time Ti and then 
computes δi
k, and Ri−1
k , ∀k, 1 ≤ k ≤ N based on the estimation
of delays dtk, ∀k, 1 ≤ k ≤ N as per Equation (1). It 
schedules, DRi to be sent to receiver k at time Ti + δi
k.
2. The DR vectors are sent to the receivers at the scheduled
times which are received after a delay of dak, ∀k, 1 ≤ k ≤
N where dak ≤ or > dtk. The receivers send the value of
dak back to the sender (the receiver can compute this value
based on the time stamps on the DR vector as described 
earlier).
3. The sender computes Ri
k as described earlier and illustrated
in Figure 2. The sender also recomputes (using exponential
averaging method similar to round-trip time estimation by
TCP [10]) the estimate of delay dtk from the new value of
dak for receiver k.
4. Go back to Step 1 to compute DRi+1 when it is required
and follow the steps of the algorithm to schedule and send
this DR vector to the receivers.
4.6 Handling Cases in Practice
So far we implicity assumed that DRi is sent out to all receivers
before a decision is made to compute the next DR vector DRi+1,
and the receivers send the value of dak corresponding to DRi and
this information reaches the sender before it computes DRi+1 so
that it can compute Ri+1
k and then use it in the computation of δi+1
k .
Two issues need consideration with respect to the above algorithm
when it is used in practice.
• It may so happen that a new DR vector is computed even
before the previous DR vector is sent out to all receivers.
How will this situation be handled?
• What happens if the feedback does not arrive before DRi+1
is computed and scheduled to be sent?
Let us consider the first scenario. We assume that DRi has been
scheduled to be sent and the scheduling instants are such that δi
1 <
δi
2 < · · · < δi
N . Assume that DRi+1 is to be computed (because
the real path has deviated exceeding a threshold from the path 
exported by DRi) at time Ti+1 where Ti + δi
k < Ti+1 < Ti + δi
k+1.
This means, DRi has been sent only to receivers up to k in the
scheduled order. In our algorithm, in this case, the scheduled delay
ordering queue is flushed which means DRi is not sent to receivers
still queued to receive it, but a new scheduling order is computed
for all the receivers to send DRi+1.
For those receivers who have been sent DRi, assume for now
that daj, 1 ≤ j ≤ k has been received from all receivers (the 
scenario where daj has not been received will be considered as a part
of the second scenario later). For these receivers, Ei
j, 1 ≤ j ≤ k
can be computed. For those receivers j, k + 1 ≤ j ≤ N to
whom DRi was not sent Ei
j does not apply. Consider a receiver
j, k + 1 ≤ j ≤ N to whom DRi was not sent. Refer to 
Figure 3. For such a receiver j, when DRi+1 is to be scheduled and
6
timeTi
Exported path
dtj
A
B
C
D
Ti-1
Gi
j
DRi+1 computed by sender
and DRi for receiver k+1 to
N is removed from queue
DRi+1 scheduled
for receiver k+1
Ti+1
G
H
E
F
DRi scheduled
for receiver j
DRi computed
by sender
Placed path
at receiver k+1
Gi+1
j
Figure 3: Schedule computation when DRi is not sent to 
receiver j, k + 1 ≤ j ≤ N.
δi+1
j needs to be computed, the total export error is the accumulated
relative export error at time Ti when schedule for DRi was 
computed, plus the integral of the distance between the two trajectories
AC and BD of Figure 3 over the time interval [Ti, Ti+1 + δi+1
j +
dtj]. Note that this integral is given by Err(DRi, Ti, Ti+1) +
Err(DRi+1, Ti+1, Ti+1 + δi+1
j + dtj). Therefore, instead of Ei
j
of Equation (1), we use the value Ri−1
j + Err(DRi, Ti, Ti+1) +
Err(DRi+1, Ti+1, Ti+1 + δi+1
j + dtj) where Ri−1
j is relative 
export error used when the schedule for DRi was computed.
Now consider the second scenario. Here the feedback dak 
corresponding to DRi has not arrived before DRi+1 is computed and
scheduled. In this case, Ri
k cannot be computed. Thus, in the 
computation of δk for DRi+1, this will be assumed to be zero. We
do assume that a reliable mechanism is used to send dak back to
the sender. When this information arrives at a later time, Ri
k will
be computed and accumulated to future relative export errors (for
example Ri+1
k if dak is received before DRi+2 is computed) and
used in the computation of δk when a future DR vector is to be
scheduled (for example DRi+2).
4.7 Experimental Results
In order to evaluate the effectiveness and quantify benefits 
obtained through the use of the scheduling algorithm, we implemented
the proposed algorithm in BZFlag (Battle Zone Flag) [11] game.
It is a first-person shooter game where the players in teams drive
tanks and move within a battle field. The aim of the players is to
navigate and capture flags belonging to the other team and bring
them back to their own area. The players shoot each other"s tanks
using shooting bullets. The movement of the tanks as well as that
of the shots are exchanged among the players using DR vectors.
We have modified the implementation of BZFlag to 
incorporate synchronized clocks among the players and the server and 
exchange time-stamps with the DR vector. We set up a testbed with
four players running the instrumented version of BZFlag, with one
as a sender and the rest as receivers. The scheduling approach and
the base case where each DR vector was sent to all the receivers
concurrently at every trigger point were implemented in the same
run by tagging the DR vectors according to the type of approach
used to send the DR vector. NISTNet [12] was used to introduce
delays across the sender and the three receivers. Mean delays of
800ms, 500ms and 200ms were introduced between the sender and
first, second and the third receiver, respectively. We introduce a
variance of 100 msec (to the mean delay of each receiver) to model
variability in delay. The sender logged the errors of each receiver
every 100 milliseconds for both the scheduling approach and the
base case. The sender also calculated the standard deviation and
the mean of the accumulated export error of all the receivers every
100 milliseconds. Figure 4 plots the mean and standard deviation
of the accumulated export error of all the receivers in the 
scheduling case against the base case. Note that the x-axis of these graphs
(and the other graphs that follow) represents the system time when
the snapshot of the game was taken.
Observe that the standard deviation of the error with scheduling
is much lower as compared to the base case. This implies that the
accumulated errors of the receivers in the scheduling case are closer
to one another. This shows that the scheduling approach achieves
fairness among the receivers even if they are at different distances
(i.e, latencies) from the sender.
Observe that the mean of the accumulated error increased 
multifold with scheduling in comparison to the base case. Further 
exploration for the reason for the rise in the mean led to the conclusion
that every time the DR vectors are scheduled in a way to equalize
the total error, it pushes each receivers total error higher. Also, as
the accumulated error has an estimated component, the schedule is
not accurate to equalize the errors for the receivers, leading to the
DR vector reaching earlier or later than the actual schedule. In 
either case, the error is not equalized and if the DR vector reaches
late, it actually increases the error for a receiver beyond the highest
accumulated error. This means that at the next trigger, this receiver
will be the one with highest error and every other receiver"s error
will be pushed to this error value. This flip-flop effect leads to
the increase in the accumulated error for all the receivers.
The scheduling for fairness leads to the decrease in standard 
deviation (i.e., increases the fairness among different players), but it
comes at the cost of higher mean error, which may not be a 
desirable feature. This led us to explore different ways of equalizing the
accumulated errors. The approach discussed in the following 
section is a heuristic approach based on the following idea. Using the
same amount of DR vectors over time as in the base case, instead
of sending the DR vectors to all the receivers at the same frequency
as in the base case, if we can increase the frequency of sending
the DR vectors to the receiver with higher accumulated error and
decrease the frequency of sending DR vectors to the receiver with
lower accumulated error, we can equalize the export error of all
receivers over time. At the same time we wish to decrease the 
error of the receiver with the highest accumulated error in the base
case (of course, this receiver would be sent more DR vectors than
in the base case). We refer to such an algorithm as a budget based
algorithm.
5. BUDGET BASED ALGORITHM
In a game, the sender of an entity sends DR vectors to all the
receivers every time a threshold is crossed by the entity. Lower
the threshold, more DR vectors are generated during a given time
period. Since the DR vectors are sent to all the receivers and the
network delay between the sender-receiver pairs cannot be avoided,
the before export error 3
with the most distant player will always
3
Note that after export error is eliminated by using synchronized
clock among the players.
7
0
1000
2000
3000
4000
5000
15950 16000 16050 16100 16150 16200 16250 16300
MeanAccumulatedError
Time in Seconds
Base Case
Scheduling Algorithm #1
0
50
100
150
200
250
300
350
400
450
500
15950 16000 16050 16100 16150 16200 16250 16300
StandardDeviationofAccumulatedError
Time in Seconds
Base Case
Scheduling Algorithm #1
Figure 4: Mean and standard deviation of error with scheduling and without (i.e., base case).
be higher than the rest. In order to mitigate the imbalance in the
error, we propose to send DR vectors selectively to different 
players based on the accumulated errors of these players. The budget
based algorithm is based on this idea and there are two variations
of it. One is a probabilistic budget based scheme and the other, a
deterministic budget base scheme.
5.1 Probabilistic budget based scheme
The probabilistic budget based scheme has three main steps: a)
lower the dead reckoning threshold but at the same time keep the
total number of DRs sent the same as the base case, b) at every
trigger, probabilistically pick a player to send the DR vector to,
and c) send the DR vector to the chosen player. These steps are
described below.
The lowering of DR threshold is implemented as follows. 
Lowering the threshold is equivalent to increasing the number of trigger
points where DR vectors are generated. Suppose the threshold is
such that the number of triggers caused by it in the base case is t
and at each trigger n DR vectors sent by the sender, which results
in a total of nt DR vectors. Our goal is to keep the total number of
DR vectors sent by the sender fixed at nt, but lower the number of
DR vectors sent at each trigger (i.e., do not send the DR vector to
all the receivers). Let n and t be the number of DR vectors sent
at each trigger and number of triggers respectively in the modified
case. We want to ensure n t = nt. Since we want to increase the
number of trigger points, i.e, t > t, this would mean that n < n.
That is, not all receivers will be sent the DR vector at every trigger.
In the probabilistic budget based scheme, at each trigger, a 
probability is calculated for each receiver to be sent a DR vector and
only one receiver is sent the DR (n = 1). This probability is based
on the relative weights of the receivers" accumulated errors. That
is, a receiver with a higher accumulated error will have a higher
probability of being sent the DR vector. Consider that the 
accumulated error for three players are a1, a2 and a3 respectively.
Then the probability of player 1 receiving the DR vector would
be a1
a1+a2+a3
. Similarly for the other players. Once the player is
picked, the DR vector is sent to that player.
To compare the probabilistic budget based algorithm with the
base case, we needed to lower the threshold for the base case (for
fair comparison). As the dead reckoning threshold in the base
case was already very fine, it was decided that instead of 
lowering the threshold, the probabilistic budget based approach would
be compared against a modified base case that would use the 
normal threshold as the budget based algorithm but the base case was
modified such that every third trigger would be actually used to
send out a DR vector to all the three receivers used in our 
experiments. This was called as the 1/3 base case as it resulted in 1/3
number of DR vectors being sent as compared to the base case.
The budget per trigger for the probability based approach was 
calculated as one DR vector at each trigger as compared to three DR
vectors at every third trigger in the 1/3 base case; thus the two cases
lead to the same number of DR vectors being sent out over time.
In order to evaluate the effectiveness of the probabilistic budget
based algorithm, we instrumented the BZFlag game to use this 
approach. We used the same testbed consisting of one sender and
three receivers with delays of 800ms, 500ms and 200ms from the
sender and with low delay variance (100ms) and moderate delay
variance (180ms). The results are shown in Figures 5 and 6. As
mentioned earlier, the x-axis of these graphs represents the system
time when the snapshot of the game was taken. Observe from the
figures that the standard deviation of the accumulated error among
the receivers with the probabilistic budget based algorithm is less
than the 1/3 base case and the mean is a little higher than the 1/3
base case. This implies that the game is fairer as compared to the
1/3 base case at the cost of increasing the mean error by a small
amount as compared to the 1/3 base case.
The increase in mean error in the probabilistic case compared to
the 1/3 base case can be attributed to the fact that the even though
the probabilistic approach on average sends the same number of
DR vectors as the 1/3 base case, it sometimes sends DR vectors to
a receiver less frequently and sometimes more frequently than the
1/3 base case due to its probabilistic nature. When a receiver does
not receive a DR vector for a long time, the receiver"s trajectory
is more and more off of the sender"s trajectory and hence the rate
of buildup of the error at the receiver is higher. At times when
a receiver receives DR vectors more frequently, it builds up error
at a lower rate but there is no way of reversing the error that was
built up when it did not receive a DR vector for a long time. This
leads the receivers to build up more error in the probabilistic case
as compared to the 1/3 base case where the receivers receive a DR
vector almost periodically.
8
0
200
400
600
800
1000
15950 16000 16050 16100 16150 16200 16250 16300
MeanAccumulatedError
Time in Seconds
1/3 Base Case
Deterministic Algorithm
Probabilistic Algorithm
0
50
100
150
200
250
300
350
400
450
500
15950 16000 16050 16100 16150 16200 16250 16300
StandardDeviationofAccumulatedError
Time in Seconds
1/3 Base Case
Deterministic Algorithm
Probabilistic Algorithm
Figure 5: Mean and standard deviation of error for different algorithms (including budget based algorithms) for low delay variance.
0
200
400
600
800
1000
16960 16980 17000 17020 17040 17060 17080 17100 17120 17140 17160 17180
MeanAccumulatedError
Time in Seconds
1/3 Base Case
Deterministic Algorithm
Probabilistic Algorithm
0
50
100
150
200
250
300
16960 16980 17000 17020 17040 17060 17080 17100 17120 17140 17160 17180
StandardDeviationofAccumulatedError
Time in Seconds
1/3 Base Case
Deterministic Algorithm
Probabilistic Algorithm
Figure 6: Mean and standard deviation of error for different algorithms (including budget based algorithms) for moderate delay
variance.
5.2 Deterministic budget based scheme
To bound the increase in mean error we decided to modify the
budget based algorithm to be deterministic. The first two steps
of the algorithm are the same as in the probabilistic algorithm; the
trigger points are increased to lower the threshold and accumulated
errors are used to compute the probability that a receiver will 
receiver a DR vector. Once these steps are completed, a deterministic
schedule for the receiver is computed as follows:
1. If there is any receiver(s) tagged to receive a DR vector at
the current trigger, the sender sends out the DR vector to the
respective receiver(s). If at least one receiver was sent a DR
vector, the sender calculates the probabilities of each receiver
receiving a DR vector as explained before and follows steps
2 to 6, else it does not do anything.
2. For each receiver, the probability value is multiplied with the
budget available at each trigger (which is set to 1 as explained
below) to give the frequency of sending the DR vector to each
receiver.
3. If any of the receiver"s frequency after multiplying with the
budget goes over 1, the receiver"s frequency is set as 1 and
the surplus amount is equally distributed to all the receivers
by adding the amount to their existing frequencies. This 
process is repeated until all the receivers have a frequency of
less than or equal to 1. This is due to the fact that at a trigger
we cannot send more than one DR vector to the respective
receiver. That will be wastage of DR vectors by sending 
redundant information.
4. (1/frequency) gives us the schedule at which the sender should
send DR vectors to the respective receiver. Credit obtained
previously (explained in step 5) if any is subtracted from the
schedule. Observe that the resulting value of the schedule
might not be an integer; hence, the value is rounded off by
taking the ceiling of the schedule. For example, if the 
frequency is 1/3.5, this implies that we would like to have a DR
vector sent every 3.5 triggers. However, we are constrained
to send it at the 4th trigger giving us a credit of 0.5. When we
do send the DR vector next time, we would be able to send it
9
on the 3rd trigger because of the 0.5 credit.
5. The difference between the schedule and the ceiling of the
schedule is the credit that the receiver has obtained which
is remembered for the future and used at the next time as
explained in step 4.
6. For each of those receivers who were sent a DR vector at
the current trigger, the receivers are tagged to receive the
next DR vector at the trigger that happens exactly schedule
(the ceiling of the schedule) number of times away from the
current trigger. Observe that no other receiver"s schedule is
modified at this point as they all are running a schedule 
calculated at some previous point of time. Those schedules will
be automatically modified at the trigger when they are 
scheduled to receive the next DR vector. At the first trigger, the
sender sends the DR vector to all the receivers and uses a
relative probability of 1/n for each receiver and follows the
steps 2 to 6 to calculate the next schedule for each receiver
in the same way as mentioned for other triggers. This 
algorithm ensures that every receiver has a guaranteed schedule
of receiving DR vectors and hence there is no irregularity in
sending the DR vector to any receiver as was observed in the
budget based probabilistic algorithm.
We used the testbed described earlier (three receivers with 
varying delays) to evaluate the deterministic algorithm using the budget
of 1 DR vector per trigger so as to use the same number of DR
vectors as in the 1/3 base case. Results from our experiments are
shown in Figures 5 and 6. It can be observed that the standard 
deviation of error in the deterministic budget based algorithm is less
than the 1/3 base case and also has the same mean error as the 1/3
base case. This indicates that the deterministic algorithm is more
fair than the 1/3 base case and at the same time does not increase
the mean error thereby leading to a better game quality compared
to the probabilistic algorithm.
In general, when comparing the deterministic approach to the
probabilistic approach, we found that the mean accumulated 
error was always less in the deterministic approach. With respect to
standard deviation of the accumulated error, we found that in the
fixed or low variance cases, the deterministic approach was 
generally lower, but in higher variance cases, it was harder to draw 
conclusions as the probabilistic approach was sometimes better than
the deterministic approach.
6. CONCLUSIONS AND FUTURE WORK
In distributed multi-player games played across the Internet, 
object and player trajectory within the game space are exchanged in
terms of DR vectors. Due to the variable delay between players,
these DR vectors reach different players at different times. There is
unfair advantage gained by receivers who are closer to the sender
of the DR as they are able to render the sender"s position more
accurately in real time. In this paper, we first developed a model
for estimating the error in rendering player trajectories at the 
receivers. We then presented an algorithm based on scheduling the
DR vectors to be sent to different players at different times thereby
equalizing the error at different players. This algorithm is aimed
at making the game fair to all players, but tends to increase the
mean error of the players. To counter this effect, we presented
budget based algorithms where the DR vectors are still 
scheduled to be sent at different players at different times but the 
algorithm balances the need for fairness with the requirement that the
error of the worst case players (who are furthest from the sender)
are not increased compared to the base case (where all DR vectors
are sent to all players every time a DR vector is generated). We 
presented two variations of the budget based algorithms and through
experimentation showed that the algorithms reduce the standard 
deviation of the error thereby making the game more fair and at the
same time has comparable mean error to the base case.
7. REFERENCES
[1] S.Aggarwal, H. Banavar, A. Khandelwal, S. Mukherjee, and
S. Rangarajan, Accuracy in Dead-Reckoning based
Distributed Multi-Player Games, Proceedings of ACM
SIGCOMM 2004 Workshop on Network and System Support
for Games (NetGames 2004), Aug. 2004.
[2] L. Gautier and C. Diot, Design and Evaluation of MiMaze,
a Multiplayer Game on the Internet, in Proc. of IEEE
Multimedia (ICMCS"98), 1998.
[3] M. Mauve, Consistency in Replicated Continuous
Interactive Media, in Proc. of the ACM Conference on
Computer Supported Cooperative Work (CSCW"00), 2000,
pp. 181-190.
[4] S.K. Singhal and D.R. Cheriton, Exploiting Position
History for Efficient Remote Rendering in Networked
Virtual Reality, Presence: Teleoperators and Virtual
Environments, vol. 4, no. 2, pp. 169-193, 1995.
[5] C. Diot and L. Gautier, A Distributed Architecture for
Multiplayer Interactive Applications on the Internet, in
IEEE Network Magazine, 1999, vol. 13, pp. 6-15.
[6] L. Pantel and L.C. Wolf, On the Impact of Delay on
Real-Time Multiplayer Games, in Proc. of ACM
NOSSDAV"02, May 2002.
[7] Y. Lin, K. Guo, and S. Paul, Sync-MS: Synchronized
Messaging Service for Real-Time Multi-Player Distributed
Games, in Proc. of 10th IEEE International Conference on
Network Protocols (ICNP), Nov 2002.
[8] K. Guo, S. Mukherjee, S. Rangarajan, and S. Paul, A Fair
Message Exchange Framework for Distributed Multi-Player
Games, in Proc. of NetGames2003, May 2003.
[9] N. E. Baughman and B. N. Levine, Cheat-Proof Playout for
Centralized and Distributed Online Games, in Proc. of IEEE
INFOCOM"01, April 2001.
[10] M. Allman and V. Paxson, On Estimating End-to-End
Network Path Properties, in Proc. of ACM SIGCOMM"99,
Sept. 1999.
[11] BZFlag Forum, BZFlag Game, URL:
http://www.bzflag.org.
[12] Nation Institute of Standards and Technology, NIST Net,
URL: http://snad.ncsl.nist.gov/nistnet/.
10
TSAR: A Two Tier Sensor Storage Architecture Using
Interval Skip Graphs
∗
Peter Desnoyers, Deepak Ganesan, and Prashant Shenoy
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
pjd@cs.umass.edu, dganesan@cs.umass.edu, shenoy@cs.umass.edu
ABSTRACT
Archival storage of sensor data is necessary for applications that query,
mine, and analyze such data for interesting features and trends. We argue
that existing storage systems are designed primarily for flat hierarchies of
homogeneous sensor nodes and do not fully exploit the multi-tier nature
of emerging sensor networks, where an application can comprise tens of
tethered proxies, each managing tens to hundreds of untethered sensors.
We present TSAR, a fundamentally different storage architecture that 
envisions separation of data from metadata by employing local archiving at
the sensors and distributed indexing at the proxies. At the proxy tier, TSAR
employs a novel multi-resolution ordered distributed index structure, the
Interval Skip Graph, for efficiently supporting spatio-temporal and value
queries. At the sensor tier, TSAR supports energy-aware adaptive 
summarization that can trade off the cost of transmitting metadata to the proxies
against the overhead of false hits resulting from querying a coarse-grain 
index. We implement TSAR in a two-tier sensor testbed comprising 
Stargatebased proxies and Mote-based sensors. Our experiments demonstrate the
benefits and feasibility of using our energy-efficient storage architecture in
multi-tier sensor networks.
Categories and Subject Descriptors: C.2.4 [Computer - 
Communication Networks]: Distributed Systems
General Terms: Algorithms, performance, experimentation.
1. Introduction
1.1 Motivation
Many different kinds of networked data-centric sensor 
applications have emerged in recent years. Sensors in these applications
sense the environment and generate data that must be processed,
filtered, interpreted, and archived in order to provide a useful 
infrastructure to its users. To achieve its goals, a typical sensor 
application needs access to both live and past sensor data. Whereas
access to live data is necessary in monitoring and surveillance 
applications, access to past data is necessary for applications such as
mining of sensor logs to detect unusual patterns, analysis of 
historical trends, and post-mortem analysis of particular events. Archival
storage of past sensor data requires a storage system, the key 
attributes of which are: where the data is stored, whether it is indexed,
and how the application can access this data in an energy-efficient
manner with low latency.
There have been a spectrum of approaches for constructing 
sensor storage systems. In the simplest, sensors stream data or events
to a server for long-term archival storage [3], where the server 
often indexes the data to permit efficient access at a later time. Since
sensors may be several hops from the nearest base station, network
costs are incurred; however, once data is indexed and archived, 
subsequent data accesses can be handled locally at the server without
incurring network overhead. In this approach, the storage is 
centralized, reads are efficient and cheap, while writes are expensive.
Further, all data is propagated to the server, regardless of whether
it is ever used by the application.
An alternate approach is to have each sensor store data or events
locally (e.g., in flash memory), so that all writes are local and incur
no communication overheads. A read request, such as whether an
event was detected by a particular sensor, requires a message to
be sent to the sensor for processing. More complex read requests
are handled by flooding. For instance, determining if an intruder
was detected over a particular time interval requires the request to
be flooded to all sensors in the system. Thus, in this approach,
the storage is distributed, writes are local and inexpensive, while
reads incur significant network overheads. Requests that require
flooding, due to the lack of an index, are expensive and may waste
precious sensor resources, even if no matching data is stored at
those sensors. Research efforts such as Directed Diffusion [17]
have attempted to reduce these read costs, however, by intelligent
message routing.
Between these two extremes lie a number of other sensor storage
systems with different trade-offs, summarized in Table 1. The 
geographic hash table (GHT) approach [24, 26] advocates the use of
an in-network index to augment the fully distributed nature of 
sensor storage. In this approach, each data item has a key associated
with it, and a distributed or geographic hash table is used to map
keys to nodes that store the corresponding data items. Thus, writes
cause data items to be sent to the hashed nodes and also trigger 
updates to the in-network hash table. A read request requires a lookup
in the in-network hash table to locate the node that stores the data
39
item; observe that the presence of an index eliminates the need for
flooding in this approach.
Most of these approaches assume a flat, homogeneous 
architecture in which every sensor node is energy-constrained. In this
paper, we propose a novel storage architecture called TSAR1
that
reflects and exploits the multi-tier nature of emerging sensor 
networks, where the application is comprised of tens of tethered 
sensor proxies (or more), each controlling tens or hundreds of 
untethered sensors. TSAR is a component of our PRESTO [8] predictive
storage architecture, which combines archival storage with caching
and prediction. We believe that a fundamentally different storage
architecture is necessary to address the multi-tier nature of future
sensor networks. Specifically, the storage architecture needs to 
exploit the resource-rich nature of proxies, while respecting resource
constraints at the remote sensors. No existing sensor storage 
architecture explicitly addresses this dichotomy in the resource 
capabilities of different tiers.
Any sensor storage system should also carefully exploit current
technology trends, which indicate that the capacities of flash 
memories continue to rise as per Moore"s Law, while their costs continue
to plummet. Thus it will soon be feasible to equip each sensor with
1 GB of flash storage for a few tens of dollars. An even more 
compelling argument is the energy cost of flash storage, which can be
as much as two orders of magnitude lower than that for 
communication. Newer NAND flash memories offer very low write and
erase energy costs - our comparison of a 1GB Samsung NAND
flash storage [16] and the Chipcon CC2420 802.15.4 wireless radio
[4] in Section 6.2 indicates a 1:100 ratio in per-byte energy cost 
between the two devices, even before accounting for network protocol
overheads. These trends, together with the energy-constrained 
nature of untethered sensors, indicate that local storage offers a viable,
energy-efficient alternative to communication in sensor networks.
TSAR exploits these trends by storing data or events locally on
the energy-efficient flash storage at each sensor. Sensors send 
concise identifying information, which we term metadata, to a nearby
proxy; depending on the representation used, this metadata may be
an order of magnitude or more smaller than the data itself, 
imposing much lower communication costs. The resource-rich proxies
interact with one another to construct a distributed index of the
metadata reported from all sensors, and thus an index of the 
associated data stored at the sensors. This index provides a unified,
logical view of the distributed data, and enables an application to
query and read past data efficiently - the index is used to 
pinpoint all data that match a read request, followed by messages to
retrieve that data from the corresponding sensors. In-network 
index lookups are eliminated, reducing network overheads for read
requests. This separation of data, which is stored at the sensors,
and the metadata, which is stored at the proxies, enables TSAR to
reduce energy overheads at the sensors, by leveraging resources at
tethered proxies.
1.2 Contributions
This paper presents TSAR, a novel two-tier storage architecture
for sensor networks. To the best of our knowledge, this is the first
sensor storage system that is explicitly tailored for emerging 
multitier sensor networks. Our design and implementation of TSAR has
resulted in four contributions.
At the core of the TSAR architecture is a novel distributed index
structure based on interval skip graphs that we introduce in this
paper. This index structure can store coarse summaries of sensor
data and organize them in an ordered manner to be easily 
search1
TSAR: Tiered Storage ARchitecture for sensor networks.
able. This data structure has O(log n) expected search and update
complexity. Further, the index provides a logically unified view of
all data in the system.
Second, at the sensor level, each sensor maintains a local archive
that stores data on flash memory. Our storage architecture is fully
stateless at each sensor from the perspective of the metadata index;
all index structures are maintained at the resource-rich proxies, and
only direct requests or simple queries on explicitly identified 
storage locations are sent to the sensors. Storage at the remote sensor
is in effect treated as appendage of the proxy, resulting in low 
implementation complexity, which makes it ideal for small, 
resourceconstrained sensor platforms. Further, the local store is optimized
for time-series access to archived data, as is typical in many 
applications. Each sensor periodically sends a summary of its data to a
proxy. TSAR employs a novel adaptive summarization technique
that adapts the granularity of the data reported in each summary to
the ratio of false hits for application queries. More fine grain 
summaries are sent whenever more false positives are observed, thereby
balancing the energy cost of metadata updates and false positives.
Third, we have implemented a prototype of TSAR on a multi-tier
testbed comprising Stargate-based proxies and Mote-based sensors.
Our implementation supports spatio-temporal, value, and 
rangebased queries on sensor data.
Fourth, we conduct a detailed experimental evaluation of TSAR
using a combination of EmStar/EmTOS [10] and our prototype.
While our EmStar/EmTOS experiments focus on the scalability of
TSAR in larger settings, our prototype evaluation involves latency
and energy measurements in a real setting. Our results demonstrate
the logarithmic scaling property of the sparse skip graph and the
low latency of end-to-end queries in a duty-cycled multi-hop 
network .
The remainder of this paper is structured as follows. Section 2
presents key design issues that guide our work. Section 3 and 4
present the proxy-level index and the local archive and 
summarization at a sensor, respectively. Section 5 discusses our prototype 
implementation, and Section 6 presents our experimental results. We
present related work in Section 7 and our conclusions in Section 8.
2. Design Considerations
In this section, we first describe the various components of a
multi-tier sensor network assumed in our work. We then present a
description of the expected usage models for this system, followed
by several principles addressing these factors which guide the 
design of our storage system.
2.1 System Model
We envision a multi-tier sensor network comprising multiple tiers
- a bottom tier of untethered remote sensor nodes, a middle tier of
tethered sensor proxies, and an upper tier of applications and user
terminals (see Figure 1).
The lowest tier is assumed to form a dense deployment of 
lowpower sensors. A canonical sensor node at this tier is equipped
with low-power sensors, a micro-controller, and a radio as well as
a significant amount of flash memory (e.g., 1GB). The common
constraint for this tier is energy, and the need for a long lifetime
in spite of a finite energy constraint. The use of radio, processor,
RAM, and the flash memory all consume energy, which needs to
be limited. In general, we assume radio communication to be 
substantially more expensive than accesses to flash memory.
The middle tier consists of power-rich sensor proxies that have
significant computation, memory and storage resources and can use
40
Table 1: Characteristics of sensor storage systems
System Data Index Reads Writes Order preserving
Centralized store Centralized Centralized index Handled at store Send to store Yes
Local sensor store Fully distributed No index Flooding, diffusion Local No
GHT/DCS [24] Fully distributed In-network index Hash to node Send to hashed node No
TSAR/PRESTO Fully distributed Distributed index at proxies Proxy lookup + sensor query Local plus index update Yes
User
Unified Logical Store
Queries
(time, space, value)
Query
Response
Cache
Query forwarding
Proxy
Remote
Sensors
Local Data Archive
on Flash Memory
Interval
Skip Graph
Query
forwarding
summaries
start index
end index
linear
traversal
Query
Response
Cache-miss
triggered
query forwarding
summaries
Figure 1: Architecture of a multi-tier sensor network.
these resources continuously. In urban environments, the proxy tier
would comprise a tethered base-station class nodes (e.g., Crossbow
Stargate), each with with multiple radios-an 802.11 radio that
connects it to a wireless mesh network and a low-power radio (e.g.
802.15.4) that connects it to the sensor nodes. In remote sensing
applications [10], this tier could comprise a similar Stargate node
with a solar power cell. Each proxy is assumed to manage several
tens to hundreds of lower-tier sensors in its vicinity. A typical 
sensor network deployment will contain multiple geographically 
distributed proxies. For instance, in a building monitoring application,
one sensor proxy might be placed per floor or hallway to monitor
temperature, heat and light sensors in their vicinity.
At the highest tier of our infrastructure are applications that query
the sensor network through a query interface[20]. In this work, we
focus on applications that require access to past sensor data. To
support such queries, the system needs to archive data on a 
persistent store. Our goal is to design a storage system that exploits the
relative abundance of resources at proxies to mask the scarcity of
resources at the sensors.
2.2 Usage Models
The design of a storage system such as TSAR is affected by the
queries that are likely to be posed to it. A large fraction of queries
on sensor data can be expected to be spatio-temporal in nature.
Sensors provide information about the physical world; two key 
attributes of this information are when a particular event or activity
occurred and where it occurred. Some instances of such queries 
include the time and location of target or intruder detections (e.g., 
security and monitoring applications), notifications of specific types
of events such as pressure and humidity values exceeding a 
threshold (e.g., industrial applications), or simple data collection queries
which request data from a particular time or location (e.g., weather
or environment monitoring).
Expected queries of such data include those requesting ranges
of one or more attributes; for instance, a query for all image data
from cameras within a specified geographic area for a certain 
period of time. In addition, it is often desirable to support efficient
access to data in a way that maintains spatial and temporal 
ordering. There are several ways of supporting range queries, such as
locality-preserving hashes such as are used in DIMS [18]. 
However, the most straightforward mechanism, and one which naturally
provides efficient ordered access, is via the use of order-preserving
data structures. Order-preserving structures such as the well-known
B-Tree maintain relationships between indexed values and thus 
allow natural access to ranges, as well as predecessor and successor
operations on their key values.
Applications may also pose value-based queries that involve 
determining if a value v was observed at any sensor; the query 
returns a list of sensors and the times at which they observed this
value. Variants of value queries involve restricting the query to a
geographical region, or specifying a range (v1, v2) rather than a
single value v. Value queries can be handled by indexing on the
values reported in the summaries. Specifically, if a sensor reports
a numerical value, then the index is constructed on these values. A
search involves finding matching values that are either contained in
the search range (v1, v2) or match the search value v exactly.
Hybrid value and spatio-temporal queries are also possible. Such
queries specify a time interval, a value range and a spatial region
and request all records that match these attributes - find all 
instances where the temperature exceeded 100o
F at location R 
during the month of August. These queries require an index on both
time and value.
In TSAR our focus is on range queries on value or time, with
planned extensions to include spatial scoping.
2.3 Design Principles
Our design of a sensor storage system for multi-tier networks is
based on the following set of principles, which address the issues
arising from the system and usage models above.
• Principle 1: Store locally, access globally: Current 
technology allows local storage to be significantly more 
energyefficient than network communication, while technology
trends show no signs of erasing this gap in the near future.
For maximum network life a sensor storage system should
leverage the flash memory on sensors to archive data locally,
substituting cheap memory operations for expensive radio
transmission. But without efficient mechanisms for retrieval,
the energy gains of local storage may be outweighed by 
communication costs incurred by the application in searching for
data. We believe that if the data storage system provides
the abstraction of a single logical store to applications, as
41
does TSAR, then it will have additional flexibility to 
optimize communication and storage costs.
• Principle 2: Distinguish data from metadata: Data must
be identified so that it may be retrieved by the application
without exhaustive search. To do this, we associate 
metadata with each data record - data fields of known syntax
which serve as identifiers and may be queried by the storage
system. Examples of this metadata are data attributes such as
location and time, or selected or summarized data values. We
leverage the presence of resource-rich proxies to index 
metadata for resource-constrained sensors. The proxies share this
metadata index to provide a unified logical view of all data in
the system, thereby enabling efficient, low-latency lookups.
Such a tier-specific separation of data storage from metadata
indexing enables the system to exploit the idiosyncrasies of
multi-tier networks, while improving performance and 
functionality.
• Principle 3: Provide data-centric query support: In a sensor
application the specific location (i.e. offset) of a record in a
stream is unlikely to be of significance, except if it conveys
information concerning the location and/or time at which the
information was generated. We thus expect that applications
will be best served by a query interface which allows them
to locate data by value or attribute (e.g. location and time),
rather than a read interface for unstructured data. This in turn
implies the need to maintain metadata in the form of an index
that provides low cost lookups.
2.4 System Design
TSAR embodies these design principles by employing local 
storage at sensors and a distributed index at the proxies. The key 
features of the system design are as follows:
In TSAR, writes occur at sensor nodes, and are assumed to 
consist of both opaque data as well as application-specific metadata.
This metadata is a tuple of known types, which may be used by the
application to locate and identify data records, and which may be
searched on and compared by TSAR in the course of locating data
for the application. In a camera-based sensing application, for 
instance, this metadata might include coordinates describing the field
of view, average luminance, and motion values, in addition to basic
information such as time and sensor location. Depending on the
application, this metadata may be two or three orders of magnitude
smaller than the data itself, for instance if the metadata consists of
features extracted from image or acoustic data.
In addition to storing data locally, each sensor periodically sends
a summary of reported metadata to a nearby proxy. The summary
contains information such as the sensor ID, the interval (t1, t2)
over which the summary was generated, a handle identifying the
corresponding data record (e.g. its location in flash memory),
and a coarse-grain representation of the metadata associated with
the record. The precise data representation used in the summary
is application-specific; for instance, a temperature sensor might
choose to report the maximum and minimum temperature values
observed in an interval as a coarse-grain representation of the 
actual time series.
The proxy uses the summary to construct an index; the index
is global in that it stores information from all sensors in the 
system and it is distributed across the various proxies in the system.
Thus, applications see a unified view of distributed data, and can
query the index at any proxy to get access to data stored at any
sensor. Specifically, each query triggers lookups in this distributed
index and the list of matches is then used to retrieve the 
corresponding data from the sensors. There are several distributed index and
lookup methods which might be used in this system; however, the
index structure described in Section 3 is highly suited for the task.
Since the index is constructed using a coarse-grain summary,
instead of the actual data, index lookups will yield approximate
matches. The TSAR summarization mechanism guarantees that 
index lookups will never yield false negatives - i.e. it will never miss
summaries which include the value being searched for. However,
index lookups may yield false positives, where a summary matches
the query but when queried the remote sensor finds no matching
value, wasting network resources. The more coarse-grained the
summary, the lower the update overhead and the greater the 
fraction of false positives, while finer summaries incur update overhead
while reducing query overhead due to false positives. Remote 
sensors may easily distinguish false positives from queries which result
in search hits, and calculate the ratio between the two; based on this
ratio, TSAR employs a novel adaptive technique that dynamically
varies the granularity of sensor summaries to balance the metadata
overhead and the overhead of false positives.
3. Data Structures
At the proxy tier, TSAR employs a novel index structure called
the Interval Skip Graph, which is an ordered, distributed data 
structure for finding all intervals that contain a particular point or range
of values. Interval skip graphs combine Interval Trees [5], an
interval-based binary search tree, with Skip Graphs [1], a ordered,
distributed data structure for peer-to-peer systems [13]. The 
resulting data structure has two properties that make it ideal for 
sensor networks. First, it has O(log n) search complexity for 
accessing the first interval that matches a particular value or range, and
constant complexity for accessing each successive interval. 
Second, indexing of intervals rather than individual values makes the
data structure ideal for indexing summaries over time or value.
Such summary-based indexing is a more natural fit for 
energyconstrained sensor nodes, since transmitting summaries incurs less
energy overhead than transmitting all sensor data.
Definitions: We assume that there are Np proxies and Ns 
sensors in a two-tier sensor network. Each proxy is responsible for
multiple sensor nodes, and no assumption is made about the 
number of sensors per proxy. Each sensor transmits interval summaries
of data or events regularly to one or more proxies that it is 
associated with, where interval i is represented as [lowi, highi]. These
intervals can correspond to time or value ranges that are used for
indexing sensor data. No assumption is made about the size of an
interval or about the amount of overlap between intervals.
Range queries on the intervals are posed by users to the network
of proxies and sensors; each query q needs to determine all index
values that overlap the interval [lowq, highq]. The goal of the 
interval skip graph is to index all intervals such that the set that overlaps
a query interval can be located efficiently. In the rest of this section,
we describe the interval skip graph in greater detail.
3.1 Skip Graph Overview
In order to inform the description of the Interval Skip Graph, we
first provide a brief overview of the Skip Graph data structure; for
a more extensive description the reader is referred to [1]. Figure 2
shows a skip graph which indexes 8 keys; the keys may be seen
along the bottom, and above each key are the pointers associated
with that key. Each data element, consisting of a key and its 
associated pointers, may reside on a different node in the network,
42
7 9 13 17 21 25 311
level 0
level 1
level 2
key
single skip graph element
(each may be on different node)
find(21)
node-to-node messages
Figure 2: Skip Graph of 8 Elements
[6,14] [9,12] [14,16] [15,23] [18,19] [20,27] [21,30][2,5]
5 14 14 16 23 23 27 30
[low,high]
max
contains(13)
match
no
match
halt
Figure 3: Interval Skip Graph
[6,14]
[9,12]
[14,16]
[15,23]
[18,19]
[20,27]
[21,30]
[2,5]
Node 1
Node 2
Node 3
level 2
level 1
level 0
Figure 4: Distributed Interval Skip Graph
and pointers therefore identify both a remote node as well as a data
element on that node. In this figure we may see the following 
properties of a skip graph:
• Ordered index: The keys are members of an ordered data
type, for instance integers. Lookups make use of ordered
comparisons between the search key and existing index 
entries. In addition, the pointers at the lowest level point 
directly to the successor of each item in the index.
• In-place indexing: Data elements remain on the nodes
where they were inserted, and messages are sent between
nodes to establish links between those elements and others
in the index.
• Log n height: There are log2 n pointers associated with each
element, where n is the number of data elements indexed.
Each pointer belongs to a level l in [0... log2 n − 1], and
together with some other pointers at that level forms a chain
of n/2l
elements.
• Probabilistic balance: Rather than relying on re-balancing
operations which may be triggered at insert or delete, skip
graphs implement a simple random balancing mechanism
which maintains close to perfect balance on average, with
an extremely low probability of significant imbalance.
• Redundancy and resiliency: Each data element forms an
independent search tree root, so searches may begin at any
node in the network, eliminating hot spots at a single search
root. In addition the index is resilient against node failure;
data on the failed node will not be accessible, but remaining
data elements will be accessible through search trees rooted
on other nodes.
In Figure 2 we see the process of searching for a particular value
in a skip graph. The pointers reachable from a single data element
form a binary tree: a pointer traversal at the highest level skips over
n/2 elements, n/4 at the next level, and so on. Search consists
of descending the tree from the highest level to level 0, at each
level comparing the target key with the next element at that level
and deciding whether or not to traverse. In the perfectly balanced
case shown here there are log2 n levels of pointers, and search will
traverse 0 or 1 pointers at each level. We assume that each data
element resides on a different node, and measure search cost by the
number messages sent (i.e. the number of pointers traversed); this
will clearly be O(log n).
Tree update proceeds from the bottom, as in a B-Tree, with the
root(s) being promoted in level as the tree grows. In this way, for
instance, the two chains at level 1 always contain n/2 entries each,
and there is never a need to split chains as the structure grows. The
update process then consists of choosing which of the 2l
chains to
insert an element into at each level l, and inserting it in the proper
place in each chain.
Maintaining a perfectly balanced skip graph as shown in 
Figure 2 would be quite complex; instead, the probabilistic balancing
method introduced in Skip Lists [23] is used, which trades off a
small amount of overhead in the expected case in return for simple
update and deletion. The basis for this method is the observation
that any element which belongs to a particular chain at level l can
only belong to one of two chains at level l+1. To insert an element
we ascend levels starting at 0, randomly choosing one of the two
possible chains at each level, an stopping when we reach an empty
chain.
One means of implementation (e.g. as described in [1]) is to
assign each element an arbitrarily long random bit string. Each
chain at level l is then constructed from those elements whose bit
strings match in the first l bits, thus creating 2l
possible chains
at each level and ensuring that each chain splits into exactly two
chains at the next level. Although the resulting structure is not
perfectly balanced, following the analysis in [23] we can show that
the probability of it being significantly out of balance is extremely
small; in addition, since the structure is determined by the random
number stream, input data patterns cannot cause the tree to become
imbalanced.
3.2 Interval Skip Graph
A skip graph is designed to store single-valued entries. In this
section, we introduce a novel data structure that extends skip graphs
to store intervals [lowi, highi] and allows efficient searches for all
intervals covering a value v, i.e. {i : lowi ≤ v ≤ highi}. Our data
structure can be extended to range searches in a straightforward
manner.
The interval skip graph is constructed by applying the method of
augmented search trees, as described by Cormen, Leiserson, and
Rivest [5] and applied to binary search trees to create an Interval
Tree. The method is based on the observation that a search structure
based on comparison of ordered keys, such as a binary tree, may
also be used to search on a secondary key which is non-decreasing
in the first key.
Given a set of intervals sorted by lower bound - lowi ≤
lowi+1 - we define the secondary key as the cumulative maximum,
maxi = maxk=0...i (highk). The set of intervals intersecting a
value v may then be found by searching for the first interval (and
thus the interval with least lowi) such that maxi ≥ v. We then
43
traverse intervals in increasing order lower bound, until we find the
first interval with lowi > v, selecting those intervals which 
intersect v.
Using this approach we augment the skip graph data structure, as
shown in Figure 3, so that each entry stores a range (lower bound
and upper bound) and a secondary key (cumulative maximum of
upper bound). To efficiently calculate the secondary key maxi for
an entry i, we take the greatest of highi and the maximum values
reported by each of i"s left-hand neighbors.
To search for those intervals containing the value v, we first
search for v on the secondary index, maxi, and locate the first entry
with maxi ≥ v. (by the definition of maxi, for this data element
maxi = highi.) If lowi > v, then this interval does not contain
v, and no other intervals will, either, so we are done. Otherwise we
traverse the index in increasing order of mini, returning matching
intervals, until we reach an entry with mini > v and we are done.
Searches for all intervals which overlap a query range, or which
completely contain a query range, are straightforward extensions
of this mechanism.
Lookup Complexity: Lookup for the first interval that matches
a given value is performed in a manner very similar to an interval
tree. The complexity of search is O(log n). The number of 
intervals that match a range query can vary depending on the amount of
overlap in the intervals being indexed, as well as the range specified
in the query.
Insert Complexity: In an interval tree or interval skip list, the
maximum value for an entry need only be calculated over the 
subtree rooted at that entry, as this value will be examined only when
searching within the subtree rooted at that entry. For a simple 
interval skip graph, however, this maximum value for an entry must be
computed over all entries preceding it in the index, as searches may
begin anywhere in the data structure, rather than at a distinguished
root element. It may be easily seen that in the worse case the 
insertion of a single interval (one that covers all existing intervals in
the index) will trigger the update of all entries in the index, for a
worst-case insertion cost of O(n).
3.3 Sparse Interval Skip Graph
The final extensions we propose take advantage of the 
difference between the number of items indexed in a skip graph and the
number of systems on which these items are distributed. The cost
in network messages of an operation may be reduced by 
arranging the data structure so that most structure traversals occur locally
on a single node, and thus incur zero network cost. In addition,
since both congestion and failure occur on a per-node basis, we
may eliminate links without adverse consequences if those links
only contribute to load distribution and/or resiliency within a 
single node. These two modifications allow us to achieve reductions
in asymptotic complexity of both update and search.
As may be in Section 3.2, insert and delete cost on an 
interval skip graph has a worst case complexity of O(n), compared to
O(log n) for an interval tree. The main reason for the difference
is that skip graphs have a full search structure rooted at each 
element, in order to distribute load and provide resilience to system
failures in a distributed setting. However, in order to provide load
distribution and failure resilience it is only necessary to provide a
full search structure for each system. If as in TSAR the number
of nodes (proxies) is much smaller than the number of data 
elements (data summaries indexed), then this will result in significant
savings.
Implementation: To construct a sparse interval skip graph, we
ensure that there is a single distinguished element on each system,
the root element for that system; all searches will start at one of
these root elements. When adding a new element, rather than 
splitting lists at increasing levels l until the element is in a list with no
others, we stop when we find that the element would be in a list 
containing no root elements, thus ensuring that the element is reachable
from all root elements. An example of applying this optimization
may be seen in Figure 5. (In practice, rather than designating 
existing data elements as roots, as shown, it may be preferable to insert
null values at startup.)
When using the technique of membership vectors as in [1], this
may be done by broadcasting the membership vectors of each root
element to all other systems, and stopping insertion of an element
at level l when it does not share an l-bit prefix with any of the Np
root elements. The expected number of roots sharing a log2Np-bit
prefix is 1, giving an expected expected height for each element of
log2Np +O(1). An alternate implementation, which distributes 
information concerning root elements at pointer establishment time,
is omitted due to space constraints; this method eliminates the need
for additional messages.
Performance: In a (non-interval) sparse skip graph, since the
expected height of an inserted element is now log2 Np + O(1),
expected insertion complexity is O(log Np), rather than O(log n),
where Np is the number of root elements and thus the number of
separate systems in the network. (In the degenerate case of a 
single system we have a skip list; with splitting probability 0.5 the
expected height of an individual element is 1.) Note that since
searches are started at root elements of expected height log2 n,
search complexity is not improved.
For an interval sparse skip graph, update performance is 
improved considerably compared to the O(n) worst case for the 
nonsparse case. In an augmented search structure such as this, an 
element only stores information for nodes which may be reached from
that element-e.g. the subtree rooted at that element, in the case of
a tree. Thus, when updating the maximum value in an interval tree,
the update is only propagated towards the root. In a sparse interval
skip graph, updates to a node only propagate towards the Np root
elements, for a worst-case cost of Np log2 n.
Shortcut search: When beginning a search for a value v, rather
than beginning at the root on that proxy, we can find the element
that is closest to v (e.g. using a secondary local index), and then
begin the search at that element. The expected distance between
this element and the search terminus is log2 Np, and the search
will now take on average log2 Np + O(1) steps. To illustrate this
optimization, in Figure 4 depending on the choice of search root, a
search for [21, 30] beginning at node 2 may take 3 network hops,
traversing to node 1, then back to node 2, and finally to node 3
where the destination is located, for a cost of 3 messages. The
shortcut search, however, locates the intermediate data element on
node 2, and then proceeds directly to node 3 for a cost of 1 message.
Performance: This technique may be applied to the primary key
search which is the first of two insertion steps in an interval skip
graph. By combining the short-cut optimization with sparse 
interval skip graphs, the expected cost of insertion is now O(log Np),
independent of the size of the index or the degree of overlap of the
inserted intervals.
3.4 Alternative Data Structures
Thus far we have only compared the sparse interval skip graph
with similar structures from which it is derived. A comparison with
several other data structures which meet at least some of the 
requirements for the TSAR index is shown in Table 2.
44
Table 2: Comparison of Distributed Index Structures
Range Query Support Interval Representation Re-balancing Resilience Small Networks Large Networks
DHT, GHT no no no yes good good
Local index, flood query yes yes no yes good bad
P-tree, RP* (distributed B-Trees) yes possible yes no good good
DIMS yes no yes yes yes yes
Interval Skipgraph yes yes no yes good good
[6,14] [9,12] [14,16] [15,23] [18,19] [20,27] [21,30][2,5]
Roots Node 1
Node 2
Figure 5: Sparse Interval Skip Graph
The hash-based systems, DHT [25] and GHT [26], lack the 
ability to perform range queries and are thus not well-suited to indexing
spatio-temporal data. Indexing locally using an appropriate 
singlenode structure and then flooding queries to all proxies is a 
competitive alternative for small networks; for large networks the linear
dependence on the number of proxies becomes an issue. Two 
distributed B-Trees were examined - P-Trees [6] and RP* [19]. Each
of these supports range queries, and in theory could be modified
to support indexing of intervals; however, they both require 
complex re-balancing, and do not provide the resilience characteristics
of the other structures. DIMS [18] provides the ability to perform
spatio-temporal range queries, and has the necessary resilience to
failures; however, it cannot be used index intervals, which are used
by TSAR"s data summarization algorithm.
4. Data Storage and Summarization
Having described the proxy-level index structure, we turn to the
mechanisms at the sensor tier. TSAR implements two key 
mechanisms at the sensor tier. The first is a local archival store at each
sensor node that is optimized for resource-constrained devices. The
second is an adaptive summarization technique that enables each
sensor to adapt to changing data and query characteristics. The rest
of this section describes these mechanisms in detail.
4.1 Local Storage at Sensors
Interval skip graphs provide an efficient mechanism to lookup
sensor nodes containing data relevant to a query. These queries are
then routed to the sensors, which locate the relevant data records
in the local archive and respond back to the proxy. To enable such
lookups, each sensor node in TSAR maintains an archival store of
sensor data. While the implementation of such an archival store
is straightforward on resource-rich devices that can run a database,
sensors are often power and resource-constrained. Consequently,
the sensor archiving subsystem in TSAR is explicitly designed to
exploit characteristics of sensor data in a resource-constrained 
setting.
Timestamp
Calibration
Parameters
Opaque DataData/Event Attributes size
Figure 6: Single storage record
Sensor data has very distinct characteristics that inform our 
design of the TSAR archival store. Sensors produce time-series data
streams, and therefore, temporal ordering of data is a natural and
simple way of storing archived sensor data. In addition to 
simplicity, a temporally ordered store is often suitable for many sensor data
processing tasks since they involve time-series data processing. 
Examples include signal processing operations such as FFT, wavelet
transforms, clustering, similarity matching, and target detection.
Consequently, the local archival store is a collection of records,
designed as an append-only circular buffer, where new records are
appended to the tail of the buffer. The format of each data record is
shown in Figure 6. Each record has a metadata field which includes
a timestamp, sensor settings, calibration parameters, etc. Raw 
sensor data is stored in the data field of the record. The data field
is opaque and application-specific-the storage system does not
know or care about interpreting this field. A camera-based sensor,
for instance, may store binary images in this data field. In order
to support a variety of applications, TSAR supports variable-length
data fields; as a result, record sizes can vary from one record to
another.
Our archival store supports three operations on records: create,
read, and delete. Due to the append-only nature of the store, 
creation of records is simple and efficient. The create operation simply
creates a new record and appends it to the tail of the store. Since
records are always written at the tail, the store need not maintain
a free space list. All fields of the record need to be specified at
creation time; thus, the size of the record is known a priori and the
store simply allocates the the corresponding number of bytes at the
tail to store the record. Since writes are immutable, the size of a
record does not change once it is created.
proxy
proxy
proxy
record
3 record
summary
local archive in
flash memory
data summary
start,end offset
time interval
sensor
summary
sent to proxy
Insert summaries
into interval skip graph
Figure 7: Sensor Summarization
45
The read operation enables stored records to be retrieved in 
order to answer queries. In a traditional database system, efficient
lookups are enabled by maintaining a structure such as a B-tree that
indexes certain keys of the records. However, this can be quite 
complex for a small sensor node with limited resources. Consequently,
TSAR sensors do not maintain any index for the data stored in their
archive. Instead, they rely on the proxies to maintain this metadata
index-sensors periodically send the proxy information 
summarizing the data contained in a contiguous sequence of records, as well
as a handle indicating the location of these records in flash memory.
The mechanism works as follows: In addition to the summary
of sensor data, each node sends metadata to the proxy containing
the time interval corresponding to the summary, as well as the start
and end offsets of the flash memory location where the raw data
corresponding is stored (as shown in Figure 7). Thus, random 
access is enabled at granularity of a summary-the start offset of each
chunk of records represented by a summary is known to the proxy.
Within this collection, records are accessed sequentially. When a
query matches a summary in the index, the sensor uses these offsets
to access the relevant records on its local flash by sequentially 
reading data from the start address until the end address. Any 
queryspecific operation can then be performed on this data. Thus, no
index needs to be maintained at the sensor, in line with our goal
of simplifying sensor state management. The state of the archive
is captured in the metadata associated with the summaries, and is
stored and maintained at the proxy.
While we anticipate local storage capacity to be large, eventually
there might be a need to overwrite older data, especially in high
data rate applications. This may be done via techniques such as
multi-resolution storage of data [9], or just simply by overwriting
older data. When older data is overwritten, a delete operation is
performed, where an index entry is deleted from the interval skip
graph at the proxy and the corresponding storage space in flash
memory at the sensor is freed.
4.2 Adaptive Summarization
The data summaries serve as glue between the storage at the 
remote sensor and the index at the proxy. Each update from a sensor
to the proxy includes three pieces of information: the summary, a
time period corresponding to the summary, and the start and end
offsets for the flash archive. In general, the proxy can index the
time interval representing a summary or the value range reported
in the summary (or both). The former index enables quick lookups
on all records seen during a certain interval, while the latter index
enables quick lookups on all records matching a certain value.
As described in Section 2.4, there is a trade-off between the 
energy used in sending summaries (and thus the frequency and 
resolution of those summaries) and the cost of false hits during queries.
The coarser and less frequent the summary information, the less
energy required, while false query hits in turn waste energy on 
requests for non-existent data.
TSAR employs an adaptive summarization technique that 
balances the cost of sending updates against the cost of false positives.
The key intuition is that each sensor can independently identify the
fraction of false hits and true hits for queries that access its local
archive. If most queries result in true hits, then the sensor 
determines that the summary can be coarsened further to reduce update
costs without adversely impacting the hit ratio. If many queries
result in false hits, then the sensor makes the granularity of each
summary finer to reduce the number and overhead of false hits.
The resolution of the summary depends on two 
parametersthe interval over which summaries of the data are constructed and
transmitted to the proxy, as well as the size of the 
applicationspecific summary. Our focus in this paper is on the interval over
which the summary is constructed. Changing the size of the data
summary can be performed in an application-specific manner (e.g.
using wavelet compression techniques as in [9]) and is beyond the
scope of this paper. Currently, TSAR employs a simple 
summarization scheme that computes the ratio of false and true hits and 
decreases (increases) the interval between summaries whenever this
ratio increases (decreases) beyond a threshold.
5. TSAR Implementation
We have implemented a prototype of TSAR on a multi-tier 
sensor network testbed. Our prototype employs Crossbow Stargate
nodes to implement the proxy tier. Each Stargate node employs a
400MHz Intel XScale processor with 64MB RAM and runs the
Linux 2.4.19 kernel and EmStar release 2.1. The proxy nodes
are equipped with two wireless radios, a Cisco Aironet 340-based
802.11b radio and a hostmote bridge to the Mica2 sensor nodes
using the EmStar transceiver. The 802.11b wireless network is
used for inter-proxy communication within the proxy tier, while
the wireless bridge enables sensor-proxy communication. The 
sensor tier consists of Crossbow Mica2s and Mica2dots, each 
consisting of a 915MHz CC1000 radio, a BMAC protocol stack, a 4 Mb
on-board flash memory and an ATMega 128L processor. The 
sensor nodes run TinyOS 1.1.8. In addition to the on-board flash, the
sensor nodes can be equipped with external MMC/SD flash cards
using a custom connector. The proxy nodes can be equipped with
external storage such as high-capacity compact flash (up to 4GB),
6GB micro-drives, or up to 60GB 1.8inch mobile disk drives.
Since sensor nodes may be several hops away from the nearest
proxy, the sensor tier employs multi-hop routing to communicate
with the proxy tier. In addition, to reduce the power consumption
of the radio while still making the sensor node available for queries,
low power listening is enabled, in which the radio receiver is 
periodically powered up for a short interval to sense the channel for
transmissions, and the packet preamble is extended to account for
the latency until the next interval when the receiving radio wakes
up. Our prototype employs the MultiHopLEPSM routing protocol
with the BMAC layer configured in the low-power mode with a
11% duty cycle (one of the default BMAC [22] parameters)
Our TSAR implementation on the Mote involves a data 
gathering task that periodically obtains sensor readings and logs these
reading to flash memory. The flash memory is assumed to be a
circular append-only store and the format of the logged data is 
depicted in Figure 6. The Mote sends a report to the proxy every N
readings, summarizing the observed data. The report contains: (i)
the address of the Mote, (ii) a handle that contains an offset and the
length of the region in flash memory containing data referred to by
the summary, (iii) an interval (t1, t2) over which this report is 
generated, (iv) a tuple (low, high) representing the minimum and the
maximum values observed at the sensor in the interval, and (v) a 
sequence number. The sensor updates are used to construct a sparse
interval skip graph that is distributed across proxies, via network
messages between proxies over the 802.11b wireless network.
Our current implementation supports queries that request records
matching a time interval (t1, t2) or a value range (v1, v2). Spatial
constraints are specified using sensor IDs. Given a list of matching
intervals from the skip graph, TSAR supports two types of 
messages to query the sensor: lookup and fetch. A lookup message
triggers a search within the corresponding region in flash memory
and returns the number of matching records in that memory region
(but does not retrieve data). In contrast, a fetch message not only
46
0
10
20
30
40
50
60
70
80
128512 1024 2048 4096
NumberofMessages
Index size (entries)
Insert (skipgraph)
Insert (sparse skipgraph)
Initial lookup
(a) James Reserve Data
0
10
20
30
40
50
60
70
80
512 1024 2048 4096
NumberofMessages
Index size (entries)
Insert (skipgraph)
Insert (sparse skipgraph)
Initial lookup
(b) Synthetic Data
Figure 8: Skip Graph Insert Performance
triggers a search but also returns all matching data records to the
proxy. Lookup messages are useful for polling a sensor, for 
instance, to determine if a query matches too many records.
6. Experimental Evaluation
In this section, we evaluate the efficacy of TSAR using our 
prototype and simulations. The testbed for our experiments consists
of four Stargate proxies and twelve Mica2 and Mica2dot sensors;
three sensors each are assigned to each proxy. Given the limited
size of our testbed, we employ simulations to evaluate the 
behavior of TSAR in larger settings. Our simulation employs the EmTOS
emulator [10], which enables us to run the same code in simulation
and the hardware platform.
Rather than using live data from a real sensor, to ensure 
repeatable experiments, we seed each sensor node with a dataset
(i.e., a trace) that dictates the values reported by that node to the
proxy. One section of the flash memory on each sensor node is
programmed with data points from the trace; these observations
are then replayed during an experiment, logged to the local archive
(located in flash memory, as well), and reported to the proxy. The
first dataset used to evaluate TSAR is a temperature dataset from
James Reserve [27] that includes data from eleven temperature 
sensor nodes over a period of 34 days. The second dataset is 
synthetically generated; the trace for each sensor is generated using a
uniformly distributed random walk though the value space.
Our experimental evaluation has four parts. First, we run 
EmTOS simulations to evaluate the lookup, update and delete overhead
for sparse interval skip graphs using the real and synthetic datasets.
Second, we provide summary results from micro-benchmarks of
the storage component of TSAR, which include empirical 
characterization of the energy costs and latency of reads and writes for the
flash memory chip as well as the whole mote platform, and 
comparisons to published numbers for other storage and 
communication technologies. These micro-benchmarks form the basis for our
full-scale evaluation of TSAR on a testbed of four Stargate proxies
and twelve Motes. We measure the end-to-end query latency in our
multi-hop testbed as well as the query processing overhead at the
mote tier. Finally, we demonstrate the adaptive summarization 
capability at each sensor node. The remainder of this section presents
our experimental results.
6.1 Sparse Interval Skip Graph Performance
This section evaluates the performance of sparse interval skip
graphs by quantifying insert, lookup and delete overheads.
We assume a proxy tier with 32 proxies and construct sparse 
interval skip graphs of various sizes using our datasets. For each skip
0
5
10
15
20
25
30
35
409620481024512
NumberofMessages
Index size (entries)
Initial lookup
Traversal
(a) James Reserve Data
0
2
4
6
8
10
12
14
409620481024512
NumberofMessages
Index size (entries)
Initial lookup
Traversal
(b) Synthetic Data
Figure 9: Skip Graph Lookup Performance
0
10
20
30
40
50
60
70
1 4 8 16 24 32 48
Numberofmessages
Number of proxies
Skipgraph insert
Sparse skipgraph insert
Initial lookup
(a) Impact of Number of
Proxies
0
20
40
60
80
100
120
512 1024 2048 4096
NumberofMessages
Index size (entries)
Insert (redundant)
Insert (non-redundant)
Lookup (redundant)
Lookup (non-redundant)
(b) Impact of Redundant
Summaries
Figure 10: Skip Graph Overheads
graph, we evaluate the cost of inserting a new value into the index.
Each entry was deleted after its insertion, enabling us to quantify
the delete overhead as well. Figure 8(a) and (b) quantify the insert
overhead for our two datasets: each insert entails an initial traversal
that incurs log n messages, followed by neighbor pointer update at
increasing levels, incurring a cost of 4 log n messages. Our results
demonstrate this behavior, and show as well that performance of
delete-which also involves an initial traversal followed by pointer
updates at each level-incurs a similar cost.
Next, we evaluate the lookup performance of the index 
structure. Again, we construct skip graphs of various sizes using our
datasets and evaluate the cost of a lookup on the index structure.
Figures 9(a) and (b) depict our results. There are two components
for each lookup-the lookup of the first interval that matches the
query and, in the case of overlapping intervals, the subsequent 
linear traversal to identify all matching intervals. The initial lookup
can be seen to takes log n messages, as expected. The costs of
the subsequent linear traversal, however, are highly data dependent.
For instance, temperature values for the James Reserve data exhibit
significant spatial correlations, resulting in significant overlap 
between different intervals and variable, high traversal cost (see 
Figure 9(a)). The synthetic data, however, has less overlap and incurs
lower traversal overhead as shown in Figure 9(b).
Since the previous experiments assumed 32 proxies, we evaluate
the impact of the number of proxies on skip graph performance. We
vary the number of proxies from 10 to 48 and distribute a skip graph
with 4096 entries among these proxies. We construct regular 
interval skip graphs as well as sparse interval skip graphs using these
entries and measure the overhead of inserts and lookups. Thus, the
experiment also seeks to demonstrate the benefits of sparse skip
graphs over regular skip graphs. Figure 10(a) depicts our results.
In regular skip graphs, the complexity of insert is O(log2n) in the
47
expected case (and O(n) in the worst case) where n is the number
of elements. This complexity is unaffected by changing the 
number of proxies, as indicated by the flat line in the figure. Sparse
skip graphs require fewer pointer updates; however, their overhead
is dependent on the number of proxies, and is O(log2Np) in the
expected case, independent of n. This can be seen to result in 
significant reduction in overhead when the number of proxies is small,
which decreases as the number of proxies increases.
Failure handling is an important issue in a multi-tier sensor 
architecture since it relies on many components-proxies, sensor nodes
and routing nodes can fail, and wireless links can fade. Handling
of many of these failure modes is outside the scope of this 
paper; however, we consider the case of resilience of skip graphs
to proxy failures. In this case, skip graph search (and subsequent
repair operations) can follow any one of the other links from a
root element. Since a sparse skip graph has search trees rooted
at each node, searching can then resume once the lookup request
has routed around the failure. Together, these two properties 
ensure that even if a proxy fails, the remaining entries in the skip
graph will be reachable with high probability-only the entries on
the failed proxy and the corresponding data at the sensors becomes
inaccessible.
To ensure that all data on sensors remains accessible, even in the
event of failure of a proxy holding index entries for that data, we 
incorporate redundant index entries. TSAR employs a simple 
redundancy scheme where additional coarse-grain summaries are used
to protect regular summaries. Each sensor sends summary data
periodically to its local proxy, but less frequently sends a 
lowerresolution summary to a backup proxy-the backup summary 
represents all of the data represented by the finer-grained summaries,
but in a lossier fashion, thus resulting in higher read overhead (due
to false hits) if the backup summary is used. The cost of 
implementing this in our system is low - Figure 10(b) shows the overhead of
such a redundancy scheme, where a single coarse summary is send
to a backup for every two summaries sent to the primary proxy.
Since a redundant summary is sent for every two summaries, the
insert cost is 1.5 times the cost in the normal case. However, these
redundant entries result in only a negligible increase in lookup 
overhead, due the logarithmic dependence of lookup cost on the index
size, while providing full resilience to any single proxy failure.
6.2 Storage Microbenchmarks
Since sensors are resource-constrained, the energy consumption
and the latency at this tier are important measures for evaluating the
performance of a storage architecture. Before performing an 
endto-end evaluation of our system, we provide more detailed 
information on the energy consumption of the storage component used
to implement the TSAR local archive, based on empirical 
measurements. In addition we compare these figures to those for other 
local storage technologies, as well as to the energy consumption of
wireless communication, using information from the literature. For
empirical measurements we measure energy usage for the storage
component itself (i.e. current drawn by the flash chip), as well as
for the entire Mica2 mote.
The power measurements in Table 3 were performed for the
AT45DB041 [15] flash memory on a Mica2 mote, which is an older
NOR flash device. The most promising technology for low-energy
storage on sensing devices is NAND flash, such as the Samsung
K9K4G08U0M device [16]; published power numbers for this 
device are provided in the table. Published energy requirements for
wireless transmission using the Chipcon [4] CC2420 radio (used
in MicaZ and Telos motes) are provided for comparison, assuming
Energy Energy/byte
Mote flash
Read 256 byte page 58µJ* /
136µJ* total
0.23µJ*
Write 256 byte page 926µJ* /
1042µJ* total
3.6µJ*
NAND Flash
Read 512 byte page 2.7µJ 1.8nJ
Write 512 byte page 7.8µJ 15nJ
Erase 16K byte sector 60µJ 3.7nJ
CC2420 radio
Transmit 8 bits
(-25dBm)
0.8µJ 0.8µJ
Receive 8 bits 1.9µJ 1.9µJ
Mote AVR processor
In-memory search,
256 bytes
1.8µJ 6.9nJ
Table 3: Storage and Communication Energy Costs (*measured
values)
0
200
400
600
800
1000
1 2 3
Latency(ms)
Number of hops
(a) Multi-hop query 
performance
0
100
200
300
400
500
1 5121024 2048 4096
Latency(ms)
Index size (entries)
Sensor communication
Proxy communication
Sensor lookup, processing
(b) Query Performance
Figure 11: Query Processing Latency
zero network and protocol overhead. Comparing the total energy
cost for writing flash (erase + write) to the total cost for 
communication (transmit + receive), we find that the NAND flash is almost
150 times more efficient than radio communication, even assuming
perfect network protocols.
6.3 Prototype Evaluation
This section reports results from an end-to-end evaluation of the
TSAR prototype involving both tiers. In our setup, there are four
proxies connected via 802.11 links and three sensors per proxy. The
multi-hop topology was preconfigured such that sensor nodes were
connected in a line to each proxy, forming a minimal tree of depth
0
400
800
1200
1600
0 20 40 60 80 100 120 140 160
Retrievallatency(ms)
Archived data retrieved (bytes)
(a) Data Query and Fetch
Time
0
2
4
6
8
10
12 4 8 16 32
Latency(ms)
Number of 34-byte records searched
(b) Sensor query 
processing delay
Figure 12: Query Latency Components
48
3. Due to resource constraints we were unable to perform 
experiments with dozens of sensor nodes, however this topology ensured
that the network diameter was as large as for a typical network of
significantly larger size.
Our evaluation metric is the end-to-end latency of query 
processing. A query posed on TSAR first incurs the latency of a sparse
skip graph lookup, followed by routing to the appropriate sensor
node(s). The sensor node reads the required page(s) from its local
archive, processes the query on the page that is read, and transmits
the response to the proxy, which then forwards it to the user. We
first measure query latency for different sensors in our multi-hop
topology. Depending on which of the sensors is queried, the total
latency increases almost linearly from about 400ms to 1 second, as
the number of hops increases from 1 to 3 (see Figure 11(a)).
Figure 11(b) provides a breakdown of the various components
of the end-to-end latency. The dominant component of the total
latency is the communication over one or more hops. The 
typical time to communicate over one hop is approximately 300ms.
This large latency is primarily due to the use of a duty-cycled MAC
layer; the latency will be larger if the duty cycle is reduced (e.g.
the 2% setting as opposed to the 11.5% setting used in this 
experiment), and will conversely decrease if the duty cycle is increased.
The figure also shows the latency for varying index sizes; as 
expected, the latency of inter-proxy communication and skip graph
lookups increases logarithmically with index size. Not surprisingly,
the overhead seen at the sensor is independent of the index size.
The latency also depends on the number of packets transmitted
in response to a query-the larger the amount of data retrieved by a
query, the greater the latency. This result is shown in Figure 12(a).
The step function is due to packetization in TinyOS; TinyOS sends
one packet so long as the payload is smaller than 30 bytes and splits
the response into multiple packets for larger payloads. As the data
retrieved by a query is increased, the latency increases in steps,
where each step denotes the overhead of an additional packet.
Finally, Figure 12(b) shows the impact of searching and 
processing flash memory regions of increasing sizes on a sensor. Each
summary represents a collection of records in flash memory, and
all of these records need to be retrieved and processed if that 
summary matches a query. The coarser the summary, the larger the
memory region that needs to be accessed. For the search sizes 
examined, amortization of overhead when searching multiple flash
pages and archival records, as well as within the flash chip and its
associated driver, results in the appearance of sub-linear increase
in latency with search size. In addition, the operation can be seen
to have very low latency, in part due to the simplicity of our query
processing, requiring only a compare operation with each stored
element. More complex operations, however, will of course incur
greater latency.
6.4 Adaptive Summarization
When data is summarized by the sensor before being reported
to the proxy, information is lost. With the interval summarization
method we are using, this information loss will never cause the
proxy to believe that a sensor node does not hold a value which it in
fact does, as all archived values will be contained within the interval
reported. However, it does cause the proxy to believe that the sensor
may hold values which it does not, and forward query messages to
the sensor for these values. These false positives constitute the cost
of the summarization mechanism, and need to be balanced against
the savings achieved by reducing the number of reports. The goal
of adaptive summarization is to dynamically vary the summary size
so that these two costs are balanced.
0
0.1
0.2
0.3
0.4
0.5
0 5 10 15 20 25 30
Fractionoftruehits
Summary size (number of records)
(a) Impact of summary
size
0
5
10
15
20
25
30
35
0 5000 10000 15000 20000 25000 30000
Summarizationsize(num.records)
Normalized time (units)
query rate 0.2
query rate 0.03
query rate 0.1
(b) Adaptation to query
rate
Figure 13: Impact of Summarization Granularity
Figure 13(a) demonstrates the impact of summary granularity
on false hits. As the number of records included in a summary
is increased, the fraction of queries forwarded to the sensor which
match data held on that sensor (true positives) decreases. Next,
in Figure 13(b) we run the a EmTOS simulation with our 
adaptive summarization algorithm enabled. The adaptive algorithm 
increases the summary granularity (defined as the number of records
per summary) when Cost(updates)
Cost(falsehits)
> 1 + and reduces it if
Cost(updates)
Cost(falsehits)
> 1 − , where is a small constant. To 
demonstrate the adaptive nature of our technique, we plot a time series
of the summarization granularity. We begin with a query rate of 1
query per 5 samples, decrease it to 1 every 30 samples, and then
increase it again to 1 query every 10 samples. As shown in 
Figure 13(b), the adaptive technique adjusts accordingly by sending
more fine-grain summaries at higher query rates (in response to the
higher false hit rate), and fewer, coarse-grain summaries at lower
query rates.
7. Related Work
In this section, we review prior work on storage and indexing
techniques for sensor networks. While our work addresses both
problems jointly, much prior work has considered them in isolation.
The problem of archival storage of sensor data has received
limited attention in sensor network literature. ELF [7] is a 
logstructured file system for local storage on flash memory that 
provides load leveling and Matchbox is a simple file system that is
packaged with the TinyOS distribution [14]. Both these systems
focus on local storage, whereas our focus is both on storage at the
remote sensors as well as providing a unified view of distributed
data across all such local archives. Multi-resolution storage [9] is
intended for in-network storage and search in systems where there
is significant data in comparison to storage resources. In contrast,
TSAR addresses the problem of archival storage in two-tier systems
where sufficient resources can be placed at the edge sensors. The
RISE platform [21] being developed as part of the NODE project
at UCR addresses the issues of hardware platform support for large
amounts of storage in remote sensor nodes, but not the indexing
and querying of this data.
In order to efficiently access a distributed sensor store, an index
needs to be constructed of the data. Early work on sensor networks
such as Directed Diffusion [17] assumes a system where all useful
sensor data was stored locally at each sensor, and spatially scoped
queries are routed using geographic co-ordinates to locations where
the data is stored. Sources publish the events that they detect, and
sinks with interest in specific events can subscribe to these events.
The Directed Diffusion substrate routes queries to specific locations
49
if the query has geographic information embedded in it (e.g.: find
temperature in the south-west quadrant), and if not, the query is
flooded throughout the network.
These schemes had the drawback that for queries that are not 
geographically scoped, search cost (O(n) for a network of n nodes)
may be prohibitive in large networks with frequent queries. 
Local storage with in-network indexing approaches address this 
issue by constructing indexes using frameworks such as Geographic
Hash Tables [24] and Quad Trees [9]. Recent research has seen
a growing body of work on data indexing schemes for sensor 
networks[26][11][18]. One such scheme is DCS [26], which provides
a hash function for mapping from event name to location. DCS
constructs a distributed structure that groups events together 
spatially by their named type. Distributed Index of Features in 
Sensornets (DIFS [11]) and Multi-dimensional Range Queries in Sensor
Networks (DIM [18]) extend the data-centric storage approach to
provide spatially distributed hierarchies of indexes to data.
While these approaches advocate in-network indexing for sensor
networks, we believe that indexing is a task that is far too 
complicated to be performed at the remote sensor nodes since it involves
maintaining significant state and large tables. TSAR provides a 
better match between resource requirements of storage and indexing
and the availability of resources at different tiers. Thus complex
operations such as indexing and managing metadata are performed
at the proxies, while storage at the sensor remains simple.
In addition to storage and indexing techniques specific to sensor
networks, many distributed, peer-to-peer and spatio-temporal index
structures are relevant to our work. DHTs [25] can be used for
indexing events based on their type, quad-tree variants such as 
Rtrees [12] can be used for optimizing spatial searches, and K-D
trees [2] can be used for multi-attribute search. While this paper
focuses on building an ordered index structure for range queries, we
will explore the use of other index structures for alternate queries
over sensor data.
8. Conclusions
In this paper, we argued that existing sensor storage systems
are designed primarily for flat hierarchies of homogeneous sensor
nodes and do not fully exploit the multi-tier nature of emerging 
sensor networks. We presented the design of TSAR, a fundamentally
different storage architecture that envisions separation of data from
metadata by employing local storage at the sensors and distributed
indexing at the proxies. At the proxy tier, TSAR employs a novel
multi-resolution ordered distributed index structure, the Sparse 
Interval Skip Graph, for efficiently supporting spatio-temporal and
range queries. At the sensor tier, TSAR supports energy-aware
adaptive summarization that can trade-off the energy cost of 
transmitting metadata to the proxies against the overhead of false hits 
resulting from querying a coarser resolution index structure. We 
implemented TSAR in a two-tier sensor testbed comprising 
Stargatebased proxies and Mote-based sensors. Our experimental 
evaluation of TSAR demonstrated the benefits and feasibility of 
employing our energy-efficient low-latency distributed storage architecture
in multi-tier sensor networks.
9. REFERENCES
[1] James Aspnes and Gauri Shah. Skip graphs. In Fourteenth Annual ACM-SIAM
Symposium on Discrete Algorithms, pages 384-393, Baltimore, MD, USA,
12-14 January 2003.
[2] Jon Louis Bentley. Multidimensional binary search trees used for associative
searching. Commun. ACM, 18(9):509-517, 1975.
[3] Philippe Bonnet, J. E. Gehrke, and Praveen Seshadri. Towards sensor database
systems. In Proceedings of the Second International Conference on Mobile
Data Management., January 2001.
[4] Chipcon. CC2420 2.4 GHz IEEE 802.15.4 / ZigBee-ready RF transceiver, 2004.
[5] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
Introduction to Algorithms. The MIT Press and McGraw-Hill, second edition
edition, 2001.
[6] Adina Crainiceanu, Prakash Linga, Johannes Gehrke, and Jayavel
Shanmugasundaram. Querying Peer-to-Peer Networks Using P-Trees.
Technical Report TR2004-1926, Cornell University, 2004.
[7] Hui Dai, Michael Neufeld, and Richard Han. ELF: an efficient log-structured
flash file system for micro sensor nodes. In SenSys "04: Proceedings of the 2nd
international conference on Embedded networked sensor systems, pages
176-187, New York, NY, USA, 2004. ACM Press.
[8] Peter Desnoyers, Deepak Ganesan, Huan Li, and Prashant Shenoy. PRESTO: A
predictive storage architecture for sensor networks. In Tenth Workshop on Hot
Topics in Operating Systems (HotOS X)., June 2005.
[9] Deepak Ganesan, Ben Greenstein, Denis Perelyubskiy, Deborah Estrin, and
John Heidemann. An evaluation of multi-resolution storage in sensor networks.
In Proceedings of the First ACM Conference on Embedded Networked Sensor
Systems (SenSys)., 2003.
[10] L. Girod, T. Stathopoulos, N. Ramanathan, J. Elson, D. Estrin, E. Osterweil,
and T. Schoellhammer. A system for simulation, emulation, and deployment of
heterogeneous sensor networks. In Proceedings of the Second ACM Conference
on Embedded Networked Sensor Systems, Baltimore, MD, 2004.
[11] B. Greenstein, D. Estrin, R. Govindan, S. Ratnasamy, and S. Shenker. DIFS: A
distributed index for features in sensor networks. Elsevier Journal of ad-hoc
Networks, 2003.
[12] Antonin Guttman. R-trees: a dynamic index structure for spatial searching. In
SIGMOD "84: Proceedings of the 1984 ACM SIGMOD international
conference on Management of data, pages 47-57, New York, NY, USA, 1984.
ACM Press.
[13] Nicholas Harvey, Michael B. Jones, Stefan Saroiu, Marvin Theimer, and Alec
Wolman. Skipnet: A scalable overlay network with practical locality properties.
In In proceedings of the 4th USENIX Symposium on Internet Technologies and
Systems (USITS "03), Seattle, WA, March 2003.
[14] Jason Hill, Robert Szewczyk, Alec Woo, Seth Hollar, David Culler, and
Kristofer Pister. System architecture directions for networked sensors. In
Proceedings of the Ninth International Conference on Architectural Support for
Programming Languages and Operating Systems (ASPLOS-IX), pages 93-104,
Cambridge, MA, USA, November 2000. ACM.
[15] Atmel Inc. 4-megabit 2.5-volt or 2.7-volt DataFlash AT45DB041B, 2005.
[16] Samsung Semiconductor Inc. K9W8G08U1M, K9K4G08U0M: 512M x 8 bit /
1G x 8 bit NAND flash memory, 2003.
[17] Chalermek Intanagonwiwat, Ramesh Govindan, and Deborah Estrin. Directed
diffusion: A scalable and robust communication paradigm for sensor networks.
In Proceedings of the Sixth Annual International Conference on Mobile
Computing and Networking, pages 56-67, Boston, MA, August 2000. ACM
Press.
[18] Xin Li, Young-Jin Kim, Ramesh Govindan, and Wei Hong. Multi-dimensional
range queries in sensor networks. In Proceedings of the First ACM Conference
on Embedded Networked Sensor Systems (SenSys)., 2003. to appear.
[19] Witold Litwin, Marie-Anne Neimat, and Donovan A. Schneider. RP*: A family
of order preserving scalable distributed data structures. In VLDB "94:
Proceedings of the 20th International Conference on Very Large Data Bases,
pages 342-353, San Francisco, CA, USA, 1994.
[20] Samuel Madden, Michael Franklin, Joseph Hellerstein, and Wei Hong. TAG: a
tiny aggregation service for ad-hoc sensor networks. In OSDI, Boston, MA,
2002.
[21] A. Mitra, A. Banerjee, W. Najjar, D. Zeinalipour-Yazti, D.Gunopulos, and
V. Kalogeraki. High performance, low power sensor platforms featuring
gigabyte scale storage. In SenMetrics 2005: Third International Workshop on
Measurement, Modeling, and Performance Analysis of Wireless Sensor
Networks, July 2005.
[22] J. Polastre, J. Hill, and D. Culler. Versatile low power media access for wireless
sensor networks. In Proceedings of the Second ACM Conference on Embedded
Networked Sensor Systems (SenSys), November 2004.
[23] William Pugh. Skip lists: a probabilistic alternative to balanced trees. Commun.
ACM, 33(6):668-676, 1990.
[24] S. Ratnasamy, D. Estrin, R. Govindan, B. Karp, L. Yin S. Shenker, and F. Yu.
Data-centric storage in sensornets. In ACM First Workshop on Hot Topics in
Networks, 2001.
[25] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker. A scalable
content addressable network. In Proceedings of the 2001 ACM SIGCOMM
Conference, 2001.
[26] S. Ratnasamy, B. Karp, L. Yin, F. Yu, D. Estrin, R. Govindan, and S. Shenker.
GHT - a geographic hash-table for data-centric storage. In First ACM
International Workshop on Wireless Sensor Networks and their Applications,
2002.
[27] N. Xu, E. Osterweil, M. Hamilton, and D. Estrin.
http://www.lecs.cs.ucla.edu/˜nxu/ess/. James Reserve Data.
50
A Holistic Approach to High-Performance Computing:
Xgrid Experience
David Przybyla
Ringling School of Art and Design
2700 North Tamiami Trail
Sarasota, Florida 34234
941-309-4720
dprzybyl@ringling.edu
Karissa Miller
Ringling School of Art and Design
2700 North Tamiami Trail
Sarasota, Florida 34234
941-359-7670
kmiller@ringling.edu
Mahmoud Pegah
Ringling School of Art and Design
2700 North Tamiami Trail
Sarasota, Florida 34234
941-359-7625
mpegah@ringling.edu
ABSTRACT
The Ringling School of Art and Design is a fully accredited 
fouryear college of visual arts and design. With a student to computer
ratio of better than 2-to-1, the Ringling School has achieved
national recognition for its large-scale integration of technology
into collegiate visual art and design education. We have found
that Mac OS X is the best operating system to train future artists
and designers. Moreover, we can now buy Macs to run high-end
graphics, nonlinear video editing, animation, multimedia, web
production, and digital video applications rather than expensive
UNIX workstations. As visual artists cross from paint on canvas
to creating in the digital realm, the demand for a 
highperformance computing environment grows. In our public
computer laboratories, students use the computers most often
during the workday; at night and on weekends the computers see
only light use. In order to harness the lost processing time for
tasks such as video rendering, we are testing Xgrid, a suite of Mac
OS X applications recently developed by Apple for parallel and
distributed high-performance computing.
As with any new technology deployment, IT managers need to
consider a number of factors as they assess, plan, and implement
Xgrid. Therefore, we would like to share valuable information we
learned from our implementation of an Xgrid environment with
our colleagues. In our report, we will address issues such as
assessing the needs for grid computing, potential applications,
management tools, security, authentication, integration into
existing infrastructure, application support, user training, and user
support. Furthermore, we will discuss the issues that arose and the
lessons learned during and after the implementation process.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed 
Systemsdistributed applications.
General Terms
Management, Documentation, Performance, Design, Economics,
Reliability, Experimentation.
1. INTRODUCTION
Grid computing does not have a single, universally accepted
definition. The technology behind grid computing model is not
new. Its roots lie in early distributed computing models that date
back to early 1980s, where scientists harnessed the computing
power of idle workstations to let compute intensive applications
to run on multiple workstations, which dramatically shortening
processing times. Although numerous distributed computing
models were available for discipline-specific scientific
applications, only recently have the tools became available to use
general-purpose applications on a grid. Consequently, the grid
computing model is gaining popularity and has become a show
piece of "utility computing". Since in the IT industry, various
computing models are used interchangeably with grid computing,
we first sort out the similarities and difference between these
computing models so that grid computing can be placed in
perspective.
1.1 Clustering
A cluster is a group of machines in a fixed configuration united to
operate and be managed as a single entity to increase robustness
and performance. The cluster appears as a single high-speed
system or a single highly available system. In this model,
resources can not enter and leave the group as necessary. There
are at least two types of clusters: parallel clusters and 
highavailability clusters. Clustered machines are generally in spatial
proximity, such as in the same server room, and dedicated solely
to their task.
In a high-availability cluster, each machine provides the same
service. If one machine fails, another seamlessly takes over its
workload. For example, each computer could be a web server for
a web site. Should one web server "die," another provides the
service, so that the web site rarely, if ever, goes down.
A parallel cluster is a type of supercomputer. Problems are split
into many parts, and individual cluster members are given part of
the problem to solve. An example of a parallel cluster is
composed of Apple Power Mac G5 computers at Virginia Tech
University [1].
1.2 Distributed Computing
Distributed computing spatially expands network services so that
the components providing the services are separated. The major
objective of this computing model is to consolidate processing
power over a network. A simple example is spreading services
such as file and print serving, web serving, and data storage across
multiple machines rather than a single machine handling all the
tasks. Distributed computing can also be more fine-grained, where
even a single application is broken into parts and each part located
on different machines: a word processor on one server, a spell
checker on a second server, etc.
1.3 Utility Computing
Literally, utility computing resembles common utilities such as
telephone or electric service. A service provider makes computing
resources and infrastructure management available to a customer
as needed, and charges for usage rather than a flat rate. The
important thing to note is that resources are only used as needed,
and not dedicated to a single customer.
1.4 Grid Computing
Grid computing contains aspects of clusters, distributed
computing, and utility computing. In the most basic sense, grid
turns a group of heterogeneous systems into a centrally managed
but flexible computing environment that can work on tasks too
time intensive for the individual systems. The grid members are
not necessarily in proximity, but must merely be accessible over a
network; the grid can access computers on a LAN, WAN, or
anywhere in the world via the Internet. In addition, the computers
comprising the grid need not be dedicated to the grid; rather, they
can function as normal workstations, and then advertise their
availability to the grid when not in use.
The last characteristic is the most fundamental to the grid
described in this paper. A well-known example of such an ad
hoc grid is the SETI@home project [2] of the University of
California at Berkeley, which allows any person in the world with
a computer and an Internet connection to donate unused processor
time for analyzing radio telescope data.
1.5 Comparing the Grid and Cluster
A computer grid expands the capabilities of the cluster by loosing
its spatial bounds, so that any computer accessible through the
network gains the potential to augment the grid. A fundamental
grid feature is that it scales well. The processing power of any
machine added to the grid is immediately availably for solving
problems. In addition, the machines on the grid can be 
generalpurpose workstations, which keep down the cost of expanding the
grid.
2. ASSESSING THE NEED FOR GRID
COMPUTING
Effective use of a grid requires a computation that can be divided
into independent (i.e., parallel) tasks. The results of each task
cannot depend on the results of any other task, and so the
members of the grid can solve the tasks in parallel. Once the tasks
have been completed, the results can be assembled into the
solution. Examples of parallelizable computations are the
Mandelbrot set of fractals, the Monte Carlo calculations used in
disciplines such as Solid State Physics, and the individual frames
of a rendered animation. This paper is concerned with the last
example.
2.1 Applications Appropriate for Grid
Computing
The applications used in grid computing must either be
specifically designed for grid use, or scriptable in such a way that
they can receive data from the grid, process the data, and then
return results. In other words, the best candidates for grid
computing are applications that run the same or very similar
computations on a large number of pieces of data without any
dependencies on the previous calculated results. Applications
heavily dependent on data handling rather than processing power
are generally more suitable to run on a traditional environment
than on a grid platform. Of course, the applications must also run
on the computing platform that hosts the grid. Our interest is in
using the Alias Maya application [3] with Apple"s Xgrid [4] on
Mac OS X.
Commercial applications usually have strict license requirements.
This is an important concern if we install a commercial
application such as Maya on all members of our grid. By its
nature, the size of the grid may change as the number of idle
computers changes. How many licenses will be required? Our
resolution of this issue will be discussed in a later section.
2.2 Integration into the Existing
Infrastructure
The grid requires a controller that recognizes when grid members
are available, and parses out job to available members. The
controller must be able to see members on the network. This does
not require that members be on the same subnet as the controller,
but if they are not, any intervening firewalls and routers must be
configured to allow grid traffic.
3. XGRID
Xgrid is Apple"s grid implementation. It was inspired by Zilla, a
desktop clustering application developed by NeXT and acquired
by Apple. In this report we describe the Xgrid Technology
Preview 2, a free download that requires Mac OS X 10.2.8 or later
and a minimum 128 MB RAM [5].
Xgrid, leverages Apple"s traditional ease of use and configuration.
If the grid members are on the same subnet, by default Xgrid
automatically discovers available resources through Rendezvous
[6]. Tasks are submitted to the grid through a GUI interface or by
the command line. A System Preference Pane controls when each
computer is available to the grid.
It may be best to view Xgrid as a facilitator. The Xgrid
architecture handles software and data distribution, job execution,
and result aggregation. However, Xgrid does not perform the
actual calculations.
3.1 Xgrid Components
Xgrid has three major components: the client, controller, and the
agent. Each component is included in the default installation, and
any computer can easily be configured to assume any role. In
120
fact, for testing purposes, a computer can simultaneously assume
all roles in local mode. The more typical production use is
called cluster mode.
The client submits jobs to the controller through the Xgrid GUI or
command line. The client defines how the job will be broken into
tasks for the grid. If any files or executables must be sent as part
of a job, they must reside on the client or at a location accessible
to the client. When a job is complete, the client can retrieve the
results from the controller. A client can only connect to a single
controller at a time.
The controller runs the GridServer process. It queues tasks
received from clients, distributes those tasks to the agents, and
handles failover if an agent cannot complete a task. In Xgrid
Technology Preview 2, a controller can handle a maximum of
10,000 agent connections. Only one controller can exist per
logical grid.
The agents run the GridAgent process. When the GridAgent
process starts, it registers with a controller; an agent can only be
connected to one controller at a time. Agents receive tasks from
their controller, perform the specified computations, and then
send the results back to the controller. An agent can be configured
to always accept tasks, or to just accept them when the computer
is not otherwise busy.
3.2 Security and Authentication
By default, Xgrid requires two passwords. First, a client needs a
password to access a controller. Second, the controller needs a
password to access an agent. Either password requirement can be
disabled. Xgrid uses two-way-random mutual authentication
protocol with MD5 hashes. At this time, data encryption is only
used for passwords.
As mentioned earlier, an agent registers with a controller when the
GridAgent process starts. There is no native method for the
controller to reject agents, and so it must accept any agent that
registers. This means that any agent could submit a job that
consumes excessive processor and disk space on the agents. Of
course, since Mac OS X is a BSD-based operating system, the
controller could employ Unix methods of restricting network
connections from agents.
The Xgrid daemons run as the user nobody, which means the
daemons can read, write, or execute any file according to world
permissions. Thus, Xgrid jobs can execute many commands and
write to /tmp and /Volumes. In general, this is not a major security
risk, but is does require a level of trust between all members of the
grid.
3.3 Using Xgrid
3.3.1 Installation
Basic Xgrid installation and configuration is described both in
Apple documentation [5] and online at the University of Utah web
site [8]. The installation is straightforward and offers no options
for customization. This means that every computer on which
Xgrid is installed has the potential to be a client, controller, or
agent.
3.3.2 Agent and Controller Configuration
The agents and controllers can be configured through the Xgrid
Preference Pane in the System Preferences or XML files in
/Library/Preferences. Here the GridServer and GridAgent
processes are started, passwords set, and the controller discovery
method used by agents is selected. By default, agents use
Rendezvous to find a controller, although the agents can also be
configured to look for a specific host.
The Xgrid Preference Pane also sets whether the Agents will
always accept jobs, or only accept jobs when idle. In Xgrid terms,
idle either means that the Xgrid screen saver has activated, or the
mouse and keyboard have not been used for more than 15
minutes. Even if the agent is configured to always accept tasks, if
the computer is being used these tasks will run in the background
at a low priority.
However, if an agent only accepts jobs when idle, any unfinished
task being performed when the computer ceases being idle are
immediately stopped and any intermediary results lost. Then the
controller assigns the task to another available member of the
grid.
Advertising the controller via Rendezvous can be disabled by
editing /Library/Preferences/com.apple.xgrid.controller.plist. This,
however, will not prevent an agent from connecting to the
controller by hostname.
3.3.3 Sending Jobs from an Xgrid Client
The client sends jobs to the controller either through the Xgrid
GUI or the command line. The Xgrid GUI submits jobs via small
applications called plug-ins. Sample plug-ins are provided by
Apple, but they are only useful as simple testing or as examples of
how to create a custom plug-in. If we are to employ Xgrid for
useful work, we will require a custom plug-in.
James Reynolds details the creation of custom plug-ins on the
University of Utah Mac OS web site [8]. Xgrid stores plug-ins in
/Library/Xgrid/Plug-ins or ~/Library/Xgrid/Plug-ins, depending
on whether the plug-in was installed with Xgrid or created by a
user.
The core plug-in parameter is the command, which includes the
executable the agents will run. Another important parameter is the
working directory. This directory contains necessary files that
are not installed on the agents or available to them over a network.
The working directory will always be copied to each agent, so it is
best to keep this directory small. If the files are installed on the
agents or available over a network, the working directory
parameter is not needed.
The command line allows the options available with the GUI
plug-in, but it can be slightly more cumbersome. However, the
command line probably will be the method of choice for serious
work. The command arguments must be included in a script
unless they are very basic. This can be a shell, perl, or python
script, as long as the agent can interpret it.
3.3.4 Running the Xgrid Job
When the Xgrid job is started, the command tells the controller
how to break the job into tasks for the agents. Then the command
is tarred and gzipped and sent to each agent; if there is a working
directory, this is also tarred and gzipped and sent to the agents.
121
The agents extract these files into /tmp and run the task. Recall
that since the GridAgent process runs as the user nobody,
everything associated with the command must be available to
nobody.
Executables called by the command should be installed on the
agents unless they are very simple. If the executable depends on
libraries or other files, it may not function properly if transferred,
even if the dependent files are referenced in the working directory.
When the task is complete, the results are available to the client.
In principle, the results are sent to the client, but whether this
actually happens depends on the command. If the results are not
sent to the client, they will be in /tmp on each agent. When
available, a better solution is to direct the results to a network
volume accessible to the client.
3.4 Limitations and Idiosyncrasies
Since Xgrid is only in its second preview release, there are some
rough edges and limitations. Apple acknowledges some
limitations [7]. For example, the controller cannot determine
whether an agent is trustworthy and the controller always copies
the command and working directory to the agent without checking
to see if these exist on the agent.
Other limitations are likely just a by-product of an unfinished
work. Neither the client nor controller can specify which agents
will receive the tasks, which is particularly important if the agents
contain a variety of processor types and speeds and the user wants
to optimize the calculations. At this time, the best solution to this
problem may be to divide the computers into multiple logical
grids. There is also no standard way to monitor the progress of a
running job on each agent. The Xgrid GUI and command line
indicate which agents are working on tasks, but gives no
indication of progress.
Finally, at this time only Mac OS X clients can submit jobs to the
grid. The framework exists to allow third parties to write plug-ins
for other Unix flavors, but Apple has not created them.
4. XGRID IMPLEMENTATION
Our goal is an Xgrid render farm for Alias Maya. The Ringling
School has about 400 Apple Power Mac G4"s and G5"s in 13
computer labs. The computers range from 733 MHz 
singleprocessor G4"s and 500 MHz and 1 GHz dual-processor G4"s to
1.8 GHz dual-processor G5"s. All of these computers are lightly
used in the evening and on weekends and represent an enormous
processing resource for our student rendering projects.
4.1 Software Installation
During our Xgrid testing, we loaded software on each computer
multiple times, including the operating systems. We saved time by
facilitating our installations with the remote administration
daemon (radmind) software developed at the University of
Michigan [9], [10].
Everything we installed for testing was first created as a radmind
base load or overload. Thus, Mac OS X, Mac OS X Developer
Tools, Xgrid, POV-Ray [11], and Alias Maya were stored on a
radmind server and then installed on our test computers when
needed.
4.2 Initial Testing
We used six 1.8 GHz dual-processor Apple Power Mac G5"s for
our Xgrid tests. Each computer ran Mac OS X 10.3.3 and
contained 1 GB RAM. As shown in Figure 1, one computer
served as both client and controller, while the other five acted as
agents.
Before attempting Maya rendering with Xgrid, we performed
basic calculations to cement our understanding of Xgrid. Apple"s
Xgrid documentation is sparse, so finding helpful web sites
facilitated our learning.
We first ran the Mandelbrot set plug-in provided by Apple, which
allowed us to test the basic functionality of our grid. Then we
performed benchmark rendering with the Open Source
Application POV-Ray, as described by Daniel Côté [12] and
James Reynolds [8]. Our results showed that one dual-processor
G5 rendering the benchmark POV-Ray image took 104 minutes.
Breaking the image into three equal parts and using Xgrid to send
the parts to three agents required 47 minutes. However, two
agents finished their rendering in 30 minutes, while the third
agent used 47 minutes; the entire render was only as fast as the
slowest agent.
These results gave us two important pieces of information. First,
the much longer rendering time for one of the tasks indicated that
we should be careful how we split jobs into tasks for the agents.
All portions of the rendering will not take equal amounts of time,
even if the pixel size is the same. Second, since POV-Ray cannot
take advantage of both processors in a G5, neither can an Xgrid
task running POV-Ray. Alias Maya does not have this limitation.
4.3 Rendering with Alias Maya 6
We first installed Alias Maya 6 for Mac OS X on the
client/controller and each agent. Maya 6 requires licenses for use
as a workstation application. However, if it is just used for
rendering from the command line or a script, no license is needed.
We thus created a minimal installation of Maya as a radmind
overload. The application was installed in a hidden directory
inside /Applications. This was done so that normal users of the
workstations would not find and attempt to run Maya, which
would fail because these installations are not licensed for such
use.
In addition, Maya requires the existence of a directory ending in
the path /maya. The directory must be readable and writable by
the Maya user. For a user running Maya on a Mac OS X
workstation, the path would usually be ~/Documents/maya.
Unless otherwise specified, this directory will be the default
location for Maya data and output files. If the directory does not
Figure 1. Xgrid test grid.
Client/
Controller
Agent 1
Agent 2
Agent 3
Agent 4
Agent 5
Network
Volume
Jobs
Data
Data
122
exist, Maya will try to create it, even if the user specifies that the
data and output files exist in other locations.
However, Xgrid runs as the user nobody, which does not have a
home directory. Maya is unable to create the needed directory,
and looks instead for /Alias/maya. This directory also does not
exist, and the user nobody has insufficient rights to create it. Our
solution was to manually create /Alias/maya and give the user
nobody read and write permissions.
We also created a network volume for storage of both the
rendering data and the resulting rendered frames. This avoided
sending the Maya files and associated textures to each agent as
part of a working directory. Such a solution worked well for us
because our computers are geographically close on a LAN; if
greater distance had separated the agents from the
client/controller, specifying a working directory may have been a
better solution.
Finally, we created a custom GUI plug-in for Xgrid. The plug-in
command calls a Perl script with three arguments. Two arguments
specify the beginning and end frames of the render and the third
argument the number of frames in each job (which we call the
cluster size). The script then calculates the total number of jobs
and parses them out to the agents. For example, if we begin at
frame 201 and end at frame 225, with 5 frames for each job, the
plug-in will create 5 jobs and send them out to the agents.
Once the jobs are sent to the agents, the script executes the
/usr/sbin/Render command on each agent with the parameters
appropriate for the particular job. The results are sent to the
network volume.
With the setup described, we were able to render with Alias Maya
6 on our test grid. Rendering speed was not important at this time;
our first goal was to implement the grid, and in that we succeeded.
4.3.1 Pseudo Code for Perl Script in Custom Xgrid
Plug-in
In this section we summarize in simplified pseudo code format the
Perl script used in our Xgrig plug-in.
agent_jobs{
• Read beginning frame, end frame, and cluster size of
render.
• Check whether the render can be divided into an integer
number of jobs based on the cluster size.
• If there are not an integer number of jobs, reduce the cluster
size of the last job and set its last frame to the end frame of
the render.
• Determine the start frame and end frame for each job.
• Execute the Render command.
}
4.4 Lessons Learned
Rendering with Maya from the Xgrid GUI was not trivial. The
lack of Xgrid documentation and the requirements of Maya
combined into a confusing picture, where it was difficult to decide
the true cause of the problems we encountered. Trial and error
was required to determine the best way to set up our grid.
The first hurdle was creating the directory /Alias/maya with read
and write permissions for the user nobody. The second hurdle was
learning that we got the best performance by storing the rendering
data on a network volume.
The last major hurdle was retrieving our results from the agents.
Unlike the POV-Ray rendering tests, our initial Maya results were
never returned to the client; instead, Maya stored the results in
/tmp on each agent. Specifying in the plug-in where to send the
results would not change this behavior. We decided this was
likely a Maya issue rather than an Xgrid issue, and the solution
was to send the results to the network volume via the Perl script.
5. FUTURE PLANS
Maya on Xgrid is not yet ready to be used by the students of
Ringling School. In order to do this, we must address at least the
following concerns.
• Continue our rendering tests through the command line
rather than the GUI plug-in. This will be essential for the
following step.
• Develop an appropriate interface for users to send jobs to the
Xgrid controller. This will probably be an extension to the
web interface of our existing render farm, where the student
specifies parameters that are placed in a script that issues the
Render command.
• Perform timed Maya rendering tests with Xgrid. Part of this
should compare the rendering times for Power Mac G4"s and
G5"s.
6. CONCLUSION
Grid computing continues to advance. Recently, the IT industry
has witnessed the emergence of numerous types of contemporary
grid applications in addition to the traditional grid framework for
compute intensive applications. For instance, peer-to-peer
applications such as Kazaa, are based on storage grids that do not
share processing power but instead an elegant protocol to swap
files between systems. Although in our campuses we discourage
students from utilizing peer-to-peer applications from music
sharing, the same protocol can be utilized on applications such as
decision support and data mining. The National Virtual
Collaboratory grid project [13] will link earthquake researchers
across the U.S. with computing resources, allowing them to share
extremely large data sets, research equipment, and work together
as virtual teams over the Internet.
There is an assortment of new grid players in the IT world
expanding the grid computing model and advancing the grid
technology to the next level. SAP [14] is piloting a project to
grid-enable SAP ERP applications, Dell [15] has partnered with
Platform Computing to consolidate computing resources and
provide grid-enabled systems for compute intensive applications,
Oracle has integrated support for grid computing in their 10g
release [16], United Devices [17] offers hosting service for 
gridon-demand, and Sun Microsystems continues their research and
development of Sun"s N1 Grid engine [18] which combines grid
and clustering platforms.
Simply, the grid computing is up and coming. The potential
benefits of grid computing are colossal in higher education
learning while the implementation costs are low. Today, it would
be difficult to identify an application with as high a return on
investment as grid computing in information technology divisions
in higher education institutions. It is a mistake to overlook this
technology with such a high payback.
123
7. ACKNOWLEDGMENTS
The authors would like to thank Scott Hanselman of the IT team
at the Ringling School of Art and Design for providing valuable
input in the planning of our Xgrid testing. We would also like to
thank the posters of the Xgrid Mailing List [13] for providing
insight into many areas of Xgrid.
8. REFERENCES
[1] Apple Academic Research,
http://www.apple.com/education/science/profiles/vatech/.
[2] SETI@home: Search for Extraterrestrial Intelligence at
home. http://setiathome.ssl.berkeley.edu/.
[3] Alias, http://www.alias.com/.
[4] Apple Computer, Xgrid, http://www.apple.com/acg/xgrid/.
[5] Xgrid Guide, http://www.apple.com/acg/xgrid/, 2004.
[6] Apple Mac OS X Features,
http://www.apple.com/macosx/features/rendezvous/.
[7] Xgrid Manual Page, 2004.
[8] James Reynolds, Xgrid Presentation, University of Utah,
http://www.macos.utah.edu:16080/xgrid/, 2004.
[9] Research Systems Unix Group, Radmind, University of
Michigan, http://rsug.itd.umich.edu/software/radmind.
[10]Using the Radmind Command Line Tools to Maintain
Multiple Mac OS X Machines,

http://rsug.itd.umich.edu/software/radmind/files/radmindtutorial-0.8.1.pdf.
[11]POV-Ray, http://www.povray.org/.
[12]Daniel Côté, Xgrid example: Parallel graphics rendering in
POVray, http://unu.novajo.ca/simple/, 2004.
[13]NEESgrid, http://www.neesgrid.org/.
[14]SAP, http://www.sap.com/.
[15]Platform Computing, http://platform.com/.
[16]Grid, http://www.oracle.com/technologies/grid/.
[17]United Devices, Inc., http://ud.com/.
[18]N1 Grid Engine 6, http://www.sun.com/
software/gridware/index.html/.
[19]Xgrig Users Mailing List,

http://www.lists.apple.com/mailman/listinfo/xgridusers/.
124
Congestion Games with Load-Dependent Failures:
Identical Resources
Michal Penn
Technion - IIT
Haifa, Israel
mpenn@ie.technion.ac.il
Maria Polukarov
Technion - IIT
Haifa, Israel
pmasha@tx.technion.ac.il
Moshe Tennenholtz
Technion - IIT
Haifa, Israel
moshet@ie.technion.ac.il
ABSTRACT
We define a new class of games, congestion games with 
loaddependent failures (CGLFs), which generalizes the well-known
class of congestion games, by incorporating the issue of 
resource failures into congestion games. In a CGLF, agents
share a common set of resources, where each resource has a
cost and a probability of failure. Each agent chooses a 
subset of the resources for the execution of his task, in order to
maximize his own utility. The utility of an agent is the 
difference between his benefit from successful task completion
and the sum of the costs over the resources he uses. CGLFs
possess two novel features. It is the first model to 
incorporate failures into congestion settings, which results in a
strict generalization of congestion games. In addition, it is
the first model to consider load-dependent failures in such
framework, where the failure probability of each resource
depends on the number of agents selecting this resource.
Although, as we show, CGLFs do not admit a potential
function, and in general do not have a pure strategy Nash
equilibrium, our main theorem proves the existence of a pure
strategy Nash equilibrium in every CGLF with identical 
resources and nondecreasing cost functions.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems; I.2.11 [Artificial Intelligence]: Distributed 
Artificial Intelligence -multiagent systems
General Terms
Theory, Economics
1. INTRODUCTION
We study the effects of resource failures in congestion 
settings. This study is motivated by a variety of situations
in multi-agent systems with unreliable components, such as
machines, computers etc. We define a model for congestion
games with load-dependent failures (CGLFs) which provides
simple and natural description of such situations. In this
model, we are given a finite set of identical resources (service
providers) where each element possesses a failure 
probability describing the probability of unsuccessful completion of
its assigned tasks as a (nondecreasing) function of its 
congestion. There is a fixed number of agents, each having
a task which can be carried out by any of the resources.
For reliability reasons, each agent may decide to assign his
task, simultaneously, to a number of resources. Thus, the
congestion on the resources is not known in advance, but
is strategy-dependent. Each resource is associated with a
cost, which is a (nonnegative) function of the congestion 
experienced by this resource. The objective of each agent is to
maximize his own utility, which is the difference between his
benefit from successful task completion and the sum of the
costs over the set of resources he uses. The benefits of the
agents from successful completion of their tasks are allowed
to vary across the agents.
The resource cost function describes the cost suffered by
an agent for selecting that resource, as a function of the
number of agents who have selected it. Thus, it is natural
to assume that these functions are nonnegative. In addition,
in many real-life applications of our model the resource cost
functions have a special structure. In particular, they can
monotonically increase or decrease with the number of the
users, depending on the context. The former case is 
motivated by situations where high congestion on a resource
causes longer delay in its assigned tasks execution and as
a result, the cost of utilizing this resource might be higher.
A typical example of such situation is as follows. Assume
we need to deliver an important package. Since there is no
guarantee that a courier will reach the destination in time,
we might send several couriers to deliver the same package.
The time required by each courier to deliver the package
increases with the congestion on his way. In addition, the
payment to a courier is proportional to the time he spends
in delivering the package. Thus, the payment to the courier
increases when the congestion increases. The latter case 
(decreasing cost functions) describes situations where a group
of agents using a particular resource have an opportunity to
share its cost among the group"s members, or, the cost of
210
using a resource decreases with the number of users, 
according to some marketing policy.
Our results
We show that CGLFs and, in particular, CGLFs with
nondecreasing cost functions, do not admit a 
potential function. Therefore, the CGLF model can not be
reduced to congestion games. Nevertheless, if the 
failure probabilities are constant (do not depend on the
congestion) then a potential function is guaranteed to
exist.
We show that CGLFs and, in particular, CGLFs with
decreasing cost functions, do not possess pure 
strategy Nash equilibria. However, as we show in our main
result, there exists a pure strategy Nash 
equilibrium in any CGLF with nondecreasing cost
functions.
Related work
Our model extends the well-known class of congestion games
[11]. In a congestion game, every agent has to choose from a
finite set of resources, where the utility (or cost) of an agent
from using a particular resource depends on the number of
agents using it, and his total utility (cost) is the sum of
the utilities (costs) obtained from the resources he uses. An
important property of these games is the existence of pure
strategy Nash equilibria. Monderer and Shapley [9] 
introduced the notions of potential function and potential game
and proved that the existence of a potential function implies
the existence of a pure strategy Nash equilibrium. They
observed that Rosenthal [11] proved his theorem on 
congestion games by constructing a potential function (hence,
every congestion game is a potential game). Moreover, they
showed that every finite potential game is isomorphic to a
congestion game; hence, the classes of finite potential games
and congestion games coincide.
Congestion games have been extensively studied and 
generalized. In particular, Leyton-Brown and Tennenholtz [5]
extended the class of congestion games to the class of 
localeffect games. In a local-effect game, each agent"s payoff is
effected not only by the number of agents who have chosen
the same resources as he has chosen, but also by the number
of agents who have chosen neighboring resources (in a given
graph structure). Monderer [8] dealt with another type of
generalization of congestion games, in which the resource
cost functions are player-specific (PS-congestion games). He
defined PS-congestion games of type q (q-congestion games),
where q is a positive number, and showed that every game
in strategic form is a q-congestion game for some q. 
Playerspecific resource cost functions were discussed for the first
time by Milchtaich [6]. He showed that simple and 
strategysymmetric PS-congestion games are not potential games,
but always possess a pure strategy Nash equilibrium. 
PScongestion games were generalized to weighted congestion
games [6] (or, ID-congestion games [7]), in which the 
resource cost functions are not only player-specific, but also
depend on the identity of the users of the resource. 
Ackermann et al. [1] showed that weighted congestion games
admit pure strategy Nash equilibria if the strategy space of
each player consists of the bases of a matroid on the set of
resources.
Much of the work on congestion games has been inspired
by the fact that every such game has a pure strategy Nash
equilibrium. In particular, Fabrikant et al. [3] studied
the computational complexity of finding pure strategy Nash
equilibria in congestion games. Intensive study has also
been devoted to quantify the inefficiency of equilibria in
congestion games. Koutsoupias and Papadimitriou [4] 
proposed the worst-case ratio of the social welfare achieved
by a Nash equilibrium and by a socially optimal strategy
profile (dubbed the price of anarchy) as a measure of the
performance degradation caused by lack of coordination.
Christodoulou and Koutsoupias [2] considered the price of
anarchy of pure equilibria in congestion games with linear
cost functions. Roughgarden and Tardos [12] used this 
approach to study the cost of selfish routing in networks with
a continuum of users.
However, the above settings do not take into 
consideration the possibility that resources may fail to execute their
assigned tasks. In the computer science context of 
congestion games, where the alternatives of concern are machines,
computers, communication lines etc., which are obviously
prone to failures, this issue should not be ignored.
Penn, Polukarov and Tennenholtz were the first to 
incorporate the issue of failures into congestion settings [10].
They introduced a class of congestion games with failures
(CGFs) and proved that these games, while not being 
isomorphic to congestion games, always possess Nash equilibria
in pure strategies. The CGF-model significantly differs from
ours. In a CGF, the authors considered the delay associated
with successful task completion, where the delay for an agent
is the minimum of the delays of his successful attempts and
the aim of each agent is to minimize his expected delay. In
contrast with the CGF-model, in our model we consider the
total cost of the utilized resources, where each agent wishes
to maximize the difference between his benefit from a 
successful task completion and the sum of his costs over the
resources he uses.
The above differences imply that CGFs and CGLFs 
possess different properties. In particular, if in our model the
resource failure probabilities were constant and known in 
advance, then a potential function would exist. This, however,
does not hold for CGFs; in CGFs, the failure probabilities
are constant but there is no potential function. 
Furthermore, the procedures proposed by the authors in [10] for
the construction of a pure strategy Nash equilibrium are
not valid in our model, even in the simple, agent-symmetric
case, where all agents have the same benefit from successful
completion of their tasks.
Our work provides the first model of congestion settings
with resource failures, which considers the sum of 
congestiondependent costs over utilized resources, and therefore, does
not extend the CGF-model, but rather generalizes the classic
model of congestion games. Moreover, it is the first model
to consider load-dependent failures in the above context.
211
Organization
The rest of the paper is organized as follows. In Section 2
we define our model. In Section 3 we present our results.
In 3.1 we show that CGLFs, in general, do not have pure
strategy Nash equilibria. In 3.2 we focus on CGLFs with
nondecreasing cost functions (nondecreasing CGLFs). We
show that these games do not admit a potential function.
However, in our main result we show the existence of pure
strategy Nash equilibria in nondecreasing CGLFs. Section
4 is devoted to a short discussion. Many of the proofs are
omitted from this conference version of the paper, and will
appear in the full version.
2. THE MODEL
The scenarios considered in this work consist of a finite set
of agents where each agent has a task that can be carried
out by any element of a set of identical resources (service
providers). The agents simultaneously choose a subset of
the resources in order to perform their tasks, and their aim
is to maximize their own expected payoff, as described in
the sequel.
Let N be a set of n agents (n ∈ N), and let M be a set
of m resources (m ∈ N). Agent i ∈ N chooses a 
strategy σi ∈ Σi which is a (potentially empty) subset of the
resources. That is, Σi is the power set of the set of 
resources: Σi = P(M). Given a subset S ⊆ N of the agents,
the set of strategy combinations of the members of S is
denoted by ΣS = ×i∈SΣi, and the set of strategy 
combinations of the complement subset of agents is denoted by
Σ−S (Σ−S = ΣN S = ×i∈N SΣi). The set of pure strategy
profiles of all the agents is denoted by Σ (Σ = ΣN ).
Each resource is associated with a cost, c(·), and a 
failure probability, f(·), each of which depends on the 
number of agents who use this resource. We assume that the
failure probabilities of the resources are independent. Let
σ = (σ1, . . . , σn) ∈ Σ be a pure strategy profile. The
(m-dimensional) congestion vector that corresponds to σ is
hσ
= (hσ
e )e∈M , where hσ
e =
˛
˛{i ∈ N : e ∈ σi}
˛
˛. The 
failure probability of a resource e is a monotone nondecreasing
function f : {1, . . . , n} → [0, 1) of the congestion 
experienced by e. The cost of utilizing resource e is a function
c : {1, . . . , n} → R+ of the congestion experienced by e.
The outcome for agent i ∈ N is denoted by xi ∈ {S, F},
where S and F, respectively, indicate whether the task 
execution succeeded or failed. We say that the execution of
agent"s i task succeeds if the task of agent i is successfully
completed by at least one of the resources chosen by him.
The benefit of agent i from his outcome xi is denoted by
Vi(xi), where Vi(S) = vi, a given (nonnegative) value, and
Vi(F) = 0.
The utility of agent i from strategy profile σ and his 
outcome xi, ui(σ, xi), is the difference between his benefit from
the outcome (Vi(xi)) and the sum of the costs of the 
resources he has used:
ui(σ, xi) = Vi(xi) −
X
e∈σi
c(hσ
e ) .
The expected utility of agent i from strategy profile σ, Ui(σ),
is, therefore:
Ui(σ) = 1 −
Y
e∈σi
f(hσ
e )
!
vi −
X
e∈σi
c(hσ
e ) ,
where 1 −
Q
e∈σi
f(hσ
e ) denotes the probability of successful
completion of agent i"s task. We use the convention thatQ
e∈∅ f(hσ
e ) = 1. Hence, if agent i chooses an empty set
σi = ∅ (does not assign his task to any resource), then his
expected utility, Ui(∅, σ−i), equals zero.
3. PURE STRATEGY NASH EQUILIBRIA
IN CGLFS
In this section we present our results on CGLFs. We 
investigate the property of the (non-)existence of pure strategy
Nash equilibria in these games. We show that this class of
games does not, in general, possess pure strategy equilibria.
Nevertheless, if the resource cost functions are 
nondecreasing then such equilibria are guaranteed to exist, despite the
non-existence of a potential function.
3.1 Decreasing Cost Functions
We start by showing that the class of CGLFs and, in 
particular, the subclass of CGLFs with decreasing cost 
functions, does not, in general, possess Nash equilibria in pure
strategies.
Consider a CGLF with two agents (N = {1, 2}) and two
resources (M = {e1, e2}). The cost function of each resource
is given by c(x) = 1
xx , where x ∈ {1, 2}, and the failure
probabilities are f(1) = 0.01 and f(2) = 0.26. The benefits
of the agents from successful task completion are v1 = 1.1
and v2 = 4. Below we present the payoff matrix of the game.
∅ {e1} {e2} {e1, e2}
∅ U1 = 0 U1 = 0 U1 = 0 U1 = 0
U2 = 0 U2 = 2.96 U2 = 2.96 U2 = 1.9996
{e1} U1 = 0.089 U1 = 0.564 U1 = 0.089 U1 = 0.564
U2 = 0 U2 = 2.71 U2 = 2.96 U2 = 2.7396
{e2} U1 = 0.089 U1 = 0.089 U1 = 0.564 U1 = 0.564
U2 = 0 U2 = 2.96 U2 = 2.71 U2 = 2.7396
{e1, e2} U1 = −0.90011 U1 = −0.15286 U1 = −0.15286 U1 = 0.52564
U2 = 0 U2 = 2.71 U2 = 2.71 U2 = 3.2296
Table 1: Example for non-existence of pure strategy Nash
equilibria in CGLFs.
It can be easily seen that for every pure strategy profile σ
in this game there exist an agent i and a strategy σi ∈ Σi
such that Ui(σ−i, σi) > Ui(σ). That is, every pure strategy
profile in this game is not in equilibrium.
However, if the cost functions in a given CGLF do not
decrease in the number of users, then, as we show in the
main result of this paper, a pure strategy Nash equilibrium
is guaranteed to exist.
212
3.2 Nondecreasing Cost Functions
This section focuses on the subclass of CGLFs with 
nondecreasing cost functions (henceforth, nondecreasing CGLFs).
We show that nondecreasing CGLFs do not, in general, 
admit a potential function. Therefore, these games are not
congestion games. Nevertheless, we prove that all such games
possess pure strategy Nash equilibria.
3.2.1 The (Non-)Existence of a Potential Function
Recall that Monderer and Shapley [9] introduced the 
notions of potential function and potential game, where 
potential game is defined to be a game that possesses a potential
function. A potential function is a real-valued function over
the set of pure strategy profiles, with the property that the
gain (or loss) of an agent shifting to another strategy while
the other agents" strategies are kept unchanged, equals to
the corresponding increment of the potential function. The
authors [9] showed that the classes of finite potential games
and congestion games coincide.
Here we show that the class of CGLFs and, in particular,
the subclass of nondecreasing CGLFs, does not admit a 
potential function, and therefore is not included in the class of
congestion games. However, for the special case of constant
failure probabilities, a potential function is guaranteed to
exist. To prove these statements we use the following 
characterization of potential games [9].
A path in Σ is a sequence τ = (σ0
→ σ1
→ · · · ) such
that for every k ≥ 1 there exists a unique agent, say agent
i, such that σk
= (σk−1
−i , σi) for some σi = σk−1
i in Σi. A
finite path τ = (σ0
→ σ1
→ · · · → σK
) is closed if σ0
= σK
.
It is a simple closed path if in addition σl
= σk
for every
0 ≤ l = k ≤ K − 1. The length of a simple closed path is
defined to be the number of distinct points in it; that is, the
length of τ = (σ0
→ σ1
→ · · · → σK
) is K.
Theorem 1. [9] Let G be a game in strategic form with
a vector U = (U1, . . . , Un) of utility functions. For a finite
path τ = (σ0
→ σ1
→ · · · → σK
), let U(τ) =
PK
k=1[Uik (σk
)−
Uik (σk−1
)], where ik is the unique deviator at step k. Then,
G is a potential game if and only if U(τ) = 0 for every 
simple closed path τ of length 4.
Load-Dependent Failures
Based on Theorem 1, we present the following 
counterexample that demonstrates the non-existence of a potential
function in CGLFs.
We consider the following agent-symmetric game G in
which two agents (N = {1, 2}) wish to assign a task to two
resources (M = {e1, e2}). The benefit from a successful task
completion of each agent equals v, and the failure 
probability function strictly increases with the congestion. Consider
the simple closed path of length 4 which is formed by
α = (∅, {e2}) , β = ({e1}, {e2}) ,
γ = ({e1}, {e1, e2}) , δ = (∅, {e1, e2}) :
{e2} {e1, e2}
∅ U1 = 0 U1 = 0
U2 = (1 − f(1)) v − c(1) U2 =
`
1 − f(1)2
´
v − 2c(1)
{e1} U1 = (1 − f(1)) v − c(1) U1 = (1 − f(2)) v − c(2)
U2 = (1 − f(1)) v − c(1) U2 = (1 − f(1)f(2)) v − c(1) − c(2)
Table 2: Example for non-existence of potentials in CGLFs.
Therefore,
U1(α) − U1(β) + U2(β) − U2(γ) + U1(γ) − U1(δ)
+U2(δ) − U2(α) = v (1 − f(1)) (f(1) − f(2)) = 0.
Thus, by Theorem 1, nondecreasing CGLFs do not 
admit potentials. As a result, they are not congestion games.
However, as presented in the next section, the special case
in which the failure probabilities are constant, always 
possesses a potential function.
Constant Failure Probabilities
We show below that CGLFs with constant failure 
probabilities always possess a potential function. This follows from
the fact that the expected benefit (revenue) of each agent in
this case does not depend on the choices of the other agents.
In addition, for each agent, the sum of the costs over his 
chosen subset of resources, equals the payoff of an agent 
choosing the same strategy in the corresponding congestion game.
Assume we are given a game G with constant failure 
probabilities. Let τ = (α → β → γ → δ → α) be an arbitrary
simple closed path of length 4. Let i and j denote the active
agents (deviators) in τ and z ∈ Σ−{i,j} be a fixed 
strategy profile of the other agents. Let α = (xi, xj, z), β =
(yi, xj, z), γ = (yi, yj, z), δ = (xi, yj, z), where xi, yi ∈ Σi
and xj, yj ∈ Σj. Then,
U(τ) = Ui(xi, xj, z) − Ui(yi, xj, z)
+Uj(yi, xj, z) − Uj(yi, yj, z)
+Ui(yi, yj, z) − Ui(xi, yj, z)
+Uj(xi, yj, z) − Uj(xi, xj, z)
=

1 − f|xi|

vi −
X
e∈xi
c(h
(xi,xj ,z)
e ) − . . .
−

1 − f|xj |

vj +
X
e∈xj
c(h
(xi,xj ,z)
e )
=
» 
1 − f|xi|

vi − . . . −

1 − f|xj |

vj

−
» X
e∈xi
c(h
(xi,xj ,z)
e ) − . . . −
X
e∈xj
c(h
(xi,xj ,z)
e )

.
Notice that
» 
1 − f|xi|

vi − . . . −

1 − f|xj |

vj

= 0, as
a sum of a telescope series. The remaining sum equals 0, by
applying Theorem 1 to congestion games, which are known
to possess a potential function. Thus, by Theorem 1, G is a
potential game.
213
We note that the above result holds also for the more
general settings with non-identical resources (having 
different failure probabilities and cost functions) and general cost
functions (not necessarily monotone and/or nonnegative).
3.2.2 The Existence of a Pure Strategy Nash 
Equilibrium
In the previous section, we have shown that CGLFs and,
in particular, nondecreasing CGLFs, do not admit a 
potential function, but this fact, in general, does not contradict
the existence of an equilibrium in pure strategies. In this
section, we present and prove the main result of this 
paper (Theorem 2) which shows the existence of pure strategy
Nash equilibria in nondecreasing CGLFs.
Theorem 2. Every nondecreasing CGLF possesses a Nash
equilibrium in pure strategies.
The proof of Theorem 2 is based on Lemmas 4, 7 and
8, which are presented in the sequel. We start with some
definitions and observations that are needed for their proofs.
In particular, we present the notions of A-, D- and S-stability
and show that a strategy profile is in equilibrium if and only
if it is A-, D- and S- stable. Furthermore, we prove the
existence of such a profile in any given nondecreasing CGLF.
Definition 3. For any strategy profile σ ∈ Σ and for any
agent i ∈ N, the operation of adding precisely one resource
to his strategy, σi, is called an A-move of i from σ. 
Similarly, the operation of dropping a single resource is called a
D-move, and the operation of switching one resource with
another is called an S-move.
Clearly, if agent i deviates from strategy σi to strategy σi
by applying a single A-, D- or S-move, then max {|σi σi|,
|σi σi|} = 1, and vice versa, if max {|σi σi|, |σi σi|} =
1 then σi is obtained from σi by applying exactly one such
move. For simplicity of exposition, for any pair of sets A
and B, let µ(A, B) = max {|A B|, |B A|}.
The following lemma implies that any strategy profile, in
which no agent wishes unilaterally to apply a single A-, 
Dor S-move, is a Nash equilibrium. More precisely, we show
that if there exists an agent who benefits from a unilateral
deviation from a given strategy profile, then there exists a
single A-, D- or S-move which is profitable for him as well.
Lemma 4. Given a nondecreasing CGLF, let σ ∈ Σ be a
strategy profile which is not in equilibrium, and let i ∈ N
such that ∃xi ∈ Σi for which Ui(σ−i, xi) > Ui(σ). Then,
there exists yi ∈ Σi such that Ui(σ−i, yi) > Ui(σ) and µ(yi, σi)
= 1.
Therefore, to prove the existence of a pure strategy Nash
equilibrium, it suffices to look for a strategy profile for which
no agent wishes to unilaterally apply an A-, D- or S-move.
Based on the above observation, we define A-, D- and 
Sstability as follows.
Definition 5. A strategy profile σ is said to be A-stable
(resp., D-stable, S-stable) if there are no agents with a
profitable A- (resp., D-, S-) move from σ. Similarly, we
define a strategy profile σ to be DS-stable if there are no
agents with a profitable D- or S-move from σ.
The set of all DS-stable strategy profiles is denoted by
Σ0
. Obviously, the profile (∅, . . . , ∅) is DS-stable, so Σ0
is not empty. Our goal is to find a DS-stable profile for
which no profitable A-move exists, implying this profile is
in equilibrium. To describe how we achieve this, we define
the notions of light (heavy) resources and (nearly-) even
strategy profiles, which play a central role in the proof of
our main result.
Definition 6. Given a strategy profile σ, resource e is
called σ-light if hσ
e ∈ arg mine∈M hσ
e and σ-heavy otherwise.
A strategy profile σ with no heavy resources will be termed
even. A strategy profile σ satisfying |hσ
e − hσ
e | ≤ 1 for all
e, e ∈ M will be termed nearly-even.
Obviously, every even strategy profile is nearly-even. In
addition, in a nearly-even strategy profile, all heavy resources
(if exist) have the same congestion. We also observe that the
profile (∅, . . . , ∅) is even (and DS-stable), so the subset of
even, DS-stable strategy profiles is not empty.
Based on the above observations, we define two types of
an A-move that are used in the sequel. Suppose σ ∈ Σ0
is a nearly-even DS-stable strategy profile. For each agent
i ∈ N, let ei ∈ arg mine∈M σi hσ
e . That is, ei is a 
lightest resource not chosen previously by i. Then, if there 
exists any profitable A-move for agent i, then the A-move
with ei is profitable for i as well. This is since if agent i
wishes to unilaterally add a resource, say a ∈ M σi, then
Ui (σ−i, (σi ∪ {a})) > Ui(σ). Hence,
1 −
Y
e∈σi
f(hσ
e )f(hσ
a + 1)
!
vi −
X
e∈σi
c(hσ
e ) − c(hσ
a + 1)
> 1 −
Y
e∈σi
f(hσ
e )
!
vi −
X
e∈σi
c(hσ
e )
⇒ vi
Y
e∈σi
f(hσ
e ) >
c(hσ
a + 1)
1 − f(hσ
a + 1)
≥
c(hσ
ei
+ 1)
1 − f(hσ
ei
+ 1)
⇒ Ui (σ−i, (σi ∪ {ei})) > Ui(σ) .
If no agent wishes to change his strategy in this 
manner, i.e. Ui(σ) ≥ Ui(σ−i, σi ∪{ei}) for all i ∈ N, then by the
above Ui(σ) ≥ Ui(σ−i, σi ∪{a}) for all i ∈ N and a ∈ M σi.
Hence, σ is A-stable and by Lemma 4, σ is a Nash 
equilibrium strategy profile. Otherwise, let N(σ) denote the subset
of all agents for which there exists ei such that a unilateral
addition of ei is profitable. Let a ∈ arg minei : i∈N(σ) hσ
ei
. Let
also i ∈ N(σ) be the agent for which ei = a. If a is σ-light,
then let σ = (σ−i, σi ∪ {a}). In this case we say that σ is
obtained from σ by a one-step addition of resource a, and a
is called an added resource. If a is σ-heavy then there exists
a σ-light resource b and an agent j such that a ∈ σj and
b /∈ σj. Then let σ =
`
σ−{i,j}, σi ∪ {a}, (σj {a}) ∪ {b}
´
.
In this case we say that σ is obtained from σ by a two-step
addition of resource b, and b is called an added resource.
We notice that, in both cases, the congestion of each 
resource in σ is the same as in σ, except for the added 
resource, for which its congestion in σ increased by 1. Thus,
since the added resource is σ-light and σ is nearly-even, σ
is nearly-even. Then, the following lemma implies the 
Sstability of σ .
214
Lemma 7. In a nondecreasing CGLF, every nearly-even
strategy profile is S-stable.
Coupled with Lemma 7, the following lemma shows that
if σ is a nearly-even and DS-stable strategy profile, and σ is
obtained from σ by a one- or two-step addition of resource
a, then the only potential cause for a non-DS-stability of σ
is the existence of an agent k ∈ N with σk = σk, who wishes
to drop the added resource a.
Lemma 8. Let σ be a nearly-even DS-stable strategy 
profile of a given nondecreasing CGLF, and let σ be obtained
from σ by a one- or two-step addition of resource a. Then,
there are no profitable D-moves for any agent i ∈ N with
σi = σi. For an agent i ∈ N with σi = σi, the only possible
profitable D-move (if exists) is to drop the added resource a.
We are now ready to prove our main result - Theorem
2. Let us briefly describe the idea behind the proof. By
Lemma 4, it suffices to prove the existence of a strategy
profile which is A-, D- and S-stable. We start with the set
of even and DS-stable strategy profiles which is obviously
not empty. In this set, we consider the subset of strategy
profiles with maximum congestion and maximum sum of the
agents" utilities. Assuming on the contrary that every 
DSstable profile admits a profitable A-move, we show the 
existence of a strategy profile x in the above subset, such that a
(one-step) addition of some resource a to x results in a 
DSstable strategy. Then by a finite series of one- or two-step
addition operations we obtain an even, DS-stable strategy
profile with strictly higher congestion on the resources, 
contradicting the choice of x. The full proof is presented below.
Proof of Theorem 2: Let Σ1
⊆ Σ0
be the subset of
all even, DS-stable strategy profiles. Observe that since
(∅, . . . , ∅) is an even, DS-stable strategy profile, then Σ1
is not empty, and minσ∈Σ0
˛
˛{e ∈ M : e is σ−heavy}
˛
˛ = 0.
Then, Σ1
could also be defined as
Σ1
= arg min
σ∈Σ0
˛
˛{e ∈ M : e is σ−heavy}
˛
˛ ,
with hσ
being the common congestion.
Now, let Σ2
⊆ Σ1
be the subset of Σ1
consisting of all
those profiles with maximum congestion on the resources.
That is,
Σ2
= arg max
σ∈Σ1
hσ
.
Let UN (σ) =
P
i∈N Ui(σ) denotes the group utility of the
agents, and let Σ3
⊆ Σ2
be the subset of all profiles in Σ2
with maximum group utility. That is,
Σ3
= arg max
σ∈Σ2
X
i∈N
Ui(σ) = arg max
σ∈Σ2
UN (σ) .
Consider first the simple case in which maxσ∈Σ1 hσ
= 0.
Obviously, in this case, Σ1
= Σ2
= Σ3
= {x = (∅, . . . , ∅)}.
We show below that by performing a finite series of 
(onestep) addition operations on x, we obtain an even, 
DSstable strategy profile y with higher congestion, that is with
hy
> hx
= 0, in contradiction to x ∈ Σ2
. Let z ∈ Σ0
be
a nearly-even (not necessarily even) DS-stable profile such
that mine∈M hz
e = 0, and note that the profile x satisfies
the above conditions. Let N(z) be the subset of agents for
which a profitable A-move exists, and let i ∈ N(z). 
Obviously, there exists a z-light resource a such that Ui(z−i, zi ∪
{a}) > Ui(z) (otherwise, arg mine∈M hz
e ⊆ zi, in 
contradiction to mine∈M hz
e = 0). Consider the strategy profile
z = (z−i, zi ∪ {a}) which is obtained from z by a (one-step)
addition of resource a by agent i. Since z is nearly-even and
a is z-light, we can easily see that z is nearly-even. Then,
Lemma 7 implies that z is S-stable. Since i is the only agent
using resource a in z , by Lemma 8, no profitable D-moves
are available. Thus, z is a DS-stable strategy profile. 
Therefore, since the number of resources is finite, there is a finite
series of one-step addition operations on x = (∅, . . . , ∅) that
leads to strategy profile y ∈ Σ1
with hy
= 1 > 0 = hx
, in
contradiction to x ∈ Σ2
.
We turn now to consider the other case where maxσ∈Σ1 hσ
≥ 1. In this case we select from Σ3
a strategy profile x,
as described below, and use it to contradict our contrary
assumption. Specifically, we show that there exists x ∈ Σ3
such that for all j ∈ N,
vjf(hx
)|xj |−1
≥
c(hx
+ 1)
1 − f(hx + 1)
. (1)
Let x be a strategy profile which is obtained from x by
a (one-step) addition of some resource a ∈ M by some
agent i ∈ N(x) (note that x is nearly-even). Then, (1)
is derived from and essentially equivalent to the inequality
Uj(x ) ≥ Uj(x−j, xj {a}), for all a ∈ xj. That is, after
performing an A-move with a by i, there is no profitable
D-move with a. Then, by Lemmas 7 and 8, x is DS-stable.
Following the same lines as above, we construct a procedure
that initializes at x and achieves a strategy profile y ∈ Σ1
with hy
> hx
, in contradiction to x ∈ Σ2
.
Now, let us confirm the existence of x ∈ Σ3
that 
satisfies (1). Let x ∈ Σ3
and let M(x) be the subset of all
resources for which there exists a profitable (one-step) 
addition. First, we show that (1) holds for all j ∈ N such that
xj ∩M(x) = ∅, that is, for all those agents with one of their
resources being desired by another agent.
Let a ∈ M(x), and let x be the strategy profile that is
obtained from x by the (one-step) addition of a by agent i.
Assume on the contrary that there is an agent j with a ∈ xj
such that
vjf(hx
)|xj |−1
<
c(hx
+ 1)
1 − f(hx + 1)
.
Let x = (x−j, xj {a}). Below we demonstrate that x
is a DS-stable strategy profile and, since x and x 
correspond to the same congestion vector, we conclude that x
lies in Σ2
. In addition, we show that UN (x ) > UN (x), 
contradicting the fact that x ∈ Σ3
.
To show that x ∈ Σ0
we note that x is an even strategy
profile, and thus no S-moves may be performed for x . In
addition, since hx
= hx
and x ∈ Σ0
, there are no profitable
D-moves for any agent k = i, j. It remains to show that
there are no profitable D-moves for agents i and j as well.
215
Since Ui(x ) > Ui(x), we get
vif(hx
)|xi|
>
c(hx
+ 1)
1 − f(hx + 1)
⇒ vif(hx
)|xi |−1
= vif(hx
)|xi|
>
c(hx
+ 1)
1 − f(hx + 1)
>
c(hx
)
1 − f(hx)
=
c(hx
)
1 − f(hx )
,
which implies Ui(x ) > Ui(x−i, xi {b}), for all b ∈ xi .
Thus, there are no profitable D-moves for agent i. By the
DS-stability of x, for agent j and for all b ∈ xj, we have
Uj(x) ≥ Uj(x−j, xj {b}) ⇒ vjf(hx
)|xj |−1
≥
c(hx
)
1 − f(hx)
.
Then,
vjf(hx
)|xj |−1
> vjf(hx
)|xj |
= vjf(hx
)|xj |−1
≥
c(hx
)
1 − f(hx)
=
c(hx
)
1 − f(hx )
⇒ Uj(x ) > Uj(x−j, xj {b}), for all b ∈ xi. Therefore, x
is DS-stable and lies in Σ2
.
To show that UN (x ), the group utility of x , satisfies
UN (x ) > UN (x), we note that hx
= hx
, and thus Uk(x ) =
Uk(x), for all k ∈ N {i, j}. Therefore, we have to show
that Ui(x ) + Uj(x ) > Ui(x) + Uj(x), or Ui(x ) − Ui(x) >
Uj(x) − Uj(x ). Observe that
Ui(x ) > Ui(x) ⇒ vif(hx
)|xi|
>
c(hx
+ 1)
1 − f(hx + 1)
and
Uj(x ) < Uj(x ) ⇒ vjf(hx
)|xj |−1
<
c(hx
+ 1)
1 − f(hx + 1)
,
which yields
vif(hx
)|xi|
> vjf(hx
)|xj |−1
.
Thus, Ui(x ) − Ui(x)
=

1 − f(hx
)|xi|+1

vi − (|xi| + 1) c(hx
)
−
h
1 − f(hx
)|xi|

vi − |xi|c(hx
)
i
= vif(hx
)|xi|
(1 − f(hx
)) − c(hx
)
> vjf(hx
)|xj |−1
(1 − f(hx
)) − c(hx
)
=

1 − f(hx
)|xj |

vj − |xj|c(hx
)
−
h
1 − f(hx
)|xj |−1

vj − (|xi| − 1) c(hx
)
i
= Uj(x) − Uj(x ) .
Therefore, x lies in Σ2
and satisfies UN (x ) > UN (x), in
contradiction to x ∈ Σ3
.
Hence, if x ∈ Σ3
then (1) holds for all j ∈ N such that
xj ∩M(x) = ∅. Now let us see that there exists x ∈ Σ3
such
that (1) holds for all the agents. For that, choose an agent
i ∈ arg mink∈N vif(hx
)|xk|
. If there exists a ∈ xi ∩ M(x)
then i satisfies (1), implying by the choice of agent i, that
the above obviously yields the correctness of (1) for any
agent k ∈ N. Otherwise, if no resource in xi lies in M(x),
then let a ∈ xi and a ∈ M(x). Since a ∈ xi, a /∈ xi,
and hx
a = hx
a , then there exists agent j such that a ∈ xj
and a /∈ xj. One can easily check that the strategy 
profile x =
`
x−{i,j}, (xi {a}) ∪ {a }, (xj {a }) ∪ {a}
´
lies
in Σ3
. Thus, x satisfies (1) for agent i, and therefore, for
any agent k ∈ N.
Now, let x ∈ Σ3
satisfy (1). We show below that by
performing a finite series of one- and two-step addition 
operations on x, we can achieve a strategy profile y that lies
in Σ1
, such that hy
> hx
, in contradiction to x ∈ Σ2
. Let
z ∈ Σ0
be a nearly-even (not necessarily even), DS-stable
strategy profile, such that
vi
Y
e∈zi {b}
f(hz
e) ≥
c(hz
b + 1)
1 − f(hz
b + 1)
, (2)
for all i ∈ N and for all z-light resource b ∈ zi. We note that
for profile x ∈ Σ3
⊆ Σ1
, with all resources being x-light,
conditions (2) and (1) are equivalent. Let z be obtained
from z by a one- or two-step addition of a z-light resource
a. Obviously, z is nearly-even. In addition, hz
e ≥ hz
e for
all e ∈ M, and mine∈M hz
e ≥ mine∈M hz
e. To complete the
proof we need to show that z is DS-stable, and, in addition,
that if mine∈M hz
e = mine∈M hz
e then z has property (2).
The DS-stability of z follows directly from Lemmas 7 and 8,
and from (2) with respect to z. It remains to prove property
(2) for z with mine∈M hz
e = mine∈M hz
e. Using (2) with
respect to z, for any agent k with zk = zk and for any 
zlight resource b ∈ zk, we get
vk
Y
e∈zk
{b}
f(hz
e ) ≥ vk
Y
e∈zk {b}
f(hz
e)
≥
c(hz
b + 1)
1 − f(hz
b + 1)
=
c(hz
b + 1)
1 − f(hz
b + 1)
,
as required. Now let us consider the rest of the agents.
Assume z is obtained by the one-step addition of a by agent
i. In this case, i is the only agent with zi = zi. The required
property for agent i follows directly from Ui(z ) > Ui(z). In
the case of a two-step addition, let z =
`
z−{i,j}, zi ∪ {b},
(zj {b}) ∪ {a}), where b is a z-heavy resource. For agent
i, from Ui(z−i, zi ∪ {b}) > Ui(z) we get
1 −
Y
e∈zi
f(hz
e)f(hz
b + 1)
!
vi −
X
e∈zi
c(hz
e) − c(hz
b + 1)
> 1 −
Y
e∈zi
f(hz
e)
!
vi −
X
e∈zi
c(hz
e)
⇒ vi
Y
e∈zi
f(hz
e) >
c(hz
b + 1)
1 − f(hz
b + 1)
, (3)
and note that since hz
b ≥ hz
e for all e ∈ M and, in 
particular, for all z -light resources, then
c(hz
b + 1)
1 − f(hz
b + 1)
≥
c(hz
e + 1)
1 − f(hz
e + 1)
, (4)
for any z -light resource e .
216
Now, since hz
e ≥ hz
e for all e ∈ M and b is z-heavy, then
vi
Y
e∈zi {e }
f(hz
e ) ≥ vi
Y
e∈zi {e }
f(hz
e)
= vi
Y
e∈(zi∪{b}) {e }
f(hz
e) ≥ vi
Y
e∈zi
f(hz
e) ,
for any z -light resource e . The above, coupled with (3)
and (4), yields the required. For agent j we just use (2)
with respect to z and the equality hz
b = hz
a . For any z -light
resource e ,
vj
Y
e∈zj {e }
f(hz
e ) ≥ vi
Y
e∈zi {e }
f(hz
e)
≥
c(hz
e + 1)
1 − f(hz
e + 1)
=
c(hz
e + 1)
1 − f(hz
e + 1)
.
Thus, since the number of resources is finite, there is a finite
series of one- and two-step addition operations on x that
leads to strategy profile y ∈ Σ1
with hy
> hx
, in 
contradiction to x ∈ Σ2
. This completes the proof.
4. DISCUSSION
In this paper, we introduce and investigate congestion 
settings with unreliable resources, in which the probability of a
resource"s failure depends on the congestion experienced by
this resource. We defined a class of congestion games with
load-dependent failures (CGLFs), which generalizes the 
wellknown class of congestion games. We study the existence of
pure strategy Nash equilibria and potential functions in the
presented class of games. We show that these games do not,
in general, possess pure strategy equilibria. Nevertheless,
if the resource cost functions are nondecreasing then such
equilibria are guaranteed to exist, despite the non-existence
of a potential function.
The CGLF-model can be modified to the case where the
agents pay only for non-faulty resources they selected. Both
the model discussed in this paper and the modified one are
reasonable. In the full version we will show that the 
modified model leads to similar results. In particular, we can
show the existence of a pure strategy equilibrium for 
nondecreasing CGLFs also in the modified model.
In future research we plan to consider various extensions
of CGLFs. In particular, we plan to consider CGLFs where
the resources may have different costs and failure 
probabilities, as well as CGLFs in which the resource failure 
probabilities are mutually dependent. In addition, it is of 
interest to develop an efficient algorithm for the computation of
pure strategy Nash equilibrium, as well as discuss the social
(in)efficiency of the equilibria.
5. REFERENCES
[1] H. Ackermann, H. R¨oglin, and B. V¨ocking. Pure nash
equilibria in player-specific and weighted congestion
games. In WINE-06, 2006.
[2] G. Christodoulou and E. Koutsoupias. The price of
anarchy of finite congestion games. In Proceedings of
the 37th Annual ACM Symposium on Theory and
Computing (STOC-05), 2005.
[3] A. Fabrikant, C. Papadimitriou, and K. Talwar. The
complexity of pure nash equilibria. In STOC-04, pages
604-612, 2004.
[4] E. Koutsoupias and C. Papadimitriou. Worst-case
equilibria. In Proceedings of the 16th Annual
Symposium on Theoretical Aspects of Computer
Science, pages 404-413, 1999.
[5] K. Leyton-Brown and M. Tennenholtz. Local-effect
games. In IJCAI-03, 2003.
[6] I. Milchtaich. Congestion games with player-specific
payoff functions. Games and Economic Behavior,
13:111-124, 1996.
[7] D. Monderer. Solution-based congestion games.
Advances in Mathematical Economics, 8:397-407,
2006.
[8] D. Monderer. Multipotential games. In IJCAI-07,
2007.
[9] D. Monderer and L. Shapley. Potential games. Games
and Economic Behavior, 14:124-143, 1996.
[10] M. Penn, M. Polukarov, and M. Tennenholtz.
Congestion games with failures. In Proceedings of the
6th ACM Conference on Electronic Commerce
(EC-05), pages 259-268, 2005.
[11] R. Rosenthal. A class of games possessing
pure-strategy nash equilibria. International Journal of
Game Theory, 2:65-67, 1973.
[12] T. Roughgarden and E. Tardos. How bad is selfish
routing. Journal of the ACM, 49(2):236-259, 2002.
217
Assured Service Quality by Improved Fault Management
Service-Oriented Event Correlation
Andreas Hanemann
Munich Network Management
Team
Leibniz Supercomputing
Center
Barer Str. 21, D-80333
Munich, Germany
hanemann@lrz.de
Martin Sailer
Munich Network Management
Team
University of Munich (LMU)
Oettingenstr. 67, D-80538
Munich, Germany
sailer@nm.ifi.lmu.de
David Schmitz
Munich Network Management
Team
Leibniz Supercomputing
Center
Barer Str. 21, D-80333
Munich, Germany
schmitz@lrz.de
ABSTRACT
The paradigm shift from device-oriented to service-oriented
management has also implications to the area of event 
correlation. Today"s event correlation mainly addresses the 
correlation of events as reported from management tools. 
However, a correlation of user trouble reports concerning services
should also be performed. This is necessary to improve the
resolution time and to reduce the effort for keeping the 
service agreements. We refer to such a type of correlation as
service-oriented event correlation. The necessity to use this
kind of event correlation is motivated in the paper.
To introduce service-oriented event correlation for an IT
service provider, an appropriate modeling of the correlation
workflow and of the information is necessary. Therefore, we
examine the process management frameworks IT 
Infrastructure Library (ITIL) and enhanced Telecom Operations Map
(eTOM) for their contribution to the workflow modeling in
this area. The different kinds of dependencies that we find
in our general scenario are then used to develop a 
workflow for the service-oriented event correlation. The MNM
Service Model, which is a generic model for IT service 
management proposed by the Munich Network Management
(MNM) Team, is used to derive an appropriate information
modeling. An example scenario, the Web Hosting Service
of the Leibniz Supercomputing Center (LRZ), is used to
demonstrate the application of service-oriented event 
correlation.
Categories and Subject Descriptors
C.2.4 [Computer Systems Organization]: 
ComputerCommunication Networks-Distributed Applications
General Terms
Management, Performance, Reliability
1. INTRODUCTION
In huge networks a single fault can cause a burst of failure
events. To handle the flood of events and to find the root
cause of a fault, event correlation approaches like rule-based
reasoning, case-based reasoning or the codebook approach
have been developed. The main idea of correlation is to
condense and structure events to retrieve meaningful 
information. Until now, these approaches address primarily the
correlation of events as reported from management tools or
devices. Therefore, we call them device-oriented.
In this paper we define a service as a set of functions
which are offered by a provider to a customer at a customer
provider interface. The definition of a service is therefore
more general than the definition of a Web Service, but a
Web Service is included in this service definition. As
a consequence, the results are applicable for Web Services
as well as for other kinds of services. A service level 
agreement (SLA) is defined as a contract between customer and
provider about guaranteed service performance.
As in today"s IT environments the offering of such services
with an agreed service quality becomes more and more 
important, this change also affects the event correlation. It
has become a necessity for providers to offer such 
guarantees for a differentiation from other providers. To avoid SLA
violations it is especially important for service providers to
identify the root cause of a fault in a very short time or even
act proactively. The latter refers to the case of recognizing
the influence of a device breakdown on the offered services.
As in this scenario the knowledge about services and their
SLAs is used we call it service-oriented. It can be addressed
from two directions.
Top-down perspective: Several customers report a 
problem in a certain time interval. Are these trouble 
reports correlated? How to identify a resource as being
the problem"s root cause?
183
Bottom-up perspective: A device (e.g., router, server)
breaks down. Which services, and especially which
customers, are affected by this fault?
The rest of the paper is organized as follows. In Section
2 we describe how event correlation is performed today and
present a selection of the state-of-the-art event correlation
techniques. Section 3 describes the motivation for 
serviceoriented event correlation and its benefits. After having
motivated the need for such type of correlation we use two
well-known IT service management models to gain 
requirements for an appropriate workflow modeling and present
our proposal for it (see Section 4). In Section 5 we present
our information modeling which is derived from the MNM
Service Model. An application of the approach for a web
hosting scenario is performed in Section 6. The last section
concludes the paper and presents future work.
2. TODAY"S EVENT CORRELATION
TECHNIQUES
In [11] the task of event correlation is defined as a 
conceptual interpretation procedure in the sense that a new 
meaning is assigned to a set of events that happen in a certain
time interval. We can distinguish between three aspects
for event correlation.
Functional aspect: The correlation focuses on functions
which are provided by each network element. It is also
regarded which other functions are used to provide a
specific function.
Topology aspect: The correlation takes into account how
the network elements are connected to each other and
how they interact.
Time aspect: When explicitly regarding time constraints,
a start and end time has to be defined for each event.
The correlation can use time relationships between the
events to perform the correlation. This aspect is only
mentioned in some papers [11], but it has to be treated
in an event correlation system.
In the event correlation it is also important to distinguish
between the knowledge acquisition/representation and the
correlation algorithm. Examples of approaches to 
knowledge acquisition/representation are Gruschke"s dependency
graphs [6] and Ensel"s dependency detection by neural 
networks [3]. It is also possible to find the dependencies by
analyzing interactions [7]. In addition, there is an approach
to manage service dependencies with XML and to define a
resource description framework [4].
To get an overview about device-oriented event correlation
a selection of several event correlation techniques being used
for this kind of correlation is presented.
Model-based reasoning: Model-based reasoning (MBR)
[15, 10, 20] represents a system by modeling each of its
components. A model can either represent a physical
entity or a logical entity (e.g., LAN, WAN, domain,
service, business process). The models for physical
entities are called functional model, while the models
for all logical entities are called logical model. A 
description of each model contains three categories of 
information: attributes, relations to other models, and
behavior. The event correlation is a result of the 
collaboration among models.
As all components of a network are represented with
their behavior in the model, it is possible to perform
simulations to predict how the whole network will 
behave.
A comparison in [20] showed that a large MBR system
is not in all cases easy to maintain. It can be difficult to
appropriately model the behavior for all components
and their interactions correctly and completely.
An example system for MBR is NetExpert[16] from
OSI which is a hybrid MBR/RBR system (in 2000 OSI
was acquired by Agilent Technologies).
Rule-based reasoning: Rule-based reasoning (RBR) [15,
10] uses a set of rules for event correlation. The rules
have the form conclusion if condition. The condition
uses received events and information about the system,
while the conclusion contains actions which can either
lead to system changes or use system parameters to
choose the next rule.
An advantage of the approach is that the rules are
more or less human-readable and therefore their effect
is intuitive. The correlation has proved to be very fast
in practice by using the RETE algorithm.
In the literature [20, 1] it is claimed that RBR 
systems are classified as relatively inflexible. Frequent
changes in the modeled IT environment would lead to
many rule updates. These changes would have to be
performed by experts as no automation has currently
been established. In some systems information about
the network topology which is needed for the event 
correlation is not used explicitly, but is encoded into the
rules. This intransparent usage would make rule 
updates for topology changes quite difficult. The system
brittleness would also be a problem for RBR systems.
It means that the system fails if an unknown case 
occurs, because the case cannot be mapped onto similar
cases. The output of RBR systems would also be 
difficult to predict, because of unforeseen rule interactions
in a large rule set. According to [15] an RBR system
is only appropriate if the domain for which it is used
is small, nonchanging, and well understood.
The GTE IMPACT system [11] is an example of a 
rulebased system. It also uses MBR (GTE has merged
with Bell Atlantic in 1998 and is now called Verizon
[19]).
Codebook approach: The codebook approach [12, 21] has
similarities to RBR, but takes a further step and 
encodes the rules into a correlation matrix.
The approach starts using a dependency graph with
two kinds of nodes for the modeling. The first kind
of nodes are the faults (denoted as problems in the
cited papers) which have to be detected, while the 
second kind of nodes are observable events (symptoms in
the papers) which are caused by the faults or other
events. The dependencies between the nodes are 
denoted as directed edges. It is possible to choose weights
for the edges, e.g., a weight for the probability that
184
fault/event A causes event B. Another possible 
weighting could indicate time dependencies. There are 
several possibilities to reduce the initial graph. If, e.g.,
a cyclic dependency of events exists and there are no
probabilities for the cycles" edges, all events can be
treated as one event.
After a final input graph is chosen, the graph is 
transformed into a correlation matrix where the columns
contain the faults and the rows contain the events.
If there is a dependency in the graph, the weight of
the corresponding edge is put into the according 
matrix cell. In case no weights are used, the matrix cells
get the values 1 for dependency and 0 otherwise. 
Afterwards, a simplification can be done, where events
which do not help to discriminate faults are deleted.
There is a trade-off between the minimization of the
matrix and the robustness of the results. If the matrix
is minimized as much as possible, some faults can only
be distinguished by a single event. If this event cannot
be reliably detected, the event correlation system 
cannot discriminate between the two faults. A measure
how many event observation errors can be 
compensated by the system is the Hamming distance. The
number of rows (events) that can be deleted from the
matrix can differ very much depending on the 
relationships [15].
The codebook approach has the advantage that it uses
long-term experience with graphs and coding. This
experience is used to minimize the dependency graph
and to select an optimal group of events with respect
to processing time and robustness against noise.
A disadvantage of the approach could be that similar
to RBR frequent changes in the environment make it
necessary to frequently edit the input graph.
SMARTS InCharge [12, 17] is an example of such a
correlation system.
Case-based reasoning: In contrast to other approaches
case-based reasoning (CBR) [14, 15] systems do not
use any knowledge about the system structure. The
knowledge base saves cases with their values for system
parameters and successful recovery actions for these
cases. The recovery actions are not performed by the
CBR system in the first place, but in most cases by a
human operator.
If a new case appears, the CBR system compares the
current system parameters with the system 
parameters in prior cases and tries to find a similar one. To
identify such a match it has to be defined for which 
parameters the cases can differ or have to be the same.
If a match is found, a learned action can be performed
automatically or the operator can be informed with a
recovery proposal.
An advantage of this approach is that the ability to
learn is an integral part of it which is important for
rapid changing environments.
There are also difficulties when applying the approach
[15]. The fields which are used to find a similar case
and their importance have to be defined appropriately.
If there is a match with a similar case, an adaptation
of the previous solution to the current one has to be
found.
An example system for CBR is SpectroRx from 
Cabletron Systems. The part of Cabletron that developed
SpectroRx became an independent software company
in 2002 and is now called Aprisma Management 
Technologies [2].
In this section four event correlation approaches were 
presented which have evolved into commercial event correlation
systems. The correlation approaches have different focuses.
MBR mainly deals with the knowledge acquisition and 
representation, while RBR and the codebook approach 
propose a correlation algorithm. The focus of CBR is its ability
to learn from prior cases.
3. MOTIVATION OF SERVICE-ORIENTED
EVENT CORRELATION
Fig. 1 shows a general service scenario upon which we
will discuss the importance of a service-oriented correlation.
Several services like SSH, a web hosting service, or a video
conference service are offered by a provider to its customers
at the customer provider interface. A customer can allow
several users to use a subscribed service. The quality and
cost issues of the subscribed services between a customer
and a provider are agreed in SLAs. On the provider side
the services use subservices for their provisioning. In case
of the services mentioned above such subservices are DNS,
proxy service, and IP service. Both services and subservices
depend on resources upon which they are provisioned. As
displayed in the figure a service can depend on more than
one resource and a resource can be used by one or more
services.
SSH
DNS
proxy
IP
service dependency resource dependency
user a
user b
user c
customer SLA
web a
video conf.
SSH sun1
provider
video conf.
web
services
subservices
resources
Figure 1: Scenario
To get a common understanding, we distinguish between
different types of events:
Resource event: We use the term resource event for 
network events and system events. A network event refers
to events like node up/down or link up/down whereas
system events refer to events like server down or 
authentication failure.
Service event: A service event indicates that a service
does not work properly. A trouble ticket which is 
generated from a customer report is a kind of such an
185
event. Other service events can be generated by the
provider of a service, if the provider himself detects a
service malfunction.
In such a scenario the provider may receive service events
from customers which indicate that SSH, web hosting 
service, and video conference service are not available. When
referring to the service hierarchy, the provider can conclude
in such a case that all services depend on DNS. Therefore,
it seems more likely that a common resource which is 
necessary for this service does not work properly or is not 
available than to assume three independent service failures. In
contrast to a resource-oriented perspective where all of the
service events would have to be processed separately, the 
service events can be linked together. Their information can
be aggregated and processed only once. If, e.g., the problem
is solved, one common message to the customers that their
services are available again is generated and distributed by
using the list of linked service events. This is certainly a 
simplified example. However, it shows the general principle of
identifying the common subservices and common resources
of different services.
It is important to note that the service-oriented 
perspective is needed to integrate service aspects, especially QoS 
aspects. An example of such an aspect is that a fault does not
lead to a total failure of a service, but its QoS parameters,
respectively agreed service levels, at the customer-provider
interface might not be met. A degradation in service 
quality which is caused by high traffic load on the backbone
is another example. In the resource-oriented perspective it
would be possible to define events which indicate that there
is a link usage higher than a threshold, but no mechanism
has currently been established to find out which services are
affected and whether a QoS violation occurs.
To summarize, the reasons for the necessity of a 
serviceoriented event correlation are the following:
Keeping of SLAs (top-down perspective): The time
interval between the first symptom (recognized either
by provider, network management tools, or customers)
that a service does not perform properly and the 
verified fault repair needs to be minimized. This is 
especially needed with respect to SLAs as such agreements
often contain guarantees like a mean time to repair.
Effort reduction (top-down perspective): If several
user trouble reports are symptoms of the same fault,
fault processing should be performed only once and
not several times. If the fault has been repaired, the
affected customers should be informed about this 
automatically.
Impact analysis (bottom-up perspective): In case of
a fault in a resource, its influence on the associated 
services and affected customers can be determined. This
analysis can be performed for short term (when there
is currently a resource failure) or long term (e.g., 
network optimization) considerations.
4. WORKFLOW MODELING
In the following we examine the established IT process
management frameworks IT Infrastructure Library (ITIL)
and enhanced Telecom Operations Map (eTOM). The aim is
find out where event correlation can be found in the process
structure and how detailed the frameworks currently are.
After that we present our solution for a workflow modeling
for the service-oriented event correlation.
4.1 IT Infrastructure Library (ITIL)
The British Office of Government Commerce (OGC) and
the IT Service Management Forum (itSMF) [9] provide a
collection of best practices for IT processes in the area of
IT service management which is called ITIL. The service
management is described by 11 modules which are grouped
into Service Support Set (provider internal processes) and
Service Delivery Set (processes at the customer-provider 
interface). Each module describes processes, functions, roles,
and responsibilities as well as necessary databases and 
interfaces. In general, ITIL describes contents, processes, and
aims at a high abstraction level and contains no information
about management architectures and tools.
The fault management is divided into Incident 
Management process and Problem Management process.
Incident Management: The Incident Management 
contains the service desk as interface to customers (e.g.,
receives reports about service problems). In case of
severe errors structured queries are transferred to the
Problem Management.
Problem Management: The Problem Management"s
tasks are to solve problems, take care of keeping 
priorities, minimize the reoccurrence of problems, and to
provide management information. After receiving 
requests from the Incident Management, the problem
has to be identified and information about necessary
countermeasures is transferred to the Change 
Management.
The ITIL processes describe only what has to be done, but
contain no information how this can be actually performed.
As a consequence, event correlation is not part of the 
modeling. The ITIL incidents could be regarded as input for the
service-oriented event correlation, while the output could be
used as a query to the ITIL Problem Management.
4.2 Enhanced Telecom Operations Map
(eTOM)
The TeleManagement Forum (TMF) [18] is an 
international non-profit organization from service providers and
suppliers in the area of telecommunications services. Similar
to ITIL a process-oriented framework has been developed at
first, but the framework was designed for a narrower focus,
i.e., the market of information and communications service
providers. A horizontal grouping into processes for 
customer care, service development & operations, network & 
systems management, and partner/supplier is performed. The
vertical grouping (fulfillment, assurance, billing) reflects the
service life cycle.
In the area of fault management three processes have been
defined along the horizontal process grouping.
Problem Handling: The purpose of this process is to 
receive trouble reports from customers and to solve them
by using the Service Problem Management. The aim
is also to keep the customer informed about the 
current status of the trouble report processing as well as
about the general network status (e.g., planned 
maintenance). It is also a task of this process to inform the
186
QoS/SLA management about the impact of current
errors on the SLAs.
Service Problem Management: In this process reports
about customer-affecting service failures are received
and transformed. Their root causes are identified and
a problem solution or a temporary workaround is 
established. The task of the Diagnose Problem 
subprocess is to find the root cause of the problem by
performing appropriate tests. Nothing is said how this
can be done (e.g., no event correlation is mentioned).
Resource Trouble Management: A subprocess of the
Resource Trouble Management is responsible for 
resource failure event analysis, alarm correlation & 
filtering, and failure event detection & reporting. 
Another subprocess is used to execute different tests to
find a resource failure. There is also another 
subprocess which keeps track about the status of the trouble
report processing. This subprocess is similar to the
functionality of a trouble ticket system.
The process description in eTOM is not very detailed. It
is useful to have a check list which aspects for these processes
have to be taken into account, but there is no detailed 
modeling of the relationships and no methodology for applying
the framework. Event correlation is only mentioned in the
resource management, but it is not used in the service level.
4.3 Workflow Modeling for the 
Service-Oriented Event Correlation
Fig. 2 shows a general service scenario which we will use
as basis for the workflow modeling for the service-oriented
event correlation. We assume that the dependencies are
already known (e.g., by using the approaches mentioned
in Section 2). The provider offers different services which
depend on other services called subservices (service 
dependency). Another kind of dependency exists between 
services/subservices and resources. These dependencies are
called resource dependencies. These two kinds of 
dependencies are in most cases not used for the event correlation
performed today. This resource-oriented event correlation
deals only with relationships on the resource level (e.g., 
network topology).
service dependency
resources
subservices
provider
services
resource dependency
Figure 2: Different kinds of dependencies for the
service-oriented event correlation
The dependencies depicted in Figure 2 reflect a situation
with no redundancy in the service provisioning. The 
relationships can be seen as AND relationships. In case of 
redundancy, if e.g., a provider has 3 independent web servers,
another modeling (see Figure 3) should be used (OR 
relationship). In such a case different relationships are possible.
The service could be seen as working properly if one of the
servers is working or a certain percentage of them is working.
services
) AND relationship b) OR relationship
resources
Figure 3: Modeling of no redundancy (a) and of
redundancy (b)
As both ITIL and eTOM contain no description how event
correlation and especially service-oriented event correlation
should actually be performed, we propose the following 
design for such a workflow (see Fig. 4). The additional 
components which are not part of a device-oriented event 
correlation are depicted with a gray background. The workflow is
divided into the phases fault detection, fault diagnosis, and
fault recovery.
In the fault detection phase resource events and service
events can be generated from different sources. The 
resource events are issued during the use of a resource, e.g.,
via SNMP traps. The service events are originated from 
customer trouble reports, which are reported via the Customer
Service Management (see below) access point. In addition
to these two passive ways to get the events, a provider
can also perform active tests. These tests can either deal
with the resources (resource active probing) or can assume
the role of a virtual customer and test a service or one of its
subservices by performing interactions at the service access
points (service active probing).
The fault diagnosis phase is composed of three event 
correlation steps. The first one is performed by the resource
event correlator which can be regarded as the event 
correlator in today"s commercial systems. Therefore, it deals only
with resource events. The service event correlator does a
correlation of the service events, while the aggregate event
correlator finally performs a correlation of both resource and
service events. If the correlation result in one of the 
correlation steps shall be improved, it is possible to go back to
the fault detection phase and start the active probing to get
additional events. These events can be helpful to confirm a
correlation result or to reduce the list of possible root causes.
After the event correlation an ordered list of possible root
causes is checked by the resource management. When the
root cause is found, the failure repair starts. This last step
is performed in the fault recovery phase.
The next subsections present different elements of the
event correlation process.
4.4 Customer Service Management and 
Intelligent Assistant
The Customer Service Management (CSM) access point
was proposed by [13] as a single interface between customer
187
fault
detection
fault
diagnosis
resource
active
probing
resource event
resource
event
correlator
resource
management
candidate
list
fault
recovery
resource
usage
service
active
probing
intelligent
assistant
service
event
correlator
aggregate
event
correlator
service eventCSM AP
Figure 4: Event correlation workflow
and provider. Its functionality is to provide information
to the customer about his subscribed services, e.g., reports
about the fulfillment of agreed SLAs. It can also be used to
subscribe services or to allow the customer to manage his
services in a restricted way. Reports about problems with a
service can be sent to the customer via CSM. The CSM is
also contained in the MNM Service Model (see Section 5).
To reduce the effort for the provider"s first level support,
an Intelligent Assistant can be added to the CSM. The 
Intelligent Assistant structures the customer"s information about
a service problem. The information which is needed for a
preclassification of the problem is gathered from a list of
questions to the customer. The list is not static as the 
current question depends on the answers to prior questions or
from the result of specific tests. A decision tree is used
to structure the questions and tests. The tests allow the
customer to gain a controlled access to the provider"s 
management. At the LRZ a customer of the E-Mail Service can,
e.g., use the Intelligent Assistant to perform ping requests
to the mail server. But also more complex requests could be
possible, e.g., requests of a combination of SNMP variables.
4.5 Active Probing
Active probing is useful for the provider to check his 
offered services. The aim is to identify and react to problems
before a customer notices them. The probing can be done
from a customer point of view or by testing the resources
which are part of the services. It can also be useful to 
perform tests of subservices (own subservices or subservices 
offered by suppliers).
Different schedules are possible to perform the active 
probing. The provider could select to test important services
and resources in regular time intervals. Other tests could
be initiated by a user who traverses the decision tree of the
Intelligent Assistant including active tests. Another 
possibility for the use of active probing is a request from the event
correlator, if the current correlation result needs to be 
improved. The results of active probing are reported via service
or resource events to the event correlator (or if the test was
demanded by the Intelligent Assistant the result is reported
to it, too). While the events that are received from 
management tools and customers denote negative events (something
does not work), the events from active probing should also
contain positive events for a better discrimination.
4.6 Event Correlator
The event correlation should not be performed by a single
event correlator, but by using different steps. The reason
for this are the different characteristics of the dependencies
(see Fig. 1).
On the resource level there are only relationships between
resources (network topology, systems configuration). An 
example for this could be a switch linking separate LANs. If
the switch is down, events are reported that other network
components which are located behind the switch are also not
reachable. When correlating these events it can be figured
out that the switch is the likely error cause. At this stage,
the integration of service events does not seem to be helpful.
The result of this step is a list of resources which could be
the problem"s root cause. The resource event correlator is
used to perform this step.
In the service-oriented scenario there are also service and
resource dependencies. As next step in the event 
correlation process the service events should be correlated with
each other using the service dependencies, because the 
service dependencies have no direct relationship to the resource
level. The result of this step, which is performed by the 
service event correlator, is a list of services/subservices which
could contain a failure in a resource. If, e.g., there are 
service events from customers that two services do not work
and both services depend on a common subservice, it seems
more likely that the resource failure can be found inside the
subservice. The output of this correlation is a list of 
services/subservices which could be affected by a failure in an
associated resource.
In the last step the aggregate event correlator matches
the lists from resource event correlator and service event
correlator to find the problem"s possible root cause. This is
done by using the resource dependencies.
The event correlation techniques presented in Section 2
could be used to perform the correlation inside the three
event correlators. If the dependencies can be found precisely,
an RBR or codebook approach seems to be appropriate. A
case database (CBR) could be used if there are cases which
could not be covered by RBR or the codebook approach.
These cases could then be used to improve the modeling in
a way that RBR or the codebook approach can deal with
them in future correlations.
5. INFORMATION MODELING
In this section we use a generic model for IT service 
management to derive the information necessary for the event
correlation process.
5.1 MNM Service Model
The MNM Service Model [5] is a generic model for IT 
service management. A distinction is made between customer
side and provider side. The customer side contains the 
basic roles customer and user, while the provider side contains
the role provider. The provider makes the service available
to the customer side. The service as a whole is divided into
usage which is accessed by the role user and management
which is used by the role customer.
The model consists of two main views. The Service View
(see Fig. 5) shows a common perspective of the service for
customer and provider. Everything that is only important
188
for the service realization is not contained in this view. For
these details another perspective, the Realization View, is
defined (see Fig. 6).
customer domain
supplies supplies
provider domain
«role»
provider
accesses uses concludes accesses
implements observesrealizes
provides directs
substantiates
usesuses
manages
implementsrealizes
manages
service
concludes
QoS
parameters
usage
functionality
service
access point
management
functionality
service implementation service management implementation
service
agreement
customersideprovidersidesideindependent
«role»
user
«role»
customer
CSM
access point
service
client
CSM
client
Figure 5: Service View
The Service View contains the service for which the 
functionality is defined for usage as well as for management. There
are two access points (service access point and CSM access
point) where user and customer can access the usage and
management functionality, respectively. Associated to each
service is a list of QoS parameters which have to be met by
the service at the service access point. The QoS surveillance
is performed by the management.
provider domain
implements observesrealizes
provides directs
implementsrealizes
accesses uses concludes accesses
usesuses
manages
side independent
side independent
manages
manages
concludes
acts as
service implementation service management implementation
manages
uses
acts as
service
logic
sub-service
client
service
client
CSM
client
uses
resources
usesuses
service 
management logic
sub-service
management client
basic 
management functionality
«role»
customer
«role»
provider
«role»
user
Figure 6: Realization View
In the Realization View the service implementation and the
service management implementation are described in detail.
For both there are provider-internal resources and 
subservices. For the service implementation a service logic uses
internal resources (devices, knowledge, staff) and external
subservices to provide the service. Analogous, the service
management implementation includes a service management
logic using basic management functionalities [8] and external
management subservices.
The MNM Service Model can be used for a similar 
modeling of the used subservices, i.e., the model can be applied
recursively.
As the service-oriented event correlation has to use 
dependencies of a service from subservices and resources, the
model is used in the following to derive the needed 
information for service events.
5.2 Information Modeling for Service Events
Today"s event correlation deals mainly with events which
are originated from resources. Beside a resource identifier
these events contain information about the resource status,
e.g., SNMP variables. To perform a service-oriented event
correlation it is necessary to define events which are related
to services. These events can be generated from the 
provider"s own service surveillance or from customer reports
at the CSM interface. They contain information about the
problems with the agreed QoS. In our information 
modeling we define an event superclass which contains common
attributes (e.g., time stamp). Resource event and service
event inherit from this superclass.
Derived from the MNM Service Model we define the 
information necessary for a service event.
Service: As a service event shall represent the problems of
a single service, a unique identification of the affected
service is contained here.
Event description: This field has to contain a description
of the problem. Depending on the interactions at the
service access point (Service View) a classification of
the problem into different categories should be defined.
It should also be possible to add an informal 
description of the problem.
QoS parameters: For each service QoS parameters 
(Service View) are defined between the provider and the
customer. This field represents a list of these QoS 
parameters and agreed service levels. The list can help
the provider to set the priority of a problem with 
respect to the service levels agreed.
Resource list: This list contains the resources (Realization
View) which are needed to provide the service. This
list is used by the provider to check if one of these
resources causes the problem.
Subservice service event identification: In the service
hierarchy (Realization View) the service, for which this
service event has been issued, may depend on 
subservices. If there is a suspicion that one of these 
subservices causes the problem, child service events are
issued from this service event for the subservices. In
such a case this field contains links to the 
corresponding events.
Other event identifications: In the event correlation
process the service event can be correlated with other
service events or with resource events. This field then
contains links to other events which have been 
correlated to this service event. This is useful to, e.g., send a
common message to all affected customers when their
subscribed services are available again.
Issuer"s identification: This field can either contain an
identification of the customer who reported the 
problem, an identification of a service provider"s employee
189
(in case the failure has been detected by the provider"s
own service active probing) or a link to a parent 
service event. The identification is needed, if there are
ambiguities in the service event or the issuer should
be informed (e.g., that the service is available again).
The possible issuers refer to the basic roles (customer,
provider) in the Service Model.
Assignee: To keep track of the processing the name and
address of the provider"s employee who is solving or
solved the problem is also noted. This is a 
specialization of the provider role in the Service Model.
Dates: This field contains key dates in the processing of the
service event such as initial date, problem 
identification date, resolution date. These dates are important
to keep track how quick the problems have been solved.
Status: This field represents the service event"s actual 
status (e.g., active, suspended, solved).
Priority: The priority shows which importance the service
event has from the provider"s perspective. The 
importance is derived from the service agreement, especially
the agreed QoS parameters (Service View).
The fields date, status, and other service events are not
derived directly from the Service Model, but are necessary
for the event correlation process.
6. APPLICATION OF 
SERVICE-ORIENTED EVENT CORRELATION FOR A
WEB HOSTING SCENARIO
The Leibniz Supercomputing Center is the joint 
computing center for the Munich universities and research 
institutions. It also runs the Munich Scientific Network and offers
related services. One of these services is the Virtual WWW
Server, a web hosting offer for smaller research institutions.
It currently has approximately 200 customers.
A subservice of the Virtual WWW Server is the 
Storage Service which stores the static and dynamic web pages
and uses caching techniques for a fast access. Other 
subservices are DNS and IP service. When a user accesses a
hosted web site via one of the LRZ"s Virtual Private 
Networks the VPN service is also used. The resources of the
Virtual WWW Server include a load balancer and 5 
redundant servers. The network connections are also part of the
resources as well as the Apache web server application 
running on the servers. Figure 7 shows the dependencies of the
Virtual WWW Server.
6.1 Customer Service Management and 
Intelligent Assistant
The Intelligent Assistant that is available at the Leibniz
Supercomputing Center can currently be used for 
connectivity or performance problems or problems with the LRZ
E-Mail Service. A selection of possible customer problem
reports for the Virtual WWW Server is given in the 
following:
• The hosted web site is not reachable.
• The web site access is (too) slow.
• The web site contains outdated content.
server
serverserver
server
server server
server
server
server
outgoing
connection
hosting of LRZ"s
own pages
content
caching
server
emergency
server
webmail
server dynamic
web pages
static
web pages
DNS ProxyIP Storage
Resources:
Services:
Virtual WWW Server
five redundant servers
AFS
NFS
DBload balancer
Figure 7: Dependencies of the Virtual WWW
Server
• The transfer of new content to the LRZ does not 
change the provided content.
• The web site looks strange (e.g., caused by problems
with HTML version)
This customer reports have to be mapped onto failures
in resources. For, e.g., an unreachable web site different
root causes are possible like a DNS problem, connectivity
problem, wrong configuration of the load balancer.
6.2 Active Probing
In general, active probing can be used for services or 
resources. For the service active probing of the Virtual WWW
Server a virtual customer could be installed. This customer
does typical HTTP requests of web sites and compares the
answer with the known content. To check the up-to-dateness
of a test web site, the content could contain a time stamp.
The service active probing could also include the testing of
subservices, e.g., sending requests to the DNS.
The resource active probing performs tests of the resources.
Examples are connectivity tests, requests to application 
processes, and tests of available disk space.
6.3 Event Correlation for the Virtual WWW
Server
Figure 8 shows the example processing. At first, a 
customer who takes a look at his hosted web site reports that
the content that he had changed is not displayed correctly.
This report is transferred to the service management via
the CSM interface. An Intelligent Assistant could be used
to structure the customer report. The service management
translates the customer report into a service event.
Independent from the customer report the service 
provider"s own service active probing tries to change the content
of a test web site. Because this is not possible, a service
event is issued.
Meanwhile, a resource event has been reported to the
event correlator, because an access of the content caching
server to one of the WWW servers failed. As there are no
other events at the moment the resource event correlation
190
customer CSM
service
mgmt
event
correlator
resource
mgmt
customer reports:
"web site content
not up−to−date"
service active
probing reports:
"web site content
change not
possible"
event:
"retrieval of server
content failed"event forward
resource
event
correlation
service
event
correlation
aggregate
event
correlation
link failure
report
event forward
check WWW server
check link
result display
link repair
result display
result forward
customer report
Figure 8: Example processing of a customer report
cannot correlate this event to other events. At this stage
it would be possible that the event correlator asks the 
resource management to perform an active probing of related
resources.
Both service events are now transferred to the service
event correlator and are correlated. From the correlation
of these events it seems likely that either the WWW server
itself or the link to the WWW server is the problem"s root
cause. A wrong web site update procedure inside the 
content caching server seems to be less likely as this would only
explain the customer report and not the service active 
probing result. At this stage a service active probing could be
started, but this does not seem to be useful as this 
correlation only deals with the Web Hosting Service and its
resources and not with other services.
After the separate correlation of both resource and service
events, which can be performed in parallel, the aggregate
event correlator is used to correlate both types of events.
The additional resource event makes it seem much more
likely that the problems are caused by a broken link to the
WWW server or by the WWW server itself and not by the
content caching server. In this case the event correlator asks
the resource management to check the link and the WWW
server. The decision between these two likely error causes
can not be further automated here.
Later, the resource management finds out that a broken
link is the failure"s root cause. It informs the event correlator
about this and it can be determined that this explains all
previous events. Therefore, the event correlation can be
stopped at this point.
Depending on the provider"s customer relationship 
management the finding of the root cause and an expected repair
time could be reported to the customers. After the link has
been repaired, it is possible to report this event via the CSM
interface.
Even though many details of this event correlation process
could also be performed differently, the example showed an
important advantage of the service-oriented event 
correlation. The relationship between the service provisioning and
the provider"s resources is explicitly modeled. This allows a
mapping of the customer report onto the provider-internal
resources.
6.4 Event Correlation for Different Services
If a provider like the LRZ offers several services the 
serviceoriented event correlation can be used to reveal relationships
that are not obvious in the first place. If the LRZ E-Mail
Service and its events are viewed in relationship with the
events for the Virtual WWW Server, it is possible to identify
failures in common subservices and resources. Both services
depend on the DNS which means that customer reports like
I cannot retrieve new e-mail and The web site of my 
research institute is not available can have a common cause,
e.g., the DNS does not work properly.
7. CONCLUSION AND FUTURE WORK
In our paper we showed the need for a service-oriented
event correlation. For an IT service provider this new kind
of event correlation makes it possible to automatically map
problems with the current service quality onto resource 
failures. This helps to find the failure"s root cause earlier and
to reduce costs for SLA violations. In addition, customer
reports can be linked together and therefore the processing
effort can be reduced.
To receive these benefits we presented our approach for
performing the service-oriented event correlation as well as
a modeling of the necessary correlation information. In the
future we are going to apply our workflow and information
modeling for services offered by the Leibniz Supercomputing
Center going further into details.
Several issues have not been treated in detail so far, e.g.,
the consequences for the service-oriented event correlation if
a subservice is offered by another provider. If a service does
not perform properly, it has to be determined whether this
is caused by the provider himself or by the subservice. In
the latter case appropriate information has to be exchanged
between the providers via the CSM interface. Another issue
is the use of active probing in the event correlation process
which can improve the result, but can also lead to a 
correlation delay.
Another important point is the precise definition of 
dependency which has also been left out by many other 
publications. To avoid having to much dependencies in a certain
situation one could try to check whether the dependencies
currently exist. In case of a download from a web site there
is only a dependency from the DNS subservice at the 
beginning, but after the address is resolved a download 
failure is unlikely to have been caused by the DNS. Another
possibility to reduce the dependencies is to divide a service
into its possible user interactions (e.g., an e-mail service into
transactions like get mail, sent mail, etc) and to define the
dependencies for each user interaction.
Acknowledgments
The authors wish to thank the members of the Munich 
Network Management (MNM) Team for helpful discussions and
valuable comments on previous versions of the paper. The
MNM Team, directed by Prof. Dr. Heinz-Gerd Hegering, is a
191
group of researchers of the Munich Universities and the 
Leibniz Supercomputing Center of the Bavarian Academy of 
Sciences. Its web server is located at wwwmnmteam.informatik.
uni-muenchen.de.
8. REFERENCES
[1] K. Appleby, G. Goldszmidt, and M. Steinder.
Yemanja - A Layered Event Correlation Engine for
Multi-domain Server Farms. In Proceedings of the
Seventh IFIP/IEEE International Symposium on
Integrated Network Management, pages 329-344.
IFIP/IEEE, May 2001.
[2] Spectrum, Aprisma Corporation.
http://www.aprisma.com.
[3] C. Ensel. New Approach for Automated Generation of
Service Dependency Models. In Network Management
as a Strategy for Evolution and Development; Second
Latin American Network Operation and Management
Symposium (LANOMS 2001). IEEE, August 2001.
[4] C. Ensel and A. Keller. An Approach for Managing
Service Dependencies with XML and the Resource
Description Framework. Journal of Network and
Systems Management, 10(2), June 2002.
[5] M. Garschhammer, R. Hauck, H.-G. Hegering,
B. Kempter, M. Langer, M. Nerb, I. Radisic,
H. Roelle, and H. Schmidt. Towards generic Service
Management Concepts - A Service Model Based
Approach. In Proceedings of the Seventh IFIP/IEEE
International Symposium on Integrated Network
Management, pages 719-732. IFIP/IEEE, May 2001.
[6] B. Gruschke. Integrated Event Management: Event
Correlation using Dependency Graphs. In Proceedings
of the 9th IFIP/IEEE International Workshop on
Distributed Systems: Operations & Management
(DSOM 98). IEEE/IFIP, October 1998.
[7] M. Gupta, A. Neogi, M. Agarwal, and G. Kar.
Discovering Dynamic Dependencies in Enterprise
Environments for Problem Determination. In
Proceedings of the 14th IFIP/IEEE Workshop on
Distributed Sytems: Operations and Management.
IFIP/IEEE, October 2003.
[8] H.-G. Hegering, S. Abeck, and B. Neumair. Integrated
Management of Networked Systems - Concepts,
Architectures and their Operational Application.
Morgan Kaufmann Publishers, 1999.
[9] IT Infrastructure Library, Office of Government
Commerce and IT Service Management Forum.
http://www.itil.co.uk.
[10] G. Jakobson and M. Weissman. Alarm Correlation.
IEEE Network, 7(6), November 1993.
[11] G. Jakobson and M. Weissman. Real-time
Telecommunication Network Management: Extending
Event Correlation with Temporal Constraints. In
Proceedings of the Fourth IEEE/IFIP International
Symposium on Integrated Network Management, pages
290-301. IEEE/IFIP, May 1995.
[12] S. Kliger, S. Yemini, Y. Yemini, D. Ohsie, and
S. Stolfo. A Coding Approach to Event Correlation. In
Proceedings of the Fourth IFIP/IEEE International
Symposium on Integrated Network Management, pages
266-277. IFIP/IEEE, May 1995.
[13] M. Langer, S. Loidl, and M. Nerb. Customer Service
Management: A More Transparent View To Your
Subscribed Services. In Proceedings of the 9th
IFIP/IEEE International Workshop on Distributed
Systems: Operations & Management (DSOM 98),
Newark, DE, USA, October 1998.
[14] L. Lewis. A Case-based Reasoning Approach for the
Resolution of Faults in Communication Networks. In
Proceedings of the Third IFIP/IEEE International
Symposium on Integrated Network Management.
IFIP/IEEE, 1993.
[15] L. Lewis. Service Level Management for Enterprise
Networks. Artech House, Inc., 1999.
[16] NETeXPERT, Agilent Technologies.
http://www.agilent.com/comms/OSS.
[17] InCharge, Smarts Corporation.
http://www.smarts.com.
[18] Enhanced Telecom Operations Map, TeleManagement
Forum. http://www.tmforum.org.
[19] Verizon Communications. http://www.verizon.com.
[20] H. Wietgrefe, K.-D. Tuchs, K. Jobmann, G. Carls,
P. Froelich, W. Nejdl, and S. Steinfeld. Using Neural
Networks for Alarm Correlation in Cellular Phone
Networks. In International Workshop on Applications
of Neural Networks to Telecommunications
(IWANNT), May 1997.
[21] S. Yemini, S. Kliger, E. Mozes, Y. Yemini, and
D. Ohsie. High Speed and Robust Event Correlation.
IEEE Communiations Magazine, 34(5), May 1996.
192
Network Monitors and Contracting Systems:
Competition and Innovation
Paul Laskowski John Chuang
UC Berkeley
{paul,chuang}@sims.berkeley.edu
ABSTRACT
Today"s Internet industry suffers from several well-known
pathologies, but none is as destructive in the long term as its
resistance to evolution. Rather than introducing new services, ISPs
are presently moving towards greater commoditization. It is
apparent that the network"s primitive system of contracts does not
align incentives properly. In this study, we identify the network"s
lack of accountability as a fundamental obstacle to correcting this
problem: Employing an economic model, we argue that optimal
routes and innovation are impossible unless new monitoring
capability is introduced and incorporated with the contracting
system. Furthermore, we derive the minimum requirements a
monitoring system must meet to support first-best routing and
innovation characteristics. Our work does not constitute a new
protocol; rather, we provide practical and specific guidance for the
design of monitoring systems, as well as a theoretical framework to
explore the factors that influence innovation.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems; J.4 [Social And Behavioral Sciences]: Economics
General Terms
Economics, Theory, Measurement, Design, Legal Aspects.
1. INTRODUCTION
Many studies before us have noted the Internet"s resistance to new
services and evolution. In recent decades, numerous ideas have
been developed in universities, implemented in code, and even
written into the routers and end systems of the network, only to
languish as network operators fail to turn them on on a large scale.
The list includes Multicast, IPv6, IntServ, and DiffServ. Lacking
the incentives just to activate services, there seems to be little hope
of ISPs devoting adequate resources to developing new ideas. In the
long term, this pathology stands out as a critical obstacle to the
network"s continued success (Ratnasamy, Shenker, and McCanne
provide extensive discussion in [11]).
On a smaller time scale, ISPs shun new services in favor of cost
cutting measures. Thus, the network has characteristics of a
commodity market. Although in theory, ISPs have a plethora of
routing policies at their disposal, the prevailing strategy is to route in
the cheapest way possible [2]. On one hand, this leads directly to
suboptimal routing. More importantly, commoditization in the short
term is surely related to the lack of innovation in the long term.
When the routing decisions of others ignore quality characteristics,
ISPs are motivated only to lower costs. There is simply no reward
for introducing new services or investing in quality improvements.
In response to these pathologies and others, researchers have put
forth various proposals for improving the situation. These can be
divided according to three high-level strategies: The first attempts
to improve the status quo by empowering end-users. Clark, et al.,
suggest that giving end-users control over routing would lead to
greater service diversity, recognizing that some payment mechanism
must also be provided [5]. Ratnasamy, Shenker, and McCanne
postulate a link between network evolution and user-directed
routing [11]. They propose a system of Anycast to give end-users
the ability to tunnel their packets to an ISP that introduces a
desirable protocol. The extra traffic to the ISP, the authors suggest,
will motivate the initial investment.
The second strategy suggests a revision of the contracting system.
This is exemplified by MacKie-Mason and Varian, who propose a
smart market to control access to network resources [10]. Prices
are set to the market-clearing level based on bids that users associate
to their traffic. In another direction, Afergan and Wroclawski
suggest that prices should be explicitly encoded in the routing
protocols [2]. They argue that such a move would improve stability
and align incentives.
The third high-level strategy calls for greater network
accountability. In this vein, Argyraki, et al., propose a system of
packet obituaries to provide feedback as to which ISPs drop packets
[3]. They argue that such feedback would help reveal which ISPs
were adequately meeting their contractual obligations. Unlike the
first two strategies, we are not aware of any previous studies that
have connected accountability with the pathologies of
commoditization or lack of innovation.
It is clear that these three strategies are closely linked to each other
(for example, [2], [5], and [9] each argue that giving end-users
routing control within the current contracting system is
problematic). Until today, however, the relationship between them
has been poorly understood. There is currently little theoretical
foundation to compare the relative merits of each proposal, and a
particular lack of evidence linking accountability with innovation
and service differentiation. This paper will address both issues.
We will begin by introducing an economic network model that
relates accountability, contracts, competition, and innovation. Our
model is highly stylized and may be considered preliminary: it is
based on a single source sending data to a single destination.
Nevertheless, the structure is rich enough to expose previously
unseen features of network behavior. We will use our model for
two main purposes:
First, we will use our model to argue that the lack of accountability
in today"s network is a fundamental obstacle to overcoming the
pathologies of commoditization and lack of innovation. In other
words, unless new monitoring capabilities are introduced, and
integrated with the system of contracts, the network cannot achieve
optimal routing and innovation characteristics. This result provides
motivation for the remainder of the paper, in which we explore how
accountability can be leveraged to overcome these pathologies and
create a sustainable industry. We will approach this problem from a
clean-slate perspective, deriving the level of accountability needed
to sustain an ideal competitive structure.
When we say that today"s Internet has poor accountability, we mean
that it reveals little information about the behavior - or misbehavior
- of ISPs. This well-known trait is largely rooted in the network"s
history. In describing the design philosophy behind the Internet
protocols, Clark lists accountability as the least important among
seven second level goals. [4] Accordingly, accountability
received little attention during the network"s formative years. Clark
relates this to the network"s military context, and finds that had the
network been designed for commercial development, accountability
would have been a top priority.
Argyraki, et al., conjecture that applying the principles of layering
and transparency may have led to the network"s lack of
accountability [3]. According to these principles, end hosts should
be informed of network problems only to the extent that they are
required to adapt. They notice when packet drops occur so that they
can perform congestion control and retransmit packets. Details of
where and why drops occur are deliberately concealed.
The network"s lack of accountability is highly relevant to a
discussion of innovation because it constrains the system of
contracts. This is because contracts depend upon external
institutions to function - the judge in the language of incomplete
contract theory, or simply the legal system. Ultimately, if a judge
cannot verify that some condition holds, she cannot enforce a
contract based on that condition. Of course, the vast majority of
contracts never end up in court. Especially when a judge"s ruling is
easily predicted, the parties will typically comply with the contract
terms on their own volition. This would not be possible, however,
without the judge acting as a last resort.
An institution to support contracts is typically complex, but we
abstract it as follows: We imagine that a contract is an algorithm
that outputs a payment transfer among a set of ISPs (the parties) at
every time. This payment is a function of the past and present
behaviors of the participants, but only those that are verifiable.
Hence, we imagine that a contract only accepts proofs as inputs.
We will call any process that generates these proofs a contractible
monitor. Such a monitor includes metering or sensing devices on
the physical network, but it is a more general concept. Constructing
a proof of a particular behavior may require readings from various
devices distributed among many ISPs. The contractible monitor
includes whatever distributed algorithmic mechanism is used to
motivate ISPs to share this private information.
Figure 1 demonstrates how our model of contracts fits together. We
make the assumption that all payments are mediated by contracts.
This means that without contractible monitors that attest to, say,
latency, payments cannot be conditioned on latency.
Figure 1: Relationship between monitors and contracts
With this model, we may conclude that the level of accountability in
today"s Internet only permits best effort contracts. Nodes cannot
condition payments on either quality or path characteristics.
Is there anything wrong with best-effort contracts? The reader
might wonder why the Internet needs contracts at all. After all, in
non-network industries, traditional firms invest in research and
differentiate their products, all in the hopes of keeping their
customers and securing new ones. One might believe that such
market forces apply to ISPs as well. We may adopt this as our null
hypothesis:
Null hypothesis: Market forces are sufficient to maintain service
diversity and innovation on a network, at least to the same extent
as they do in traditional markets.
There is a popular intuitive argument that supports this hypothesis,
and it may be summarized as follows:
Intuitive argument supporting null hypothesis:
1. Access providers try to increase their quality to get more
consumers.
2. Access providers are themselves customers for second hop
ISPs, and the second hops will therefore try to provide 
highquality service in order to secure traffic from access
providers. Access providers try to select high quality transit
because that increases their quality.
3. The process continues through the network, giving every
ISP a competitive reason to increase quality.
We are careful to model our network in continuous time, in order to
capture the essence of this argument. We can, for example, specify
equilibria in which nodes switch to a new next hop in the event of a
quality drop.
Moreover, our model allows us to explore any theoretically possible
punishments against cheaters, including those that are costly for
end-users to administer. By contrast, customers in the real world
rarely respond collectively, and often simply seek the best deal
currently offered. These constraints limit their ability to punish
cheaters.
Even with these liberal assumptions, however, we find that we must
reject our null hypothesis. Our model will demonstrate that
identifying a cheating ISP is difficult under low accountability,
limiting the threat of market driven punishment. We will define an
index of commoditization and show that it increases without bound
as data paths grow long. Furthermore, we will demonstrate a
framework in which an ISP"s maximum research investment
decreases hyperbolically with its distance from the end-user.
Network
Behavior
Monitor Contract
Proof
Payments
184
To summarize, we argue that the Internet"s lack of accountability
must be addressed before the pathologies of commoditization and
lack of innovation can be resolved. This leads us to our next topic:
How can we leverage accountability to overcome these pathologies?
We approach this question from a clean-slate perspective. Instead
of focusing on incremental improvements, we try to imagine how an
ideal industry would behave, then derive the level of accountability
needed to meet that objective. According to this approach, we first
craft a new equilibrium concept appropriate for network
competition. Our concept includes the following requirements:
First, we require that punishing ISPs that cheat is done without
rerouting the path. Rerouting is likely to prompt end-users to switch
providers, punishing access providers who administer punishments
correctly. Next, we require that the equilibrium cannot be
threatened by a coalition of ISPs that exchanges illicit side
payments. Finally, we require that the punishment mechanism that
enforces contracts does not punish innocent nodes that are not in the
coalition.
The last requirement is somewhat unconventional from an economic
perspective, but we maintain that it is crucial for any reasonable
solution. Although ISPs provide complementary services when they
form a data path together, they are likely to be horizontal
competitors as well. If innocent nodes may be punished, an ISP
may decide to deliberately cheat and draw punishment onto itself
and its neighbors. By cheating, the ISP may save resources, thereby
ensuring that the punishment is more damaging to the other ISPs,
which probably compete with the cheater directly for some
customers. In the extreme case, the cheater may force the other
ISPs out of business, thereby gaining a monopoly on some routes.
Applying this equilibrium concept, we derive the monitors needed
to maintain innovation and optimize routes. The solution is
surprisingly simple: contractible monitors must report the quality of
the rest of the path, from each ISP to the destination. It turns out
that this is the correct minimum accountability requirement, as
opposed to either end-to-end monitors or hop-by-hop monitors, as
one might initially suspect.
Rest of path monitors can be implemented in various ways. They
may be purely local algorithms that listen for packet echoes.
Alternately, they can be distributed in nature. We describe a way to
construct a rest of path monitor out of monitors for individual ISP
quality and for the data path. This requires a mechanism to
motivate ISPs to share their monitor outputs with each other. The
rest of path monitor then includes the component monitors and the
distributed algorithmic mechanism that ensures that information is
shared as required. This example shows that other types of monitors
may be useful as building blocks, but must be combined to form rest
of path monitors in order to achieve ideal innovation characteristics.
Our study has several practical implications for future protocol
design. We show that new monitors must be implemented and
integrated with the contracting system before the pathologies of
commoditization and lack of innovation can be overcome.
Moreover, we derive exactly what monitors are needed to optimize
routes and support innovation. In addition, our results provide
useful input for clean-slate architectural design, and we use several
novel techniques that we expect will be applicable to a variety of
future research.
The rest of this paper is organized as follows: In section 2, we lay
out our basic network model. In section 3, we present a 
lowaccountability network, modeled after today"s Internet. We
demonstrate how poor monitoring causes commoditization and a
lack of innovation. In section 4, we present verifiable monitors, and
show that proofs, even without contracts, can improve the status
quo. In section 5, we turn our attention to contractible monitors.
We show that rest of path monitors can support competition games
with optimal routing and innovation. We further show that rest of
path monitors are required to support such competition games. We
continue by discussing how such monitors may be constructed using
other monitors as building blocks. In section 6, we conclude and
present several directions for future research.
2. BASIC NETWORK MODEL
A source, S, wants to send data to destination, D. S and D are nodes
on a directed, acyclic graph, with a finite set of intermediate nodes,
{ }NV ,...2,1= , representing ISPs. All paths lead to D, and every
node not connected to D has at least two choices for next hop.
We will represent quality by a finite dimensional vector space, Q,
called the quality space. Each dimension represents a distinct
network characteristic that end-users care about. For example,
latency, loss probability, jitter, and IP version can each be assigned
to a dimension.
To each node, i, we associate a vector in the quality space, Qqi ∈ .
This corresponds to the quality a user would experience if i were the
only ISP on the data path. Let N
Q∈q be the vector of all node
qualities.
Of course, when data passes through multiple nodes, their qualities
combine in some way to yield a path quality. We represent this by
an associative binary operation, *: QQQ →× . For path
( )nvvv ,...,, 21 , the quality is given by nvvv qqq ∗∗∗ ...21
. The *
operation reflects the characteristics of each dimension of quality.
For example, * can act as an addition in the case of latency,
multiplication in the case of loss probability, or a 
minimumargument function in the case of security.
When data flows along a complete path from S to D, the source and
destination, generally regarded as a single player, enjoy utility given
by a function of the path quality, →Qu : . Each node along the
path, i, experiences some cost of transmission, ci.
2.1 Game Dynamics
Ultimately, we are most interested in policies that promote
innovation on the network. In this study, we will use innovation in
a fairly general sense. Innovation describes any investment by an
ISP that alters its quality vector so that at least one potential data
path offers higher utility. This includes researching a new routing
algorithm that decreases the amount of jitter users experience. It
also includes deploying a new protocol that supports quality of
service. Even more broadly, buying new equipment to decrease
S D
185
latency may also be regarded as innovation. Innovation
may be thought of as the micro-level process by which
the network evolves.
Our analysis is limited in one crucial respect: We focus
on inventions that a single ISP can implement to improve
the end-user experience. This excludes technologies that
require adoption by all ISPs on the network to function.
Because such technologies do not create a competitive
advantage, rewarding them is difficult and may require
intellectual property or some other market distortion. We
defer this interesting topic to future work.
At first, it may seem unclear how a large-scale distributed process
such as innovation can be influenced by mechanical details like
networks monitors. Our model must draw this connection in a
realistic fashion.
The rate of innovation depends on the profits that potential
innovators expect in the future. The reward generated by an
invention must exceed the total cost to develop it, or the inventor
will not rationally invest. This reward, in turn, is governed by the
competitive environment in which the firm operates, including the
process by which firms select prices, and agree upon contracts with
each other. Of course, these decisions depend on how routes are
established, and how contracts determine actual monetary
exchanges.
Any model of network innovation must therefore relate at least three
distinct processes: innovation, competition, and routing. We select
a game dynamics that makes the relation between these processes as
explicit as possible. This is represented schematically in Figure 2.
The innovation stage occurs first, at time 2−=t . In this stage, each
agent decides whether or not to make research investments. If she
chooses not to, her quality remains fixed. If she makes an
investment, her quality may change in some way. It is not
necessary for us to specify how such changes take place. The
agents" choices in this stage determine the vector of qualities, q,
common knowledge for the rest of the game.
Next, at time 1−=t , agents participate in the competition stage, in
which contracts are agreed upon. In today"s industry, these
contracts include prices for transit access, and peering agreements.
Since access is provided on a best-effort basis, a transit agreement
can simply be represented by its price. Other contracting systems
we will explore will require more detail.
Finally, beginning at 0=t , firms participate in the routing stage.
Other research has already employed repeated games to study
routing, for example [1], [12]. Repetition reveals interesting effects
not visible in a single stage game, such as informal collusion to
elevate prices in [12]. We use a game in continuous time in order to
study such properties. For example, we will later ask whether a
player will maintain higher quality than her contracts require, in the
hope of keeping her customer base or attracting future customers.
Our dynamics reflect the fact that ISPs make innovation decisions
infrequently. Although real firms have multiple opportunities to
innovate, each opportunity is followed by a substantial length of
time in which qualities are fixed. The decision to invest focuses on
how the firm"s new quality will improve the contracts it can enter
into. Hence, our model places innovation at the earliest stage,
attempting to capture a single investment decision. Contracting
decisions are made on an intermediate time scale, thus appearing
next in the dynamics. Routing decisions are made very frequently,
mainly to maximize immediate profit flows, so they appear in the
last stage.
Because of this ordering, our model does not allow firms to route
strategically to affect future innovation or contracting decisions. In
opposition, Afergan and Wroclawski argue that contracts are formed
in response to current traffic patterns, in a feedback loop [2].
Although we are sympathetic to their observation, such an addition
would make our analysis intractable. Our model is most realistic
when contracting decisions are infrequent.
Throughout this paper, our solution concept will be a subgame
perfect equilibrium (SPE). An SPE is a strategy point that is a Nash
equilibrium when restricted to each subgame. Three important
subgames have been labeled in Figure 2. The innovation game
includes all three stages. The competition game includes only the
competition stage and the routing stage. The routing game includes
only the routing stage.
An SPE guarantees that players are forward-looking. This means,
for example, that in the competition stage, firms must act rationally,
maximizing their expected profits in the routing stage. They cannot
carry out threats they made in the innovation stage if it lowers their
expected payoff.
Our schematic already suggests that the routing game is crucial for
promoting innovation. To support innovation, the competition
game must somehow reward ISPs with high quality. But that
means that the routing game must tend to route to nodes with high
quality. If the routing game always selects the lowest-cost routes,
for example, innovation will not be supported. We will support this
observation with analysis later.
2.2 The Routing Game
The routing game proceeds in continuous time, with all players
discounting by a common factor, r. The outputs from previous
stages, q and the set of contracts, are treated as exogenous
parameters for this game. For each time 0≥t , each node must
select a next hop to route data to. Data flows across the resultant
path, causing utility flow to S and D, and a flow cost to the nodes on
the path, as described above. Payment flows are also created, based
on the contracts in place.
Relating our game to the familiar repeated prisoners" dilemma,
imagine that we are trying to impose a high quality, but costly path.
As we argued loosely above, such paths must be sustainable in order
to support innovation. Each ISP on the path tries to maximize her
own payment, net of costs, so she may not want to cooperate with
our plan. Rather, if she can find a way to save on costs, at the
expense of the high quality we desire, she will be tempted to do so.
Innovation Game Competition Game Routing Game
Innovation
stage
Competition
stage
Routing
stageQualities
(q)
Contracts
(prices)
Profits
t = -2 t = -1 t ∈ [ 0 , )
Figure 2: Game Dynamics
186
Analogously to the prisoners" dilemma, we will call such a decision
cheating. A little more formally,
Cheating refers to any action that an ISP can take, contrary to
some target strategy point that we are trying to impose, that
enhances her immediate payoff, but compromises the quality of
the data path.
One type of cheating relates to the data path. Each node on the path
has to pay the next node to deliver its traffic. If the next node offers
high quality transit, we may expect that a lower quality node will
offer a lower price. Each node on the path will be tempted to route
to a cheaper next hop, increasing her immediate profits, but
lowering the path quality. We will call this type of action cheating
in route.
Another possibility we can model, is that a node finds a way to save
on its internal forwarding costs, at the expense of its own quality.
We will call this cheating internally to distinguish it from cheating
in route. For example, a node might drop packets beyond the rate
required for congestion control, in order to throttle back TCP flows
and thus save on forwarding costs [3]. Alternately, a node
employing quality of service could give high priority packets a
lower class of service, thus saving on resources and perhaps
allowing itself to sell more high priority service.
If either cheating in route or cheating internally is profitable, the
specified path will not be an equilibrium. We assume that cheating
can never be caught instantaneously. Rather, a cheater can always
enjoy the payoff from cheating for some positive time, which we
label 0t . This includes the time for other players to detect and react
to the cheating. If the cheater has a contract which includes a
customer lock-in period, 0t also includes the time until customers
are allowed to switch to a new ISP. As we will see later, it is
socially beneficial to decrease 0t , so such lock-in is detrimental to
welfare.
3. PATHOLOGIES OF A 
LOWACCOUNTABILITY NETWORK
In order to motivate an exploration of monitoring systems, we begin
in this section by considering a network with a poor degree of
accountability, modeled after today"s Internet. We will show how
the lack of monitoring necessarily leads to poor routing and
diminishes the rate of innovation. Thus, the network"s lack of
accountability is a fundamental obstacle to resolving these
pathologies.
3.1 Accountability in the Current Internet
First, we reflect on what accountability characteristics the present
Internet has. Argyraki, et al., point out that end hosts are given
minimal information about packet drops [3]. Users know when
drops occur, but not where they occur, nor why. Dropped packets
may represent the innocent signaling of congestion, or, as we
mentioned above, they may be a form of cheating internally. The
problem is similar for other dimensions of quality, or in fact more
acute. Finding an ISP that gives high priority packets a lower class
of service, for example, is further complicated by the lack of even
basic diagnostic tools.
In fact, it is similarly difficult to identify an ISP that cheats in route.
Huston notes that Internet traffic flows do not always correspond to
routing information [8]. An ISP may hand a packet off to a
neighbor regardless of what routes that neighbor has advertised.
Furthermore, blocks of addresses are summarized together for
distant hosts, so a destination may not even be resolvable until
packets are forwarded closer.
One might argue that diagnostic tools like ping and traceroute can
identify cheaters. Unfortunately, Argyraki, et al., explain that these
tools only reveal whether probe packets are echoed, not the fate of
past packets [3]. Thus, for example, they are ineffective in detecting
low-frequency packet drops. Even more fundamentally, a
sophisticated cheater can always spot diagnostic packets and give
them special treatment.
As a further complication, a cheater may assume different aliases
for diagnostic packets arriving over different routes. As we will see
below, this gives the cheater a significant advantage in escaping
punishment for bad behavior, even if the data path is otherwise
observable.
3.2 Modeling Low-Accountability
As the above evidence suggests, the current industry allows for very
little insight into the behavior of the network. In this section, we
attempt to capture this lack of accountability in our model. We
begin by defining a monitor, our model of the way that players
receive external information about network behavior,
A monitor is any distributed algorithmic mechanism that runs on
the network graph, and outputs, to specific nodes, informational
statements about current or past network behavior.
We assume that all external information about network behavior is
mediated in this way. The accountability properties of the Internet
can be represented by the following monitors:
E2E (End to End): A monitor that informs S/D about what the
total path quality is at any time (this is the quality they
experience).
ROP (Rest of Path): A monitor that informs each node along the
data path what the quality is for the rest of the path to the
destination.
PRc (Packets Received): A monitor that tells nodes how much
data they accept from each other, so that they can charge by
volume. It is important to note, however, that this information is
aggregated over many source-destination pairs. Hence, for the
sake of realism, it cannot be used to monitor what the data path is.
Players cannot measure the qualities of other, single nodes, just the
rest of the path. Nodes cannot see the path past the next hop. This
last assumption is stricter than needed for our results. The critical
ingredient is that nodes cannot verify that the path avoids a specific
hop. This holds, for example, if the path is generally visible, except
nodes can use different aliases for different parents. Similar results
also hold if alternate paths always converge after some integer
number, m, of hops.
It is important to stress that E2E and ROP are not the contractible
monitors we described in the introduction - they do not generate
proofs. Thus, even though a player observes certain information,
she generally cannot credibly share it with another player. For
example, if a node after the first hop starts cheating, the first hop
will detect the sudden drop in quality for the rest of the path, but the
first hop cannot make the source believe this observation - the
187
source will suspect that the first hop was the cheater, and fabricated
the claim against the rest of the path.
Typically, E2E and ROP are envisioned as algorithms that run on a
single node, and listen for packet echoes. This is not the only way
that they could be implemented, however; an alternate strategy is to
aggregate quality measurements from multiple points in the
network. These measurements can originate in other monitors,
located at various ISPs. The monitor then includes the component
monitors as well as whatever mechanisms are in place to motivate
nodes to share information honestly as needed. For example, if the
source has monitors that reveal the qualities of individual nodes,
they could be combined with path information to create an ROP
monitor.
Since we know that contracts only accept proofs as input, we can
infer that payments in this environment can only depend on the
number of packets exchanged between players. In other words,
contracts are best-effort. For the remainder of this section, we will
assume that contracts are also linear - there is a constant payment
flow so long as a node accepts data, and all conditions of the
contract are met. Other, more complicated tariffs are also possible,
and are typically used to generate lock-in. We believe that our
parameter t0 is sufficient to describe lock-in effects, and we believe
that the insights in this section apply equally to any tariffs that are
bounded so that the routing game remains continuous at infinity.
Restricting attention to linear contracts allows us to represent some
node i"s contract by its price, pi.
Because we further know that nodes cannot observe the path after
the next hop, we can infer that contracts exist only between
neighboring nodes on the graph. We will call this arrangement of
contracts bilateral. When a competition game exclusively uses
bilateral contracts, we will call it a bilateral contract competition
game.
We first focus on the routing game and ask whether a high quality
route can be maintained, even when a low quality route is cheaper.
Recall that this is a requirement in order for nodes to have any
incentive to innovate. If nodes tend to route to low price next hops,
regardless of quality, we say that the network is commoditized. To
measure this tendency, we define an index of commoditization as
follows:
For a node on the data path, i, define its quality premium,
minppd ji −= , where pj is the flow payment to the next hop in
equilibrium, and pmin is the price of the lowest cost next hop.
Definition: The index of commoditization, CI , is the average,
over each node on the data path, i, of i"s flow profit as a fraction
of i"s quality premium, ( ) ijii dpcp /−− .
CI ranges from 0, when each node spends all of its potential profit
on its quality premium, to infinite, when a node absorbs positive
profit, but uses the lowest price next hop. A high value for CI
implies that nodes are spending little of their money inflow on
purchasing high quality for the rest of the path. As the next claim
shows, this is exactly what happens as the path grows long:
Claim 1. If the only monitors are E2E, ROP, and PRc, ∞→CI
as ∞→n , where n is the number of nodes on the data path.
To show that this is true, we first need the following lemma, which
will establish the difficulty of punishing nodes in the network.
First a bit of notation: Recall that a cheater can benefit from its
actions for 00 >t before other players can react. When a node
cheats, it can expect a higher profit flow, at least until it is caught
and other players react, perhaps by diverting traffic. Let node i"s
normal profit flow be iπ , and her profit flow during cheating be
some greater value, yi. We will call the ratio, iiy π/ , the
temptation to cheat.
Lemma 1. If the only monitors are E2E, ROP, and PRc, the
discounted time, −nt
rt
e
0
, needed to punish a cheater increases at
least as fast as the product of the temptations to cheat along the data
path,
∏ −−
≥
0
0
pathdataon
0
t
rt
i i
i
t
rt
e
y
e
n
π
(1)
Corollary. If nodes share a minimum temptation to cheat, π/y ,
the discounted time needed to punish cheating increases at least
exponentially in the length of the data path, n,
−−
≥
0
00
t
rt
nt
rt
e
y
e
n
π
(2)
Since it is the discounted time that increases exponentially, the
actual time increases faster than exponentially. If n is so large that
tn is undefined, the given path cannot be maintained in equilibrium.
Proof. The proof proceeds by induction on the number of nodes on
the equilibrium data path, n. For 1=n , there is a single node, say i.
By cheating, the node earns extra profit ( ) −
−
0
0
t
rt
ii ey π . If node i
is then punished until time 1t , the extra profit must be cancelled out
by the lost profit between time 0t and 1t , −1
0
t
t
rt
i eπ . A little
manipulation gives −−
=
01
00
t
rt
i
i
t
rt
e
y
e
π
, as required.
For 1>n , assume for induction that the claim holds for 1−n . The
source does not know whether the cheater is the first hop, or after
the first hop. Because the source does not know the data path after
the first hop, it is unable to punish nodes beyond it. If it chooses a
new first hop, it might not affect the rest of the data path. Because
of this, the source must rely on the first hop to punish cheating
nodes farther along the path. The first hop needs discounted time,
∏ −0
0
hopfirstafter
t
rt
i i
i e
y
π
, to accomplish this by assumption. So
the source must give the first hop this much discounted time in order
to punish defectors further down the line (and the source will expect
poor quality during this period).
Next, the source must be protected against a first hop that cheats,
and pretends that the problem is later in the path. The first hop can
188
do this for the full discounted time, ∏ −0
0
hopfirstafter
t
rt
i i
i e
y
π
, so
the source must punish the first hop long enough to remove the extra
profit it can make. Following the same argument as for 1=n , we
can show that the full discounted time is ∏ −0
0
pathdataon
t
rt
i i
i e
y
π
,
which completes the proof.
The above lemma and its corollary show that punishing cheaters
becomes more and more difficult as the data path grows long, until
doing so is impossible. To capture some intuition behind this result,
imagine that you are an end user, and you notice a sudden drop in
service quality. If your data only travels through your access
provider, you know it is that provider"s fault. You can therefore
take your business elsewhere, at least for some time. This threat
should motivate your provider to maintain high quality.
Suppose, on the other hand, that your data traverses two providers.
When you complain to your ISP, he responds, yes, we know your
quality went down, but it"s not our fault, it"s the next ISP. Give us
some time to punish them and then normal quality will resume. If
your access provider is telling the truth, you will want to listen,
since switching access providers may not even route around the
actual offender. Thus, you will have to accept lower quality service
for some longer time. On the other hand, you may want to punish
your access provider as well, in case he is lying. This means you
have to wait longer to resume normal service. As more ISPs are
added to the path, the time increases in a recursive fashion.
With this lemma in hand, we can return to prove Claim 1.
Proof of Claim 1. Fix an equilibrium data path of length n. Label
the path nodes 1,2,…,n. For each node i, let i"s quality premium be
'11 ++ −= iii ppd . Then we have,
[ ]
=
−
=
−
+
+
=
−
+
++
=
+
−=−
−−
−−
=
−−
−
=
−−
=
n
i
i
n
i iii
iii
n
i iii
ii
n
i i
iii
C
g
npcp
pcp
n
pcp
pp
nd
pcp
n
I
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
'1
'11
, (3)
where gi is node i"s temptation to cheat by routing to the lowest
price next hop. Lemma 1 tells us that Tg
n
i
i <∏
=1
, where
( )01 rt
eT −
−= . It requires a bit of calculus to show that IC is
minimized by setting each gi equal to n
T /1
. However, as ∞→n ,
we have 1/1
→n
T , which shows that ∞→CI .
According to the claim, as the data path grows long, it increasingly
resembles a lowest-price path. Since lowest-price routing does not
support innovation, we may speculate that innovation degrades with
the length of the data path. Though we suspect stronger claims are
possible, we can demonstrate one such result by including an extra
assumption:
Available Bargain Path: A competitive market exists for 
lowcost transit, such that every node can route to the destination for
no more than flow payment, lp .
Claim 2. Under the available bargain path assumption, if node i , a
distance n from S, can invest to alter its quality, and the source will
spend no more than sP for a route including node i"s new quality,
then the payment to node i, p, decreases hyperbolically with n,
( )
( ) s
n
l P
n
T
pp
1
1/1
−
+≤
−
, (4)
where ( )01 rt
eT −
−= is the bound on the product of temptations
from the previous claim. Thus, i will spend no more than
( )
( )−
+
−
s
n
l P
n
T
p
r 1
1 1/1
on this quality improvement, which
approaches the bargain path"s payment,
r
pl
, as ∞→n .
The proof is given in the appendix. As a node gets farther from the
source, its maximum payment approaches the bargain price, pl.
Hence, the reward for innovation is bounded by the same amount.
Large innovations, meaning substantially more expensive than
rpl / , will not be pursued deep into the network.
Claim 2 can alternately be viewed as a lower bound on how much it
costs to elicit innovation in a network. If the source S wants node i
to innovate, it needs to get a motivating payment, p, to i during the
routing stage. However, it must also pay the nodes on the way to i a
premium in order to motivate them to route properly. The claim
shows that this premium increases with the distance to i, until it
dwarfs the original payment, p.
Our claims stand in sharp contrast to our null hypothesis from the
introduction. Comparing the intuitive argument that supported our
hypothesis with these claims, we can see that we implicitly used an
oversimplified model of market pressure (as either present or not).
As is now clear, market pressure relies on the decisions of
customers, but these are limited by the lack of information. Hence,
competitive forces degrade as the network deepens.
4. VERIFIABLE MONITORS
In this section, we begin to introduce more accountability into the
network. Recall that in the previous section, we assumed that
players couldn"t convince each other of their private information.
What would happen if they could? If a monitor"s informational
signal can be credibly conveyed to others, we will call it a verifiable
monitor. The monitor"s output in this case can be thought of as a
statement accompanied by a proof, a string that can be processed by
any player to determine that the statement is true.
A verifiable monitor is a distributed algorithmic mechanism that
runs on the network graph, and outputs, to specific nodes, proofs
about current or past network behavior.
Along these lines, we can imagine verifiable counterparts to E2E
and ROP. We will label these E2Ev and ROPv. With these
monitors, each node observes the quality of the rest of the path and
can also convince other players of these observations by giving
them a proof.
189
By adding verifiability to our monitors, identifying a single cheater
is straightforward. The cheater is the node that cannot produce
proof that the rest of path quality decreased. This means that the
negative results of the previous section no longer hold. For
example, the following lemma stands in contrast to Lemma 1.
Lemma 2. With monitors E2Ev, ROPv, and PRc, and provided that
the node before each potential cheater has an alternate next hop that
isn"t more expensive, it is possible to enforce any data path in SPE
so long as the maximum temptation is less than what can be deterred
in finite time,
−
≤
0
0
max
1
t
rt
er
y
π
(5)
Proof. This lemma follows because nodes can share proofs to
identify who the cheater is. Only that node must be punished in
equilibrium, and the preceding node does not lose any payoff in
administering the punishment.
With this lemma in mind, it is easy to construct counterexamples to
Claim 1 and Claim 2 in this new environment.
Unfortunately, there are at least four reasons not to be satisfied with
this improved monitoring system. The first, and weakest reason is
that the maximum temptation remains finite, causing some
distortion in routes or payments. Each node along a route must
extract some positive profit unless the next hop is also the cheapest.
Of course, if t0 is small, this effect is minimal.
The second, and more serious reason is that we have always given
our source the ability to commit to any punishment. Real world
users are less likely to act collectively, and may simply search for
the best service currently offered. Since punishment phases are
generally characterized by a drop in quality, real world end-users
may take this opportunity to shop for a new access provider. This
will make nodes less motivated to administer punishments.
The third reason is that Lemma 2 does not apply to cheating by
coalitions. A coalition node may pretend to punish its successor,
but instead enjoy a secret payment from the cheating node.
Alternately, a node may bribe its successor to cheat, if the
punishment phase is profitable, and so forth. The required
discounted time for punishment may increase exponentially in the
number of coalition members, just as in the previous section!
The final reason not to accept this monitoring system is that when a
cheater is punished, the path will often be routed around not just the
offender, but around other nodes as well. Effectively, innocent
nodes will be punished along with the guilty. In our abstract model,
this doesn"t cause trouble since the punishment falls off the
equilibrium path. The effects are not so benign in the real world.
When ISPs lie in sequence along a data path, they contribute
complementary services, and their relationship is vertical. From the
perspective of other source-destination pairs, however, these same
firms are likely to be horizontal competitors. Because of this, a
node might deliberately cheat, in order to trigger punishment for
itself and its neighbors. By cheating, the node will save money to
some extent, so the cheater is likely to emerge from the punishment
phase better off than the innocent nodes. This may give the cheater
a strategic advantage against its competitors. In the extreme, the
cheater may use such a strategy to drive neighbors out of business,
and thereby gain a monopoly on some routes.
5. CONTRACTIBLE MONITORS
At the end of the last section, we identified several drawbacks that
persist in an environment with E2Ev, ROPv, and PRc. In this
section, we will show how all of these drawbacks can be overcome.
To do this, we will require our third and final category of monitor:
A contractible monitor is simply a verifiable monitor that generates
proofs that can serve as input to a contract. Thus, contractible is
jointly a property of the monitor and the institutions that must verify
its proofs. Contractibility requires that a court,
1. Can verify the monitor"s proofs.
2. Can understand what the proofs and contracts represent to
the extent required to police illegal activity.
3. Can enforce payments among contracting parties.
Understanding the agreements between companies has traditionally
been a matter of reading contracts on paper. This may prove to be a
harder task in a future network setting. Contracts may plausibly be
negotiated by machine, be numerous, even per-flow, and be further
complicated by the many dimensions of quality.
When a monitor (together with institutional infrastructure) meets
these criteria, we will label it with a subscript c, for contractible.
The reader may recall that this is how we labeled the packets
received monitor, PRc, which allows ISPs to form contracts with
per-packet payments. Similarly, E2Ec and ROPc are contractible
versions of the monitors we are now familiar with.
At the end of the previous section, we argued for some desirable
properties that we"d like our solution to have. Briefly, we would
like to enforce optimal data paths with an equilibrium concept that
doesn"t rely on re-routing for punishment, is coalition proof, and
doesn"t punish innocent nodes when a coalition cheats. We will call
such an equilibrium a fixed-route coalition-proof 
protect-theinnocent equilibrium.
As the next claim shows, ROPc allows us to create a system of
linear (price, quality) contracts under just such an equilibrium.
Claim 3. With ROPc, for any feasible and consistent assignment of
rest of path qualities to nodes, and any corresponding payment
schedule that yields non-negative payoffs, these qualities can be
maintained with bilateral contracts in a fixed-route coalition-proof
protect-the-innocent equilibrium.
Proof: Fix any data path consistent with the given rest of path
qualities. Select some monetary punishment, P, large enough to
prevent any cheating for time t0 (the discounted total payment from
the source will work). Let each node on the path enter into a
contract with its parent, which fixes an arbitrary payment schedule
so long as the rest of path quality is as prescribed. When the parent
node, which has ROPc, submits a proof that the rest of path quality
is less than expected, the contract awards her an instantaneous
transfer, P, from the downstream node. Such proofs can be
submitted every 0t for the previous interval.
Suppose now that a coalition, C, decides to cheat. The source
measures a decrease in quality, and according to her contract, is
awarded P from the first hop. This means that there is a net outflow
of P from the ISPs as a whole. Suppose that node i is not in C. In
order for the parent node to claim P from i, it must submit proof that
the quality of the path starting at i is not as prescribed. This means
190
that there is a cheater after i. Hence, i would also have detected a
change in quality, so i can claim P from the next node on the path.
Thus, innocent nodes are not punished. The sequence of payments
must end by the destination, so the net outflow of P must come from
the nodes in C. This establishes all necessary conditions of the
equilibrium.
Essentially, ROPc allows for an implementation of (price, quality)
contracts. Building upon this result, we can construct competition
games in which nodes offer various qualities to each other at
specified prices, and can credibly commit to meet these
performance targets, even allowing for coalitions and a desire to
damage other ISPs.
Example 1. Define a Stackelberg price-quality competition game
as follows: Extend the partial order of nodes induced by the graph
to any complete ordering, such that downstream nodes appear
before their parents. In this order, each node selects a contract to
offer to its parents, consisting of a rest of path quality, and a linear
price. In the routing game, each node selects a next hop at every
time, consistent with its advertised rest of path quality. The
Stackelberg price-quality competition game can be implemented in
our model with ROPc monitors, by using the strategy in the proof,
above. It has the following useful property:
Claim 4. The Stackelberg price-quality competition game yields
optimal routes in SPE.
The proof is given in the appendix. This property is favorable from
an innovation perspective, since firms that invest in high quality will
tend to fall on the optimal path, gaining positive payoff. In general,
however, investments may be over or under rewarded. Extra
conditions may be given under which innovation decisions approach
perfect efficiency for large innovations. We omit the full analysis
here.
Example 2. Alternately, we can imagine that players report their
private information to a central authority, which then assigns all
contracts. For example, contracts could be computed to implement
the cost-minimizing VCG mechanism proposed by Feigenbaum, et
al. in [7]. With ROPc monitors, we can adapt this mechanism to
maximize welfare. For node, i, on the optimal path, L, the net
payment must equal, essentially, its contribution to the welfare of S,
D, and the other nodes. If L" is an optimal path in the graph with i
removed, the profit flow to i is,
( ) ( )
∈≠∈
+−−
',
'
Lj
j
ijLj
jLL ccququ , (6)
where Lq and 'Lq are the qualities of the two paths. Here, (price,
quality) contracts ensure that nodes report their qualities honestly.
The incentive structure of the VCG mechanism is what motivates
nodes to report their costs accurately.
A nice feature of this game is that individual innovation decisions
are efficient, meaning that a node will invest in an innovation
whenever the investment cost is less than the increased welfare of
the optimal data path. Unfortunately, the source may end up paying
more than the utility of the path.
Notice that with just E2Ec, a weaker version of Claim 3 holds.
Bilateral (price, quality) contracts can be maintained in an
equilibrium that is fixed-route and coalition-proof, but not 
protectthe-innocent. This is done by writing contracts to punish everyone
on the path when the end to end quality drops. If the path length is
n, the first hop pays nP to the source, the second hop pays ( )Pn 1−
to the first, and so forth. This ensures that every node is punished
sufficiently to make cheating unprofitable. For the reasons we gave
previously, we believe that this solution concept is less than ideal,
since it allows for malicious nodes to deliberately trigger
punishments for potential competitors.
Up to this point, we have adopted fixed-route coalition-proof
protect-the-innocent equilibrium as our desired solution concept,
and shown that ROPc monitors are sufficient to create some
competition games that are desirable in terms of service diversity
and innovation. As the next claim will show, rest of path
monitoring is also necessary to construct such games under our
solution concept.
Before we proceed, what does it mean for a game to be desirable
from the perspective of service diversity and innovation? We will
use a very weak assumption, essentially, that the game is not fully
commoditized for any node. The claim will hold for this entire class
of games.
Definition: A competition game is nowhere-commoditized if for
each node, i, not adjacent to D, there is some assignment of qualities
and marginal costs to nodes, such that the optimal data path includes
i, and i has a positive temptation to cheat.
In the case of linear contracts, it is sufficient to require that ∞<CI ,
and that every node make positive profit under some assignment of
qualities and marginal costs.
Strictly speaking, ROPc monitors are not the only way to construct
these desirable games. To prove the next claim, we must broaden
our notion of rest of path monitoring to include the similar ROPc"
monitor, which attests to the quality starting at its own node,
through the end of the path. Compare the two monitors below:
ROPc: gives a node proof that the path quality from the next node
to the destination is not correct.
ROPc": gives a node proof that the path quality from that node to
the destination is correct.
We present a simplified version of this claim, by including an
assumption that only one node on the path can cheat at a time
(though conspirators can still exchange side payments). We will
discuss the full version after the proof.
Claim 5. Assume a set of monitors, and a nowhere-commoditized
bilateral contract competition game that always maintains the
optimal quality in fixed-route coalition-proof protect-the-innocent
equilibrium, with only one node allowed to cheat at a time. Then
for each node, i, not adjacent to D, either i has an ROPc monitor, or
i"s children each have an ROPc" monitor.
Proof: First, because of the fixed-route assumption, punishments
must be purely monetary.
Next, when cheating occurs, if the payment does not go to the
source or destination, it may go to another coalition member,
rendering it ineffective. Thus, the source must accept some
monetary compensation, net of its normal flow payment, when
cheating occurs. Since the source only contracts with the first hop,
it must accept this money from the first hop. The source"s contract
must therefore distinguish when the path quality is normal from
when it is lowered by cheating. To do so, it can either accept proofs
191
from the source, that the quality is lower than required, or it can
accept proofs from the first hop, that the quality is correct. These
nodes will not rationally offer the opposing type of proof.
By definition, any monitor that gives the source proof that the path
quality is wrong is an ROPc monitor. Any monitor that gives the
first hop proof that the quality is correct is a ROPc" monitor. Thus,
at least one of these monitors must exist.
By the protect-the-innocent assumption, if cheating occurs, but the
first hop is not a cheater, she must be able to claim the same size
reward from the next ISP on the path, and thus pass on the
punishment. The first hop"s contract with the second must then
distinguish when cheating occurs after the first hop. By argument
similar to that for the source, either the first hop has a ROPc
monitor, or the second has a ROPc" monitor. This argument can be
iterated along the entire path to the penultimate node before D.
Since the marginal costs and qualities can be arranged to make any
path the optimal path, these statements must hold for all nodes and
their children, which completes the proof.
The two possibilities for monitor correspond to which node has the
burden of proof. In one case, the prior node must prove the
suboptimal quality to claim its reward. In the other, the subsequent
node must prove that the quality was correct to avoid penalty.
Because the two monitors are similar, it seems likely that they
require comparable costs to implement. If submitting the proofs is
costly, it seems natural that nodes would prefer to use the ROPc
monitor, placing the burden of proof on the upstream node.
Finally, we note that it is straightforward to derive the full version of
the claim, which allows for multiple cheaters. The only
complication is that cheaters can exchange side payments, which
makes any money transfers between them redundant. Because of
this, we have to further generalize our rest of path monitors, so they
are less constrained in the case that there are cheaters on either side.
5.1 Implementing Monitors
Claim 5 should not be interpreted as a statement that each node must
compute the rest of path quality locally, without input from other
nodes. Other monitors, besides ROPc and ROPc" can still be used,
loosely speaking, as building blocks. For instance, network
tomography is concerned with measuring properties of the network
interior with tools located at the edge. Using such techniques, our
source might learn both individual node qualities and the data path.
This is represented by the following two monitors:
SHOPc
i
: (source-based hop quality) A monitor that gives the
source proof of what the quality of node i is.
SPATHc: (source-based path) A monitor that gives the source
proof of what the data path is at any time, at least as far as it
matches the equilibrium path.
With these monitors, a punishment mechanism can be designed to
fulfill the conditions of Claim 5. It involves the source sharing the
proofs it generates with nodes further down the path, which use
them to determine bilateral payments. Ultimately however, the
proof of Claim 5 shows us that each node i"s bilateral contracts
require proof of the rest of path quality. This means that node i (or
possibly its children) will have to combine the proofs that they
receive to generate a proof of the rest of path quality. Thus, the
combined process is itself a rest of path monitor.
What we have done, all in all, is constructed a rest of path monitor
using SPATHc and SHOPc
i
as building blocks. Our new monitor
includes both the component monitors and whatever distributed
algorithmic mechanism exists to make sure nodes share their proofs
correctly.
This mechanism can potentially involve external institutions. For a
concrete example, suppose that when node i suspects it is getting
poor rest of path quality from its successor, it takes the downstream
node to court. During the discovery process, the court subpoenas
proofs of the path and of node qualities from the source (ultimately,
there must be some threat to ensure the source complies). Finally,
for the court to issue a judgment, one party or the other must
compile a proof of what the rest of path quality was. Hence, the
entire discovery process acts as a rest of path monitor, albeit a rather
costly monitor in this case.
Of course, mechanisms can be designed to combine these monitors
at much lower cost. Typically, such mechanisms would call for
automatic sharing of proofs, with court intervention only as a last
resort. We defer these interesting mechanisms to future work.
As an aside, intuition might dictate that SHOPc
i
generates more
information than ROPc; after all, inferring individual node qualities
seems a much harder problem. Yet, without path information,
SHOPc
i
is not sufficient for our first-best innovation result. The
proof of this demonstrates a useful technique:
Claim 6. With monitors E2E, ROP, SHOPc
i
and PRc, and a
nowhere-commoditized bilateral contract competition game, the
optimal quality cannot be maintained for all assignments of quality
and marginal cost, in fixed-route coalition-proof 
protect-theinnocent equilibrium.
Proof: Because nodes cannot verify the data path, they cannot form
a proof of what the rest of path quality is. Hence, ROPc monitors do
not exist, and therefore the requirements of Claim 5 cannot hold.
6. CONCLUSIONS AND FUTURE WORK
It is our hope that this study will have a positive impact in at least
three different ways. The first is practical: we believe our analysis
has implications for the design of future monitoring protocols and
for public policy.
For protocol designers, we first provide fresh motivation to create
monitoring systems. We have argued that the poor accountability of
the Internet is a fundamental obstacle to alleviating the pathologies
of commoditization and lack of innovation. Unless accountability
improves, these pathologies are guaranteed to remain.
Secondly, we suggest directions for future advances in monitoring.
We have shown that adding verifiability to monitors allows for
some improvements in the characteristics of competition. At the
same time, this does not present a fully satisfying solution. This
paper has suggested a novel standard for monitors to aspire to - one
of supporting optimal routes in innovative competition games under
fixed-route coalition-proof protect-the-innocent equilibrium. We
have shown that under bilateral contracts, this specifically requires
contractible rest of path monitors.
This is not to say that other types of monitors are unimportant. We
included an example in which individual hop quality monitors and a
path monitor can also meet our standard for sustaining competition.
However, in order for this to happen, a mechanism must be included
192
to combine proofs from these monitors to form a proof of rest of
path quality. In other words, the monitors must ultimately be
combined to form contractible rest of path monitors. To support
service differentiation and innovation, it may be easier to design rest
of path monitors directly, thereby avoiding the task of designing
mechanisms for combining component monitors.
As far as policy implications, our analysis points to the need for
legal institutions to enforce contracts based on quality. These
institutions must be equipped to verify proofs of quality, and police
illegal contracting behavior. As quality-based contracts become
numerous and complicated, and possibly negotiated by machine,
this may become a challenging task, and new standards and
regulations may have to emerge in response. This remains an
interesting and unexplored area for research.
The second area we hope our study will benefit is that of clean-slate
architectural design. Traditionally, clean-slate design tends to focus
on creating effective and elegant networks for a static set of
requirements. Thus, the approach is often one of engineering,
which tends to neglect competitive effects. We agree with
Ratnasamy, Shenker, and McCanne, that designing for evolution
should be a top priority [11]. We have demonstrated that the
network"s monitoring ability is critical to supporting innovation, as
are the institutions that support contracting. These elements should
feature prominently in new designs. Our analysis specifically
suggests that architectures based on bilateral contracts should
include contractible rest of path monitoring. From a clean-slate
perspective, these monitors can be transparently and fully integrated
with the routing and contracting systems.
Finally, the last contribution our study makes is methodological.
We believe that the mathematical formalization we present is
applicable to a variety of future research questions. While a
significant literature addresses innovation in the presence of
network effects, to the best of our knowledge, ours is the first model
of innovation in a network industry that successfully incorporates
the actual topological structure as input. This allows the discovery
of new properties, such as the weakening of market forces with the
number of ISPs on a data path that we observe with 
lowaccountability.
Our method also stands in contrast to the typical approach of
distributed algorithmic mechanism design. Because this field is
based on a principle-agent framework, contracts are usually
proposed by the source, who is allowed to make a take it or leave it
offer to network nodes. Our technique allows contracts to emerge
from a competitive framework, so the source is limited to selecting
the most desirable contract. We believe this is a closer reflection of
the industry.
Based on the insights in this study, the possible directions for future
research are numerous and exciting. To some degree, contracting
based on quality opens a Pandora"s Box of pressing questions: Do
quality-based contracts stand counter to the principle of network
neutrality? Should ISPs be allowed to offer a choice of contracts at
different quality levels? What anti-competitive behaviors are
enabled by quality-based contracts? Can a contracting system
support optimal multicast trees?
In this study, we have focused on bilateral contracts. This system
has seemed natural, especially since it is the prevalent system on the
current network. Perhaps its most important benefit is that each
contract is local in nature, so both parties share a common, familiar
legal jurisdiction. There is no need to worry about who will enforce
a punishment against another ISP on the opposite side of the planet,
nor is there a dispute over whose legal rules to apply in interpreting
a contract.
Although this benefit is compelling, it is worth considering other
systems. The clearest alternative is to form a contract between the
source and every node on the path. We may call these source
contracts. Source contracting may present surprising advantages.
For instance, since ISPs do not exchange money with each other, an
ISP cannot save money by selecting a cheaper next hop.
Additionally, if the source only has contracts with nodes on the
intended path, other nodes won"t even be willing to accept packets
from this source since they won"t receive compensation for carrying
them. This combination seems to eliminate all temptation for a
single cheater to cheat in route. Because of this and other
encouraging features, we believe source contracts are a fertile topic
for further study.
Another important research task is to relax our assumption that
quality can be measured fully and precisely. One possibility is to
assume that monitoring is only probabilistic or suffers from noise.
Even more relevant is the possibility that quality monitors are
fundamentally incomplete. A quality monitor can never anticipate
every dimension of quality that future applications will care about,
nor can it anticipate a new and valuable protocol that an ISP
introduces. We may define a monitor space as a subspace of the
quality space that a monitor can measure, QM ⊂ , and a
corresponding monitoring function that simply projects the full
range of qualities onto the monitor space, MQm →: .
Clearly, innovations that leave quality invariant under m are not
easy to support - they are invisible to the monitoring system. In this
environment, we expect that path monitoring becomes more
important, since it is the only way to ensure data reaches certain
innovator ISPs. Further research is needed to understand this
process.
7. ACKNOWLEDGEMENTS
We would like to thank the anonymous reviewers, Jens Grossklags,
Moshe Babaioff, Scott Shenker, Sylvia Ratnasamy, and Hal Varian
for their comments. This work is supported in part by the National
Science Foundation under ITR award ANI-0331659.
8. REFERENCES
[1] Afergan, M. Using Repeated Games to Design 
IncentiveBased Routing Systems. In Proceedings of IEEE INFOCOM
(April 2006).
[2] Afergan, M. and Wroclawski, J. On the Benefits and
Feasibility of Incentive Based Routing Infrastructure. In ACM
SIGCOMM'04 Workshop on Practice and Theory of Incentives
in Networked Systems (PINS) (August 2004).
[3] Argyraki, K., Maniatis, P., Cheriton, D., and Shenker, S.
Providing Packet Obituaries. In Third Workshop on Hot Topics
in Networks (HotNets) (November 2004).
[4] Clark, D. D. The Design Philosophy of the DARPA Internet
Protocols. In Proceedings of ACM SIGCOMM (1988).
193
[5] Clark, D. D., Wroclawski, J., Sollins, K. R., and Braden, R.
Tussle in cyberspace: Defining tomorrow's internet. In
Proceedings of ACM SIGCOMM (August 2002).
[6] Dang-Nguyen, G. and Pénard, T. Interconnection Agreements:
Strategic Behaviour and Property Rights. In Brousseau, E. and
Glachant, J.M. Eds. The Economics of Contracts: Theories and
Applications, Cambridge University Press, 2002.
[7] Feigenbaum, J., Papadimitriou, C., Sami, R., and Shenker, S.
A BGP-based Mechanism for Lowest-Cost Routing.
Distributed Computing 18 (2005), pp. 61-72.
[8] Huston, G. Interconnection, Peering, and Settlements. Telstra,
Australia.
[9] Liu, Y., Zhang, H., Gong, W., and Towsley, D. On the
Interaction Between Overlay Routing and Traffic Engineering.
In Proceedings of IEEE INFOCOM (2005).
[10] MacKie-Mason, J. and Varian, H. Pricing the Internet. In
Kahin, B. and Keller, J. Eds. Public access to the Internet.
Englewood Cliffs, NJ; Prentice-Hall, 1995.
[11] Ratnasamy, S., Shenker, S., and McCanne, S. Towards an
Evolvable Internet Architecture. In Proceeding of ACM
SIGCOMM (2005).
[12] Shakkottai, S., and Srikant, R. Economics of Network Pricing
with Multiple ISPs. In Proceedings of IEEE INFOCOM
(2005).
9. APPENDIX
Proof of Claim 2. Node i must fall on the equilibrium data path to
receive any payment. Let the prices along the data path be
ppppP nS == ,..., 21 , with marginal costs, ncc ,...,1 . We may
assume the prices on the path are greater than lp or the claim
follows trivially. Each node along the data path can cheat in route
by giving data to the bargain path at price no more than lp . So
node j"s temptation to cheat is at least
11 ++ −
−
≥
−−
−−
jj
l
jjj
ljj
pp
pp
pcp
pcp
. Then Lemma 1 gives,
1
13221
1...
−
−
−
−>
−
−
⋅⋅
−
−
−
−
≥
n
S
l
n
lll
P
pp
n
pp
pp
pp
pp
pp
pp
T (7)
This can be rearranged to give
( )
( ) s
n
l P
n
T
pp
1
1/1
−
+≤
−
, as required.
The rest of the claim simply recognizes that rp / is the greatest
reward node i can receive for its investment, so it will not invest
sums greater than this.
Proof of Claim 4. Label the nodes 1,2,.. N in the order in which
they select contracts. Let subgame n be the game that begins with n
choosing its contract. Let Ln be the set of possible paths restricted to
nodes n,…,N. That is, Ln is the set of possible routes from S to
reach some node that has already moved.
For subgame n, define the local welfare over paths nLl ∈ , and their
possible next hops, nj < as follows,
( ) ( ) j
li
ipathjl pcqqujlV −−=
∈
*, , (8)
where ql is the quality of path l in the set {n,…,N}, and pathjq and
pj are the quality and price of the contract j has offered.
For induction, assume that subgame n + 1 maximizes local welfare.
We show that subgame n does as well. If node n selects next hop k,
we can write the following relation,
( ) ( )( ) ( )( ) nknn knlVpcpknlVnlV π−=++−= ,,,,, , (9)
where n is node n"s profit if the path to n is chosen. This path is
chosen whenever ( )nlV , is maximal over Ln+1 and possible next
hops. If ( )( )knlV ,, is maximal over Ln, it is also maximal over the
paths in Ln+1 that don"t lead to n. This means that node n can choose
some n small enough so that ( )nlV , is maximal over Ln+1, so the
route will lead to k.
Conversely, if ( )( )knlV ,, is not maximal over Ln, either V is greater
for another of n"s next hops, in which case n will select that one in
order to increase n, or V is greater for some path in Ln+1 that don"t
lead to n, in which case ( )nlV , cannot be maximal for any 
nonnegative n.
Thus, we conclude that subgame n maximizes local welfare. For the
initial case, observe that this assumption holds for the source.
Finally, we deduce that subgame 1, which is the entire game,
maximizes local welfare, which is equivalent to actual welfare.
Hence, the Stackelberg price-quality game yields an optimal route.
194
StarDust: A Flexible Architecture for Passive Localization in
Wireless Sensor Networks
∗
Radu Stoleru, Pascal Vicaire, Tian He†, John A. Stankovic
Department of Computer Science, University of Virginia
†Department of Computer Science and Engineering, University of Minnesota
{stoleru, pv9f}@cs.virginia.edu, tianhe@cs.umn.edu, stankovic@cs.virginia.edu
Abstract
The problem of localization in wireless sensor networks
where nodes do not use ranging hardware, remains a 
challenging problem, when considering the required location 
accuracy, energy expenditure and the duration of the 
localization phase. In this paper we propose a framework, called
StarDust, for wireless sensor network localization based on
passive optical components. In the StarDust framework,
sensor nodes are equipped with optical retro-reflectors. An
aerial device projects light towards the deployed sensor 
network, and records an image of the reflected light. An image
processing algorithm is developed for obtaining the locations
of sensor nodes. For matching a node ID to a location we
propose a constraint-based label relaxation algorithm. We
propose and develop localization techniques based on four
types of constraints: node color, neighbor information, 
deployment time for a node and deployment location for a
node. We evaluate the performance of a localization system
based on our framework by localizing a network of 26 
sensor nodes deployed in a 120 × 60 ft2 area. The localization
accuracy ranges from 2 ft to 5 ft while the localization time
ranges from 10 milliseconds to 2 minutes.
Categories and Subject Descriptors: C.2.4 [Computer
Communications Networks]: Distributed Systems; C.3 
[Special Purpose and Application Based Systems]: Real-time and
embedded systems
General Terms: Algorithms, Measurement, Performance,
Design, Experimentation
1 Introduction
Wireless Sensor Networks (WSN) have been envisioned
to revolutionize the way humans perceive and interact with
the surrounding environment. One vision is to embed tiny
sensor devices in outdoor environments, by aerial 
deployments from unmanned air vehicles. The sensor nodes form
a network and collaborate (to compensate for the extremely
scarce resources available to each of them: computational
power, memory size, communication capabilities) to 
accomplish the mission. Through collaboration, redundancy and
fault tolerance, the WSN is then able to achieve 
unprecedented sensing capabilities.
A major step forward has been accomplished by 
developing systems for several domains: military surveillance [1]
[2] [3], habitat monitoring [4] and structural monitoring [5].
Even after these successes, several research problems remain
open. Among these open problems is sensor node 
localization, i.e., how to find the physical position of each sensor
node. Despite the attention the localization problem in WSN
has received, no universally acceptable solution has been 
developed. There are several reasons for this. On one hand,
localization schemes that use ranging are typically high end
solutions. GPS ranging hardware consumes energy, it is 
relatively expensive (if high accuracy is required) and poses
form factor challenges that move us away from the vision
of dust size sensor nodes. Ultrasound has a short range and
is highly directional. Solutions that use the radio transceiver
for ranging either have not produced encouraging results (if
the received signal strength indicator is used) or are sensitive
to environment (e.g., multipath). On the other hand, 
localization schemes that only use the connectivity information
for inferring location information are characterized by low
accuracies: ≈ 10 ft in controlled environments, 40−50 ft in
realistic ones.
To address these challenges, we propose a framework for
WSN localization, called StarDust, in which the 
complexity associated with the node localization is completely 
removed from the sensor node. The basic principle of the
framework is localization through passivity: each sensor
node is equipped with a corner-cube retro-reflector and 
possibly an optical filter (a coloring device). An aerial 
vehicle projects light onto the deployment area and records 
images containing retro-reflected light beams (they appear as
luminous spots). Through image processing techniques, the
locations of the retro-reflectors (i.e., sensor nodes) is 
deter57
mined. For inferring the identity of the sensor node present
at a particular location, the StarDust framework develops a
constraint-based node ID relaxation algorithm.
The main contributions of our work are the following. We
propose a novel framework for node localization in WSNs
that is very promising and allows for many future extensions
and more accurate results. We propose a constraint-based
label relaxation algorithm for mapping node IDs to the 
locations, and four constraints (node, connectivity, time and
space), which are building blocks for very accurate and very
fast localization systems. We develop a sensor node 
hardware prototype, called a SensorBall. We evaluate the 
performance of a localization system for which we obtain location
accuracies of 2 − 5 ft with a localization duration ranging
from 10 milliseconds to 2 minutes. We investigate the range
of a system built on our framework by considering realities
of physical phenomena that occurs during light propagation
through the atmosphere.
The rest of the paper is structured as follows. Section 2
is an overview of the state of art. The design of the 
StarDust framework is presented in Section 3. One 
implementation and its performance evaluation are in Sections 4 and
5, followed by a suite of system optimization techniques, in
Section 6. In Section 7 we present our conclusions.
2 Related Work
We present the prior work in localization in two major
categories: the range-based, and the range-free schemes.
The range-based localization techniques have been 
designed to use either more expensive hardware (and hence
higher accuracy) or just the radio transceiver. Ranging 
techniques dependent on hardware are the time-of-flight (ToF)
and the time-difference-of-arrival(TDoA). Solutions that use
the radio are based on the received signal strength indicator
(RSSI) and more recently on radio interferometry.
The ToF localization technique that is most widely used is
the GPS. GPS is a costly solution for a high accuracy 
localization of a large scale sensor network. AHLoS [6] employs
a TDoA ranging technique that requires extensive hardware
and solves relatively large nonlinear systems of equations.
The Cricket location-support system (TDoA) [7] can achieve
a location granularity of tens of inches with highly 
directional and short range ultrasound transceivers. In [2] the 
location of a sniper is determined in an urban terrain, by 
using the TDoA between an acoustic wave and a radio beacon.
The PushPin project [8] uses the TDoA between ultrasound
pulses and light flashes for node localization. The RADAR
system [9] uses the RSSI to build a map of signal strengths
as emitted by a set of beacon nodes. A mobile node is 
located by the best match, in the signal strength space, with a
previously acquired signature. In MAL [10], a mobile node
assists in measuring the distances (acting as constraints) 
between nodes until a rigid graph is generated. The localization
problem is formulated as an on-line state estimation in a 
nonlinear dynamic system [11]. A cooperative ranging that 
attempts to achieve a global positioning from distributed local
optimizations is proposed in [12]. A very recent, remarkable,
localization technique is based on radio interferometry, RIPS
[13], which utilizes two transmitters to create an interfering
signal. The frequencies of the emitters are very close to each
other, thus the interfering signal will have a low frequency
envelope that can be easily measured. The ranging technique
performs very well. The long time required for localization
and multi-path environments pose significant challenges.
Real environments create additional challenges for the
range based localization schemes. These have been 
emphasized by several studies [14] [15] [16]. To address these 
challenges, and others (hardware cost, the energy expenditure,
the form factor, the small range, localization time), several
range-free localization schemes have been proposed. Sensor
nodes use primarily connectivity information for inferring
proximity to a set of anchors. In the Centroid localization
scheme [17], a sensor node localizes to the centroid of its
proximate beacon nodes. In APIT [18] each node decides its
position based on the possibility of being inside or outside of
a triangle formed by any three beacons within node"s 
communication range. The Gradient algorithm [19], leverages
the knowledge about the network density to infer the average
one hop length. This, in turn, can be transformed into 
distances to nodes with known locations. DV-Hop [20] uses the
hop by hop propagation capability of the network to forward
distances to landmarks. More recently, several localization
schemes that exploit the sensing capabilities of sensor nodes,
have been proposed. Spotlight [21] creates well controlled
(in time and space) events in the network while the sensor
nodes detect and timestamp this events. From the 
spatiotemporal knowledge for the created events and the temporal
information provided by sensor nodes, nodes" spatial 
information can be obtained. In a similar manner, the Lighthouse
system [22] uses a parallel light beam, that is emitted by an
anchor which rotates with a certain period. A sensor node
detects the light beam for a period of time, which is 
dependent on the distance between it and the light emitting device.
Many of the above localization solutions target specific
sets of requirements and are useful for specific applications.
StarDust differs in that it addresses a particular demanding
set of requirements that are not yet solved well. StarDust is
meant for localizing air dropped nodes where node 
passiveness, high accuracy, low cost, small form factor and rapid 
localization are all required. Many military applications have
such requirements.
3 StarDust System Design
The design of the StarDust system (and its name) was 
inspired by the similarity between a deployed sensor network,
in which sensor nodes indicate their presence by emitting
light, and the Universe consisting of luminous and 
illuminated objects: stars, galaxies, planets, etc.
The main difficulty when applying the above ideas to the
real world is the complexity of the hardware that needs to
be put on a sensor node so that the emitted light can be 
detected from thousands of feet. The energy expenditure for
producing an intense enough light beam is also prohibitive.
Instead, what we propose to use for sensor node 
localization is a passive optical element called a retro-reflector.
The most common retro-reflective optical component is a
Corner-Cube Retroreflector (CCR), shown in Figure 1(a). It
consists of three mutually perpendicular mirrors. The 
inter58
(a) (b)
Figure 1. Corner-Cube Retroreflector (a) and an array of
CCRs molded in plastic (b)
esting property of this optical component is that an incoming
beam of light is reflected back, towards the source of the
light, irrespective of the angle of incidence. This is in 
contrast with a mirror, which needs to be precisely positioned to
be perpendicular to the incident light. A very common and
inexpensive implementation of an array of CCRs is the 
retroreflective plastic material used on cars and bicycles for night
time detection, shown in Figure 1(b).
In the StarDust system, each node is equipped with a
small (e.g. 0.5in2) array of CCRs and the enclosure has
self-righting capabilities that orient the array of CCRs 
predominantly upwards. It is critical to understand that the 
upward orientation does not need to be exact. Even when large
angular variations from a perfectly upward orientation are
present, a CCR will return the light in the exact same 
direction from which it came.
In the remaining part of the section, we present the 
architecture of the StarDust system and the design of its main
components.
3.1 System Architecture
The envisioned sensor network localization scenario is as
follows:
• The sensor nodes are released, possibly in a controlled
manner, from an aerial vehicle during the night.
• The aerial vehicle hovers over the deployment area and
uses a strobe light to illuminate it. The sensor nodes,
equipped with CCRs and optical filters (acting as 
coloring devices) have self-righting capabilities and 
retroreflect the incoming strobe light. The retro-reflected
light is either white, as the originating source light,
or colored, due to optical filters.
• The aerial vehicle records a sequence of two images
very close in time (msec level). One image is taken
when the strobe light is on, the other when the strobe
light is off. The acquired images are used for obtaining
the locations of sensor nodes (which appear as luminous
spots in the image).
• The aerial vehicle executes the mapping of node IDs to
the identified locations in one of the following ways: a)
by using the color of a retro-reflected light, if a sensor
node has a unique color; b) by requiring sensor nodes
to establish neighborhood information and report it to
a base station; c) by controlling the time sequence of
sensor nodes deployment and recording additional 
imLight Emitter
Sensor Node i
Transfer Function
Φi(λ)
Ψ(λ)
Φ(Ψ(λ))
Image
Processing
Node ID Matching
Radio Model
R
G(Λ,E)
Central Device
V"
V"
Figure 2. The StarDust system architecture
ages; d) by controlling the location where a sensor node
is deployed.
• The computed locations are disseminated to the sensor
network.
The architecture of the StarDust system is shown in 
Figure 2. The architecture consists of two main components:
the first is centralized and it is located on a more powerful
device. The second is distributed and it resides on all 
sensor nodes. The Central Device consists of the following: the
Light Emitter, the Image Processing module, the Node ID
Mapping module and the Radio Model. The distributed 
component of the architecture is the Transfer Function, which
acts as a filter for the incoming light. The aforementioned
modules are briefly described below:
• Light Emitter - It is a strobe light, capable of producing
very intense, collimated light pulses. The emitted light
is non-monochromatic (unlike a laser) and it is 
characterized by a spectral density Ψ(λ), a function of the
wavelength. The emitted light is incident on the CCRs
present on sensor nodes.
• Transfer Function Φ(Ψ(λ)) - This is a bandpass filter
for the incident light on the CCR. The filter allows a
portion of the original spectrum, to be retro-reflected.
From here on, we will refer to the transfer function as
the color of a sensor node.
• Image Processing - The Image Processing module 
acquires high resolution images. From these images the
locations and the colors of sensor nodes are obtained.
If only one set of pictures can be taken (i.e., one 
location of the light emitter/image analysis device), then the
map of the field is assumed to be known as well as the
distance between the imaging device and the field. The
aforementioned assumptions (field map and distance to
it) are not necessary if the images can be simultaneously
taken from different locations. It is important to remark
here that the identity of a node can not be directly 
obtained through Image Processing alone, unless a 
specific characteristic of a sensor node can be identified in
the image.
• Node ID Matching - This module uses the detected 
locations and through additional techniques (e.g., sensor
node coloring and connectivity information (G(Λ,E))
from the deployed network) to uniquely identify the
sensor nodes observed in the image. The connectivity
information is represented by neighbor tables sent from
59
Algorithm 1 Image Processing
1: Background filtering
2: Retro-reflected light recognition through intensity 
filtering
3: Edge detection to obtain the location of sensor nodes
4: Color identification for each detected sensor node
each sensor node to the Central Device.
• Radio Model - This component provides an estimate of
the radio range to the Node ID Matching module. It
is only used by node ID matching techniques that are
based on the radio connectivity in the network. The 
estimate of the radio range R is based on the sensor node
density (obtained through the Image Processing 
module) and the connectivity information (i.e., G(Λ,E)).
The two main components of the StarDust architecture
are the Image Processing and the Node ID Mapping. Their
design and analysis is presented in the sections that follow.
3.2 Image Processing
The goal of the Image Processing Algorithm (IPA) is to
identify the location of the nodes and their color. Note that
IPA does not identify which node fell where, but only what
is the set of locations where the nodes fell.
IPA is executed after an aerial vehicle records two 
pictures: one in which the field of deployment is illuminated and
one when no illuminations is present. Let Pdark be the 
picture of the deployment area, taken when no light was emitted
and Plight be the picture of the same deployment area when a
strong light beam was directed towards the sensor nodes.
The proposed IPA has several steps, as shown in 
Algorithm 1. The first step is to obtain a third picture Pfilter where
only the differences between Pdark and Plight remain. Let us
assume that Pdark has a resolution of n × m, where n is the
number of pixels in a row of the picture, while m is the 
number of pixels in a column of the picture. Then Pdark is 
composed of n × m pixels noted Pdark(i, j), i ∈ 1 ≤ i ≤ n,1 ≤
j ≤ m. Similarly Plight is composed of n × m pixels noted
Plight(i, j), 1 ≤ i ≤ n,1 ≤ j ≤ m.
Each pixel P is described by an RGB value where the R
value is denoted by PR, the G value is denoted by PG, and
the B value is denoted by PB. IPA then generates the third
picture, Pfilter, through the following transformations:
PR
filter(i, j) = PR
light(i, j)−PR
dark(i, j)
PG
filter(i, j) = PG
light(i, j)−PG
dark(i, j)
PB
filter(i, j) = PB
light(i, j)−PB
dark(i, j)
(1)
After this transformation, all the features that appeared in
both Pdark and Plight are removed from Pfilter. This simplifies
the recognition of light retro-reflected by sensor nodes.
The second step consists of identifying the elements 
contained in Pfilter that retro-reflect light. For this, an intensity
filter is applied to Pfilter. First IPA converts Pfilter into a
grayscale picture. Then the brightest pixels are identified and
used to create Preflect. This step is eased by the fact that the
reflecting nodes should appear much brighter than any other
illuminated object in the picture.
Support: Q(λk)
ni
P1
...
P2
...
PN
λ1
...
λk
...
λN
Figure 3. Probabilistic label relaxation
The third step runs an edge detection algorithm on Preflect
to identify the boundary of the nodes present. A tool such as
Matlab provides a number of edge detection techniques. We
used the bwboundaries function. For the obtained edges, the
location (x,y) (in the image) of each node is determined by
computing the centroid of the points constituting its edges.
Standard computer graphics techniques [23] are then used
to transform the 2D locations of sensor nodes detected in
multiple images into 3D sensor node locations. The color of
the node is obtained as the color of the pixel located at (x,y)
in Plight.
3.3 Node ID Matching
The goal of the Node ID Matching module is to 
obtain the identity (node ID) of a luminous spot in the 
image, detected to be a sensor node. For this, we define V =
{(x1,y1),(x2,y2),...,(xm,ym)} to be the set of locations of
the sensor nodes, as detected by the Image Processing 
module and Λ = {λ1,λ2,...,λm} to be the set of unique node IDs
assigned to the m sensor nodes, before deployment. From
here on, we refer to node IDs as labels.
We model the problem of finding the label λj of a node ni
as a probabilistic label relaxation problem, frequently used
in image processing/understanding. In the image processing
domain, scene labeling (i.e., identifying objects in an 
image) plays a major role. The goal of scene labeling is to
assign a label to each object detected in an image, such that
an appropriate image interpretation is achieved. It is 
prohibitively expensive to consider the interactions among all
the objects in an image. Instead, constraints placed among
nearby objects generate local consistencies and through 
iteration, global consistencies can be obtained.
The main idea of the sensor node localization through
probabilistic label relaxation is to iteratively compute the
probability of each label being the correct label for a 
sensor node, by taking into account, at each iteration, the 
support for a label. The support for a label can be understood
as a hint or proof, that a particular label is more likely to be
the correct one, when compared with the other potential 
labels for a sensor node. We pictorially depict this main idea
in Figure 3. As shown, node ni has a set of candidate 
labels {λ1,...,λk}. Each of the labels has a different value
for the Support function Q(λk). We defer the explanation
of how the Support function is implemented until the 
subsections that follow, where we provide four concrete 
techniques. Formally, the algorithm is outlined in Algorithm 2,
where the equations necessary for computing the new 
probability Pni(λk) for a label λk of a node ni, are expressed by the
60
Algorithm 2 Label Relaxation
1: for each sensor node ni do
2: assign equal prob. to all possible labels
3: end for
4: repeat
5: converged ← true
6: for each sensor node ni do
7: for each each label λj of ni do
8: compute the Support label λj: Equation 4
9: end for
10: compute K for the node ni: Equation 3
11: for each each label λj do
12: update probability of label λj: Equation 2
13: if |new prob.−old prob.| ≥ ε then
14: converged ← false
15: end if
16: end for
17: end for
18: until converged = true
following equations:
Ps+1
ni
(λk) =
1
Kni
Ps
ni
(λk)Qs
ni
(λk) (2)
where Kni is a normalizing constant, given by:
Kni =
N
∑
k=1
Ps
ni
(λk)Qs
ni
(λk) (3)
and Qs
ni
(λk) is:
Qs
ni
(λk) = support for label λk of node ni (4)
The label relaxation algorithm is iterative and it is 
polynomial in the size of the network(number of nodes). The
pseudo-code is shown in Algorithm 2. It initializes the 
probabilities associated with each possible label, for a node ni,
through a uniform distribution. At each iteration s, the 
algorithm updates the probability associated with each label, by
considering the Support Qs
ni
(λk) for each candidate label of
a sensor node.
In the sections that follow, we describe four different 
techniques for implementing the Support function: based on
node coloring, radio connectivity, the time of deployment
(time) and the location of deployment (space). While some
of these techniques are simplistic, they are primitives which,
when combined, can create powerful localization systems.
These design techniques have different trade-offs, which we
will present in Section 3.3.6.
3.3.1 Relaxation with Color Constraints
The unique mapping between a sensor node"s position
(identified by the image processing) and a label can be 
obtained by assigning a unique color to each sensor node. For
this we define C = {c1,c2,...,cn} to be the set of unique 
colors available and M : Λ → C to be a one-to-one mapping of
labels to colors. This mapping is known prior to the sensor
node deployment (from node manufacturing).
In the case of color constrained label relaxation, the 
support for label λk is expressed as follows:
Qs
ni
(λk) = 1 (5)
As a result, the label relaxation algorithm (Algorithm 2)
consists of the following steps: one label is assigned to each
sensor node (lines 1-3 of the algorithm), implicitly having
a probability Pni(λk) = 1 ; the algorithm executes a single
iteration, when the support function, simply, reiterates the
confidence in the unique labeling.
However, it is often the case that unique colors for each
node will not be available. It is interesting to discuss here the
influence that the size of the coloring space (i.e., |C|) has on
the accuracy of the localization algorithm. Several cases are
discussed below:
• If |C| = 0, no colors are used and the sensor nodes are
equipped with simple CCRs that reflect back all the 
incoming light (i.e., no filtering, and no coloring of the 
incoming light). From the image processing system, the
position of sensor nodes can still be obtained. Since
all nodes appear white, no single sensor node can be
uniquely identified.
• If |C| = m − 1 then there are enough unique colors for
all nodes (one node remains white, i.e. no coloring), the
problem is trivially solved. Each node can be identified,
based on its unique color. This is the scenario for the
relaxation with color constraints.
• If |C| ≥ 1, there are several options for how to 
partition the coloring space. If C = {c1} one possibility is
to assign the color c1 to a single node, and leave the 
remaining m−1 sensor nodes white, or to assign the color
c1 to more than one sensor node. One can observe that
once a color is assigned uniquely to a sensor node, in
effect, that sensor node is given the status of anchor,
or node with known location.
It is interesting to observe that there is an entire spectrum
of possibilities for how to partition the set of sensor nodes
in equivalence classes (where an equivalence class is 
represented by one color), in order to maximize the success of the
localization algorithm. One of the goals of this paper is to
understand how the size of the coloring space and its 
partitioning affect localization accuracy.
Despite the simplicity of this method of constraining the
set of labels that can be assigned to a node, we will show that
this technique is very powerful, when combined with other
relaxation techniques.
3.3.2 Relaxation with Connectivity Constraints
Connectivity information, obtained from the sensor 
network through beaconing, can provide additional information
for locating sensor nodes. In order to gather connectivity 
information, the following need to occur: 1) after deployment,
through beaconing of HELLO messages, sensor nodes build
their neighborhood tables; 2) each node sends its neighbor
table information to the Central device via a base station.
First, let us define G = (Λ,E) to be the weighted 
connectivity graph built by the Central device from the received
neighbor table information. In G the edge (λi,λj) has a
61
λ1
λ2
...
λN
ni nj
gi2,j2
λ1
λ2
...
λN
Pj,λ1
Pj,λ2
...
Pj,λN
Pi,λ1
Pi,λ1
...
Pi,λN gi2,jm
Figure 4. Label relaxation with connectivity constraints
weight gij represented by the number of beacons sent by λj
and received by λi. In addition, let R be the radio range of
the sensor nodes.
The main idea of the connectivity constrained label 
relaxation is depicted in Figure 4 in which two nodes ni and
nj have been assigned all possible labels. The confidence in
each of the candidate labels for a sensor node, is represented
by a probability, shown in a dotted rectangle.
It is important to remark that through beaconing and the
reporting of neighbor tables to the Central Device, a global
view of all constraints in the network can be obtained. It
is critical to observe that these constraints are among labels.
As shown in Figure 4 two constraints exist between nodes ni
and nj. The constraints are depicted by gi2,j2 and gi2,jM, the
number of beacons sent the labels λj2 and λjM and received
by the label λi2.
The support for the label λk of sensor node ni, resulting
from the interaction (i.e., within radio range) with sensor
node nj is given by:
Qs
ni
(λk) =
M
∑
m=1
gλkλm
Ps
nj
(λm) (6)
As a result, the localization algorithm (Algorithm 3 
consists of the following steps: all labels are assigned to each
sensor node (lines 1-3 of the algorithm), and implicitly each
label has a probability initialized to Pni(λk) = 1/|Λ|; in each
iteration, the probabilities for the labels of a sensor node are
updated, when considering the interaction with the labels of
sensor nodes within R. It is important to remark that the 
identity of the nodes within R is not known, only the candidate
labels and their probabilities. The relaxation algorithm 
converges when, during an iteration, the probability of no label
is updated by more than ε.
The label relaxation algorithm based on connectivity 
constraints, enforces such constraints between pairs of sensor
nodes. For a large scale sensor network deployment, it is not
feasible to consider all pairs of sensor nodes in the network.
Hence, the algorithm should only consider pairs of sensor
nodes that are within a reasonable communication range (R).
We assume a circular radio range and a symmetric 
connectivity. In the remaining part of the section we propose a
simple analytical model that estimates the radio range R for
medium-connected networks (less than 20 neighbors per R).
We consider the following to be known: the size of the 
deployment field (L), the number of sensor nodes deployed (N)
Algorithm 3 Localization
1: Estimate the radio range R
2: Execute the Label Relaxation Algorithm with Support
Function given by Equation 6 for neighbors less than R
apart
3: for each sensor node ni do
4: node identity is λk with max. prob.
5: end for
and the total number of unidirectional (i.e., not symmetric)
one-hop radio connections in the network (k). For our 
analysis, we uniformly distribute the sensor nodes in a square area
of length L, by using a grid of unit length L/
√
N. We use the
substitution u = L/
√
N to simplify the notation, in order to
distinguish the following cases: if u ≤ R ≤
√
2u each node
has four neighbors (the expected k = 4N); if
√
2u ≤ R ≤ 2u
each node has eight neighbors (the expected k = 8N); if
2u ≤ R ≤
√
5u each node has twelve neighbors ( the expected
k = 12N); if
√
5u ≤ R ≤ 3u each node has twenty neighbors
(the expected k = 20N)
For a given t = k/4N we take R to be the middle of the
interval. As an example, if t = 5 then R = (3 +
√
5)u/2. A
quadratic fitting for R over the possible values of t, produces
the following closed-form solution for the communication
range R, as a function of network connectivity k, assuming L
and N constant:
R(k) =
L
√
N
−0.051
k
4N
2
+0.66
k
4N
+0.6 (7)
We investigate the accuracy of our model in Section 5.2.1.
3.3.3 Relaxation with Time Constraints
Time constraints can be treated similarly with color 
constraints. The unique identification of a sensor node can be
obtained by deploying sensor nodes individually, one by one,
and recording a sequence of images. The sensor node that is
identified as new in the last picture (it was not identified in
the picture before last) must be the last sensor node dropped.
In a similar manner with color constrained label 
relaxation, the time constrained approach is very simple, but may
take too long, especially for large scale systems. While it
can be used in practice, it is unlikely that only a time 
constrained label relaxation is used. As we will see, by 
combining constrained-based primitives, realistic localization 
systems can be implemented.
The support function for the label relaxation with time
constraints is defined identically with the color constrained
relaxation:
Qs
ni
(λk) = 1 (8)
The localization algorithm (Algorithm 2 consists of the
following steps: one label is assigned to each sensor node
(lines 1-3 of the algorithm), and implicitly having a 
probability Pni(λk) = 1 ; the algorithm executes a single iteration,
62
D1
D2
D4
D
3Node
Label-1
Label-2
Label-3
Label-4
0.2
0.1
0.5
0.2
Figure 5. Relaxation with space
constraints
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0 1 2 3 4 5 6 7 8
PDF
Distance D
σ = 0.5
σ = 1
σ = 2
Figure 6. Probability distribution of
distances
-4
-3
-2
-1
0
1
2
3
4
X
-4
-3
-2
-1
0
1
2
3
4
Y
0
0.2
0.4
0.6
0.8
1
Node Density
Figure 7. Distribution of nodes
when the support function, simply, reiterates the confidence
in the unique labeling.
3.3.4 Relaxation with Space Constraints
Spatial information related to sensor deployment can also
be employed as another input to the label relaxation 
algorithm. To do that, we use two types of locations: the node 
location pn and the label location pl. The former pn is defined
as the position of nodes (xn,yn,zn) after deployment, which
can be obtained through Image Processing as mentioned in
Section 3.3. The latter pl is defined as the location (xl,yl,zl)
where a node is dropped. We use Dni
λm
to denote the 
horizontal distance between the location of the label λm and the 
location of the node ni. Clearly, Dni
λm
= (xn −xl)2 +(yn −yl)2.
At the time of a sensor node release, the one-to-one 
mapping between the node and its label is known. In other words,
the label location is the same as the node location at the 
release time. After release, the label location information is
partially lost due to the random factors such as wind and 
surface impact. However, statistically, the node locations are
correlated with label locations. Such correlation depends on
the airdrop methods employed and environments. For the
sake of simplicity, let"s assume nodes are dropped from the
air through a helicopter hovering in the air. Wind can be 
decomposed into three components X,Y and Z. Only X and
Y affect the horizontal distance a node can travel. 
According to [24], we can assume that X and Y follow an 
independent normal distribution. Therefore, the absolute value of
the wind speed follows a Rayleigh distribution. Obviously
the higher the wind speed is, the further a node would land
away horizontally from the label location. If we assume that
the distance D is a function of the wind speed V [25] [26],
we can obtain the probability distribution of D under a given
wind speed distribution. Without loss of generality, we 
assume that D is proportional to the wind speed. Therefore,
D follows the Rayleigh distribution as well. As shown in
Figure 5, the spatial-based relaxation is a recursive process
to assign the probability that a nodes has a certain label by
using the distances between the location of a node with 
multiple label locations.
We note that the distribution of distance D affects the
probability with which a label is assigned. It is not 
necessarily true that the nearest label is always chosen. For example,
if D follows the Rayleigh(σ2) distribution, we can obtain the
Probability Density Function (PDF) of distances as shown
in Figure 6. This figure indicates that the possibility of a
node to fall vertically is very small under windy conditions
(σ > 0), and that the distance D is affected by the σ. The
spatial distribution of nodes for σ = 1 is shown in Figure 7.
Strong wind with a high σ value leads to a larger node 
dispersion. More formally, given a probability density function
PDF(D), the support for label λk of sensor node ni can be
formulated as:
Qs
ni
(λk) = PDF(Dni
λk
) (9)
It is interesting to point out two special cases. First, if all
nodes are released at once (i.e., only one label location for
all released nodes), the distance D from a node to all labels
is the same. In this case, Ps+1
ni
(λk) = Ps
ni
(λk), which indicates
that we can not use the spatial-based relaxation to recursively
narrow down the potential labels for a node. Second, if nodes
are released at different locations that are far away from each
other, we have: (i) If node ni has label λk, Ps
ni
(λk) → 1 when
s → ∞, (ii) If node ni does not have label λk, Ps
ni
(λk) → 0
when s → ∞. In this second scenario, there are multiple 
labels (one label per release), hence it is possible to correlate
release times (labels) with positions on the ground. These 
results indicate that spatial-based relaxation can label the node
with a very high probability if the physical separation among
nodes is large.
3.3.5 Relaxation with Color and Connectivity 
Constraints
One of the most interesting features of the StarDust 
architecture is that it allows for hybrid localization solutions to be
built, depending on the system requirements. One example
is a localization system that uses the color and connectivity
constraints. In this scheme, the color constraints are used for
reducing the number of candidate labels for sensor nodes,
to a more manageable value. As a reminder, in the 
connectivity constrained relaxation, all labels are candidate labels
for each sensor node. The color constraints are used in the
initialization phase of Algorithm 3 (lines 1-3). After the 
initialization, the standard connectivity constrained relaxation
algorithm is used.
For a better understanding of how the label relaxation 
algorithm works, we give a concrete example, exemplified in
Figure 8. In part (a) of the figure we depict the data structures
63
11
8
4
1
12
9
7
5
3
ni nj
12
8
10
11
10
0.2
0.2
0.2
0.2
0.2
0.25
0.25
0.25
0.25
(a)
11
8
4
1
12
9
7
5
3
ni nj
12
8
10
11
10
0.2
0.2
0.2
0.2
0.2
0.32
0
0.68
0
(b)
Figure 8. A step through the algorithm. After 
initialization (a) and after the 1st iteration for node ni (b)
associated with nodes ni and nj after the initialization steps
of the algorithm (lines 1-6), as well as the number of beacons
between different labels (as reported by the network, through
G(Λ,E)). As seen, the potential labels (shown inside the 
vertical rectangles) are assigned to each node. Node ni can be
any of the following: 11,8,4,1. Also depicted in the figure
are the probabilities associated with each of the labels. After
initialization, all probabilities are equal.
Part (b) of Figure 8 shows the result of the first iteration
of the localization algorithm for node ni, assuming that node
nj is the first wi chosen in line 7 of Algorithm 3. By using
Equation 6, the algorithm computes the support Q(λi) for
each of the possible labels for node ni. Once the Q(λi)"s
are computed, the normalizing constant, given by Equation
3 can be obtained. The last step of the iteration is to update
the probabilities associated with all potential labels of node
ni, as given by Equation 2.
One interesting problem, which we explore in the 
performance evaluation section, is to assess the impact the 
partitioning of the color set C has on the accuracy of 
localization. When the size of the coloring set is smaller than the
number of sensor nodes (as it is the case for our hybrid 
connectivity/color constrained relaxation), the system designer
has the option of allowing one node to uniquely have a color
(acting as an anchor), or multiple nodes. Intuitively, by 
assigning one color to more than one node, more constraints
(distributed) can be enforced.
3.3.6 Relaxation Techniques Analysis
The proposed label relaxation techniques have different
trade-offs. For our analysis of the trade-offs, we consider
the following metrics of interest: the localization time 
(duration), the energy consumed (overhead), the network size
(scale) that can be handled by the technique and the 
localization accuracy. The parameters of interest are the following:
the number of sensor nodes (N), the energy spent for one
aerial drop (εd), the energy spent in the network for 
collecting and reporting neighbor information εb and the time Td
taken by a sensor node to reach the ground after being 
aerially deployed. The cost comparison of the different label
relaxation techniques is shown in Table 1.
As shown, the relaxation techniques based on color and
space constraints have the lowest localization duration, zero,
for all practical purposes. The scalability of the color based
relaxation technique is, however, limited to the number of
(a) (b)
Figure 9. SensorBall with self-righting capabilities (a)
and colored CCRs (b)
unique color filters that can be built. The narrower the 
Transfer Function Ψ(λ), the larger the number of unique colors
that can be created. The manufacturing costs, however, are
increasing as well. The scalability issue is addressed by all
other label relaxation techniques. Most notably, the time
constrained relaxation, which is very similar to the 
colorconstrained relaxation, addresses the scale issue, at a higher
deployment cost.
Criteria Color Connectivity Time Space
Duration 0 NTb NTd 0
Overhead εd εd +Nεb Nεd εd
Scale |C| |N| |N| |N|
Accuracy High Low High Medium
Table 1. Comparison of label relaxation techniques
4 System Implementation
The StarDust localization framework, depicted in Figure
2, is flexible in that it enables the development of new 
localization systems, based on the four proposed label 
relaxation schemes, or the inclusion of other, yet to be invented,
schemes. For our performance evaluation we implemented a
version of the StarDust framework, namely the one proposed
in Section 3.3.5, where the constraints are based on color and
connectivity.
The Central device of the StarDust system consists of the
following: the Light Emitter - we used a 
common-off-theshelf flash light (QBeam, 3 million candlepower); the 
image acquisition was done with a 3 megapixel digital camera
(Sony DSC-S50) which provided the input to the Image 
Processing algorithm, implemented in Matlab.
For sensor nodes we built a custom sensor node, called
SensorBall, with self-righting capabilities, shown in Figure
9(a). The self-righting capabilities are necessary in order to
orient the CCR predominantly upwards. The CCRs that we
used were inexpensive, plastic molded, night time warning
signs commonly available on bicycles, as shown in Figure
9(b). We remark here the low quality of the CCRs we used.
The reflectivity of each CCR (there are tens molded in the
plastic container) is extremely low, and each CCR is not built
with mirrors. A reflective effect is achieved by employing
finely polished plastic surfaces. We had 5 colors available,
in addition to the standard CCR, which reflects all the 
incoming light (white CCR). For a slightly higher price (ours
were 20cents/piece), better quality CCRs can be employed.
64
Figure 10. The field in the dark Figure 11. The illuminated field Figure 12. The difference: Figure 
10Figure 11
Higher quality (better mirrors) would translate in more 
accurate image processing (better sensor node detection) and
smaller form factor for the optical component (an array of
CCRs with a smaller area can be used).
The sensor node platform we used was the micaZ mote.
The code that runs on each node is a simple application
which broadcasts 100 beacons, and maintains a neighbor 
table containing the percentage of successfully received 
beacons, for each neighbor. On demand, the neighbor table is
reported to a base station, where the node ID mapping is 
performed.
5 System Evaluation
In this section we present the performance evaluation of
a system implementation of the StarDust localization 
framework. The three major research questions that our evaluation
tries to answer are: the feasibility of the proposed framework
(can sensor nodes be optically detected at large distances),
the localization accuracy of one actual implementation of the
StarDust framework, and whether or not atmospheric 
conditions can affect the recognition of sensor nodes in an 
image. The first two questions are investigated by evaluating
the two main components of the StarDust framework: the
Image Processing and the Node ID Matching. These 
components have been evaluated separately mainly because of
lack of adequate facilities. We wanted to evaluate the 
performance of the Image Processing Algorithm in a long range,
realistic, experimental set-up, while the Node ID Matching
required a relatively large area, available for long periods of
time (for connectivity data gathering). The third research
question is investigated through a computer modeling of 
atmospheric phenomena.
For the evaluation of the Image Processing module, we
performed experiments in a football stadium where we 
deploy 6 sensor nodes in a 3×2 grid. The distance between the
Central device and the sensor nodes is approximately 500 ft.
The metrics of interest are the number of false positives and
false negatives in the Image Processing Algorithm.
For the evaluation of the Node ID Mapping component,
we deploy 26 sensor nodes in an 120 × 60 ft2 flat area of
a stadium. In order to investigate the influence the radio
connectivity has on localization accuracy, we vary the height
above ground of the deployed sensor nodes. Two set-ups are
used: one in which the sensor nodes are on the ground, and
the second one, in which the sensor nodes are raised 3 inches
above ground. From here on, we will refer to these two
experimental set-ups as the low connectivity and the high
connectivity networks, respectively because when nodes are
on the ground the communication range is low resulting in
less neighbors than when the nodes are elevated and have a
greater communication range. The metrics of interest are:
the localization error (defined as the distance between the
computed location and the true location - known from the
manual placement), the percentage of nodes correctly 
localized, the convergence of the label relaxation algorithm, the
time to localize and the robustness of the node ID mapping
to errors in the Image Processing module.
The parameters that we vary experimentally are: the 
angle under which images are taken, the focus of the camera,
and the degree of connectivity. The parameters that we vary
in simulations (subsequent to image acquisition and 
connectivity collection) the number of colors, the number of 
anchors, the number of false positives or negatives as input
to the Node ID Matching component, the distance between
the imaging device and sensor network (i.e., range), 
atmospheric conditions (light attenuation coefficient) and CCR
reflectance (indicative of its quality).
5.1 Image Processing
For the IPA evaluation, we deploy 6 sensor nodes in a
3 × 2 grid. We take 13 sets of pictures using different 
orientations of the camera and different zooming factors. All
pictures were taken from the same location. Each set is 
composed of a picture taken in the dark and of a picture taken
with a light beam pointed at the nodes. We process the 
pictures offline using a Matlab implementation of IPA. Since we
are interested in the feasibility of identifying colored sensor
nodes at large distance, the end result of our IPA is the 2D
location of sensor nodes (position in the image). The 
transformation to 3D coordinates can be done through standard
computer graphics techniques [23].
One set of pictures obtained as part of our experiment is
shown in Figures 10 and 11. The execution of our IPA 
algorithm results in Figure 12 which filters out the background,
and Figure 13 which shows the output of the edge detection
step of IPA. The experimental results are depicted in 
Figure 14. For each set of pictures the graph shows the number
of false positives (the IPA determines that there is a node
65
Figure 13. Retroreflectors detected in Figure 12
0
1
2
3
1 2 3 4 5 6 7 8 9 10 11
Experiment Number
Count
False Positives
False Negatives
Figure 14. False Positives and Negatives for the 6 nodes
while there is none), and the number of false negatives (the
IPA determines that there is no node while there is one). In
about 45% of the cases, we obtained perfect results, i.e., no
false positives and no false negatives. In the remaining cases,
we obtained a number of false positives of at most one, and
a number of false negatives of at most two.
We exclude two pairs of pictures from Figure 14. In the
first excluded pair, we obtain 42 false positives and in the
second pair 10 false positives and 7 false negatives. By 
carefully examining the pictures, we realized that the first pair
was taken out of focus and that a car temporarily appeared
in one of the pictures of the second pair. The anomaly in
the second set was due to the fact that we waited too long to
take the second picture. If the pictures had been taken a few
milliseconds apart, the car would have been represented on
either both or none of the pictures and the IPA would have
filtered it out.
5.2 Node ID Matching
We evaluate the Node ID Matching component of our 
system by collecting empirical data (connectivity information)
from the outdoor deployment of 26 nodes in the 120×60 ft2
area. We collect 20 sets of data for the high connectivity
and low connectivity network deployments. Off-line we 
investigate the influence of coloring on the metrics of interest,
by randomly assigning colors to the sensor nodes. For one
experimental data set we generate 50 random assignments
of colors to sensor nodes. It is important to observe that, for
the evaluation of the Node ID Matching algorithm (color and
connectivity constrained), we simulate the color assignment
to sensor nodes. As mentioned in Section 4 the size of the
coloring space available to us was 5 (5 colors). Through 
simulations of color assignment (not connectivity) we are able
to investigate the influence that the size of the coloring space
has on the accuracy of localization. The value of the 
param0
10
20
30
40
50
60
0 10 20 30 40 50 60 70 80 90
Distance [feet]
Count
Connected
Not Connected
Figure 15. The number of existing and missing radio 
connections in the sparse connectivity experiment
0
10
20
30
40
50
60
70
0 10 20 30 40 50 60 70 80 90 10 11 12
Distance [feet]
Count
Connected
Not Connected
Figure 16. The number of existing and missing radio 
connections in the high connectivity experiment
eter ε used in Algorithm 2 was 0.001. The results presented
here represent averages over the randomly generated 
colorings and over all experimental data sets.
We first investigate the accuracy of our proposed Radio
Model, and subsequently use the derived values for the radio
range in the evaluation of the Node ID matching component.
5.2.1 Radio Model
From experiments, we obtain the average number of 
observed beacons (k, defined in Section 3.3.2) for the low 
connectivity network of 180 beacons and for the high 
connectivity network of 420 beacons. From our Radio Model 
(Equation 7, we obtain a radio range R = 25 ft for the low 
connectivity network and R = 40 ft for the high connectivity 
network.
To estimate the accuracy of our simple model, we plot
the number of radio links that exist in the networks, and the
number of links that are missing, as functions of the distance
between nodes. The results are shown in Figures 15 and 16.
We define the average radio range R to be the distance over
which less than 20% of potential radio links, are missing.
As shown in Figure 15, the radio range is between 20 ft and
25 ft. For the higher connectivity network, the radio range
was between 30 ft and 40 ft.
We choose two conservative estimates of the radio range:
20 ft for the low connectivity case and 35 ft for the high 
connectivity case, which are in good agreement with the values
predicted by our Radio Model.
5.2.2 Localization Error vs. Coloring Space Size
In this experiment we investigate the effect of the number
of colors on the localization accuracy. For this, we randomly
assign colors from a pool of a given size, to the sensor nodes.
66
0
5
10
15
20
25
30
35
40
45
0 5 10 15 20
Number of Colors
LocalizationError[feet]
R=15feet
R=20feet
R=25feet
Figure 17. Localization error
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15 20
Number of Colors
%CorrectLocalized[x100]
R=15feet
R=20feet
R=25feet
Figure 18. Percentage of nodes correctly localized
We then execute the localization algorithm, which uses the
empirical data. The algorithm is run for three different radio
ranges: 15, 20 and 25 ft, to investigate its influence on the
localization error.
The results are depicted in Figure 17 (localization error)
and Figure 18 (percentage of nodes correctly localized). As
shown, for an estimate of 20 ft for the radio range (as 
predicted by our Radio Model) we obtain the smallest 
localization errors, as small as 2 ft, when enough colors are used.
Both Figures 17 and 18 confirm our intuition that a larger
number of colors available significantly decrease the error in
localization.
The well known fact that relaxation algorithms do not 
always converge, was observed during our experiments. The
percentage of successful runs (when the algorithm 
converged) is depicted in Figure 19. As shown, in several 
situations, the algorithm failed to converge (the algorithm 
execution was stopped after 100 iterations per node). If the 
algorithm does not converge in a predetermined number of steps,
it will terminate and the label with the highest probability
will provide the identity of the node. It is very probable that
the chosen label is incorrect, since the probabilities of some
of labels are constantly changing (with each iteration).The
convergence of relaxation based algorithms is a well known
issue.
5.2.3 Localization Error vs. Color Uniqueness
As mentioned in the Section 3.3.1, a unique color gives a
sensor node the statute of an anchor. A sensor node that is
an anchor can unequivocally be identified through the Image
Processing module. In this section we investigate the effect
unique colors have on the localization accuracy. Specifically,
we want to experimentally verify our intuition that assigning
more nodes to a color can benefit the localization accuracy,
by enforcing more constraints, as opposed to uniquely 
assigning a color to a single node.
90
95
100
105
0 5 10 15 20
Number of Colors
ConvergenceRate[x100]
R=15feet
R=20feet
R=25feet
Figure 19. Convergence error
0
2
4
6
8
10
12
14
16
4 6 8
Number of Colors
LocalizationError[feet]
0 anchors
2 anchors
4 anchors
6 anchors
8 anchors
Figure 20. Localization error vs. number of colors
For this, we fix the number of available colors to either 4,
6 or 8 and vary the number of nodes that are given unique
colors, from 0, up to the maximum number of colors (4, 6 or
8). Naturally, if we have a maximum number of colors of 4,
we can assign at most 4 anchors. The experimental results
are depicted in Figure 20 (localization error) and Figure 21
(percentage of sensor node correctly localized). As expected,
the localization accuracy increases with the increase in the
number of colors available (larger coloring space). Also, for
a given size of the coloring space (e.g., 6 colors available), if
more colors are uniquely assigned to sensor nodes then the
localization accuracy decreases. It is interesting to observe
that by assigning colors uniquely to nodes, the benefit of 
having additional colors is diminished. Specifically, if 8 colors
are available and all are assigned uniquely, the system would
be less accurately localized (error ≈ 7 ft), when compared
to the case of 6 colors and no unique assignments of colors
(≈ 5 ft localization error).
The same trend, of a less accurate localization can be 
observed in Figure 21, which shows the percentage of nodes
correctly localized (i.e., 0 ft localization error). As shown, if
we increase the number of colors that are uniquely assigned,
the percentage of nodes correctly localized decreases.
5.2.4 Localization Error vs. Connectivity
We collected empirical data for two network deployments
with different degrees of connectivity (high and low) in 
order to assess the influence of connectivity on location 
accuracy. The results obtained from running our localization
algorithm are depicted in Figure 22 and Figure 23. We 
varied the number of colors available and assigned no anchors
(i.e., no unique assignments of colors).
In both scenarios, as expected, localization error decrease
with an increase in the number of colors. It is interesting
to observe, however, that the low connectivity scenario 
im67
0
20
40
60
80
100
120
140
4 6 8
Number of Colors
%CorrectLocalized[x100]
0 anchors
2 anchors
4 anchors
6 anchors
8 anchors
Figure 21. Percentage of nodes correctly localized vs.
number of colors
0
5
10
15
20
25
30
35
40
45
0 2 4 6 8 10 12
Number of Colors
LocalizationError[feet]
Low Connectivity
High Connectivity
Figure 22. Localization error vs. number of colors
proves the localization accuracy quicker, from the additional
number of colors available. When the number of colors 
becomes relatively large (twelve for our 26 sensor node 
network), both scenarios (low and high connectivity) have 
comparable localization errors, of less that 2 ft. The same trend
of more accurate location information is evidenced by 
Figure 23 which shows that the percentage of nodes that are
localized correctly grows quicker for the low connectivity
deployment.
5.3 Localization Error vs. Image Processing
Errors
So far we investigated the sources for error in 
localization that are intrinsic to the Node ID Matching component.
As previously presented, luminous objects can be 
mistakenly detected to be sensor nodes during the location 
detection phase of the Image Processing module. These false 
positives can be eliminated by the color recognition procedure
of the Image Processing module. More problematic are false
negatives (when a sensor node does not reflect back enough
light to be detected). They need to be handled by the 
localization algorithm. In this case, the localization algorithm
is presented with two sets of nodes of different sizes, that
need to be matched: one coming from the Image Processing
(which misses some nodes) and one coming from the 
network, with the connectivity information (here we assume a
fully connected network, so that all sensor nodes report their
connectivity information). In this experiment we investigate
how Image Processing errors (false negatives) influence the
localization accuracy.
For this evaluation, we ran our localization algorithm with
empirical data, but dropped a percentage of nodes from the
list of nodes detected by the Image Processing algorithm (we
artificially introduced false negatives in the Image 
Process0
10
20
30
40
50
60
70
80
90
100
0 2 4 6 8 10 12
Number of Colors
%CorrectLocalized[x100]
Low Connectivity
High Connectivity
Figure 23. Percentage of nodes correctly localized
0
2
4
6
8
10
12
14
0 4 8 12 16
% False Negatives [x100]
LocalizationError[feet]
4 colors
8 colors
12 colors
Figure 24. Impact of false negatives on the localization
error
ing). The effect of false negatives on localization accuracy is
depicted in Figure 24. As seen in the figure if the number of
false negatives is 15%, the error in position estimation 
doubles when 4 colors are available. It is interesting to observe
that the scenario when more colors are available (e.g., 12 
colors) is being affected more drastically than the scenario with
less colors (e.g., 4 colors). The benefit of having more colors
available is still being maintained, at least for the range of
colors we investigated (4 through 12 colors).
5.4 Localization Time
In this section we look more closely at the duration for
each of the four proposed relaxation techniques and two
combinations of them: color-connectivity and color-time.
We assume that 50 unique color filters can be manufactured,
that the sensor network is deployed from 2,400 ft 
(necessary for the time-constrained relaxation) and that the time
required for reporting connectivity grows linearly, with an
initial reporting period of 160sec, as used in a real world
tracking application [1]. The localization duration results, as
presented in Table 1, are depicted in Figure 25.
As shown, for all practical purposes the time required
by the space constrained relaxation techniques is 0sec. The
same applies to the color constrained relaxation, for which
the localization time is 0sec (if the number of colors is 
sufficient). Considering our assumptions, only for a network of
size 50 the color constrained relaxation works. The 
localization duration for all other network sizes (100, 150 and 200)
is infinite (i.e., unique color assignments to sensor nodes
can not be made, since only 50 colors are unique), when
only color constrained relaxation is used. Both the 
connectivity constrained and time constrained techniques increase
linearly with the network size (for the time constrained, the
Central device deploys sensor nodes one by one, recording
an image after the time a sensor node is expected to reach the
68
0
500
1000
1500
2000
2500
3000
Color Connectivity Time Space 
ColorConenctivity
Color-Time
Localization technique
Localizationtime[sec]
50 nodes
100 nodes
150 nodes
200 nodes
Figure 25. Localization time for different 
label relaxation schemes
0 2000 4000 6000 8000
0
0.5
1
1.5
2
2.5
3
3.5
4
r [feet]
C
r
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 26. Apparent contrast in a
clear atmosphere
0 2000 4000 6000 8000
0
0.5
1
1.5
2
2.5
3
3.5
4
r [feet]
C
r
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 27. Apparent contrast in a
hazing atmosphere
ground).
It is interesting to notice in Figure 25 the improvement in
the localization time obtained by simply combining the color
and the connectivity constrained techniques. The 
localization duration in this case is identical with the connectivity
constrained technique.
The combination of color and time constrained 
relaxations is even more interesting. For a reasonable 
localization duration of 52seconds a perfect (i.e., 0 ft localization
error) localization system can be built. In this scenario, the
set of sensor nodes is split in batches, with each batch 
having a set of unique colors. It would be very interesting to
consider other scenarios, where the strength of the space
constrained relaxation (0sec for any sensor network size) is
used for improving the other proposed relaxation techniques.
We leave the investigation and rigorous classification of such
technique combination for future work.
5.5 System Range
In this section we evaluate the feasibility of the 
StarDust localization framework when considering the realities
of light propagation through the atmosphere.
The main factor that determines the range of our system is
light scattering, which redirects the luminance of the source
into the medium (in essence equally affecting the luminosity
of the target and of the background). Scattering limits the
visibility range by reducing the apparent contrast between
the target and its background (approaches zero, as the 
distance increases). The apparent contrast Cr is quantitatively
expressed by the formula:
Cr = (Nt
r −Nb
r )/Nb
r (10)
where Nt
r and Nb
r are the apparent target radiance and 
apparent background radiance at distance r from the light source,
respectively. The apparent radiance Nt
r of a target at a 
distance r from the light source, is given by:
Nt
r = Na +
Iρte−2σr
πr2
(11)
where I is the intensity of the light source, ρt is the 
target reflectance, σ is the spectral attenuation coefficient (≈
0.12km−1 and ≈ 0.60km−1 for a clear and a hazy 
atmosphere, respectively) and Na is the radiance of the 
atmospheric backscatter, and it can be expressed as follows:
Na =
Gσ2I
2π
2σrZ
0.02σr
e−x
x2
dx (12)
where G = 0.24 is a backscatter gain. The apparent 
background radiance Nb
r is given by formulas similar with 
Equations 11 and 12, where only the target reflectance ρt is 
substituted with the background reflectance ρb. It is important
to remark that when Cr reaches its lower limit, no increase
in the source luminance or receiver sensitivity will increase
the range of the system. From Equations 11 and 12 it can be
observed that the parameter which can be controlled and can
influence the range of the system is ρt, the target reflectance.
Figures 26 and 27 depict the apparent contrast Cr as a
function of the distance r for a clear and for a hazy 
atmosphere, respectively. The apparent contrast is investigated for
reflectance coefficients ρt ranging from 0.3 to 1.0 (perfect 
reflector). For a contrast C of at least 0.5, as it can be seen in
Figure 26 a range of approximately 4,500 ft can be achieved
if the atmosphere is clear. The performance dramatically 
deteriorates, when the atmospheric conditions are problematic.
As shown in Figure 27 a range of up to 1,500 ft is 
achievable, when using highly reflective CCR components.
While our light source (3 million candlepower) was 
sufficient for a range of a few hundred feet, we remark that there
exist commercially available light sources (20 million 
candlepower) or military (150 million candlepower [27]), 
powerful enough for ranges of a few thousand feet.
6 StarDust System Optimizations
In this section we describe extensions of the proposed 
architecture that can constitute future research directions.
6.1 Chained Constraint Primitives
In this paper we proposed four primitives for 
constraintbased relaxation algorithms: color, connectivity, time and
space. To demonstrate the power that can be obtained by
combining them, we proposed and evaluated one 
combination of such primitives: color and connectivity. An 
interesting research direction to pursue could be to chain more than
two of these primitives. An example of such chain is: color,
temporal, spatial and connectivity. Other research directions
could be to use voting scheme for deciding which primitive
to use or assign different weights to different relaxation 
algorithms.
69
6.2 Location Learning
If after several iterations of the algorithm, none of the 
label probabilities for a node ni converges to a higher value, the
confidence in our labeling of that node is relatively low. It
would be interesting to associate with a node, more than one
label (implicitly more than one location) and defer the label
assignment decision until events are detected in the network
(if the network was deployed for target tracking).
6.3 Localization in Rugged Environments
The initial driving force for the StarDust localization
framework was to address the sensor node localization in 
extremely rugged environments. Canopies, dense vegetation,
extremely obstructing environments pose significant 
challenges for sensor nodes localization. The hope, and our 
original idea, was to consider the time period between the aerial
deployment and the time when the sensor node disappears
under the canopy. By recording the last visible position of a
sensor node (as seen from the aircraft) a reasonable estimate
of the sensor node location can be obtained. This would
require that sensor nodes posses self-righting capabilities,
while in mid-air. Nevertheless, we remark on the suitability
of our localization framework for rugged, non-line-of-sight
environments.
7 Conclusions
StarDust solves the localization problem for aerial 
deployments where passiveness, low cost, small form factor
and rapid localization are required. Results show that 
accuracy can be within 2 ft and localization time within 
milliseconds. StarDust also shows robustness with respect to errors.
We predict the influence the atmospheric conditions can have
on the range of a system based on the StarDust framework,
and show that hazy environments or daylight can pose 
significant challenges.
Most importantly, the properties of StarDust support
the potential for even more accurate localization solutions
as well as solutions for rugged, non-line-of-sight 
environments.
8 References
[1] T. He, S. Krishnamurthy, J. A. Stankovic, T. Abdelzaher, L. Luo,
R. Stoleru, T. Yan, L. Gu, J. Hui, and B. Krogh, An energy-efficient
surveillance system using wireless sensor networks, in MobiSys, 2004.
[2] G. Simon, M. Maroti, A. Ledeczi, G. Balogh, B. Kusy, A. Nadas,
G. Pap, J. Sallai, and K. Frampton, Sensor network-based 
countersniper system, in SenSys, 2004.
[3] A. Arora, P. Dutta, and B. Bapat, A line in the sand: A wireless sensor
network for trage detection, classification and tracking, in Computer
Networks, 2004.
[4] R. Szewczyk, A. Mainwaring, J. Polastre, J. Anderson, and D. Culler,
An analysis of a large scale habitat monitoring application, in ACM
SenSys, 2004.
[5] N. Xu, S. Rangwala, K. K. Chintalapudi, D. Ganesan, A. Broad,
R. Govindan, and D. Estrin, A wireless sensor network for structural
monitoring, in ACM SenSys, 2004.
[6] A. Savvides, C. Han, and M. Srivastava, Dynamic fine-grained 
localization in ad-hoc networks of sensors, in Mobicom, 2001.
[7] N. Priyantha, A. Chakraborty, and H. Balakrishnan, The cricket
location-support system, in Mobicom, 2000.
[8] M. Broxton, J. Lifton, and J. Paradiso, Localizing a sensor network
via collaborative processing of global stimuli, in EWSN, 2005.
[9] P. Bahl and V. N. Padmanabhan, Radar: An in-building rf-based user
location and tracking system, in IEEE Infocom, 2000.
[10] N. Priyantha, H. Balakrishnan, E. Demaine, and S. Teller, 
Mobileassisted topology generation for auto-localization in sensor networks,
in IEEE Infocom, 2005.
[11] P. N. Pathirana, A. Savkin, S. Jha, and N. Bulusu, Node localization
using mobile robots in delay-tolerant sensor networks, IEEE 
Transactions on Mobile Computing, 2004.
[12] C. Savarese, J. M. Rabaey, and J. Beutel, Locationing in distribued
ad-hoc wireless sensor networks, in ICAASSP, 2001.
[13] M. Maroti, B. Kusy, G. Balogh, P. Volgyesi, A. Nadas, K. Molnar,
S. Dora, and A. Ledeczi, Radio interferometric geolocation, in ACM
SenSys, 2005.
[14] K. Whitehouse, A. Woo, C. Karlof, F. Jiang, and D. Culler, The 
effects of ranging noise on multi-hop localization: An empirical study,
in IPSN, 2005.
[15] Y. Kwon, K. Mechitov, S. Sundresh, W. Kim, and G. Agha, Resilient
localization for sensor networks in outdoor environment, UIUC, Tech.
Rep., 2004.
[16] R. Stoleru and J. A. Stankovic, Probability grid: A location 
estimation scheme for wireless sensor networks, in SECON, 2004.
[17] N. Bulusu, J. Heidemann, and D. Estrin, GPS-less low cost outdoor
localization for very small devices, IEEE Personal Communications
Magazine, 2000.
[18] T. He, C. Huang, B. Blum, J. A. Stankovic, and T. Abdelzaher,
Range-Free localization schemes in large scale sensor networks, in
ACM Mobicom, 2003.
[19] R. Nagpal, H. Shrobe, and J. Bachrach, Organizing a global 
coordinate system from local information on an ad-hoc sensor network, in
IPSN, 2003.
[20] D. Niculescu and B. Nath, ad-hoc positioning system, in IEEE
GLOBECOM, 2001.
[21] R. Stoleru, T. He, J. A. Stankovic, and D. Luebke, A high-accuracy
low-cost localization system for wireless sensor networks, in ACM
SenSys, 2005.
[22] K. R¨omer, The lighthouse location system for smart dust, in
ACM/USENIX MobiSys, 2003.
[23] R. Y. Tsai, A versatile camera calibration technique for 
highaccuracy 3d machine vision metrology using off-the-shelf tv cameras
and lenses, IEEE JRA, 1987.
[24] C. L. Archer and M. Z. Jacobson, Spatial and temporal distributions
of U.S. winds and wind power at 80m derived from measurements,
Geophysical Research Jrnl., 2003.
[25] Team for advanced flow simulation and modeling. [Online].
Available: http://www.mems.rice.edu/TAFSM/RES/
[26] K. Stein, R. Benney, T. Tezduyar, V. Kalro, and J. Leonard, 3-D
computation of parachute fluid-structure interactions - performance and
control, in Aerodynamic Decelerator Systems Conference, 1999.
[27] Headquarters Department of the Army, Technical manual for 
searchlight infrared AN/GSS-14(V)1, 1982.
70
pTHINC: A Thin-Client Architecture
for Mobile Wireless Web
Joeng Kim, Ricardo A. Baratto, and Jason Nieh
Department of Computer Science
Columbia University, New York, NY, USA
{jk2438, ricardo, nieh}@cs.columbia.edu
ABSTRACT
Although web applications are gaining popularity on 
mobile wireless PDAs, web browsers on these systems can be
quite slow and often lack adequate functionality to access
many web sites. We have developed pTHINC, a PDA 
thinclient solution that leverages more powerful servers to run
full-function web browsers and other application logic, then
sends simple screen updates to the PDA for display. pTHINC
uses server-side screen scaling to provide high-fidelity 
display and seamless mobility across a broad range of different
clients and screen sizes, including both portrait and 
landscape viewing modes. pTHINC also leverages existing PDA
control buttons to improve system usability and maximize
available screen resolution for application display. We have
implemented pTHINC on Windows Mobile and evaluated
its performance on mobile wireless devices. Our results 
compared to local PDA web browsers and other thin-client 
approaches demonstrate that pTHINC provides superior web
browsing performance and is the only PDA thin client that
effectively supports crucial browser helper applications such
as video playback.
Categories and Subject Descriptors: C.2.4 
ComputerCommunication-Networks: Distributed Systems - client/
server
General Terms: Design, Experimentation, Performance
1. INTRODUCTION
The increasing ubiquity of wireless networks and 
decreasing cost of hardware is fueling a proliferation of mobile 
wireless handheld devices, both as standalone wireless Personal
Digital Assistants (PDA) and popular integrated PDA/cell
phone devices. These devices are enabling new forms of 
mobile computing and communication. Service providers are
leveraging these devices to deliver pervasive web access, and
mobile web users already often use these devices to access
web-enabled information such as news, email, and localized
travel guides and maps. It is likely that within a few years,
most of the devices accessing the web will be mobile.
Users typically access web content by running a web browser
and associated helper applications locally on the PDA. 
Although native web browsers exist for PDAs, they deliver
subpar performance and have a much smaller feature set
and more limited functionality than their desktop 
computing counterparts [10]. As a result, PDA web browsers are 
often not able to display web content from web sites that 
leverage more advanced web technologies to deliver a richer web
experience. This fundamental problem arises for two 
reasons. First, because PDAs have a completely different 
hardware/software environment from traditional desktop 
computers, web applications need to be rewritten and customized
for PDAs if at all possible, duplicating development costs.
Because the desktop application market is larger and more
mature, most development effort generally ends up being
spent on desktop applications, resulting in greater 
functionality and performance than their PDA counterparts. 
Second, PDAs have a more resource constrained environment
than traditional desktop computers to provide a smaller
form factor and longer battery life. Desktop web browsers
are large, complex applications that are unable to run on a
PDA. Instead, developers are forced to significantly strip
down these web browsers to provide a usable PDA web
browser, thereby crippling PDA browser functionality.
Thin-client computing provides an alternative approach
for enabling pervasive web access from handheld devices.
A thin-client computing system consists of a server and a
client that communicate over a network using a remote 
display protocol. The protocol enables graphical displays to be
virtualized and served across a network to a client device,
while application logic is executed on the server. Using the
remote display protocol, the client transmits user input to
the server, and the server returns screen updates of the 
applications from the server to the client. Using a thin-client
model for mobile handheld devices, PDAs can become 
simple stateless clients that leverage the remote server 
capabilities to execute web browsers and other helper applications.
The thin-client model provides several important 
benefits for mobile wireless web. First, standard desktop web
applications can be used to deliver web content to PDAs
without rewriting or adapting applications to execute on
a PDA, reducing development costs and leveraging existing
software investments. Second, complex web applications can
be executed on powerful servers instead of running stripped
down versions on more resource constrained PDAs, 
providing greater functionality and better performance [10]. Third,
web applications can take advantage of servers with faster
networks and better connectivity, further boosting 
application performance. Fourth, PDAs can be even simpler 
devices since they do not need to perform complex application
logic, potentially reducing energy consumption and 
extend143
ing battery life. Finally, PDA thin clients can be essentially
stateless appliances that do not need to be backed up or 
restored, require almost no maintenance or upgrades, and do
not store any sensitive data that can be lost or stolen. This
model provides a viable avenue for medical organizations to
comply with HIPAA regulations [6] while embracing mobile
handhelds in their day to day operations.
Despite these potential advantages, thin clients have been
unable to provide the full range of these benefits in delivering
web applications to mobile handheld devices. Existing thin
clients were not designed for PDAs and do not account for
important usability issues in the context of small form factor
devices, resulting in difficulty in navigating displayed web
content. Furthermore, existing thin clients are ineffective at
providing seamless mobility across the heterogeneous mix
of device display sizes and resolutions. While existing thin
clients can already provide faster performance than native
PDA web browsers in delivering HTML web content, they
do not effectively support more display-intensive web helper
applications such as multimedia video, which is increasingly
an integral part of available web content.
To harness the full potential of thin-client computing in
providing mobile wireless web on PDAs, we have developed
pTHINC (PDA THin-client InterNet Computing). pTHINC
builds on our previous work on THINC [1] to provide a 
thinclient architecture for mobile handheld devices. pTHINC
virtualizes and resizes the display on the server to efficiently
deliver high-fidelity screen updates to a broad range of 
different clients, screen sizes, and screen orientations, including
both portrait and landscape viewing modes. This enables
pTHINC to provide the same persistent web session across
different client devices. For example, pTHINC can provide
the same web browsing session appropriately scaled for 
display on a desktop computer and a PDA so that the same
cookies, bookmarks, and other meta-data are continuously
available on both machines simultaneously. pTHINC"s 
virtual display approach leverages semantic information 
available in display commands, and client-side video hardware to
provide more efficient remote display mechanisms that are
crucial for supporting more display-intensive web 
applications. Given limited display resolution on PDAs, pTHINC
maximizes the use of screen real estate for remote display
by moving control functionality from the screen to readily
available PDA control buttons, improving system usability.
We have implemented pTHINC on Windows Mobile and
demonstrated that it works transparently with existing 
applications, window systems, and operating systems, and does
not require modifying, recompiling, or relinking existing 
software. We have quantitatively evaluated pTHINC against 
local PDA web browsers and other thin-client approaches on
Pocket PC devices. Our experimental results demonstrate
that pTHINC provides superior web browsing performance
and is the only PDA thin client that effectively supports
crucial browser helper applications such as video playback.
This paper presents the design and implementation of
pTHINC. Section 2 describes the overall usage model and 
usability characteristics of pTHINC. Section 3 presents the 
design and system architecture of pTHINC. Section 4 presents
experimental results measuring the performance of pTHINC
on web applications and comparing it against native PDA
browsers and other popular PDA thin-client systems. 
Section 5 discusses related work. Finally, we present some 
concluding remarks.
2. PTHINC USAGE MODEL
pTHINC is a thin-client system that consists of a simple
client viewer application that runs on the PDA and a server
that runs on a commodity PC. The server leverages more
powerful PCs to to run web browsers and other application
logic. The client takes user input from the PDA stylus and
virtual keyboard and sends them to the server to pass to
the applications. Screen updates are then sent back from
the server to the client for display to the user.
When the pTHINC PDA client is started, the user is 
presented with a simple graphical interface where information
such as server address and port, user authentication 
information, and session settings can be provided. pTHINC first
attempts to connect to the server and perform the 
necessary handshaking. Once this process has been completed,
pTHINC presents the user with the most recent display of
his session. If the session does not exist, a new session is 
created. Existing sessions can be seamlessly continued without
changes in the session setting or server configuration.
Unlike other thin-client systems, pTHINC provides a user
with a persistent web session model in which a user can
launch a session running a web browser and associated 
applications at the server, then disconnect from that session
and reconnect to it again anytime. When a user reconnects
to the session, all of the applications continue running where
the user left off, so that the user can continue working as
though he or she never disconnected. The ability to 
disconnect and reconnect to a session at anytime is an important
benefit for mobile wireless PDA users which may have 
intermittent network connectivity.
pTHINC"s persistent web session model enables a user to
reconnect to a web session from devices other than the one
on which the web session was originally initiated. This 
provides users with seamless mobility across different devices.
If a user loses his PDA, he can easily use another PDA to
access his web session. Furthermore, pTHINC allows users
to use non-PDA devices to access web sessions as well. A
user can access the same persistent web session on a 
desktop PC as on a PDA, enabling a user to use the same web
session from any computer.
pTHINC"s persistent web session model addresses a key
problem encountered by mobile web users, the lack of a 
common web environment across computers. Web browsers 
often store important information such as bookmarks, cookies,
and history, which enable them to function in a much more
useful manner. The problem that occurs when a user moves
between computers is that this data, which is specific to a
web browser installation, cannot move with the user. 
Furthermore, web browsers often need helper applications to
process different media content, and those applications may
not be consistently available across all computers. pTHINC
addresses this problem by enabling a user to remotely use
the exact same web browser environment and helper 
applications from any computer. As a result, pTHINC can 
provide a common, consistent web browsing environment for
mobile users across different devices without requiring them
to attempt to repeatedly synchronize different web browsing
environments across multiple machines.
To enable a user to access the same web session on 
different devices, pTHINC must provide mechanisms to 
support different display sizes and resolutions. Toward this end,
pTHINC provides a zoom feature that enables a user to
zoom in and out of a display and allows the display of a web
144
Figure 1: pTHINC shortcut keys
session to be resized to fit the screen of the device being
used. For example, if the server is running a web session at
1024×768 but the client is a PDA with a display resolution
of 640×480, pTHINC will resize the desktop display to fit
the full display in the smaller screen of the PDA. pTHINC
provides the PDA user with the option to increase the size
of the display by zooming in to different parts of the display.
Users are often familiar with the general layout of commonly
visited websites, and are able to leverage this resizing 
feature to better navigate through web pages. For example,
a user can zoom out of the display to view the entire page
content and navigate hyperlinks, then zoom in to a region
of interest for a better view.
To enable a user to access the same web session on 
different devices, pTHINC must also provide mechanisms to
support different display orientations. In a desktop 
environment, users are typically accustomed to having displays
presented in landscape mode where the screen width is larger
than its height. However, in a PDA environment, the choice
is not always obvious. Some users may prefer having the
display in portrait mode, as it is easier to hold the device
in their hands, while others may prefer landscape mode in
order to minimize the amount of side-scrolling necessary
to view a web page. To accommodate PDA user 
preferences, pTHINC provides an orientation feature that enables
it to seamless rotate the display between landscape and 
portrait mode. The landscape mode is particularly useful for
pTHINC users who frequently access their web sessions on
both desktop and PDA devices, providing those users with
the same familiar landscape setting across different devices.
Because screen space is a relatively scarce resource on
PDAs, pTHINC runs in fullscreen mode to maximize the
screen area available to display the web session. To be able
to use all of the screen on the PDA and still allow the user
to control and interact with it, pTHINC reuses the typical
shortcut buttons found on PDAs to perform all the 
control functions available to the user. The buttons used by
pTHINC do not require any OS environment changes; they
are simply intercepted by the pTHINC client application
when they are pressed. Figure 1 shows how pTHINC 
utilizes the shortcut buttons to provide easy navigation and
improve the overall user experience. These buttons are not
device specific, and the layout shown is common to 
widelyused PocketPC devices. pTHINC provides six shortcuts to
support its usage model:
• Rotate Screen: The record button on the left edge is
used to rotate the screen between portrait and 
landscape mode each time the button is pressed.
• Zoom Out: The leftmost button on the bottom front
is used to zoom out the display of the web session
providing a bird"s eye view of the web session.
• Zoom In: The second leftmost button on the bottom
front is used to zoom in the display of the web session
to more clearly view content of interest.
• Directional Scroll: The middle button on the bottom
front is used to scroll around the display using a single
control button in a way that is already familiar to PDA
users. This feature is particularly useful when the user
has zoomed in to a region of the display such that only
part of the display is visible on the screen.
• Show/Hide Keyboard: The second rightmost button on
the bottom front is used to bring up a virtual keyboard
drawn on the screen for devices which have no physical
keyboard. The virtual keyboard uses standard PDA
OS mechanisms, providing portability across different
PDA environments.
• Close Session: The rightmost button on the bottom
front is used to disconnect from the pTHINC session.
pTHINC uses the PDA touch screen, stylus, and standard
user interface mechanisms to provide a user interface 
pointand-click metaphor similar to that provided by the mouse
in a traditional desktop computing environment. pTHINC
does not use a cursor since PDA environments do not 
provide one. Instead, a user can use the stylus to tap on 
different sections of the touch screen to indicate input focus. A
single tap on the touch screen generates a corresponding 
single click mouse event. A double tap on the touch screen 
generates a corresponding double click mouse event. pTHINC
provides two-button mouse emulation by using the stylus to
press down on the screen for one second to generate a right
mouse click. All of these actions are identical to the way
users already interact with PDA applications in the common
PocketPC environment. In web browsing, users can click on
hyperlinks and focus on input boxes by simply tapping on
the desired screen area of interest. Unlike local PDA web
browsers and other PDA applications, pTHINC leverages
more powerful desktop user interface metaphors to enable
users to manipulate multiple open application windows 
instead of being limited to a single application window at any
given moment. This provides increased browsing flexibility
beyond what is currently available on PDA devices. Similar
to a desktop environment, browser windows and other 
application windows can be moved around by pressing down
and dragging the stylus similar to a mouse.
3. PTHINC SYSTEM ARCHITECTURE
pTHINC builds on the THINC [1] remote display 
architecture to provide a thin-client system for PDAs. pTHINC
virtualizes the display at the server by leveraging the video
device abstraction layer, which sits below the window server
and above the framebuffer. This is a well-defined, low-level,
device-dependent layer that exposes the video hardware to
the display system. pTHINC accomplishes this through a
simple virtual display driver that intercepts drawing 
commands, packetizes, and sends them over the network.
145
While other thin-client approaches intercept display 
commands at other layers of the display subsystem, pTHINC"s
display virtualization approach provides some key benefits
in efficiently supporting PDA clients. For example, 
intercepting display commands at a higher layer between 
applications and the window system as is done by X [17] requires
replicating and running a great deal of functionality on the
PDA that is traditionally provided by the desktop window
system. Given both the size and complexity of traditional
window systems, attempting to replicate this functionality
in the restricted PDA environment would have proven to
be a daunting, and perhaps unfeasible task. Furthermore,
applications and the window system often require tight 
synchronization in their operation and imposing a wireless 
network between them by running the applications on the server
and the window system on the client would significantly 
degrade performance. On the other hand, intercepting at a
lower layer by extracting pixels out of the framebuffer as
they are rendered provides a simple solution that requires
very little functionality on the PDA client, but can also 
result in degraded performance. The reason is that by the
time the remote display server attempts to send screen 
updates, it has lost all semantic information that may have
helped it encode efficiently, and it must resort to using a
generic and expensive encoding mechanism on the server,
as well as a potentially expensive decoding mechanism on
the limited PDA client. In contrast to both the high and
low level interception approaches, pTHINC"s approach of
intercepting at the device driver provides an effective 
balance between client and server simplicity, and the ability to
efficiently encode and decode screen updates.
By using a low-level virtual display approach, pTHINC
can efficiently encode application display commands using
only a small set of low-level commands. In a PDA 
environment, this set of commands provides a crucial component
in maintaining the simplicity of the client in the 
resourceconstrained PDA environment. The display commands are
shown in Table 1, and work as follows. COPY instructs the
client to copy a region of the screen from its local framebuffer
to another location. This command improves the user 
experience by accelerating scrolling and opaque window 
movement without having to resend screen data from the server.
SFILL, PFILL, and BITMAP are commands that paint a
fixed-size region on the screen. They are useful for 
accelerating the display of solid window backgrounds, desktop
patterns, backgrounds of web pages, text drawing, and 
certain operations in graphics manipulation programs. SFILL
fills a sizable region on the screen with a single color. PFILL
replicates a tile over a screen region. BITMAP performs a
fill using a bitmap of ones and zeros as a stipple to apply
a foreground and background color. Finally, RAW is used
to transmit unencoded pixel data to be displayed verbatim
on a region of the screen. This command is invoked as a
last resort if the server is unable to employ any other 
command, and it is the only command that may be compressed
to mitigate its impact on network bandwidth.
pTHINC delivers its commands using a non-blocking, 
serverpush update mechanism, where as soon as display updates
are generated on the server, they are sent to the client.
Clients are not required to explicitly request display 
updates, thus minimizing the impact that the typical 
varying network latency of wireless links may have on the 
responsiveness of the system. Keeping in mind that resource
Command Description
COPY Copy a frame buffer area to specified 
coordinates
SFILL Fill an area with a given pixel color value
PFILL Tile an area with a given pixel pattern
BITMAP Fill a region using a bit pattern
RAW Display raw pixel data at a given location
Table 1: pTHINC Protocol Display Commands
constrained PDAs and wireless networks may not be able
to keep up with a fast server generating a large number of
updates, pTHINC is able to coalesce, clip, and discard 
updates automatically if network loss or congestion occurs, or
the client cannot keep up with the rate of updates. This
type of behavior proves crucial in a web browsing 
environment, where for example, a page may be redrawn multiple
times as it is rendered on the fly by the browser. In this
case, the PDA will only receive and render the final result,
which clearly is all the user is interesting in seeing.
pTHINC prioritizes the delivery of updates to the PDA 
using a Shortest-Remaining-Size-First (SRSF) preemptive 
update scheduler. SRSF is analogous to 
Shortest-RemainingProcessing-Time scheduling, which is known to be optimal
for minimizing mean response time in an interactive system.
In a web browsing environment, short jobs are associated
with text and basic page layout components such as the
page"s background, which are critical web content for the
user. On the other hand, large jobs are often lower priority
beautifying elements, or, even worse, web page banners
and advertisements, which are of questionable value to the
user as he or she is browsing the page. Using SRSF, pTHINC
is able to maximize the utilization of the relatively scarce
bandwidth available on the wireless connection between the
PDA and the server.
3.1 Display Management
To enable users to just as easily access their web browser
and helper applications from a desktop computer at home
as from a PDA while on the road, pTHINC provides a 
resize mechanism to zoom in and out of the display of a web
session. pTHINC resizing is completely supported by the
server, not the client. The server resamples updates to fit
within the PDAs viewport before they are transmitted over
the network. pTHINC uses Fant"s resampling algorithm to
resize pixel updates. This provides smooth, visually 
pleasing updates with properly antialiasing and has only modest
computational requirements.
pTHINC"s resizing approach has a number of advantages.
First, it allows the PDA to leverage the vastly superior 
computational power of the server to use high quality resampling
algorithms and produce higher quality updates for the PDA
to display. Second, resizing the screen does not translate into
additional resource requirements for the PDA, since it does
not need to perform any additional work. Finally, better
utilization of the wireless network is attained since rescaling
the updates reduces their bandwidth requirements.
To enable users to orient their displays on a PDA to
provide a viewing experience that best accommodates user
preferences and the layout of web pages or applications,
pTHINC provides a display rotation mechanism to switch
between landscape and portrait viewing modes. pTHINC
display rotation is completely supported by the client, not
the server. pTHINC does not explicitly recalculate the 
ge146
ometry of display updates to perform rotation, which would
be computationally expensive. Instead, pTHINC simply
changes the way data is copied into the framebuffer to switch
between display modes. When in portrait mode, data is
copied along the rows of the framebuffer from left to right.
When in landscape mode, data is copied along the columns
of the framebuffer from top to bottom. These very fast and
simple techniques replace one set of copy operations with 
another and impose no performance overhead. pTHINC 
provides its own rotation mechanism to support a wide range of
devices without imposing additional feature requirements on
the PDA. Although some newer PDA devices provide native
support for different orientations, this mechanism is not 
dynamic and requires the user to rotate the PDA"s entire user
interface before starting the pTHINC client. Windows 
Mobile provides native API mechanisms for PDA applications
to rotate their UI on the fly, but these mechanisms deliver
poor performance and display quality as the rotation is 
performed naively and is not completely accurate.
3.2 Video Playback
Video has gradually become an integral part of the World
Wide Web, and its presence will only continue to increase.
Web sites today not only use animated graphics and flash
to deliver web content in an attractive manner, but also 
utilize streaming video to enrich the web interface. Users are
able to view pre-recorded and live newscasts on CNN, watch
sports highlights on ESPN, and even search through large
collection of videos on Google Video. To allow applications
to provide efficient video playback, interfaces have been 
created in display systems that allow video device drivers to
expose their hardware capabilities back to the applications.
pTHINC takes advantage of these interfaces and its virtual
device driver approach to provide a virtual bridge between
the remote client and its hardware and the applications, and
transparently support video playback.
On top of this architecture, pTHINC uses the YUV 
colorspace to encode the video content, which provides a 
number of benefits. First, it has become increasingly common
for PDA video hardware to natively support YUV and be
able to perform the colorspace conversion and scaling 
automatically. As a result, pTHINC is able to provide fullscreen
video playback without any performance hits. Second, the
use of YUV allows for a more efficient representation of RGB
data without loss of quality, by taking advantage of the 
human eye"s ability to better distinguish differences in 
brightness than in color. In particular, pTHINC uses the YV12
format, which allows full color RGB data to be encoded 
using just 12 bits per pixel. Third, YUV data is produced
as one of the last steps of the decoding process of most
video codecs, allowing pTHINC to provide video playback
in a manner that is format independent. Finally, even if the
PDA"s video hardware is unable to accelerate playback, the
colorspace conversion process is simple enough that it does
not impose unreasonable requirements on the PDA.
A more concrete example of how pTHINC leverages the
PDA video hardware to support video playback can be seen
in our prototype implementation on the popular Dell Axim
X51v PDA, which is equipped with the Intel 2700G 
multimedia accelerator. In this case, pTHINC creates an 
offscreen buffer in video memory and writes and reads from
this memory region data on the YV12 format. When a new
video frame arrives, video data is copied from the buffer to
Figure 2: Experimental Testbed
an overlay surface in video memory, which is independent
of the normal surface used for traditional drawing. As the
YV12 data is put onto the overlay, the Intel accelerator 
automatically performs both colorspace conversion and scaling.
By using the overlay surface, pTHINC has no need to redraw
the screen once video playback is over since the overlapped
surface is unaffected. In addition, specific overlay regions
can be manipulated by leveraging the video hardware, for
example to perform hardware linear interpolation to smooth
out the frame and display it fullscreen, and to do automatic
rotation when the client runs in landscape mode.
4. EXPERIMENTAL RESULTS
We have implemented a pTHINC prototype that runs the
client on widely-used Windows Mobile-based Pocket PC 
devices and the server on both Windows and Linux operating
systems. To demonstrate its effectiveness in supporting 
mobile wireless web applications, we have measured its 
performance on web applications. We present experimental results
on different PDA devices for two popular web applications,
browsing web pages and playing video content from the web.
We compared pTHINC against native web applications 
running locally on the PDA to demonstrate the improvement
that pTHINC can provide over the traditional fat-client 
approach. We also compared pTHINC against three of the
most widely used thin clients that can run on PDAs, Citrix
Meta-FrameXP [2], Microsoft Remote Desktop [3] and VNC
(Virtual Network Computing) [16]. We follow common 
practice and refer to Citrix MetaFrameXP and Microsoft Remote
Desktop by their respective remote display protocols, ICA
(Independent Computing Architecture) and RDP (Remote
Desktop Protocol).
4.1 Experimental Testbed
We conducted our web experiments using two different
wireless Pocket PC PDAs in an isolated Wi-Fi network 
testbed, as shown in Figure 2. The testbed consisted of two
PDA client devices, a packet monitor, a thin-client server,
and a web server. Except for the PDAs, all of the other 
machines were IBM Netfinity 4500R servers with dual 933 MHz
Intel PIII CPUs and 512 MB RAM and were connected on
a switched 100 Mbps FastEthernet network. The web server
used was Apache 1.3.27, the network emulator was 
NISTNet 2.0.12, and the packet monitor was Ethereal 0.10.9. The
PDA clients connected to the testbed through a 802.11b 
Lucent Orinoco AP-2000 wireless access point. All experiments
using the wireless network were conducted within ten feet
of the access point, so we considered the amount of packet
loss to be negligible in our experiments.
Two Pocket PC PDAs were used to provide results across
both older, less powerful models and newer higher 
performance models. The older model was a Dell Axim X5 with
147
Client 1024×768 640×480 Depth Resize Clip
RDP no yes 8-bit no yes
VNC yes yes 16-bit no no
ICA yes yes 16-bit yes no
pTHINC yes yes 24-bit yes no
Table 2: Thin-client Testbed Configuration Setting
a 400 MHz Intel XScale PXA255 CPU and 64 MB RAM
running Windows Mobile 2003 and a Dell TrueMobile 1180
2.4Ghz CompactFlash card for wireless networking. The
newer model was a Dell Axim X51v with a 624 MHz Intel
XScale XPA270 CPU and 64 MB RAM running Windows
Mobile 5.0 and integrated 802.11b wireless networking. The
X51v has an Intel 2700G multimedia accelerator with 16MB
video memory. Both PDAs are capable of 16-bit color but
have different screen sizes and display resolutions. The X5
has a 3.5 inch diagonal screen with 240×320 resolution. The
X51v has a 3.7 inch diagonal screen with 480×640.
The four thin clients that we used support different 
levels of display quality as summarized in Table 2. The RDP
client only supports a fixed 640×480 display resolution on
the server with 8-bit color depth, while other platforms 
provide higher levels of display quality. To provide a fair 
comparison across all platforms, we conducted our experiments
with thin-client sessions configured for two possible 
resolutions, 1024×768 and 640×480. Both ICA and VNC were
configured to use the native PDA resolution of 16-bit color
depth. The current pTHINC prototype uses 24-bit color 
directly and the client downsamples updates to the 16-bit color
depth available on the PDA. RDP was configured using only
8-bit color depth since it does not support any better color
depth. Since both pTHINC and ICA provide the ability to
view the display resized to fit the screen, we measured both
clients with and without the display resized to fit the PDA
screen. Each thin client was tested using landscape rather
than portrait mode when available. All systems run on the
X51v could run in landscape mode because the hardware
provides a landscape mode feature. However, the X5 does
not provide this functionality. Only pTHINC directly 
supports landscape mode, so it was the only system that could
run in landscape mode on both the X5 and X51v.
To provide a fair comparison, we also standardized on
common hardware and operating systems whenever possible.
All of the systems used the Netfinity server as the thin-client
server. For the two systems designed for Windows servers,
ICA and RDP, we ran Windows 2003 Server on the server.
For the other systems which support X-based servers, VNC
and pTHINC, we ran the Debian Linux Unstable 
distribution with the Linux 2.6.10 kernel on the server. We used the
latest thin-client server versions available on each platform
at the time of our experiments, namely Citrix MetaFrame
XP Server for Windows Feature Release 3, Microsoft 
Remote Desktop built into Windows XP and Windows 2003
using RDP 5.2, and VNC 4.0.
4.2 Application Benchmarks
We used two web application benchmarks for our 
experiments based on two common application scenarios, browsing
web pages and playing video content from the web. Since
many thin-client systems including two of the ones tested
are closed and proprietary, we measured their performance
in a noninvasive manner by capturing network traffic with
a packet monitor and using a variant of slow-motion 
benchmarking [13] previously developed to measure thin-client
performance in PDA environments [10]. This measurement
methodology accounts for both the display decoupling that
can occur between client and server in thin-client systems
as well as client processing time, which may be significant
in the case of PDAs.
To measure web browsing performance, we used a web
browsing benchmark based on the Web Text Page Load Test
from the Ziff-Davis i-Bench benchmark suite [7]. The 
benchmark consists of JavaScript controlled load of 55 pages from
the web server. The pages contain both text and 
graphics with pages varying in size. The graphics are embedded
images in GIF and JPEG formats. The original i-Bench
benchmark was modified for slow-motion benchmarking by
introducing delays of several seconds between the pages 
using JavaScript. Then two tests were run, one where 
delays where added between each page, and one where pages
where loaded continuously without waiting for them to be
displayed on the client. In the first test, delays were 
sufficiently adjusted in each case to ensure that each page could
be received and displayed on the client completely without
temporal overlap in transferring the data belonging to two
consecutive pages. We used the packet monitor to record
the packet traffic for each run of the benchmark, then used
the timestamps of the first and last packet in the trace to
obtain our latency measures [10]. The packet monitor also
recorded the amount of data transmitted between the client
and the server. The ratio between the data traffic in the two
tests yields a scale factor. This scale factor shows the loss
of data between the server and the client due to inability of
the client to process the data quickly enough. The product
of the scale factor with the latency measurement produces
the true latency accounting for client processing time.
To run the web browsing benchmark, we used Mozilla
Firefox 1.0.4 running on the thin-client server for the thin
clients, and Windows Internet Explorer (IE) Mobile for 2003
and Mobile for 5.0 for the native browsers on the X5 and
X51v PDAs, respectively. In all cases, the web browser used
was sized to fill the entire display region available.
To measure video playback performance, we used a video
benchmark that consisted of playing a 34.75s MPEG-1 video
clip containing a mix of news and entertainment 
programming at full-screen resolution. The video clip is 5.11 MB and
consists of 834 352x240 pixel frames with an ideal frame rate
of 24 frames/sec. We measured video performance using
slow-motion benchmarking by monitoring resulting packet
traffic at two playback rates, 1 frames/second (fps) and 24
fps, and comparing the results to determine playback 
delays and frame drops that occur at 24 fps to measure overall
video quality [13]. For example, 100% quality means that all
video frames were played at real-time speed. On the other
hand, 50% quality could mean that half the video data was
dropped, or that the clip took twice as long to play even
though all of the video data was displayed.
To run the video benchmark, we used Windows Media
Player 9 for Windows-based thin-client servers, MPlayer 1.0
pre 6 for X-based thin-client servers, and Windows Media
Player 9 Mobile and 10 Mobile for the native video players
running locally on the X5 and X51v PDAs, respectively. In
all cases, the video player used was sized to fill the entire
display region available.
4.3 Measurements
Figures 3 and 4 show the results of running the web 
brows148
0
1
10
100
pTHINC
Resized
pTHINCICA
Resized
ICAVNCRDPLOCAL
Latency(s)
Platform
Axim X5 (640x480 or less)
Axim X51v (640x480)
Axim X5 (1024x768)
Axim X51v (1024x768)
Figure 3: Browsing Benchmark: Average Page Latency
ing benchmark. For each platform, we show results for up to
four different configurations, two on the X5 and two on the
X51v, depending on whether each configuration was 
supported. However, not all platforms could support all 
configurations. The local browser only runs at the display 
resolution of the PDA, 480×680 or less for the X51v and the
X5. RDP only runs at 640×480. Neither platform could
support 1024×768 display resolution. ICA only ran on the
X5 and could not run on the X51v because it did not work
on Windows Mobile 5.
Figure 3 shows the average latency per web page for each
platform. pTHINC provides the lowest average web 
browsing latency on both PDAs. On the X5, pTHINC performs
up to 70 times better than other thin-client systems and 8
times better than the local browser. On the X51v, pTHINC
performs up to 80 times better than other thin-client 
systems and 7 times better than the native browser. In fact,
all of the thin clients except VNC outperform the local
PDA browser, demonstrating the performance benefits of
the thin-client approach. Usability studies have shown that
web pages should take less than one second to download
for the user to experience an uninterrupted web browsing
experience [14]. The measurements show that only the thin
clients deliver subsecond web page latencies. In contrast, the
local browser requires more than 3 seconds on average per
web page. The local browser performs worse since it needs
to run a more limited web browser to process the HTML,
JavaScript, and do all the rendering using the limited 
capabilities of the PDA. The thin clients can take advantage of
faster server hardware and a highly tuned web browser to
process the web content much faster.
Figure 3 shows that RDP is the next fastest platform after
pTHINC. However, RDP is only able to run at a fixed 
resolution of 640×480 and 8-bit color depth. Furthermore, RDP
also clips the display to the size of the PDA screen so that
it does not need to send updates that are not visible on the
PDA screen. This provides a performance benefit 
assuming the remaining web content is not viewed, but degrades
performance when a user scrolls around the display to view
other web content. RDP achieves its performance with 
significantly lower display quality compared to the other thin
clients and with additional display clipping not used by other
systems. As a result, RDP performance alone does not 
provide a complete comparison with the other platforms. In
contrast, pTHINC provides the fastest performance while
at the same time providing equal or better display quality
than the other systems.
0
1
10
100
1000
pTHINC
Resized
pTHINCICA
Resized
ICAVNCRDPLOCAL
DataSize(KB)
Platform
Axim X5 (640x480 or less)
Axim X51v (640x480)
Axim X5 (1024x768)
Axim X51v (1024x768)
Figure 4: Browsing Benchmark: Average Page Data
Transferred
Since VNC and ICA provide similar display quality to
pTHINC, these systems provide a more fair comparison of
different thin-client approaches. ICA performs worse in part
because it uses higher-level display primitives that require
additional client processing costs. VNC performs worse in
part because it loses display data due to its client-pull 
delivery mechanism and because of the client processing costs
in decompressing raw pixel primitives. In both cases, their
performance was limited in part because their PDA clients
were unable to keep up with the rate at which web pages
were being displayed.
Figure 3 also shows measurements for those thin clients
that support resizing the display to fit the PDA screen,
namely ICA and pTHINC. Resizing requires additional 
processing, which results in slower average web page latencies.
The measurements show that the additional delay incurred
by ICA when resizing versus not resizing is much more 
substantial than for pTHINC. ICA performs resizing on the
slower PDA client. In contrast, pTHINC leverage the more
powerful server to do resizing, reducing the performance
difference between resizing and not resizing. Unlike ICA,
pTHINC is able to provide subsecond web page download
latencies in both cases.
Figure 4 shows the data transferred in KB per page when
running the slow-motion version of the tests. All of the 
platforms have modest data transfer requirements of roughly
100 KB per page or less. This is well within the 
bandwidth capacity of Wi-Fi networks. The measurements show
that the local browser does not transfer the least amount of
data. This is surprising as HTML is often considered to be
a very compact representation of content. Instead, RDP is
the most bandwidth efficient platform, largely as a result of
using only 8-bit color depth and screen clipping so that it
does not transfer the entire web page to the client. pTHINC
overall has the largest data requirements, slightly more than
VNC. This is largely a result of the current pTHINC 
prototype"s lack of native support for 16-bit color data in the wire
protocol. However, this result also highlights pTHINC"s 
performance as it is faster than all other systems even while
transferring more data. Furthermore, as newer PDA models
support full 24-bit color, these results indicate that pTHINC
will continue to provide good web browsing performance.
Since display usability and quality are as important as
performance, Figures 5 to 8 compare screenshots of the 
different thin clients when displaying a web page, in this case
from the popular BBC news website. Except for ICA, all of
the screenshots were taken on the X51v in landscape mode
149
Figure 5: Browser Screenshot: RDP 640x480 Figure 6: Browser Screenshot: VNC 1024x768
Figure 7: Browser Screenshot: ICA Resized 1024x768 Figure 8: Browser Screenshot: pTHINC Resized 1024x768
using the maximum display resolution settings for each 
platform given in Table 2. The ICA screenshot was taken on the
X5 since ICA does not run on the X51v. While the 
screenshots lack the visual fidelity of the actual device display, 
several observations can be made. Figure 5 shows that RDP
does not support fullscreen mode and wastes lots of screen
space for controls and UI elements, requiring the user to
scroll around in order to access the full contents of the web
browsing session. Figure 6 shows that VNC makes better
use of the screen space and provides better display quality,
but still forces the user to scroll around to view the web
page due to its lack of resizing support. Figure 7 shows
ICA"s ability to display the full web page given its resizing
support, but that its lack of landscape capability and poorer
resize algorithm significantly compromise display quality. In
contrast, Figure 8 shows pTHINC using resizing to provide
a high quality fullscreen display of the full width of the web
page. pTHINC maximizes the entire viewing region by 
moving all controls to the PDA buttons. In addition, pTHINC
leverages the server computational power to use a high 
quality resizing algorithm to resize the display to fit the PDA
screen without significant overhead.
Figures 9 and 10 show the results of running the video
playback benchmark. For each platform except ICA, we
show results for an X5 and X51v configuration. ICA could
not run on the X51v as noted earlier. The measurements
were done using settings that reflected the environment a
user would have to access a web session from both a 
desktop computer and a PDA. As such, a 1024×768 server 
display resolution was used whenever possible and the video
was shown at fullscreen. RDP was limited to 640×480 
display resolution as noted earlier. Since viewing the entire
video display is the only really usable option, we resized
the display to fit the PDA screen for those platforms that
supported this feature, namely ICA and pTHINC.
Figure 9 shows the video quality for each platform. pTHINC
is the only thin client able to provide perfect video playback
quality, similar to the native PDA video player. All of the
other thin clients deliver very poor video quality. With the
exception of RDP on the X51v which provided unacceptable
35% video quality, none of the other systems were even able
to achieve 10% video quality. VNC and ICA have the worst
quality at 8% on the X5 device.
pTHINC"s native video support enables superior video
performance, while other thin clients suffer from their 
inability to distinguish video from normal display updates.
They attempt to apply ineffective and expensive 
compression algorithms on the video data and are unable to keep up
with the stream of updates generated, resulting in dropped
frames or long playback times. VNC suffers further from
its client-pull update model because video frames are 
generated faster than the rate at which the client can process
and send requests to the server to obtain the next display
update. Figure 10 shows the total data transferred during
150
0%
20%
40%
60%
80%
100%
pTHINCICAVNCRDPLOCAL
VideoQuality
Platform
Axim X5
Axim X51v
Figure 9: Video Benchmark: Fullscreen Video Quality
0
1
10
100
pTHINCICAVNCRDPLOCAL
VideoDataSize(MB)
Platform
Axim X5
Axim X51v
Figure 10: Video Benchmark: Fullscreen Video Data
video playback for each system. The native player is the
most bandwidth efficient platform, sending less than 6 MB
of data, which corresponds to about 1.2 Mbps of bandwidth.
pTHINC"s 100% video quality requires about 25 MB of data
which corresponds to a bandwidth usage of less than 6 Mbps.
While the other thin clients send less data than THINC,
they do so because they are dropping video data, resulting
in degraded video quality.
Figures 11 to 14 compare screenshots of the different thin
clients when displaying the video clip. Except for ICA, all of
the screenshots were taken on the X51v in landscape mode
using the maximum display resolution settings for each 
platform given in Table 2. The ICA screenshot was taken on the
X5 since ICA does not run on the X51v. Figures 11 and 12
show that RDP and VNC are unable to display the entire
video frame on the PDA screen. RDP wastes screen space
for UI elements and VNC only shows the top corner of the
video frame on the screen. Figure 13 shows that ICA 
provides resizing to display the entire video frame, but did not
proportionally resize the video data, resulting in strange 
display artifacts. In contrast, Figure 14 shows pTHINC using
resizing to provide a high quality fullscreen display of the 
entire video frame. pTHINC provides visually more appealing
video display than RDP, VNC, or ICA.
5. RELATED WORK
Several studies have examined the web browsing 
performance of thin-client computing [13, 19, 10]. The ability for
thin clients to improve web browsing performance on 
wireless PDAs was first quantitatively demonstrated in a 
previous study by one of the authors [10]. This study 
demonstrated that thin clients can provide both faster web 
browsing performance and greater web browsing functionality.
The study considered a wide range of web content including
content from medical information systems. Our work builds
on this previous study and consider important issues such as
how usable existing thin clients are in PDA environments,
the trade-offs between thin-client usability and performance,
performance across different PDA devices, and the 
performance of thin clients on common web-related applications
such as video.
Many thin clients have been developed and some have
PDA clients, including Microsoft"s Remote Desktop [3], 
Citrix MetraFrame XP [2], Virtual Network Computing [16,
12], GoToMyPC [5], and Tarantella [18]. These systems
were first designed for desktop computing and retrofitted
for PDAs. Unlike pTHINC, they do not address key 
system architecture and usability issues important for PDAs.
This limits their display quality, system performance, 
available screen space, and overall usability on PDAs. pTHINC
builds on previous work by two of the authors on THINC [1],
extending the server architecture and introducing a client 
interface and usage model to efficiently support PDA devices
for mobile web applications.
Other approaches to improve the performance of mobile
wireless web browsing have focused on using transcoding
and caching proxies in conjunction with the fat client model
[11, 9, 4, 8]. They work by pushing functionality to external
proxies, and using specialized browsing applications on the
PDA device that communicate with the proxy. Our 
thinclient approach differs fundamentally from these fat-client
approaches by pushing all web browser logic to the server,
leveraging existing investments in desktop web browsers and
helper applications to work seamlessly with production 
systems without any additional proxy configuration or web
browser modifications.
With the emergence of web browsing on small display 
devices, web sites have been redesigned using mechanisms like
WAP and specialized native web browsers have been 
developed to tailor the needs of these devices. Recently, Opera
has developed the Opera Mini [15] web browser, which uses
an approach similar to the thin-client model to provide 
access across a number of mobile devices that would normally
be incapable of running a web browser. Instead of requiring
the device to process web pages, it uses a remote server to
pre-process the page before sending it to the phone.
6. CONCLUSIONS
We have introduced pTHINC, a thin-client architecture
for wireless PDAs. pTHINC provides key architectural and
usability mechanisms such as server-side screen resizing, 
clientside screen rotation using simple copy techniques, YUV video
support, and maximizing screen space for display updates
and leveraging existing PDA control buttons for UI 
elements. pTHINC transparently supports traditional 
desktop browsers and their helper applications on PDA devices
and desktop machines, providing mobile users with 
ubiquitous access to a consistent, personalized, and full-featured
web environment across heterogeneous devices. We have
implemented pTHINC and measured its performance on
web applications compared to existing thin-client systems
and native web applications. Our results on multiple 
mobile wireless devices demonstrate that pTHINC delivers web
browsing performance up to 80 times better than existing
thin-client systems, and 8 times better than a native PDA
browser. In addition, pTHINC is the only PDA thin client
151
Figure 11: Video Screenshot: RDP 640x480 Figure 12: Video Screenshot: VNC 1024x768
Figure 13: Video Screenshot: ICA Resized 1024x768
Figure 14: Video Screenshot: pTHINC Resized 1024x768
that transparently provides full-screen, full frame rate video
playback, making web sites with multimedia content 
accessible to mobile web users.
7. ACKNOWLEDGEMENTS
This work was supported in part by NSF ITR grants 
CCR0219943 and CNS-0426623, and an IBM SUR Award.
8. REFERENCES
[1] R. Baratto, L. Kim, and J. Nieh. THINC: A Virtual
Display Architecture for Thin-Client Computing. In
Proceedings of the 20th ACM Symposium on Operating
Systems Principles (SOSP), Oct. 2005.
[2] Citrix Metaframe. http://www.citrix.com.
[3] B. C. Cumberland, G. Carius, and A. Muir. Microsoft
Windows NT Server 4.0, Terminal Server Edition:
Technical Reference. Microsoft Press, Redmond, WA, 1999.
[4] A. Fox, I. Goldberg, S. D. Gribble, and D. C. Lee.
Experience With Top Gun Wingman: A Proxy-Based
Graphical Web Browser for the 3Com PalmPilot. In
Proceedings of Middleware "98, Lake District, England,
September 1998, 1998.
[5] GoToMyPC. http://www.gotomypc.com/.
[6] Health Insurance Portability and Accountability Act.
http://www.hhs.gov/ocr/hipaa/.
[7] i-Bench version 1.5. http:
//etestinglabs.com/benchmarks/i-bench/i-bench.asp.
[8] A. Joshi. On proxy agents, mobility, and web access.
Mobile Networks and Applications, 5(4):233-241, 2000.
[9] J. Kangasharju, Y. G. Kwon, and A. Ortega. Design and
Implementation of a Soft Caching Proxy. Computer
Networks and ISDN Systems, 30(22-23):2113-2121, 1998.
[10] A. Lai, J. Nieh, B. Bohra, V. Nandikonda, A. P. Surana,
and S. Varshneya. Improving Web Browsing on Wireless
PDAs Using Thin-Client Computing. In Proceedings of the
13th International World Wide Web Conference (WWW),
May 2004.
[11] A. Maheshwari, A. Sharma, K. Ramamritham, and
P. Shenoy. TranSquid: Transcoding and caching proxy for
heterogenous ecommerce environments. In Proceedings of
the 12th IEEE Workshop on Research Issues in Data
Engineering (RIDE "02), Feb. 2002.
[12] .NET VNC Viewer for PocketPC.
http://dotnetvnc.sourceforge.net/.
[13] J. Nieh, S. J. Yang, and N. Novik. Measuring Thin-Client
Performance Using Slow-Motion Benchmarking. ACM
Trans. Computer Systems, 21(1):87-115, Feb. 2003.
[14] J. Nielsen. Designing Web Usability. New Riders
Publishing, Indianapolis, IN, 2000.
[15] Opera Mini Browser.
http://www.opera.com/products/mobile/operamini/.
[16] T. Richardson, Q. Stafford-Fraser, K. R. Wood, and
A. Hopper. Virtual Network Computing. IEEE Internet
Computing, 2(1), Jan./Feb. 1998.
[17] R. W. Scheifler and J. Gettys. The X Window System.
ACM Trans. Gr., 5(2):79-106, Apr. 1986.
[18] Sun Secure Global Desktop.
http://www.sun.com/software/products/sgd/.
[19] S. J. Yang, J. Nieh, S. Krishnappa, A. Mohla, and
M. Sajjadpour. Web Browsing Performance of Wireless
Thin-Client Computing. In Proceedings of the 12th
International World Wide Web Conference (WWW), May
2003.
152
Multi-dimensional Range Queries in Sensor Networks∗
Xin Li
†
Young Jin Kim
†
Ramesh Govindan
†
Wei Hong
‡
ABSTRACT
In many sensor networks, data or events are named by 
attributes. Many of these attributes have scalar values, so one
natural way to query events of interest is to use a 
multidimensional range query. An example is: List all events
whose temperature lies between 50◦
and 60◦
, and whose
light levels lie between 10 and 15. Such queries are useful
for correlating events occurring within the network.
In this paper, we describe the design of a distributed 
index that scalably supports multi-dimensional range queries.
Our distributed index for multi-dimensional data (or DIM)
uses a novel geographic embedding of a classical index data
structure, and is built upon the GPSR geographic routing
algorithm. Our analysis reveals that, under reasonable 
assumptions about query distributions, DIMs scale quite well
with network size (both insertion and query costs scale as
O(
√
N)). In detailed simulations, we show that in practice,
the insertion and query costs of other alternatives are 
sometimes an order of magnitude more than the costs of DIMs,
even for moderately sized network. Finally, experiments on
a small scale testbed validate the feasibility of DIMs.
Categories and Subject Descriptors
C.2.4 [Computer Communication Networks]: Distributed
Systems; C.3 [Special-Purpose and Application-Based
Systems]: Embedded Systems
General Terms
Embedded Systems, Sensor Networks, Storage
1. INTRODUCTION
In wireless sensor networks, data or events will be named
by attributes [15] or represented as virtual relations in a
distributed database [18, 3]. Many of these attributes will
have scalar values: e.g., temperature and light levels, soil
moisture conditions, etc. In these systems, we argue, one
natural way to query for events of interest will be to use
multi-dimensional range queries on these attributes. For
example, scientists analyzing the growth of marine 
microorganisms might be interested in events that occurred within
certain temperature and light conditions: List all events
that have temperatures between 50◦
F and 60◦
F, and light
levels between 10 and 20.
Such range queries can be used in two distinct ways. They
can help users efficiently drill-down their search for events of
interest. The query described above illustrates this, where
the scientist is presumably interested in discovering, and
perhaps mapping the combined effect of temperature and
light on the growth of marine micro-organisms. More 
importantly, they can be used by application software running
within a sensor network for correlating events and triggering
actions. For example, if in a habitat monitoring application,
a bird alighting on its nest is indicated by a certain range
of thermopile sensor readings, and a certain range of 
microphone readings, a multi-dimensional range query on those
attributes enables higher confidence detection of the arrival
of a flock of birds, and can trigger a system of cameras.
In traditional database systems, such range queries are
supported using pre-computed indices. Indices trade-off some
initial pre-computation cost to achieve a significantly more
efficient querying capability. For sensor networks, we 
assert that a centralized index for multi-dimensional range
queries may not be feasible for energy-efficiency reasons (as
well as the fact that the access bandwidth to this central
index will be limited, particularly for queries emanating
from within the network). Rather, we believe, there will
be situations when it is more appropriate to build an 
innetwork distributed data structure for efficiently answering
multi-dimensional range queries.
In this paper, we present just such a data structure, that
we call a DIM1
. DIMs are inspired by classical database 
indices, and are essentially embeddings of such indices within
the sensor network. DIMs leverage two key ideas: in-network
1
Distributed Index for Multi-dimensional data.
63
data centric storage, and a novel locality-preserving 
geographic hash (Section 3). DIMs trace their lineage to 
datacentric storage systems [23]. The underlying mechanism in
these systems allows nodes to consistently hash an event to
some location within the network, which allows efficient 
retrieval of events. Building upon this, DIMs use a technique
whereby events whose attribute values are close are likely
to be stored at the same or nearby nodes. DIMs then use
an underlying geographic routing algorithm (GPSR [16]) to
route events and queries to their corresponding nodes in an
entirely distributed fashion.
We discuss the design of a DIM, presenting algorithms for
event insertion and querying, for maintaining a DIM in the
event of node failure, and for making DIMs robust to data or
packet loss (Section 3). We then extensively evaluate DIMs
using analysis (Section 4), simulation (Section 5), and actual
implementation (Section 6). Our analysis reveals that, 
under reasonable assumptions about query distributions, DIMs
scale quite well with network size (both insertion and query
costs scale as O(
√
N)). In detailed simulations, we show
that in practice, the event insertion and querying costs of
other alternatives are sometimes an order of magnitude the
costs of DIMs, even for moderately sized network. 
Experiments on a small scale testbed validate the feasibility of
DIMs (Section 6). Much work remains, including efficient
support for skewed data distributions, existential queries,
and node heterogeneity.
We believe that DIMs will be an essential, but perhaps
not necessarily the only, distributed data structure 
supporting efficient queries in sensor networks. DIMs will be part
of a suite of such systems that enable feature extraction [7],
simple range querying [10], exact-match queries [23], or 
continuous queries [15, 18]. All such systems will likely be
integrated to a sensor network database system such as
TinyDB [17]. Application designers could then choose the
appropriate method of information access. For instance,
a fire tracking application would use DIM to detect the
hotspots, and would then use mechanisms that enable 
continuous queries [15, 18] to track the spatio-temporal progress
of the hotspots. Finally, we note that DIMs are applicable
not just to sensor networks, but to other deeply distributed
systems (embedded networks for home and factory 
automation) as well.
2. RELATED WORK
The basic problem that this paper addresses - 
multidimensional range queries - is typically solved in database
systems using indexing techniques. The database 
community has focused mostly on centralized indices, but distributed
indexing has received some attention in the literature.
Indexing techniques essentially trade-off some data 
insertion cost to enable efficient querying. Indexing has, for long,
been a classical research problem in the database 
community [5, 2]. Our work draws its inspiration from the class
of multi-key constant branching index structures, 
exemplified by k-d trees [2], where k represents the dimensionality
of the data space. Our approach essentially represents a
geographic embedding of such structures in a sensor field.
There is one important difference. The classical indexing
structures are data-dependent (as are some indexing schemes
that use locality preserving hashes, and developed in the
theory literature [14, 8, 13]). The index structure is decided
not only by the data, but also by the order in which data
is inserted. Our current design is not data dependent. 
Finally, tangentially related to our work is the class of spatial
indexing systems [21, 6, 11].
While there has been some work on distributed indexing,
the problem has not been extensively explored. There 
exist distributed indices of a restricted kind-those that allow
exact match or partial prefix match queries. Examples of
such systems, of course, are the Internet Domain Name 
System, and the class of distributed hash table (DHT) systems
exemplified by Freenet[4], Chord[24], and CAN[19]. Our
work is superficially similar to CAN in that both construct
a zone-based overlay atop of the underlying physical 
network. The underlying details make the two systems very
different: CAN"s overlay is purely logical while our overlay
is consistent with the underlying physical topology. More
recent work in the Internet context has addressed support
for range queries in DHT systems [1, 12], but it is unclear if
these directly translate to the sensor network context.
Several research efforts have expressed the vision of a
database interface to sensor networks [9, 3, 18], and there
are examples of systems that contribute to this vision [18,
3, 17]. Our work is similar in spirit to this body of 
literature. In fact, DIMs could become an important component
of a sensor network database system such as TinyDB [17].
Our work departs from prior work in this area in two 
significant respects. Unlike these approaches, in our work the data
generated at a node are hashed (in general) to different 
locations. This hashing is the key to scaling multi-dimensional
range searches. In all the other systems described above,
queries are flooded throughout the network, and can 
dominate the total cost of the system. Our work avoids query
flooding by an appropriate choice of hashing. Madden et
al. [17] also describe a distributed index, called Semantic
Routing Trees (SRT). This index is used to direct queries
to nodes that have detected relevant data. Our work 
differs from SRT in three key aspects. First, SRT is built on
single attributes while DIM supports mulitple attributes.
Second, SRT constructs a routing tree based on historical
sensor readings, and therefore works well only for 
slowlychanging sensor values. Finally, in SRT queries are issued
from a fixed node while in DIM queries can be issued from
any node.
A similar differentiation applies with respect to work on
data-centric routing in sensor networks [15, 25], where data
generated at a node is assumed to be stored at the node,
and queries are either flooded throughout the network [15],
or each source sets up a network-wide overlay announcing its
presence so that mobile sinks can rendezvous with sources
at the nearest node on the overlay [25]. These approaches
work well for relatively long-lived queries.
Finally, our work is most close related to data-centric
storage [23] systems, which include geographic hash-tables
(GHTs) [20], DIMENSIONS [7], and DIFS [10].In a GHT,
data is hashed by name to a location within the network, 
enabling highly efficient rendezvous. GHTs are built upon the
GPSR [16] protocol and leverage some interesting properties
of that protocol, such as the ability to route to a node nearest
to a given location. We also leverage properties in GPSR (as
we describe later), but we use a locality-preserving hash to
store data, enabling efficient multi-dimensional range queries.
DIMENSIONS and DIFS can be thought of as using the
same set of primitives as GHT (storage using consistent
hashing), but for different ends: DIMENSIONS allows 
drill64
down search for features within a sensor network, while
DIFS allows range queries on a single key in addition to
other operations.
3. THE DESIGN OF DIMS
Most sensor networks are deployed to collect data from
the environment. In these networks, nodes (either 
individually or collaboratively) will generate events. An event
can generally be described as a tuple of attribute values,
A1, A2, · · · , Ak , where each attribute Ai represents a 
sensor reading, or some value corresponding to a detection
(e.g., a confidence level). The focus of this paper is the 
design of systems to efficiently answer multi-dimensional range
queries of the form: x1 − y1, x2 − y2, · · · , xk − yk . Such a
query returns all events whose attribute values fall into the
corresponding ranges. Notice that point queries, i.e., queries
that ask for events with specified values for each attribute,
are a special case of range queries.
As we have discussed in Section 1, range queries can 
enable efficient correlation and triggering within the network.
It is possible to implement range queries by flooding a query
within the network. However, as we show in later sections,
this alternative can be inefficient, particularly as the system
scales, and if nodes within the network issue such queries 
relatively frequently. The other alternative, sending all events
to an external storage node results in the access link being
a bottleneck, especially if nodes within the network issue
queries. Shenker et al. [23] also make similar arguments with
respect to data-centric storage schemes in general; DIMs are
an instance of such schemes.
The system we present in this paper, the DIM, relies upon
two foundations: a locality-preserving geographic hash, and
an underlying geographic routing scheme.
The key to resolving range queries efficiently is data 
locality: i.e., events with comparable attribute values are stored
nearby. The basic insight underlying DIM is that data 
locality can be obtained by a locality-preserving geographic
hash function. Our geographic hash function finds a 
localitypreserving mapping from the multi-dimensional space 
(described by the set of attributes) to a 2-d geographic space;
this mapping is inspired by k-d trees [2] and is described
later. Moreover, each node in the network self-organizes
to claim part of the attribute space for itself (we say that
each node owns a zone), so events falling into that space are
routed to and stored at that node.
Having established the mapping, and the zone structure,
DIMs use a geographic routing algorithm previously 
developed in the literature to route events to their corresponding
nodes, or to resolve queries. This algorithm, GPSR [16],
essentially enables the delivery of a packet to a node at a
specified location. The routing mechanism is simple: when
a node receives a packet destined to a node at location X, it
forwards the packet to the neighbor closest to X. In GPSR,
this is called greedy-mode forwarding. When no such 
neighbor exists (as when there exists a void in the network), the
node starts the packet on a perimeter mode traversal, 
using the well known right-hand rule to circumnavigate voids.
GPSR includes efficient techniques for perimeter traversal
that are based on graph planarization algorithms amenable
to distributed implementation.
For all of this to work, DIMs make two assumptions that
are consistent with the literature [23]. First, all nodes know
the approximate geographic boundaries of the network. These
boundaries may either be configured in nodes at the time of
deployment, or may be discovered using a simple protocol.
Second, each node knows its geographic location. Node 
locations can be automatically determined by a localization
system or by other means.
Although the basic idea of DIMs may seem 
straightforward, it is challenging to design a completely distributed
data structure that must be robust to packet losses and
node failures, yet must support efficient query distribution
and deal with communication voids and obstacles. We now
describe the complete design of DIMs.
3.1 Zones
The key idea behind DIMs, as we have discussed, is a 
geographic locality-preserving hash that maps a multi-attribute
event to a geographic zone. Intuitively, a zone is a 
subdivision of the geographic extent of a sensor field.
A zone is defined by the following constructive procedure.
Consider a rectangle R on the x-y plane. Intuitively, R is
the bounding rectangle that contains all sensors withing the
network. We call a sub-rectangle Z of R a zone, if Z is
obtained by dividing R k times, k ≥ 0, using a procedure
that satisfies the following property:
After the i-th division, 0 ≤ i ≤ k, R is 
partitioned into 2i
equal sized rectangles. If i is an
odd (even) number, the i-th division is parallel
to the y-axis (x-axis).
That is, the bounding rectangle R is first sub-divided into
two zones at level 0 by a vertical line that splits R into two
equal pieces, each of these sub-zones can be split into two
zones at level 1 by a horizontal line, and so on. We call the
non-negative integer k the level of zone Z, i.e. level(Z) = k.
A zone can be identified either by a zone code code(Z)
or by an address addr(Z). The code code(Z) is a 0-1 bit
string of length level(Z), and is defined as follows. If Z lies
in the left half of R, the first (from the left) bit of code(Z)
is 0, else 1. If Z lies in the bottom half of R, the second
bit of code(Z) is 0, else 1. The remaining bits of code(Z)
are then recursively defined on each of the four quadrants of
R. This definition of the zone code matches the definition
of zones given above, encoding divisions of the sensor field
geography by bit strings. Thus, in Figure 2, the zone in the
top-right corner of the rectangle R has a zone code of 1111.
Note that the zone codes collectively define a zone tree such
that individual zones are at the leaves of this tree.
The address of a zone Z, addr(Z), is defined to be the 
centroid of the rectangle defined by Z. The two representations
of a zone (its code and its address) can each be computed
from the other, assuming the level of the zone is known.
Two zones are called sibling zones if their zone codes are
the same except for the last bit. For example, if code(Z1) =
01101 and code(Z2) = 01100, then Z1 and Z2 are sibling
zones. The sibling subtree of a zone is the subtree rooted
at the left or right sibling of the zone in the zone tree. We
uniquely define a backup zone for each zone as follows: if
the sibling subtree of the zone is on the left, the backup
zone is the right-most zone in the sibling subtree; 
otherwise, the backup zone is the left-most zone in the sibling
subtree. For a zone Z, let p be the first level(Z) − 1 digits
of code(Z). Let backup(Z) be the backup zone of zone Z.
If code(Z) = p1, code(backup(Z)) = p01∗ with the most
number of trailing 1"s (∗ means 0 or 1 occurrences). If
65
code(Z) = p0, code(backup(Z)) = p10∗ with the most 
number of trailing 0"s.
3.2 Associating Zones with Nodes
Our definition of a zone is independent of the actual 
distribution of nodes in the sensor field, and only depends upon
the geographic extent (the bounding rectangle) of the sensor
field. Now we describe how zones are mapped to nodes.
Conceptually, the sensor field is logically divided into zones
and each zone is assigned to a single node. If the sensor 
network were deployed in a grid-like (i.e., very regular) fashion,
then it is easy to see that there exists a k such that each
node maps into a distinct level-k zone. In general, however,
the node placements within a sensor field are likely to be less
regular than the grid. For some k, some zones may be empty
and other zones might have more than one node situated
within them. One alternative would have been to choose
a fixed k for the overall system, and then associate nodes
with the zones they are in (and if a zone is empty, associate
the nearest node with it, for some definition of nearest).
Because it makes our overall query routing system simpler,
we allow nodes in a DIM to map to different-sized zones.
To precisely understand the associations between zones
and nodes, we define the notion of zone ownership. For any
given placement of network nodes, consider a node A. Let
ZA to be the largest zone that includes only node A and no
other node. Then, we say that A owns ZA. Notice that this
definition of ownership may leave some sections of the sensor
field un-associated with a node. For example, in Figure 2,
the zone 110 does not contain any nodes and would not have
an owner. To remedy this, for any empty zone Z, we define
the owner to be the owner of backup(Z). In our example,
that empty zone"s owner would also be the node that owns
1110, its backup zone.
Having defined the association between nodes and zones,
the next problem we tackle is: given a node placement, does
there exist a distributed algorithm that enables each node
to determine which zones it owns, knowing only the overall
boundary of the sensor network? In principle, this should
be relatively straightforward, since each node can simply
determine the location of its neighbors, and apply simple
geometric methods to determine the largest zone around it
such that no other node resides in that zone. In practice,
however, communication voids and obstacles make the 
algorithm much more challenging. In particular, resolving the
ownership of zones that do not contain any nodes is 
complicated. Equally complicated is the case where the zone
of a node is larger than its communication radius and the
node cannot determine the boundaries of its zone by local
communication alone.
Our distributed zone building algorithm defers the 
resolution of such zones until when either a query is initiated, or
when an event is inserted. The basic idea behind our 
algorithm is that each node tentatively builds up an idea of the
zone it resides in just by communicating with its neighbors
(remembering which boundaries of the zone are undecided
because there is no radio neighbor that can help resolve that
boundary). These undecided boundaries are later resolved
by a GPSR perimeter traversal when data messages are 
actually routed.
We now describe the algorithm, and illustrate it using 
examples. In our algorithm, each node uses an array bound[0..3]
to maintain the four boundaries of the zone it owns 
(rememFigure 1: A network, where circles represent sensor
nodes and dashed lines mark the network boundary.
1111
011
00
110
100
101
1110
010
Figure 2: The zone code and boundaries.
0 1
0 1
10
10
1 1
10
00
Figure 3: The Corresponding Zone Tree
ber that in this algorithm, the node only tries to determine
the zone it resides in, not the other zones it might own
because those zones are devoid of nodes). When a node
starts up, each node initializes this array to be the network
boundary, i.e., initially each node assumes its zone contains
the whole network. The zone boundary algorithm now 
relies upon GPSR"s beacon messages to learn the locations of
neighbors within radio range. Upon hearing of such a 
neighbor, the node calls the algorithm in Figure 4 to update its
zone boundaries and its code accordingly. In this algorithm,
we assume that A is the node at which the algorithm is 
executed, ZA is its zone, and a is a newly discovered neighbor
of A. (Procedure Contain(ZA, a) is used to decide if node
a is located within the current zone boundaries of node A).
Using this algorithm, then, each node can independently
and asynchronously decide its own tentative zone based on
the location of its neighbors. Figure 2 illustrates the results
of applying this algorithm for the network in Figure 1.
Figure 3 describes the corresponding zone tree. Each zone
resides at a leaf node and the code of a zone is the path from
the root to the zone if we represent the branch to the left
66
Build-Zone(a)
1 while Contain(ZA, a)
2 do if length(code(ZA)) mod 2 = 0
3 then new bound ← (bound[0] + bound[1])/2
4 if A.x < new bound
5 then bound[1] ← new bound
6 else bound[0] ← new bound
7 else new bound ← (bound[2] + bound[3])/2
8 if A.y < new bound
9 then bound[3] ← new bound
10 else bound[2] ← new bound
11 Update zone code code(ZA)
Figure 4: Zone Boundary Determination, where A.x
and A.y represent the geographic coordinate of node
A.
Insert-Event(e)
1 c ← Encode(e)
2 if Contain(ZA, c) = true and is Internal() = true
3 then Store e and exit
4 Send-Message(c, e)
Send-Message(c, m)
1 if ∃ neighbor Y, Closer(Y, owner(m), m) = true
2 then addr(m) ← addr(Y )
3 else if length(c) > length(code(m))
4 then Update code(m) and addr(m)
5 source(m) ← caller
6 if is Owner(msg) = true
7 then owner(m) ← caller"s code
8 Send(m)
Figure 5: Inserting an event in a DIM. Procedure
Closer(A, B, m) returns true if code(A) is closer to
code(m) than code(B). source(m) is used to set the source
address of message m.
child by 0 and the branch to the right child by 1. This binary
tree forms the index that we will use in the following event
and query processing procedures.
We see that the zone sizes are different and depend on
the local densities and so are the lengths of zone codes for
different nodes. Notice that in Figure 2, there is an empty
zone whose code should be 110. In this case, if the node in
zone 1111 can only hear the node in zone 1110, it sets its
boundary with the empty zone to undecided, because it did
not hear from any neighboring nodes from that direction.
As we have mentioned before, the undecided boundaries are
resolved using GPSR"s perimeter mode when an event is
inserted, or a query sent. We describe event insertion in the
next step.
Finally, this description does not describe how a node"s
zone codes are adjusted when neighboring nodes fail, or new
nodes come up. We return to this in Section 3.5.
3.3 Inserting an Event
In this section, we describe how events are inserted into
a DIM. There are two algorithms of interest: a consistent
hashing technique for mapping an event to a zone, and a
routing algorithm for storing the event at the appropriate
zone. As we shall see, these two algorithms are inter-related.
3.3.1 Hashing an Event to a Zone
In Section 3.1, we described a recursive tessellation of
the geographic extent of a sensor field. We now describe
a consistent hashing scheme for a DIM that supports range
queries on m distinct attributes2
Let us denote these attributes A1 . . . Am. For simplicity,
assume for now that the depth of every zone in the network
is k, k is a multiple of m, and that this value of k is known
to every node. We will relax this assumption shortly. 
Furthermore, for ease of discussion, we assume that all attribute
values have been normalized to be between 0 and 1.
Our hashing scheme assigns a k bit zone code to an event
as follows. For i between 1 and m, if Ai < 0.5, the i-th
bit of the zone code is assigned 0, else 1. For i between
m + 1 and 2m, if Ai−m < 0.25 or Ai−m ∈ [0.5, 0.75), the
i-th bit of the zone is assigned 0, else 1, because the next
level divisions are at 0.25 and 0.75 which divide the ranges
to [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). We repeat
this procedure until all k bits have been assigned. As an
example, consider event E = 0.3, 0.8 . For this event, the
5-bit zone code is code(ZA) = 01110.
Essentially, our hashing scheme uses the values of the 
attributes in round-robin fashion on the zone tree (such as
the one in Figure 3), in order to map an m-attribute event
to a zone code. This is reminiscent of k-d trees [2], but
is quite different from that data structure: zone trees are
spatial embeddings and do not incorporate the re-balancing
algorithms in k-d trees.
In our design of DIMs, we do not require nodes to have
zone codes of the same length, nor do we expect a node to
know the zone codes of other nodes. Rather, suppose the
encoding node is A and its own zone code is of length kA.
Then, given an event E, node A only hashes E to a zone
code of length kA. We denote the zone code assigned to an
event E by code(E). As we describe below, as the event is
routed, code(E) is refined by intermediate nodes. This lazy
evaluation of zone codes allows different nodes to use 
different length zone codes without any explicit coordination.
3.3.2 Routing an Event to its Owner
The aim of hashing an event to a zone code is to store the
event at the node within the network node that owns that
zone. We call this node the owner of the event. Consider
an event E that has just been generated at a node A. After
encoding event E, node A compares code(E) with code(A).
If the two are identical, node A store event E locally; 
otherwise, node A will attempt to route the event to its owner.
To do this, note that code(E) corresponds to some zone
Z , which is A"s current guess for the zone at which event E
should be stored. A now invokes GPSR to send a message
to addr(Z ) (the centroid of Z , Section 3.1). The message
contains the event E, code(E), and the target geographic
location for storing the event. In the message, A also marks
itself as the owner of event E. As we will see later, the
guessed zone Z , the address addr(Z ), and the owner of
E, all of them contained in the message, will be refined by
intermediate forwarding nodes.
GPSR now delivers this message to the next hop towards
addr(Z ) from A. This next hop node (call it B) does not 
immediately forward the message. Rather, it attempts to 
com2
DIM does not assume that all nodes are homogeneous in
terms of the sensors they have. Thus, in an m dimensional
DIM, a node that does not possess all m sensors can use NULL
values for the corresponding readings. DIM treats NULL as
an extreme value for range comparisons. As an aside, a 
network may have many DIM instances running concurrently.
67
pute a new zone code for E to get a new code codenew(E).
B will update the code contained in the message (and also
the geographic destination of the message) if codenew(E) is
longer than the event code in the message. In this manner,
as the event wends its way to its owner, its zone code gets
refined. Now, B compares its own code code(B) against the
owner code owner(E) contained in the incoming message.
If code(B) has a longer match with code(E) than the 
current owner owner(E), then B sets itself to be the current
owner of E, meaning that if nobody is eligible to store E,
then B will store the event (we shall see how this happens
next). If B"s zone code does not exactly match code(E), B
will invoke GPSR to deliver E to the next hop.
3.3.3 Resolving undecided zone boundaries during
insertion
Suppose that some node, say C, finds itself to be the 
destination (or eventual owner) of an event E. It does so by
noticing that code code(C) equals code(E) after locally 
recomputing a code for E. In that case, C stores E locally, but
only if all four of C"s zone boundaries are decided. When
this condition holds, C knows for sure that no other nodes
have overlapped zones with it. In this case, we call C an
internal node.
Recall, though, that because the zone discovery algorithm
Section 3.2 only uses information from immediate neighbors,
one or more of C"s boundaries may be undecided. If so, C
assumes that some other nodes have a zone that overlaps
with its own, and sets out to resolve this overlap. To do
this, C now sets itself to be the owner of E and continues
forwarding the message. Here we rely on GPSR"s 
perimeter mode routing to probe around the void that causes the
undecided boundary. Since the message starts from C and
is destined for a geographic location near C, GPSR 
guarantees that the message will be delivered back to C if no
other nodes will update the information in the message. If
the message comes back to C with itself to be the owner, C
infers that it must be the true owner of the zone and stores
E locally.
If this does not happen, there are two possibilities. The
first is that as the event traverses the perimeter, some 
intermediate node, say B whose zone overlaps with C"s marks
itself to be the owner of the event, but otherwise does not
change the event"s zone code. This node also recognizes that
its own zone overlaps with C"s and initiates a message 
exchange which causes each of them to appropriately shrink
their zone.
Figures 6 through 8 show an example of this data-driven
zone shrinking. Initially, both node A and node B have
claimed the same zone 0 because they are out of radio range
of each other. Suppose that A inserts an event E = 0.4, 0.8, 0.9 .
A encodes E to 0 and claims itself to be the owner of E.
Since A is not an internal node, it sends out E, looking for
other owner candidates of E. Once E gets to node B, B will
see in the message"s owner field A"s code that is the same as
its own. B then shrinks its zone from 0 to 01 according to
A"s location which is also recorded in the message and send
a shrink request to A. Upon receiving this request, A also
shrinks its zone from 0 to 00.
A second possibility is if some intermediate node changes
the destination code of E to a more specific value (i.e.,
longer zone code). Let us label this node D. D now tries
to initiate delivery to the centroid of the new zone. This
A
B
0
0
110
100
1111
1110
101
Figure 6: Nodes A and B have claimed the same zone.
A
B
<0.4,0.8,0.9>
Figure 7: An event/query message (filled arrows) 
triggers zone shrinking (hollow arrows).
A
B
01
00
110
100
1111
1110
101
Figure 8: The zone layout after shrinking. Now node
A and B have been mapped to different zones.
might result in a new perimeter walk that returns to D (if,
for example, D happens to be geographically closest to the
centroid of the zone). However, D would not be the owner
of the event, which would still be C. In routing to the 
centroid of this zone, the message may traverse the perimeter
and return to D. Now D notices that C was the original
owner, so it encapsulates the event and directs it to C. In
case that there indeed is another node, say X, that owns
an overlapped zone with C, X will notice this fact by 
finding in the message the same prefix of the code of one of
its zones, but with a different geographic location from its
own. X will shrink its zone to resolve the overlap. If X"s
zone is smaller than or equal to C"s zone, X will also send
a shrink request to C. Once C receives a shrink request,
it will reduce its zone appropriately and fix its undecided
boundary. In this manner, the zone formation process is
resolved on demand in a data-driven way.
68
There are several interesting effects with respect to 
perimeter walking that arise in our algorithm. The first is that
there are some cases where an event insertion might cause
the entire outer perimeter of the network to be traversed3
.
Figure 6 also works as an example where the outer 
perimeter is traversed. Event E inserted by A will eventually be
stored in node B. Before node B stores event E, if B"s 
nominal radio range does not intersect the network boundary, it
needs to send out E again as A did, because B in this case
is not an internal node. But if B"s nominal radio range 
intersects the network boundary, it then has two choices. It
can assume that there will not be any nodes outside the
network boundary and so B is an internal node. This is an
aggressive approach. On the other hand, B can also make
a conservative decision assuming that there might be some
other nodes it have not heard of yet. B will then force the
message walking another perimeter before storing it.
In some situations, especially for large zones where the
node that owns a zone is far away from the centroid of the
owned zone, there might exist a small perimeter around the
destination that does not include the owner of the zone. The
event will end up being stored at a different node than the
real owner. In order to deal with this problem, we add an
extra operation in event forwarding, called efficient neighbor
discovery. Before invoking GPSR, a node needs to check if
there exists a neighbor who is eligible to be the real owner of
the event. To do this, a node C, say, needs to know the zone
codes of its neighboring nodes. We deploy GPSR"s 
beaconing message to piggyback the zone codes for nodes. So by
simply comparing the event"s code and neighbor"s code, a
node can decide whether there exists a neighbor Y which
is more likely to be the owner of event E. C delivers E
to Y , which simply follows the decision making procedure
discussed above.
3.3.4 Summary and Pseudo-code
In summary, our event insertion procedure is designed to
nicely interact with the zone discovery mechanism, and the
event hashing mechanism. The latter two mechanisms are
kept simple, while the event insertion mechanism uses lazy
evaluation at each hop to refine the event"s zone code, and it
leverages GPSR"s perimeter walking mechanism to fix 
undecided zone boundaries. In Section 3.5, we address robustness
of event insertion to packet loss or to node failures.
Figure 5 shows the pseudo-code for inserting and 
forwarding an event e. In this pseudo code, we have omitted a 
description of the zone shrinking procedure. In the pseudo
code, procedure is Internal() is used to determine if the
caller is an internal node and procedure is Owner() is used
to determine if the caller is more eligible to be the owner of
the event than is currently claimed owner as recorded in the
message. Procedure Send-Message is used to send either
an event message or a query message. If the message 
destination address has been changed, the packet source address
needs also to be changed in order to avoid being dropped by
GPSR, since GPSR does not allow a node to see the same
packet in greedy mode twice.
3
This happens less frequently than for GHTs, where 
inserting an event to a location outside the actual (but inside
the nominal) boundary of the network will always invoke an
external perimeter walk.
3.4 Resolving and Routing Queries
DIMs support both point queries4
and range queries. 
Routing a point query is identical to routing an event. Thus, the
rest of this section details how range queries are routed.
The key challenge in routing zone queries is brought out
by the following strawman design. If the entire network was
divided evenly into zones of depth k (for some pre-defined
constant k), then the querier (the node issuing the query)
could subdivide a given range query into the relevant 
subzones and route individual requests to each of the zones.
This can be inefficient for large range queries and also hard
to implement in our design where zone sizes are not 
predefined. Accordingly, we use a slightly different technique
where a range query is initially routed to a zone 
corresponding to the entire range, and is then progressively split into
smaller subqueries. We describe this algorithm here.
The first step of the algorithm is to map a range query to
a zone code prefix. Conceptually, this is easy; in a zone tree
(Figure 3), there exists some node which contains the entire
range query in its sub-tree, and none of its children in the
tree do. The initial zone code we choose for the query is the
zone code corresponding to that tree node, and is a prefix of
the zone codes of all zones (note that these zones may not
be geographically contiguous) in the subtree. The querier
computes the zone code of Q, denoted by code(Q) and then
starts routing a query to addr(code(Q)).
Upon receiving a range query Q, a node A (where A is any
node on the query propagation path) divides it into multiple
smaller sized subqueries if there is an overlap between the
zone of A, zone(A) and the zone code associated with Q,
code(Q). Our approach to split a query Q into subqueries
is as follows. If the range of Q"s first attribute contains
the value 0.5, A divides Q into two sub-queries one of whose
first attribute ranges from 0 to 0.5, and the other from 0.5 to
1. Then A decides the half that overlaps with its own zone.
Let"s call it QA. If QA does not exist, then A stops splitting;
otherwise, it continues splitting (using the second attribute
range) and recomputing QA until QA is small enough so
that it completely falls into zone(A) and hence A can now
resolve it. For example, suppose that node A, whose code
is 0110, is to split a range query Q = 0.3 − 0.8, 0.6 − 0.9 .
The splitting steps is shown in Figure 2. After splitting,
we obtain three smaller queries q0 = 0.3 − 0.5, 0.6 − 0.75 ,
q1 = 0.3 − 0.5, 0.75 − 0.9 , and q2 = 0.5 − 0.8, 0.6 − 0.9 .
This splitting procedure is illustrated in Figure 9 which
also shows the codes of each subquery after splitting.
A then replies to subquery q0 with data stored locally
and sends subqueries q1 and q2 using the procedure outlined
above. More generally, if node A finds itself to be inside
the zone subtree that maximally covers Q, it will send the
subqueries that resulted from the split. Otherwise, if there
is no overlap between A and Q, then A forwards Q as is (in
this case Q is either the original query, or a product of an
earlier split).
Figure 10 describes the pseudo-code for the zone splitting
algorithm. As shown in the above algorithm, once a 
subquery has been recognized as belonging to the caller"s zone,
procedure Resolve is invoked to resolve the subquery and
send a reply to the querier. Every query message contains
4
By point queries, we mean the equality condition on all
indexed keys. DIM index attributes are not necessarily 
primary keys.
69
the geographic location of its initiator, so the corresponding
reply message can be delivered directly back to the 
initiator. Finally, in the process of query resolution, zones might
shrink similar to shrinkage during inserting. We omit this
in the pseudo code.
3.5 Robustness
Until now, we have not discussed the impact of node 
failures and packet losses, or node arrivals and departures on
our algorithms. Packet losses can affect query and event 
insertion, and node failures can result in lost data, while node
arrivals and departures can impact the zone structure. We
now discuss how DIMs can be made robust to these kinds
of dynamics.
3.5.1 Maintaining Zones
In previous sections, we described how the zone discovery
algorithm could leave zone boundaries undecided. These 
undecided boundaries are resolved during insertion or 
querying, using the zone shrinking procedure describe above.
When a new node joins the network, the zone discovery
mechanism (Section 3.2) will cause neighboring zones to 
appropriately adjust their zone boundaries. At this time, those
zones can also transfer to the new node those events they
store but which should belong to the new node.
Before a node turns itself off (if this is indeed possible), it
knows that its backup node (Section 3.1) will take over its
zone, and will simply send all its events to its backup node.
Node deletion may also cause zone expansion. In order to
keep the mapping between the binary zone tree"s leaf nodes
and zones, we allow zone expansion to only occur among
sibling zones (Section 3.1). The rule is: if zone(A)"s sibling
zone becomes empty, then A can expand its own zone to
include its sibling zone.
Now, we turn our attention to node failures. Node failures
are just like node deletions except that a failed node does
not have a chance to move its events to another node. But
how does a node decide if its sibling has failed? If the 
sibling is within radio range, the absence of GPSR beaconing
messages can detect this. Once it detects this, the node can
expand its zone. A different approach is needed for 
detecting siblings who are not within radio range. These are the
cases where two nodes own their zones after exchanging a
shrink message; they do not periodically exchange messages
thereafter to maintain this zone relationship. In this case,
we detect the failure in a data-driven fashion, with obvious
efficiency benefits compared to periodic keepalives. Once a
node B has failed, an event or query message that previously
should have been owned by the failed node will now be 
delivered to the node A that owns the empty zone left by node
B. A can see this message because A stands right around
the empty area left by B and is guaranteed to be visited in a
GPSR perimeter traversal. A will set itself to be the owner
of the message, and any node which would have dropped this
message due to a perimeter loop will redirect the message to
A instead. If A"s zone happens to be the sibling of B"s zone,
A can safely expand its own zone and notify its expanded
zone to its neighbors via GPSR beaconing messages.
3.5.2 Preventing Data Loss from Node Failure
The algorithms described above are robust in terms of
zone formation, but node failure can erase data. To avoid
this, DIMs can employ two kinds of replication: local 
replication to be resilient to random node failures, and mirror
replication for resilience to concurrent failure of 
geographically contiguous nodes.
Mirror replication is conceptually easy. Suppose an event
E has a zone code code(E). Then, the node that inserts
E would store two copies of E; one at the zone denoted
by code(E), and the other at the zone corresponding to the
one"s complement of code(E). This technique essentially
creates a mirror DIM. A querier would need, in parallel, to
query both the original DIM and its mirror since there is no
way of knowing if a collection of nodes has failed. Clearly,
the trade-off here is an approximate doubling of both 
insertion and query costs.
There exists a far cheaper technique to ensure resilience
to random node failures. Our local replication technique
rests on the observation that, for each node A, there exists
a unique node which will take over its zone when A fails.
This node is defined as the node responsible for A"s zone"s
backup zone (see Section 3.1). The basic idea is that A
replicates each data item it has in this node. We call this
node A"s local replica. Let A"s local replica be B. Often
B will be a radio neighbor of A and can be detected from
GPSR beacons. Sometimes, however, this is not the case,
and B will have to be explicitly discovered.
We use an explicit message for discovering the local replica.
Discovering the local replica is data-driven, and uses a 
mechanism similar to that of event insertion. Node A sends a
message whose geographic destination is a random nearby
location chosen by A. The location is close enough to A such
that GPSR will guarantee that the message will delivered
back to A. In addition, the message has three fields, one for
the zone code of A, code(A), one for the owner owner(A) of
zone(A) which is set to be empty, and one for the geographic
location of owner(A). Then the packet will be delivered in
GPSR perimeter mode. Each node that receives this 
message will compare its zone code and code(A) in the message,
and if it is more eligible to be the owner of zone(A) than
the current owner(A) recorded in the message, it will 
update the field owner(A) and the corresponding geographic
location. Once the packet comes back to A, it will know the
location of its local replica and can start to send replicas.
In a dense sensor network, the local replica of a node
is usually very near to the node, either its direct neighbor
or 1-2 hops away, so the cost of sending replicas to local
replication will not dominate the network traffic. However,
a node"s local replica itself may fail. There are two ways to
deal with this situation; periodic refreshes, or repeated 
datadriven discovery of local replicas. The former has higher
overhead, but more quickly discovers failed replicas.
3.5.3 Robustness to Packet Loss
Finally, the mechanisms for querying and event insertion
can be easily made resilient to packet loss. For event 
insertion, a simple ACK scheme suffices.
Of course, queries and responses can be lost as well. In
this case, there exists an efficient approach for error 
recovery. This rests on the observation that the querier knows
which zones fall within its query and should have responded
(we assume that a node that has no data matching a query,
but whose zone falls within the query, responds with a 
negative acknowledgment). After a conservative timeout, the
querier can re-issue the queries selectively to these zones.
If DIM cannot get any answers (positive or negative) from
70
<0.3-0.8, 0.6-0.9>
<0.5-0.8, 0.6-0.9><0.3-0.5, 0.6-0.9>
<0.3-0.5, 0.6-0.9>
<0.3-0.5, 0.6-0.9>
<0.3-0.5, 0.6-0.75> <0.3-0.5, 0.75-0.9>
0
0
1
1
1
1
Figure 9: An example of range query splitting
Resolve-Range-Query(Q)
1 Qsub ← nil
2 q0, Qsub ← Split-Query(Q)
3 if q0 = nil
4 then c ← Encode(Q)
5 if Contain(c, code(A)) = true
6 then go to step 12
7 else Send-Message(c, q0)
8 else Resolve(q0)
9 if is Internal() = true
10 then Absorb (q0)
11 else Append q0 to Qsub
12 if Qsub = nil
13 then for each subquery q ∈ Qsub
14 do c ← Encode(q)
15 Send-Message(c, q)
Figure 10: Query resolving algorithm
certain zones after repeated timeouts, it can at least return
the partial query results to the application together with the
information about the zones from which data is missing.
4. DIMS: AN ANALYSIS
In this section, we present a simple analytic performance
evaluation of DIMs, and compare their performance against
other possible approaches for implementing multi-dimensional
range queries in sensor networks. In the next section, we 
validate these analyses using detailed packet-level simulations.
Our primary metrics for the performance of a DIM are:
Average Insertion Cost measures the average number of
messages required to insert an event into the network.
Average Query Delivery Cost measures the average 
number of messages required to route a query message to
all the relevant nodes in the network.
It does not measure the number of messages required to
transmit responses to the querier; this latter number 
depends upon the precise data distribution and is the same
for many of the schemes we compare DIMs against.
In DIMs, event insertion essentially uses geographic 
routing. In a dense N-node network where the likelihood of
traversing perimeters is small, the average event insertion
cost proportional to
√
N [23].
On the other hand, the query delivery cost depends upon
the size of ranges specified in the query. Recall that our
query delivery mechanism is careful about splitting a query
into sub-queries, doing so only when the query nears the
zone that covers the query range. Thus, when the querier is
far from the queried zone, there are two components to the
query delivery cost. The first, which is proportional to
√
N,
is the cost to deliver the query near the covering zone. If
within this covering zone, there are M nodes, the message
delivery cost of splitting the query is proportional to M.
The average cost of query delivery depends upon the 
distribution of query range sizes. Now, suppose that query sizes
follow some density function f(x), then the average cost of
resolve a query can be approximated by
Ê N
1
xf(x)dx. To
give some intuition for the performance of DIMs, we 
consider four different forms for f(x): the uniform distribution
where a query range encompassing the entire network is as
likely as a point query; a bounded uniform distribution where
all sizes up to a bound B are equally likely; an algebraic 
distribution in which most queries are small, but large queries
are somewhat likely; and an exponential distribution where
most queries are small and large queries are unlikely. In all
our analyses, we make the simplifying assumption that the
size of a query is proportional to the number of nodes that
can answer that query.
For the uniform distribution P(x) ∝ c for some constant c.
If each query size from 1 . . . N is equally likely, the average
query delivery cost of uniformly distributed queries is O(N).
Thus, for uniformly distributed queries, the performance of
DIMs is comparable to that of flooding. However, for the
applications we envision, where nodes within the network
are trying to correlate events, the uniform distribution is
highly unrealistic.
Somewhat more realistic is a situation where all query
sizes are bounded by a constant B. In this case, the average
cost for resolving such a query is approximately
Ê B
1
xf(x)dx =
O(B). Recall now that all queries have to pay an 
approximate cost of O(
√
N) to deliver the query near the covering
zone. Thus, if DIM limited queries to a size proportional to√
N, the average query cost would be O(
√
N).
The algebraic distribution, where f(x) ∝ x−k
, for some
constant k between 1 and 2, has an average query resolution
cost given by
Ê N
1
xf(x)dx = O(N2−k
). In this case, if k >
1.5, the average cost of query delivery is dominated by the
cost to deliver the query to near the covering zone, given by
O(
√
N).
Finally, for the exponential distribution, f(x) = ce−cx
for
some constant c, and the average cost is just the mean of the
corresponding distribution, i.e., O(1) for large N. 
Asymptotically, then, the cost of the query for the exponential
distribution is dominated by the cost to deliver the query
near the covering zone (O(
√
N)).
Thus, we see that if queries follow either the bounded
uniform distribution, the algebraic distribution, or the 
exponential distribution, the query cost scales as the insertion
cost (for appropriate choice of constants for the bounded
uniform and the algebraic distributions).
How well does the performance of DIMs compare against
alternative choices for implementing multi-dimensional queries?
A simple alternative is called external storage [23], where all
events are stored centrally in a node outside the sensor 
network. This scheme incurs an insertion cost of O(
√
N), and
a zero query cost. However, as [23] points out, such systems
may be impractical in sensor networks since the access link
to the external node becomes a hotspot.
A second alternative implementation would store events
at the node where they are generated. Queries are flooded
71
throughout the network, and nodes that have matching data
respond. Examples of systems that can be used for this 
(although, to our knowledge, these systems do not implement
multi-dimensional range queries) are Directed Diffusion [15]
and TinyDB [17]. The flooding scheme incurs a zero 
insertion cost, but an O(N) query cost. It is easy to show that
DIMs outperform flooding as long as the ratio of the number
of insertions to the number of queries is less than
√
N.
A final alternative would be to use a geographic hash table
(GHT [20]). In this approach, attribute values are assumed
to be integers (this is actually quite a reasonable 
assumption since attribute values are often quantized), and events
are hashed on some (say, the first) attribute. A range query
is sub-divided into several sub-queries, one for each integer
in the range of the first attribute. Each sub-query is then
hashed to the appropriate location. The nodes that receive a
sub-query only return events that match all other attribute
ranges. In this approach, which we call GHT-R (GHT"s for
range queries) the insertion cost is O(
√
N). Suppose that
the range of the first attribute contains r discrete values.
Then the cost to deliver queries is O(r
√
N). Thus, 
asymptotically, GHT-R"s perform similarly to DIMs. In practice,
however, the proportionality constants are significantly 
different, and DIMs outperform GHT-Rs, as we shall show
using detailed simulations.
5. DIMS: SIMULATION RESULTS
Our analysis gives us some insight into the asymptotic
behavior of various approaches for multi-dimensional range
queries. In this section, we use simulation to compare DIMs
against flooding and GHT-R; this comparison gives us a
more detailed understanding of these approaches for 
moderate size networks, and gives us a nuanced view of the 
mechanistic differences between some of these approaches.
5.1 Simulation Methodology
We use ns-2 for our simulations. Since DIMs are 
implemented on top of GPSR, we first ported an earlier GPSR
implementation to the latest version of ns-2. We modified
the GPSR module to call our DIM implementation when
it receives any data message in transit or when it is about
to drop a message because that message traversed the entire
perimeter. This allows a DIM to modify message zone codes
in flight (Section 3), and determine the actual owner of an
event or query.
In addition, to this, we implemented in ns-2 most of the
DIM mechanisms described in Section 3. Of those 
mechanisms, the only one we did not implement is mirror 
replication. We have implemented selective query retransmission
for resiliency to packet loss, but have left the evaluation of
this mechanism to future work. Our DIM implementation
in ns-2 is 2800 lines of code.
Finally, we implemented GHT-R, our GHT-based 
multidimensional range query mechanism in ns-2. This 
implementation was relatively straightforward, given that we had
ported GPSR, and modified GPSR to detect the completion
of perimeter mode traversals.
Using this implementation, we conducted a fairly 
extensive evaluation of DIM and two alternatives (flooding, and
our GHT-R). For all our experiments, we use uniformly
placed sensor nodes with network sizes ranging from 50
nodes to 300 nodes. Each node has a radio range of 40m.
For the results presented here, each node has on average 20
nodes within its nominal radio range. We have conducted
experiments at other node densities; they are in agreement
with the results presented here.
In all our experiments, each node first generates 3 events5
on average (more precisely, for a topology of size N, we have
3N events, and each node is equally likely to generate an
event). We have conducted experiments for three different
event value distributions. Our uniform event distribution
generates 2-dimensional events and, for each dimension, 
every attribute value is equally likely. Our normal event 
distribution generates 2-dimensional events and, for each 
dimension, the attribute value is normally distributed with a
mean corresponding to the mid-point of the attribute value
range. The normal event distribution represents a skewed
data set. Finally, our trace event distribution is a collection
of 4-dimensional events obtained from a habitat monitoring
network. As we shall see, this represents a fairly skewed
data set.
Having generated events, for each simulation we 
generate queries such that, on average, each node generates 2
queries. The query sizes are determined using the four size
distributions we discussed in Section 4: uniform, 
boundeduniform, algebraic and exponential. Once a query size has
been determined, the location of the query (i.e., the actual
boundaries of the zone) are uniformly distributed. For our
GHT-R experiments, the dynamic range of the attributes
had 100 discrete values, but we restricted the query range
for any one attribute to 50 discrete values to allow those
simulations to complete in reasonable time.
Finally, using one set of simulations we evaluate the 
efficacy of local replication by turning off random fractions of
nodes and measuring the fidelity of the returned results.
The primary metrics for our simulations are the average
query and insertion costs, as defined in Section 4.
5.2 Results
Although we have examined almost all the combinations
of factors described above, we discuss only the most salient
ones here, for lack of space.
Figure 11 plots the average insertion costs for DIM and
GHT-R (for flooding, of course, the insertion costs are zero).
DIM incurs less per event overhead in inserting events 
(regardless of the actual event distribution; Figure 11 shows the
cost for uniformly distributed events). The reason for this is
interesting. In GHT-R, storing almost every event incurs a
perimeter traversal, and storing some events require 
traversing the outer perimeter of the network [20]. By contrast, in
DIM, storing an event incurs a perimeter traversal only when
a node"s boundaries are undecided. Furthermore, an 
insertion or a query in a DIM can traverse the outer perimeter
(Section 3.3), but less frequently than in GHTs.
Figure 13 plots the average query cost for a bounded 
uniform query size distribution. For this graph (and the next)
we use a uniform event distribution, since the event 
distribution does not affect the query delivery cost. For this 
simulation, our bound was 1
4
th the size of the largest possible
5
Our metrics are chosen so that the exact number of events
and queries is unimportant for our discussion. Of course,
the overall performance of the system will depend on the
relative frequency of events and queries, as we discuss in
Section 4. Since we don"t have realistic ratios for these, we
focus on the microscopic costs, rather than on the overall
system costs.
72
0
2
4
6
8
10
12
14
16
18
20
50 100 150 200 250 300
AverageCostperInsertion
Network Size
DIM
GHT-R
Figure 11: Average insertion cost for DIM and
GHT.
0.4
0.5
0.6
0.7
0.8
0.9
1
5 10 15 20 25 30
Fractionofrepliescomparedwithnon-failurecase
Fraction of failed nodes (%)
No Replication
Local Replication
Figure 12: Local replication performance.
query (e.g., a query of the form 0 − 0.5, 0 − 0.5 . Even for
this generous query size, DIMs perform quite well (almost
a third the cost of flooding). Notice, however, that 
GHTRs incur high query cost since almost any query requires as
many subqueries as the width of the first attribute"s range.
Figure 14 plots the average query cost for the exponential
distribution (the average query size for this distribution was
set to be 1
16
th the largest possible query). The superior
scaling of DIMs is evident in these graphs. Clearly, this is
the regime in which one might expect DIMs to perform best,
when most of the queries are small and large queries are
relatively rare. This is also the regime in which one would
expect to use multi-dimensional range queries: to perform
relatively tight correlations. As with the bounded uniform
distribution, GHT query cost is dominated by the cost of
sending sub-queries; for DIMs, the query splitting strategy
works quite well in keep overall query delivery costs low.
Figure 12 describes the efficacy of local replication. To
obtain this figure, we conducted the following experiment.
On a 100-node network, we inserted a number of events
uniformly distributed throughout the network, then issued
a query covering the entire network and recorded the 
answers. Knowing the expected answers for this query, we
then successively removed a fraction f of nodes randomly,
and re-issued the same query. The figure plots the fraction
of expected responses actually received, with and without
replication. As the graph shows, local replication performs
well for random failures, returning almost 90% of the 
responses when up to 30% of the nodes have failed 
simultaneously 6
.In the absence of local replication, of course, when
6
In practice, the performance of local replication is likely to
0
100
200
300
400
500
600
700
50 100 150 200 250 300
AverageCostperQueryinBoundedUnifDistribution
Network Size
DIM
flooding
GHT-R
Figure 13: Average query cost with a bounded
uniform query distribution
0
50
100
150
200
250
300
350
400
450
50 100 150 200 250 300
AverageCostperQueryinExponentialDistribution
Network Size
DIM
flooding
GHT-R
Figure 14: Average query cost with an exponential
query distribution
30% of the nodes fail, the response rate is only 70% as one
would expect.
We note that DIMs (as currently designed) are not 
perfect. When the data is highly skewed-as it was for our trace
data set from the habitat monitoring application where most
of the event values fell into within 10% of the attribute"s
range-a few DIM nodes will clearly become the bottleneck.
This is depicted in Figure 15, which shows that for DIMs,
and GHT-Rs, the maximum number of transmissions at any
network node (the hotspots) is rather high. (For less skewed
data distributions, and reasonable query size distributions,
the hotspot curves for all three schemes are comparable.)
This is a standard problem that the database indices have
dealt with by tree re-balancing. In our case, simpler 
solutions might be possible (and we discuss this in Section 7).
However, our use of the trace data demonstrates that
DIMs work for events which have more than two dimensions.
Increasing the number of dimensions does not noticeably 
degrade DIMs query cost (omitted for lack of space).
Also omitted are experiments examining the impact of
several other factors, as they do not affect our conclusions
in any way. As we expected, DIMs are comparable in 
performance to flooding when all sizes of queries are equally
likely. For an algebraic distribution of query sizes, the 
relative performance is close to that for the exponential 
distribution. For normally distributed events, the insertion costs
be much better than this. Assuming a node and its replica
don"t simultaneously fail often, a node will almost always
detect a replica failure and re-replicate, leading to near 100%
response rates.
73
0
2000
4000
6000
8000
10000
12000
50 100 150 200 250 300
MaximumHotspotonTraceDataSet
Network Size
DIM
flooding
GHT-R
Figure 15: Hotspot usage
DIM
Zone
Manager
Query
Router
Query
Processor
Event
Manager
Event
Router
GPSR interface(Event driven/Thread based)
update
useuse
update
GPSR
Upper interface(Event driven/Thread based)
Lower interface(Event driven/Thread based)
Greedy
Forwarding
Perimeter
Forwarding
Beaconing
Neighbor
List
Manager
update
use
MoteNIC (MicaRadio) IP Socket (802.11b/Ethernet)
Figure 16: Software architecture of DIM over GPSR
are comparable to that for the uniform distribution.
Finally, we note that in all our evaluations we have only
used list queries (those that request all events matching the
specified range). We expect that for summary queries (those
that expect an aggregate over matching events), the overall
cost of DIMs could be lower because the matching data are
likely to be found in one or a small number of zones. We
leave an understanding of this to future work. Also left to
future work is a detailed understanding of the impact of
location error on DIM"s mechanisms. Recent work [22] has
examined the impact of imprecise location information on
other data-centric storage mechanisms such as GHTs, and
found that there exist relatively simple fixes to GPSR that
ameliorate the effects of location error.
6. IMPLEMENTATION
We have implemented DIMs on a Linux platform suitable
for experimentation on PDAs and PC-104 class machines.
To implement DIMs, we had to develop and test an 
independent implementation of GPSR. Our GPSR implementation
is full-featured, while our DIM implementation has most of
the algorithms discussed in Section 3; some of the robustness
extensions have only simpler variants implemented.
The software architecture of DIM/GPSR system is shown
in Figure 16. The entire system (about 5000 lines of code)
is event-driven and multi-threaded. The DIM subsystem
consists of six logical components: zone management, event
maintenance, event routing, query routing, query 
processing, and GPSR interactions. The GPSR system is 
implemented as user-level daemon process. Applications are 
executed as clients. For the DIM subsystem, the GPSR module
0
2
4
6
8
10
12
14
16
0.25x0.25 0.50x0.50 0.75x0.75 1.0x1.0
Query size
Average#ofreceivedresponses
perquery
Figure 17: Number of events received for different
query sizes
0
2
4
6
8
10
12
14
16
0.25x0.25 0.50x0.50 0.75x0.75 1.0x1.0
Query sizeTotalnumberofmessages
onlyforsendingthequery
Figure 18: Query distribution cost
provides several extensions: it exports information about
neighbors, and provides callbacks during packet forwarding
and perimeter-mode termination.
We tested our implementation on a testbed consisting of 8
PC-104 class machines. Each of these boxes runs Linux and
uses a Mica mote (attached through a serial cable) for 
communication. These boxes are laid out in an office building
with a total spatial separation of over a hundred feet. We
manually measured the locations of these nodes relative to
some coordinate system and configured the nodes with their
location. The network topology is approximately a chain.
On this testbed, we inserted queries and events from a 
single designated node. Our events have two attributes which
span all combinations of the four values [0, 0.25, 0.75, 1] 
(sixteen events in all). Our queries span four sizes, returning 1,
4, 9 and 16 events respectively.
Figure 17 plots the number of events received for different
sized queries. It might appear that we received fewer events
than expected, but this graph doesn"t count the events that
were already stored at the querier. With that adjustment,
the number of responses matches our expectation. Finally,
Figure 18 shows the total number of messages required for
different query sizes on our testbed.
While these experiments do not reveal as much about the
performance range of DIMs as our simulations do, they 
nevertheless serve as proof-of-concept for DIMs. Our next step
in the implementation is to port DIMs to the Mica motes,
and integrate them into the TinyDB [17] sensor database
engine on motes.
74
7. CONCLUSIONS
In this paper, we have discussed the design and evaluation
of a distributed data structure called DIM for efficiently 
resolving multi-dimensional range queries in sensor networks.
Our design of DIMs relies upon a novel locality-preserving
hash inspired by early work in database indexing, and is
built upon GPSR. We have a working prototype, both of
GPSR and DIM, and plan to conduct larger scale 
experiments in the future.
There are several interesting future directions that we
intend to pursue. One is adaptation to skewed data 
distributions, since these can cause storage and transmission
hotspots. Unlike traditional database indices that re-balance
trees upon data insertion, in sensor networks it might be
feasible to re-structure the zones on a much larger timescale
after obtaining a rough global estimate of the data 
distribution. Another direction is support for node heterogeneity
in the zone construction process; nodes with larger storage
space assert larger-sized zones for themselves. A third is 
support for efficient resolution of existential queries-whether
there exists an event matching a multi-dimensional range.
Acknowledgments
This work benefited greatly from discussions with Fang Bian,
Hui Zhang and other ENL lab members, as well as from
comments provided by the reviewers and our shepherd Feng
Zhao.
8. REFERENCES
[1] J. Aspnes and G. Shah. Skip Graphs. In Proceedings of
14th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), Baltimore, MD, January 2003.
[2] J. L. Bentley. Multidimensional Binary Search Trees Used
for Associative Searching. Communicaions of the ACM,
18(9):475-484, 1975.
[3] P. Bonnet, J. E. Gerhke, and P. Seshadri. Towards Sensor
Database Systems. In Proceedings of the Second
International Conference on Mobile Data Management,
Hong Kong, January 2001.
[4] I. Clarke, O. Sandberg, B. Wiley, and T. W. Hong. Freenet:
A Distributed Anonymous Information Storage and
Retrieval System. In Designing Privacy Enhancing
Technologies: International Workshop on Design Issues in
Anonymity and Unobservability. Springer, New York, 2001.
[5] D. Comer. The Ubiquitous B-tree. ACM Computing
Surveys, 11(2):121-137, 1979.
[6] R. A. Finkel and J. L. Bentley. Quad Trees: A Data
Structure for Retrieval on Composite Keys. Acta
Informatica, 4:1-9, 1974.
[7] D. Ganesan, D. Estrin, and J. Heidemann. DIMENSIONS:
Why do we need a new Data Handling architecture for
Sensor Networks? In Proceedings of the First Workshop on
Hot Topics In Networks (HotNets-I), Princeton, NJ,
October 2002.
[8] A. Gionis, P. Indyk, and R. Motwani. Similarity Search in
High Dimensions via Hashing. In Proceedings of the 25th
VLDB conference, Edinburgh, Scotland, September 1999.
[9] R. Govindan, J. Hellerstein, W. Hong, S. Madden,
M. Franklin, and S. Shenker. The Sensor Network as a
Database. Technical Report 02-771, Computer Science
Department, University of Southern California, September
2002.
[10] B. Greenstein, D. Estrin, R. Govindan, S. Ratnasamy, and
S. Shenker. DIFS: A Distributed Index for Features in
Sensor Networks. In Proceedings of 1st IEEE International
Workshop on Sensor Network Protocols and Applications,
Anchorage, AK, May 2003.
[11] A. Guttman. R-trees: A Dynamic Index Structure for
Spatial Searching. In Proceedings of the ACM SIGMOD,
Boston, MA, June 1984.
[12] M. Harren, J. M. Hellerstein, R. Huebsch, B. T. Loo,
S. Shenker, and I. Stoica. Complex Queries in DHT-based
Peer-to-Peer Networks. In P. Druschel, F. Kaashoek, and
A. Rowstron, editors, Proceedings of 1st International
Workshop on Peer-to-Peer Systems (IPTPS"02), volume
2429 of LNCS, page 242, Cambridge, MA, March 2002.
Springer-Verlag.
[13] P. Indyk and R. Motwani. Approximate Nearest Neighbors:
Towards Removing the Curse of Dimensionality. In
Proceedings of the 30th Annual ACM Symposium on
Theory of Computing, Dallas, Texas, May 1998.
[14] P. Indyk, R. Motwani, P. Raghavan, and S. Vempala.
Locality-preserving Hashing in Multidimensional Spaces. In
Proceedings of the 29th Annual ACM symposium on
Theory of Computing, pages 618 - 625, El Paso, Texas,
May 1997. ACM Press.
[15] C. Intanagonwiwat, R. Govindan, and D. Estrin. Directed
Diffusion: A Scalable and Robust Communication
Paradigm for Sensor Networks. In Proceedings of the Sixth
Annual ACM/IEEE International Conference on Mobile
Computing and Networking (Mobicom 2000), Boston, MA,
August 2000.
[16] B. Karp and H. T. Kung. GPSR: Greedy Perimeter
Stateless Routing for Wireless Networks. In Proceedings of
the Sixth Annual ACM/IEEE International Conference on
Mobile Computing and Networking (Mobicom 2000),
Boston, MA, August 2000.
[17] S. Madden, M. Franklin, J. Hellerstein, and W. Hong. The
Design of an Acquisitional Query Processor for Sensor
Networks. In Proceedings of ACM SIGCMOD, San Diego,
CA, June 2003.
[18] S. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong.
TAG: a Tiny AGregation Service for ad-hoc Sensor
Networks. In Proceedings of 5th Annual Symposium on
Operating Systems Design and Implementation (OSDI),
Boston, MA, December 2002.
[19] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and
S. Shenker. A Scalable Content-Addressable Network. In
Proceedings of the ACM SIGCOMM, San Diego, CA,
August 2001.
[20] S. Ratnasamy, B. Karp, L. Yin, F. Yu, D. Estrin,
R. Govindan, and S. Shenker. GHT: A Geographic Hash
Table for Data-Centric Storage. In Proceedings of the First
ACM International Workshop on Wireless Sensor
Networks and Applications, Atlanta, GA, September 2002.
[21] H. Samet. Spatial Data Structures. In W. Kim, editor,
Modern Database Systems: The Object Model,
Interoperability and Beyond, pages 361-385. Addison
Wesley/ACM, 1995.
[22] K. Sead, A. Helmy, and R. Govindan. On the Effect of
Localization Errors on Geographic Face Routing in Sensor
Networks. In Under submission, 2003.
[23] S. Shenker, S. Ratnasamy, B. Karp, R. Govindan, and
D. Estrin. Data-Centric Storage in Sensornets. In Proc.
ACM SIGCOMM Workshop on Hot Topics In Networks,
Princeton, NJ, 2002.
[24] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and
H. Balakrishnan. Chord: A Scalable Peer-To-Peer Lookup
Service for Internet Applications. In Proceedings of the
ACM SIGCOMM, San Diego, CA, August 2001.
[25] F. Ye, H. Luo, J. Cheng, S. Lu, and L. Zhang. A Two-Tier
Data Dissemination Model for Large-scale Wireless Sensor
Networks. In Proceedings of the Eighth Annual ACM/IEEE
International Conference on Mobile Computing and
Networking (Mobicom"02), Atlanta, GA, September 2002.
75
Adaptive Duty Cycling for Energy Harvesting Systems
Jason Hsu, Sadaf Zahedi, Aman Kansal, Mani Srivastava
Electrical Engineering Department
University of California Los Angeles
{jasonh,kansal,szahedi,mbs} @ ee.ucla.edu
Vijay Raghunathan
NEC Labs America
Princeton, NJ
vijay@nec-labs.com
ABSTRACT
Harvesting energy from the environment is feasible in many
applications to ameliorate the energy limitations in sensor networks. In
this paper, we present an adaptive duty cycling algorithm that allows
energy harvesting sensor nodes to autonomously adjust their duty
cycle according to the energy availability in the environment. The
algorithm has three objectives, namely (a) achieving energy neutral
operation, i.e., energy consumption should not be more than the energy
provided by the environment, (b) maximizing the system performance
based on an application utility model subject to the above 
energyneutrality constraint, and (c) adapting to the dynamics of the energy
source at run-time. We present a model that enables harvesting sensor
nodes to predict future energy opportunities based on historical data.
We also derive an upper bound on the maximum achievable
performance assuming perfect knowledge about the future behavior of
the energy source. Our methods are evaluated using data gathered from
a prototype solar energy harvesting platform and we show that our
algorithm can utilize up to 58% more environmental energy compared
to the case when harvesting-aware power management is not used.
Categories and Subject Descriptors
C.2.4 [Computer Systems Organization]: Computer
Communication Networks-Distributed Systems
General Terms
Algorithms, Design
1. INTRODUCTION
Energy supply has always been a crucial issue in designing
battery-powered wireless sensor networks because the lifetime and
utility of the systems are limited by how long the batteries are able to
sustain the operation. The fidelity of the data produced by a sensor
network begins to degrade once sensor nodes start to run out of
battery power. Therefore, harvesting energy from the environment
has been proposed to supplement or completely replace battery
supplies to enhance system lifetime and reduce the maintenance cost
of replacing batteries periodically.
However, metrics for evaluating energy harvesting systems are
different from those used for battery powered systems.
Environmental energy is distinct from battery energy in two ways.
First it is an inexhaustible supply which, if appropriately used, can
allow the system to last forever, unlike the battery which is a limited
resource. Second, there is an uncertainty associated with its
availability and measurement, compared to the energy stored in the
battery which can be known deterministically. Thus, power
management methods based on battery status are not always
applicable to energy harvesting systems. In addition, most power
management schemes designed for battery-powered systems only
account for the dynamics of the energy consumers (e.g., CPU, radio)
but not the dynamics of the energy supply. Consequently, battery
powered systems usually operate at the lowest performance level that
meets the minimum data fidelity requirement in order to maximize
the system life. Energy harvesting systems, on the other hand, can
provide enhanced performance depending on the available energy.
In this paper, we will study how to adapt the performance of the
available energy profile. There exist many techniques to accomplish
performance scaling at the node level, such as radio transmit power
adjustment [1], dynamic voltage scaling [2], and the use of low
power modes [3]. However, these techniques require hardware
support and may not always be available on resource constrained
sensor nodes. Alternatively, a common performance scaling
technique is duty cycling. Low power devices typically provide at
least one low power mode in which the node is shut down and the
power consumption is negligible. In addition, the rate of duty cycling
is directly related to system performance metrics such as network
latency and sampling frequency. We will use duty cycle adjustment
as the primitive performance scaling technique in our algorithms.
2. RELATED WORK
Energy harvesting has been explored for several different types
of systems, such as wearable computers [4], [5], [6], sensor networks
[7], etc. Several technologies to extract energy from the environment
have been demonstrated including solar, motion-based, biochemical,
vibration-based [8], [9], [10], [11], and others are being developed
[12], [13]. While several energy harvesting sensor node platforms
have been prototyped [14], [15], [16], there is a need for systematic
power management techniques that provide performance guarantees
during system operation. The first work to take environmental
energy into account for data routing was [17], followed by [18].
While these works did demonstrate that environment aware
decisions improve performance compared to battery aware decisions,
their objective was not to achieve energy neutral operation. Our
proposed techniques attempt to maximize system performance while
maintaining energy-neutral operation.
3. SYSTEM MODEL
The energy usage considerations in a harvesting system vary
significantly from those in a battery powered system, as mentioned
earlier. We propose the model shown in Figure 1 for designing
energy management methods in a harvesting system. The functions
of the various blocks shown in the figure are discussed below. The
precise methods used in our system to achieve these functions will
be discussed in subsequent sections.
Harvested Energy Tracking: This block represents the mechanisms
used to measure the energy received from the harvesting device,
such as the solar panel. Such information is useful for determining
the energy availability profile and adapting system performance
based on it. Collecting this information requires that the node
hardware be equipped with the facility to measure the power
generated from the environment, and the Heliomote platform [14]
we used for evaluating the algorithms has this capability.
Energy Generation Model: For wireless sensor nodes with limited
storage and processing capabilities to be able to use the harvested
energy data, models that represent the essential components of this
information without using extensive storage are required. The
purpose of this block is to provide a model for the energy available
to the system in a form that may be used for making power
management decisions. The data measured by the energy tracking
block is used here to predict future energy availability. A good
prediction model should have a low prediction error and provide
predicted energy values for durations long enough to make
meaningful performance scaling decisions. Further, for energy
sources that exhibit both long-term and short-term patterns (e.g.,
diurnal and climate variations vs. weather patterns for solar energy),
the model must be able to capture both characteristics. Such a model
can also use information from external sources such as local weather
forecast service to improve its accuracy.
Energy Consumption Model: It is also important to have detailed
information about the energy usage characteristics of the system, at
various performance levels. For general applicability of our design,
we will assume that only one sleep mode is available. We assume
that the power consumption in the sleep and active modes is known.
It may be noted that for low power systems with more advanced
capabilities such as dynamic voltage scaling (DVS), multiple low
power modes, and the capability to shut down system components
selectively, the power consumption in each of the states and the
resultant effect on application performance should be known to make
power management decisions.
Energy Storage Model: This block represents the model for the
energy storage technology. Since all the generated energy may not be
used instantaneously, the harvesting system will usually have some
energy storage technology. Storage technologies (e.g., batteries and
ultra-capacitors) are non-ideal, in that there is some energy loss while
storing and retrieving energy from them. These characteristics must be
known to efficiently manage energy usage and storage. This block also
includes the system capability to measure the residual stored energy.
Most low power systems use batteries to store energy and provide
residual battery status. This is commonly based on measuring the
battery voltage which is then mapped to the residual battery energy
using the known charge to voltage relationship for the battery
technology in use. More sophisticated methods which track the flow of
energy into and out of the battery are also available.
Harvesting-aware Power Management: The inputs provided by the
previously mentioned blocks are used here to determine the suitable
power management strategy for the system. Power management
could be carried to meet different objectives in different applications.
For instance, in some systems, the harvested energy may marginally
supplement the battery supply and the objective may be to maximize
the system lifetime. A more interesting case is when the harvested
energy is used as the primary source of energy for the system with
the objective of achieving indefinitely long system lifetime. In such
cases, the power management objective is to achieve energy neutral
operation. In other words, the system should only use as much
energy as harvested from the environment and attempt to maximize
performance within this available energy budget.
4. THEORETICALLY OPTIMAL POWER
MANAGEMENT
We develop the following theory to understand the energy
neutral mode of operation. Let us define Ps(t) as the energy harvested
from the environment at time t, and the energy being consumed by
the load at that time is Pc(t). Further, we model the non-ideal storage
buffer by its round-trip efficiency η (strictly less than 1) and a
constant leakage power Pleak. Using this notation, applying the rule
of energy conservation leads to the following inequality:
0 0 00
[ ( ) ( )] [ ( ) ( )] 0
T T T
s c c s leakP t P t dt P t P t dt P dtB η
+ +
− − − ≥+ −∫ ∫ ∫ (1)
where B0 is the initial battery level and the function [X]+
= X if X > 0
and zero otherwise.
DEFINITION 1 (ρ,σ1,σ2) function: A non-negative, continuous and
bounded function P (t) is said to be a (ρ,σ1,σ2) function if and only if
for any value of finite real number T , the following are satisfied:
2 1( )
T
T P t dt Tρ σ ρ σ− ≤ ≤ +∫ (2)
This function can be used to model both energy sources and loads.
If the harvested energy profile Ps(t) is a (ρ1,σ1,σ2) function, then the
average rate of available energy over long durations becomes ρ1, and
the burstiness is bounded by σ1 and σ2 . Similarly, Pc(t) can be modeled
as a (ρ2,σ3) function, when ρ2 and σ3 are used to place an upper bound
on power consumption (the inequality on the right side) while there are
no minimum power consumption constraints.
The condition for energy neutrality, equation (1), leads to the
following theorem, based on the energy production, consumption, and
energy buffer models discussed above.
THEOREM 1 (ENERGY NEUTRAL OPERATION): Consider
a harvesting system in which the energy production profile is
characterized by a (ρ1, σ1, σ2) function, the load is characterized by
a (ρ2, σ3) function and the energy buffer is characterized by
parameters η for storage efficiency, and Pleak for leakage power. The
following conditions are sufficient for the system to achieve energy
neutrality:
ρ2 ≤ ηρ1 − Pleak (3)
B0 ≥ ησ2 + σ3 (4)
B ≥ B0 (5)
where B0 is the initial energy stored in the buffer and provides a
lower bound on the capacity of the energy buffer B. The proof is
presented in our prior work [19].
To adjust the duty cycle D using our performance scaling
algorithm, we assume the following relation between duty cycle and
the perceived utility of the system to the user: Suppose the utility of
the application to the user is represented by U(D) when the system
operates at a duty cycle D. Then,
min
1 min max
2 max
( ) 0,
( ) ,
( ) ,
U D if D D
U D k D if D D D
U D k if D D
β
= <
= + ≤ ≤
= >
This is a fairly general and simple model and the specific values of
Dmin and Dmax may be determined as per application requirements. As
an example, consider a sensor node designed to detect intrusion across
a periphery. In this case, a linear increase in duty cycle translates into a
linear increase in the detection probability. The fastest and the slowest
speeds of the intruders may be known, leading to a minimum and
Harvested Energy
Tracking
Energy Consumption
Model
Energy Storage Model

Harvestingaware Power
Mangement
Energy Generation
Model
LOAD
Figure 1. System model for an energy harvesting system.
181
maximum sensing delay tolerable, which results in the relevant Dmax
and Dmin for the sensor node. While there may be cases where the
relationship between utility and duty cycle may be non-linear, in this
paper, we restrict our focus on applications that follow this linear
model. In view of the above models for the system components and
the required performance, the objective of our power management
strategy is adjust the duty cycle D(i) dynamically so as to maximize
the total utility U(D) over a period of time, while ensuring energy
neutral operation for the sensor node.
Before discussing the performance scaling methods for harvesting
aware duty cycle adaptation, let us first consider the optimal power
management strategy that is possible for a given energy generation
profile. For the calculation of the optimal strategy, we assume
complete knowledge of the energy availability profile at the node,
including the availability in the future. The calculation of the optimal is
a useful tool for evaluating the performance of our proposed algorithm.
This is particularly useful for our algorithm since no prior algorithms
are available to serve as a baseline for comparison.
Suppose the time axis is partitioned into discrete slots of duration
ΔT, and the duty cycle adaptation calculation is carried out over a
window of Nw such time slots. We define the following energy profile
variables, with the index i ranging over {1,…, Nw}: Ps(i) is the power
output from the harvested source in time slot i, averaged over the slot
duration, Pc is the power consumption of the load in active mode, and
D(i) is the duty cycle used in slot i, whose value is to be determined.
B(i) is the residual battery energy at the beginning of slot i. Following
this convention, the battery energy left after the last slot in the window
is represented by B(Nw+1). The values of these variables will depend
on the choice of D(i).
The energy used directly from the harvested source and the energy
stored and used from the battery must be accounted for differently.
Figure 2 shows two possible cases for Ps(i) in a time slot. Ps(i) may
either be less than or higher than Pc , as shown on the left and right
respectively. When Ps(i) is lower than Pc, some of the energy used by
the load comes from the battery, while when Ps(i) is higher than Pc, all
the energy used is supplied directly from the harvested source. The
crosshatched area shows the energy that is available for storage into
the battery while the hashed area shows the energy drawn from the
battery. We can write the energy used from the battery in any slot i as:
( ) ( ) ( ) ( )[ ] ( ) ( ){ } ( ) ( )[ ]1 1c cs s sB i B i TD i P P i TP i D i TD i P i Pη η
+ +
− + = Δ − − Δ − − − (6)
In equation (6), the first term on the right hand side measures the
energy drawn from the battery when Ps(i) < Pc, the next term measures
the energy stored into the battery when the node is in sleep mode, and
the last term measures the energy stored into the battery in active mode
if Ps(i) > Pc. For energy neutral operation, we require the battery at the
end of the window of Nw slots to be greater than or equal to the starting
battery. Clearly, battery level will go down when the harvested energy
is not available and the system is operated from stored energy.
However, the window Nw is judiciously chosen such that over that
duration, we expect the environmental energy availability to complete
a periodic cycle. For instance, in the case of solar energy harvesting,
Nw could be chosen to be a twenty-four hour duration, corresponding
to the diurnal cycle in the harvested energy. This is an approximation
since an ideal choice of the window size would be infinite, but a finite
size must be used for analytical tractability. Further, the battery level
cannot be negative at any time, and this is ensured by having a large
enough initial battery level B0 such that node operation is sustained
even in the case of total blackout during a window period. Stating the
above constraints quantitatively, we can express the calculation of the
optimal duty cycles as an optimization problem below:
1
max ( )
wN
i
D i
=
∑ (7)
( ) ( ) ( ) ( ) ( ) ( ){ } ( ) ( )1 1c s s s cB i B i TD i P P i TP i D i TD i P i Pη η
+ +
⎡ ⎤ ⎡ ⎤− + = Δ − − Δ − − −⎣ ⎦ ⎣ ⎦
(8)
0(1)B B= (9)
0( 1)wB N B+ ≥ (10)
min w( ) i {1,...,N }D i D≥ ∀ ∈ (11)
max w( ) i {1,...,N }D i D≤ ∀ ∈ (12)
The solution to the optimization problem yields the duty cycles
that must be used in every slot and the evolution of residual battery
over the course of Nw slots. Note that while the constraints above
contain the non-linear function [x]+
, the quantities occurring within
that function are all known constants. The variable quantities occur
only in linear terms and hence the above optimization problem can
be solved using standard linear programming techniques, available
in popular optimization toolboxes.
5. HARVESTING-AWARE POWER
MANAGEMENT
We now present a practical algorithm for power management that
may be used for adapting the performance based on harvested energy
information. This algorithm attempts to achieve energy neutral
operation without using knowledge of the future energy availability
and maximizes the achievable performance within that constraint.
The harvesting-aware power management strategy consists of
three parts. The first part is an instantiation of the energy generation
model which tracks past energy input profiles and uses them to
predict future energy availability. The second part computes the
optimal duty cycles based on the predicted energy, and this step
uses our computationally tractable method to solve the optimization
problem. The third part consists of a method to dynamically adapt
the duty cycle in response to the observed energy generation profile
in real time. This step is required since the observed energy
generation may deviate significantly from the predicted energy
availability and energy neutral operation must be ensured with the
actual energy received rather than the predicted values.
5.1. Energy Prediction Model
We use a prediction model based on Exponentially Weighted
Moving-Average (EWMA). The method is designed to exploit the
diurnal cycle in solar energy but at the same time adapt to the seasonal
variations. A historical summary of the energy generation profile is
maintained for this purpose. While the storage data size is limited to a
vector length of Nw values in order to minimize the memory overheads
of the power management algorithm, the window size is effectively
infinite as each value in the history window depends on all the
observed data up to that instant. The window size is chosen to be 24
hours and each time slot is taken to be 30 minutes as the variation in
generated power by the solar panel using this setting is less than 10%
between each adjacent slots. This yields Nw = 48. Smaller slot
durations may be used at the expense of a higher Nw.
The historical summary maintained is derived as follows. On a
typical day, we expect the energy generation to be similar to the energy
generation at the same time on the previous days. The value of energy
generated in a particular slot is maintained as a weighted average of the
energy received in the same time-slot during all observed days. The
weights are exponential, resulting in decaying contribution from older
Figure 2. Two possible cases for energy calculations
Slot i Slot k
Pc
Pc
P(i)
P(i)
Active Sleep
182
data. More specifically, the historical average maintained for each slot
is given by:
1 (1 )k k kx x xα α−= + −
where α is the value of the weighting factor, kx is the observed value
of energy generated in the slot, and 1kx −
is the previously stored
historical average. In this model, the importance of each day relative to
the previous one remains constant because the same weighting factor
was used for all days.
The average value derived for a slot is treated as an estimate of
predicted energy value for the slot corresponding to the subsequent
day. This method helps the historical average values adapt to the
seasonal variations in energy received on different days. One of the
parameters to be chosen in the above prediction method is the
parameter α, which is a measure of rate of shift in energy pattern over
time. Since this parameter is affected by the characteristics of the
energy and sensor node location, the system should have a training
period during which this parameter will be determined. To determine a
good value of α, we collected energy data over 72 days and compared
the average error of the prediction method for various values of α. The
error based on the different values of α is shown in Figure 3. This
curve suggests an optimum value of α = 0.15 for minimum prediction
error and this value will be used in the remainder of this paper.
5.2. Low-complexity Solution
The energy values predicted for the next window of Nw slots are
used to calculated the desired duty cycles for the next window,
assuming the predicted values match the observed values in the future.
Since our objective is to develop a practical algorithm for embedded
computing systems, we present a simplified method to solve the linear
programming problem presented in Section 4. To this end, we define
the sets S and D as follows:
{ }
{ }
| ( ) 0
| ( ) 0
s c
c s
S i P i P
D i P P i
= − ≥
= − >
The two sets differ by the condition that whether the node operation
can be sustained entirely from environmental energy. In the case that
energy produced from the environment is not sufficient, battery will be
discharged to supplement the remaining energy. Next we sum up both
sides of (6) over the entire Nw window and rewrite it with the new
notation.
1
1 1 1
( )[ ( )] ( ) ( ) ( ) ( )[ ( ) ]
Nw Nw Nw
i i c s s s s c
i i D i i i S
B B TD i P P i TP i TP i D i TD i P i Pη η η+
= ∈ = = ∈
− = Δ − − Δ + Δ − Δ −∑ ∑ ∑ ∑ ∑
The term on the left hand side is actually the battery energy used over
the entire window of Nw slots, which can be set to 0 for energy neutral
operation. After some algebraic manipulation, this yields:
1
1
( ) ( ) ( ) 1 ( )
Nw
c
s s c
i i D i S
P
P i D i P i P D i
η η= ∈ ∈
⎛ ⎞⎛ ⎞
= + − +⎜ ⎟⎜ ⎟
⎝ ⎠⎝ ⎠
∑ ∑ ∑ (13)
The term on the left hand side is the total energy received in Nw
slots. The first term on the right hand side can be interpreted as the
total energy consumed during the D slots and the second term is the
total energy consumed during the S slots. We can now replace three
constraints (8), (9), and (10) in the original problem with (13), restating
the optimization problem as follows:
1
max ( )
wN
i
D i
=
∑
1
1
( ) ( ) ( ) 1 ( )
Nw
c
s s c
i i D i S
P
P i D i P i P D i
η η= ∈ ∈
⎛ ⎞⎛ ⎞
= + − +⎜ ⎟⎜ ⎟
⎝ ⎠⎝ ⎠
∑ ∑ ∑
min
max
D(i) D {1,...,Nw)
D(i) D {1,...,Nw)
i
i
≥ ∀ ∈
≤ ∀ ∈
This form facilitates a low complexity solution that doesn"t require
a general linear programming solver. Since our objective is to
maximize the total system utility, it is preferable to set the duty cycle to
Dmin for time slots where the utility per unit energy is the least. On the
other hand, we would also like the time slots with the highest Ps to
operate at Dmax because of better efficiency of using energy directly
from the energy source. Combining these two characteristics, we
define the utility co-efficient for each slot i as follows:
1
( ) 1
1 ( ) 1
c
c
s
P for i S
W i P
P i for i D
η η
∈⎧
⎪
= ⎛ ⎞⎛ ⎞⎨
+ − ∈⎜ ⎟⎜ ⎟⎪
⎝ ⎠⎝ ⎠⎩
where W(i) is a representation of how efficient the energy usage in a
particular time slot i is. A larger W(i) indicates more system utility per
unit energy in slot i and vice versa. The algorithm starts by assuming
D(i) =Dmin for i = {1…NW} because of the minimum duty cycle
requirement, and computes the remaining system energy R by:
1
1
( ) ( ) ( ) 1 ( ) (14)
Nw
c
s s c
i i D i S
P
R P i D i P i P D i
η η= ∈ ∈
⎛ ⎞⎛ ⎞
= − + − −⎜ ⎟⎜ ⎟
⎝ ⎠⎝ ⎠
∑ ∑ ∑
A negative R concludes that the optimization problem is infeasible,
meaning the system cannot achieve energy neutrality even at the
minimum duty cycle. In this case, the system designer is responsible
for increasing the environment energy availability (e.g., by using larger
solar panels). If R is positive, it means the system has excess energy
that is not being used, and this may be allocated to increase the duty
cycle beyond Dmin for some slots. Since our objective is to maximize
the total system utility, the most efficient way to allocate the excess
energy is to assign duty cycle Dmax to the slots with the highest W(i).
So, the coefficients W(i) are arranged in decreasing order and duty
cycle Dmax is assigned to the slots beginning with the largest
coefficients until the excess energy available, R (computed by (14) in
every iteration), is insufficient to assign Dmax to another slot. The
remaining energy, RLast, is used to increase the duty cycle to some
value between Dmin and Dmax in the slot with the next lower coefficient.
Denoting this slot with index j, the duty cycle is given by:
D(j)=
min
/
( ( ) ) / ( )
Last c
Last
s c s
R P if j D
DR
if j S
P j P P jη
∈⎧ ⎫
⎪ ⎪
+⎨ ⎬
∈⎪ ⎪− −⎩ ⎭
The above solution to the optimization problem requires only simple
arithmetic calculations and one sorting step which can be easily
implemented on an embedded platform, as opposed to implementing a
general linear program solver.
5.3. Slot-by-slot continual duty cycle adaptiation.
The observed energy values may vary greatly from the predicted
ones, such as due to the effect of clouds or other sudden changes. It is
thus important to adapt the duty cycles calculated using the predicted
values, to the actual energy measurements in real time to ensure energy
neutrality. Denote the initial duty cycle assignments for each time slot i
computed using the predicted energy values as D(i) = {1, ...,Nw}. First
we compute the difference between predicted power level Ps(i) and
actual power level observed, Ps"(i) in every slot i. Then, the excess
energy in slot i, denoted by X, can be obtained as follows:
( ) '( ) '( )
1
( ) '( ) ( )[ ( ) '( )](1 ) '( )
s s s c
s s s s s c
P i P i if P i P
X
P i P i D i P i P i if P i P
η
− >⎧
⎪
= ⎨
− − − − ≤⎪
⎩
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
2.6
2.7
2.8
2.9
3
alpha
AvgError(mA)
Figure 3. Choice of prediction parameter.
183
The upper term accounts for the energy difference when actual
received energy is more than the power drawn by the load. On the
other hand, if the energy received is less than Pc, we will need to
account for the extra energy used from the battery by the load, which is
a function of duty cycle used in time slot i and battery efficiency factor
η. When more energy is received than predicted, X is positive and that
excess energy is available for use in the subsequent solutes, while if X
is negative, that energy must be compensated from subsequent slots.
CASE I: X<0. In this case, we want to reduce the duty cycles used in
the future slots in order to make up for this shortfall of energy. Since
our objective function is to maximize the total system utility, we have
to reduce the duty cycles for time slots with the smallest normalized
utility coefficient, W(i). This is accomplished by first sorting the
coefficient W(j) ,where j>i. in decreasing order, and then iteratively
reducing Dj to Dmin until the total reduction in energy consumption is
the same as X.
CASE II: X>0. Here, we want to increase the duty cycles used in the
future to utilize this excess energy we received in recent time slot. In
contrast to Case I, the duty cycles of future time slots with highest
utility coefficient W(i) should be increased first in order to maximize
the total system utility.
Suppose the duty cycle is changed by d in slot j. Define a quantity
R(j,d) as follows:
⎪
⎩
⎪
⎨
⎧
<=⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
−+
>⋅
=
lji
l
ljl
PPifP
P
d
PPifP
djR
　　　
1
1
　　d　　
),(
ηη
The precise procedure to adapt the duty cycle to account for the
above factors is presented in Algorithm 1. This calculation is
performed at the end of every slot to set the duty cycle for the next
slot. We claim that our duty cycling algorithm is energy neutral
because an surplus of energy at the previous time slot will always
translate to additional energy opportunity for future time slots, and
vice versa. The claim may be violated in cases of severe energy
shortages especially towards the end of window. For example, a large
deficit in energy supply can"t be restored if there is no future energy
input until the end of the window. In such case, this offset will be
carried over to the next window so that long term energy neutrality is
still maintained.
6. EVALUATION
Our adaptive duty cycling algorithm was evaluated using an actual
solar energy profile measured using a sensor node called Heliomote,
capable of harvesting solar energy [14]. This platform not only tracks
the generated energy but also the energy flow into and out of the
battery to provide an accurate estimate of the stored energy.
The energy harvesting platform was deployed in a residential area
in Los Angeles from the beginning of June through the middle of
August for a total of 72 days. The sensor node used is a Mica2 mote
running at a fixed 40% duty cycle with an initially full battery. Battery
voltage and net current from the solar panels are sampled at a period of
10 seconds. The energy generation profile for that duration, measured
by tracking the output current from the solar cell is shown in Figure 4,
both on continuous and diurnal scales. We can observe that although
the energy profile varies from day to day, it still exhibits a general
pattern over several days.
0 10 20 30 40 50 60 70
0
10
20
30
40
50
60
70
Day
mA
0 5 10 15 20
0
10
20
30
40
50
60
70
Hour
mA
6.1. Prediction Model
We first evaluate the performance of the prediction model, which
is judged by the amount of absolute error it made between the
predicted and actual energy profile. Figure 5 shows the average error
of each time slot in mA over the entire 72 days. Generally, the amount
of error is larger during the day time because that"s when the factor of
weather can cause deviations in received energy, while the prediction
made for night time is mostly correct.
6.2. Adaptive Duty cycling algorithm
Prior methods to optimize performance while achieving energy
neutral operation using harvested energy are scarce. Instead, we
compare the performance of our algorithm against two extremes: the
theoretical optimal calculated assuming complete knowledge about
future energy availability and a simple approach which attempts to
achieve energy neutrality using a fixed duty cycle without accounting
for battery inefficiency.
The optimal duty cycles are calculated for each slot using the
future knowledge of actual received energy for that slot. For the simple
approach, the duty cycle is kept constant within each day and is
Figure 4 Solar Energy Profile (Left: Continuous, Right: Diurnal)
Input: D: Initial duty cycle, X: Excess energy due to error in the
prediction, P: Predicted energy profile, i: index of current time slot
Output: D: Updated duty cycles in one or more subsequent slots
AdaptDutyCycle()
Iteration: At each time slot do:
if X > 0
Wsorted = W{1, ...,Nw} sorted in decending order.
Q := indices of Wsorted
for k = 1 to |Q|
if Q(k) ≤ i or D(Q(k)) ≥ Dmax //slot is already passed
continue
if R(Q(k), Dmax − D(Q(k))) < X
D(Q(k)) = Dmax
X = X − R(j, Dmax − D(Q(k)))
else
//X insufficient to increase duty cycle to Dmax
if P (Q(k)) > Pl
D(Q(k)) = D(Q(k)) + X/Pl
else
D(Q(k)) = D(Q(k)) +
( ( ))(1 1 ))c s
X
P P Q kη η+ −
if X < 0
Wsorted = W{1, ...,Nw} sorted in ascending order.
Q := indices of Wsorted
for k = 1 to |Q|
if Q(k) ≤ I or D(Q(k)) ≤ Dmin
continue
if R(Q(k), Dmax − D(Q(k))) > X
D(Q(k)) = Dmin
X = X − R(j, Dmin − D(Q(k)))
else
if P (Q(k)) > Pc
D(Q(k)) = D(Q(k)) + X/Pc
else
D(Q(k)) = D(Q(k)) +
( ( ))(1 1 ))c s
X
P P Q kη η+ −
ALGORITHM 1 Pseudocode for the duty-cycle adaptation algorithm
Figure 5. Average Predictor Error in mA
0 5 10 15 20 25
0
2
4
6
8
10
12
Time(H)
abserror(mA)
184
computed by taking the ratio of the predicted energy availability and
the maximum usage, and this guarantees that the senor node will never
deplete its battery running at this duty cycle.
{1.. )
( )s w c
i Nw
D P i N Pη
∈
= ⋅ ⋅∑
We then compare the performance of our algorithm to the two
extremes with varying battery efficiency. Figure 6 shows the results,
using Dmax = 0.8 and Dmin = 0.3. The battery efficiency was varied
from 0.5 to 1 on the x-axis and solar energy utilizations achieved by
the three algorithms are shown on the y-axis. It shows the fraction of
net received energy that is used to perform useful work rather than lost
due to storage inefficiency.
As can be seen from the figure, battery efficiency factor has great
impact on the performance of the three different approaches. The three
approaches all converges to 100% utilization if we have a perfect
battery (η=1), that is, energy is not lost by storing it into the batteries.
When battery inefficiency is taken into account, both the adaptive and
optimal approach have much better solar energy utilization rate than
the simple one. Additionally, the result also shows that our adaptive
duty cycle algorithm performs extremely close to the optimal.
0.4 0.5 0.6 0.7 0.8 0.9 1
0.4
0.5
0.6
0.7
0.8
0.9
1
Eta-batery roundtrip efficiency
SolarEnergyUtilization(%)
Optimal
Adaptive
Simple
We also compare the performance of our algorithm with different
values of Dmin and Dmax for η=0.7, which is typical of NiMH batteries.
These results are shown in Table 1 as the percentage of energy saved
by the optimal and adaptive approaches, and this is the energy which
would normally be wasted in the simple approach. The figures and
table indicate that our real time algorithm is able to achieve a
performance very close to the optimal feasible. In addition, these
results show that environmental energy harvesting with appropriate
power management can achieve much better utilization of the
environmental energy.
Dmax
Dmin
0.8
0.05
0.8
0.1
0.8
0.3
0.5
0.2
0.9
0.2
1.0
0.2
Adaptive 51.0% 48.2% 42.3% 29.4% 54.7% 58.7%
Optimal 52.3% 49.6% 43.7% 36.7% 56.6% 60.8%
7. CONCLUSIONS
We discussed various issues in power management for systems
powered using environmentally harvested energy. Specifically, we
designed a method for optimizing performance subject to the
constraint of energy neutral operation. We also derived a theoretically
optimal bound on the performance and showed that our proposed
algorithm operated very close to the optimal. The proposals were
evaluated using real data collected using an energy harvesting sensor
node deployed in an outdoor environment.
Our method has significant advantages over currently used
methods which are based on a conservative estimate of duty cycle and
can only provide sub-optimal performance. However, this work is only
the first step towards optimal solutions for energy neutral operation. It
is designed for a specific power scaling method based on adapting the
duty cycle. Several other power scaling methods, such as DVS, 
submodule power switching and the use of multiple low power modes are
also available. It is thus of interest to extend our methods to exploit
these advanced capabilities.
8. ACKNOWLEDGEMENTS
This research was funded in part through support provided by
DARPA under the PAC/C program, the National Science Foundation
(NSF) under award #0306408, and the UCLA Center for Embedded
Networked Sensing (CENS). Any opinions, findings, conclusions or
recommendations expressed in this paper are those of the authors and
do not necessarily reflect the views of DARPA, NSF, or CENS.
REFERENCES
[1] R Ramanathan, and R Hain, Toplogy Control of Multihop Wireless
Networks Using Transmit Power Adjustment in Proc. Infocom. Vol 2.
26-30 pp. 404-413. March 2000
[2] T.A. Pering, T.D. Burd, and R. W. Brodersen,  The simulation and
evaluation of dynamic voltage scaling algorithms, in Proc. ACM
ISLPED, pp. 76-81, 1998
[3] L. Benini and G. De Micheli, Dynamic Power Management: Design
Techniques and CAD Tools. Kluwer Academic Publishers, Norwell, MA,
1997.
[4] John Kymisis, Clyde Kendall, Joseph Paradiso, and Neil Gershenfeld.
Parasitic power harvesting in shoes. In ISWC, pages 132-139. IEEE
Computer Society press, October 1998.
[5] Nathan S. Shenck and Joseph A. Paradiso. Energy scavenging with 
shoemounted piezoelectrics. IEEE Micro, 21(3):30ñ42, May-June 2001.
[6] T Starner. Human-powered wearable computing. IBM Systems Journal,
35(3-4), 1996.
[7] Mohammed Rahimi, Hardik Shah, Gaurav S. Sukhatme, John
Heidemann, and D. Estrin. Studying the feasibility of energy harvesting in
a mobile sensor network. In ICRA, 2003.
[8] ChrisMelhuish. The ecobot project. www.ias.uwe.ac.uk/energy
autonomy/EcoBot web page.html.
[9] Jan M.Rabaey, M. Josie Ammer, Julio L. da Silva Jr., Danny Patel, and
Shad Roundy. Picoradio supports ad-hoc ultra-low power wireless
networking. IEEE Computer, pages 42-48, July 2000.
[10] Joseph A. Paradiso and Mark Feldmeier. A compact, wireless, 
selfpowered pushbutton controller. In ACM Ubicomp, pages 299-304,
Atlanta, GA, USA, September 2001. Springer-Verlag Berlin Heidelberg.
[11] SE Wright, DS Scott, JB Haddow, andMA Rosen. The upper limit to solar
energy conversion. volume 1, pages 384 - 392, July 2000.
[12] Darpa energy harvesting projects.
http://www.darpa.mil/dso/trans/energy/projects.html.
[13] Werner Weber. Ambient intelligence: industrial research on a visionary
concept. In Proceedings of the 2003 international symposium on Low
power electronics and design, pages 247-251. ACM Press, 2003.
[14] V Raghunathan, A Kansal, J Hsu, J Friedman, and MB Srivastava,
"Design Considerations for Solar Energy Harvesting Wireless Embedded
Systems," (IPSN/SPOTS), April 2005.
[15] Xiaofan Jiang, Joseph Polastre, David Culler, Perpetual Environmentally
Powered Sensor Networks, (IPSN/SPOTS), April 25-27, 2005.
[16] Chulsung Park, Pai H. Chou, and Masanobu Shinozuka, "DuraNode:
Wireless Networked Sensor for Structural Health Monitoring," to appear
in Proceedings of the 4th IEEE International Conference on Sensors,
Irvine, CA, Oct. 31 - Nov. 1, 2005.
[17] Aman Kansal and Mani B. Srivastava. An environmental energy
harvesting framework for sensor networks. In International symposium on
Low power electronicsand design, pages 481-486. ACM Press, 2003.
[18] Thiemo Voigt, Hartmut Ritter, and Jochen Schiller. Utilizing solar power
in wireless sensor networks. In LCN, 2003.
[19] A. Kansal, J. Hsu, S. Zahedi, and M. B. Srivastava. Power management
in energy harvesting sensor networks. Technical Report 
TR-UCLA-NESL200603-02, Networked and Embedded Systems Laboratory, UCLA,
March 2006.
Figure 6. Duty Cycles achieved with respect to η
TABLE 1. Energy Saved by adaptive and optimal approach.
185
A Hierarchical Process Execution Support for Grid
Computing
Fábio R. L. Cicerre
Institute of Computing
State University of Campinas
Campinas, Brazil
fcicerre@ic.unicamp.br
Edmundo R. M. Madeira
Institute of Computing
State University of Campinas
Campinas, Brazil
edmundo@ic.unicamp.br
Luiz E. Buzato
Institute of Computing
State University of Campinas
Campinas, Brazil
buzato@ic.unicamp.br
ABSTRACT
Grid is an emerging infrastructure used to share resources
among virtual organizations in a seamless manner and to
provide breakthrough computing power at low cost. 
Nowadays there are dozens of academic and commercial products
that allow execution of isolated tasks on grids, but few 
products support the enactment of long-running processes in a
distributed fashion. In order to address such subject, this
paper presents a programming model and an infrastructure
that hierarchically schedules process activities using 
available nodes in a wide grid environment. Their advantages
are automatic and structured distribution of activities and
easy process monitoring and steering.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems-distributed applications
General Terms
Design, Performance, Management, Algorithms
1. INTRODUCTION
Grid computing is a model for wide-area distributed and
parallel computing across heterogeneous networks in 
multiple administrative domains. This research field aims to 
promote sharing of resources and provides breakthrough 
computing power over this wide network of virtual 
organizations in a seamless manner [8]. Traditionally, as in Globus
[6], Condor-G [9] and Legion [10], there is a minimal 
infrastructure that provides data resource sharing, computational
resource utilization management, and distributed execution.
Specifically, considering distributed execution, most of the
existing grid infrastructures supports execution of isolated
tasks, but they do not consider their task interdependencies
as in processes (workflows) [12]. This deficiency restricts
better scheduling algorithms, distributed execution 
coordination and automatic execution recovery.
There are few proposed middleware infrastructures that
support process execution over the grid. In general, they
model processes by interconnecting their activities through
control and data dependencies. Among them, WebFlow
[1] emphasizes an architecture to construct distributed 
processes; Opera-G [3] provides execution recovering and 
steering, GridFlow [5] focuses on improved scheduling algorithms
that take advantage of activity dependencies, and SwinDew
[13] supports totally distributed execution on peer-to-peer
networks. However, such infrastructures contain 
scheduling algorithms that are centralized by process [1, 3, 5], or
completely distributed, but difficult to monitor and control
[13].
In order to address such constraints, this paper proposes a
structured programming model for process description and a
hierarchical process execution infrastructure. The 
programming model employs structured control flow to promote 
controlled and contextualized activity execution. 
Complementary, the support infrastructure, which executes a process
specification, takes advantage of the hierarchical structure
of a specified process in order to distribute and schedule
strong dependent activities as a unit, allowing a better 
execution performance and fault-tolerance and providing 
localized communication.
The programming model and the support infrastructure,
named X avantes, are under implementation in order to show
the feasibility of the proposed model and to demonstrate its
two major advantages: to promote widely distributed 
process execution and scheduling, but in a controlled, 
structured and localized way.
Next Section describes the programming model, and 
Section 3, the support infrastructure for the proposed grid 
computing model. Section 4 demonstrates how the support
infrastructure executes processes and distributes activities.
Related works are presented and compared to the proposed
model in Section 5. The last Section concludes this paper
encompassing the advantages of the proposed hierarchical
process execution support for the grid computing area and
lists some future works.
87 Middleware 2004 Companion
ProcessElement
Process Activity Controller
1
*
1
*
Figure 1: High-level framework of the programming
model
2. PROGRAMMING MODEL
The programming model designed for the grid computing
architecture is very similar to the specified to the Business
Process Execution Language (BPEL) [2]. Both describe 
processes in XML [4] documents, but the former specifies 
processes strictly synchronous and structured, and has more
constructs for structured parallel control. The rationale 
behind of its design is the possibility of hierarchically distribute
the process control and coordination based on structured
constructs, differently from BPEL, which does not allow 
hierarchical composition of processes.
In the proposed programming model, a process is a set of
interdependent activities arranged to solve a certain 
problem. In detail, a process is composed of activities, 
subprocesses, and controllers (see Figure 1). Activities represent
simple tasks that are executed on behalf of a process; 
subprocesses are processes executed in the context of a 
parent process; and controllers are control elements used to
specify the execution order of these activities and 
subprocesses. Like structured languages, controllers can be nested
and then determine the execution order of other controllers.
Data are exchanged among process elements through 
parameters. They are passed by value, in case of simple 
objects, or by reference, if they are remote objects shared
among elements of the same controller or process. External
data can be accessed through data sources, such as relational
databases or distributed objects.
2.1 Controllers
Controllers are structured control constructs used to 
define the control flow of processes. There are sequential and
parallel controllers.
The sequential controller types are: block, switch, for
and while. The block controller is a simple sequential 
construct, and the others mimic equivalent structured 
programming language constructs. Similarly, the parallel types are:
par, parswitch, parfor and parwhile. They extend the 
respective sequential counterparts to allow parallel execution
of process elements.
All parallel controller types fork the execution of one or
more process elements, and then, wait for each execution to
finish. Indeed, they contain a fork and a join of execution.
Aiming to implement a conditional join, all parallel 
controller types contain an exit condition, evaluated all time
that an element execution finishes, in order to determine
when the controller must end.
The parfor and parwhile are the iterative versions of
the parallel controller types. Both fork executions while
the iteration condition is true. This provides flexibility to
determine, at run-time, the number of process elements to
execute simultaneously.
When compared to workflow languages, the parallel 
controller types represent structured versions of the workflow
control constructors, because they can nest other controllers
and also can express fixed and conditional forks and joins,
present in such languages.
2.2 Process Example
This section presents an example of a prime number search
application that receives a certain range of integers and 
returns a set of primes contained in this range. The whole
computation is made by a process, which uses a parallel
controller to start and dispatch several concurrent activities
of the same type, in order to find prime numbers. The 
portion of the XML document that describes the process and
activity types is shown below.
<PROCESS_TYPE NAME="FindPrimes">
<IN_PARAMETER TYPE="int" NAME="min"/>
<IN_PARAMETER TYPE="int" NAME="max"/>
<IN_PARAMETER TYPE="int" NAME="numPrimes"/>
<IN_PARAMETER TYPE="int" NAME="numActs"/>
<BODY>
<PRE_CODE>
setPrimes(new RemoteHashSet());
parfor.setMin(getMin());
parfor.setMax(getMax());
parfor.setNumPrimes(getNumPrimes());
parfor.setNumActs(getNumActs());
parfor.setPrimes(getPrimes());
parfor.setCounterBegin(0);
parfor.setCounterEnd(getNumActs()-1);
</PRE_CODE>
<PARFOR NAME="parfor">
<IN_PARAMETER TYPE="int" NAME="min"/>
<IN_PARAMETER TYPE="int" NAME="max"/>
<IN_PARAMETER TYPE="int" NAME="numPrimes"/>
<IN_PARAMETER TYPE="int" NAME="numActs"/>
<IN_PARAMETER
TYPE="RemoteCollection" NAME="primes"/>
<ITERATE>
<PRE_CODE>
int range=
(getMax()-getMin()+1)/getNumActs();
int minNum = range*getCounter()+getMin();
int maxNum = minNum+range-1;
if (getCounter() == getNumActs()-1)
maxNum = getMax();
findPrimes.setMin(minNum);
findPrimes.setMax(maxNum);
findPrimes.setNumPrimes(getNumPrimes());
findPrimes.setPrimes(getPrimes());
</PRE_CODE>
<ACTIVITY
TYPE="FindPrimes" NAME="findPrimes"/>
</ITERATE>
</PARFOR>
</BODY>
<OUT_PARAMETER
TYPE="RemoteCollection" NAME="primes"/>
</PROCESS_TYPE>
Middleware for Grid Computing 88
<ACTIVITY_TYPE NAME="FindPrimes">
<IN_PARAMETER TYPE="int" NAME="min"/>
<IN_PARAMETER TYPE="int" NAME="max"/>
<IN_PARAMETER TYPE="int" NAME="numPrimes"/>
<IN_PARAMETER
TYPE="RemoteCollection" NAME="primes"/>
<CODE>
for (int num=getMin(); num<=getMax(); num++) {
// stop, required number of primes was found
if (primes.size() >= getNumPrimes())
break;
boolean prime = true;
for (int i=2; i<num; i++) {
if (num % i == 0) {
prime = false;
break;
}
}
if (prime) {
primes.add(new Integer(num));
}
}
</CODE>
</ACTIVITY_TYPE>
Firstly, a process type that finds prime numbers, named
FindPrimes, is defined. It receives, through its input 
parameters, a range of integers in which prime numbers have
to be found, the number of primes to be returned, and the
number of activities to be executed in order to perform this
work. At the end, the found prime numbers are returned as
a collection through its output parameter.
This process contains a PARFOR controller aiming to 
execute a determined number of parallel activities. It iterates
from 0 to getNumActs() - 1, which determines the number
of activities, starting a parallel activity in each iteration. In
such case, the controller divides the whole range of numbers
in subranges of the same size, and, in each iteration, starts a
parallel activity that finds prime numbers in a specific 
subrange. These activities receive a shared object by reference
in order to store the prime numbers just found and control
if the required number of primes has been reached.
Finally, it is defined the activity type, FindPrimes, used
to find prime numbers in each subrange. It receives, through
its input parameters, the range of numbers in which it has
to find prime numbers, the total number of prime numbers
to be found by the whole process, and, passed by reference,
a collection object to store the found prime numbers. 
Between its CODE markers, there is a simple code to find prime
numbers, which iterates over the specified range and 
verifies if the current integer is a prime. Additionally, in each
iteration, the code verifies if the required number of primes,
inserted in the primes collection by all concurrent activities,
has been reached, and exits if true.
The advantage of using controllers is the possibility of the
support infrastructure determines the point of execution the
process is in, allowing automatic recovery and monitoring,
and also the capability of instantiating and dispatching 
process elements only when there are enough computing 
resources available, reducing unnecessary overhead. Besides,
due to its structured nature, they can be easily composed
and the support infrastructure can take advantage of this
in order to distribute hierarchically the nested controllers to
Group Server
Group
Java Virtual Machine
RMI JDBC
Group Manager
Process Server
Java Virtual Machine
RMI JDBC
Process Coordinator
Worker
Java Virtual Machine
RMI
Activity Manager
Repository
Figure 2: Infrastructure architecture
different machines over the grid, allowing enhanced 
scalability and fault-tolerance.
3. SUPPORT INFRASTRUCTURE
The support infrastructure comprises tools for 
specification, and services for execution and monitoring of 
structured processes in highly distributed, heterogeneous and 
autonomous grid environments. It has services to monitor
availability of resources in the grid, to interpret processes
and schedule activities and controllers, and to execute 
activities.
3.1 Infrastructure Architecture
The support infrastructure architecture is composed of
groups of machines and data repositories, which preserves
its administrative autonomy. Generally, localized machines
and repositories, such as in local networks or clusters, form
a group. Each machine in a group must have a Java Virtual
Machine (JVM) [11], and a Java Runtime Library, besides
a combination of the following grid support services: group
manager (GM), process coordinator (PC) and activity 
manager (AM). This combination determines what kind of group
node it represents: a group server, a process server, or 
simply a worker (see Figure 2).
In a group there are one or more group managers, but
only one acts as primary and the others, as replicas. They
are responsible to maintain availability information of group
machines. Moreover, group managers maintain references to
data resources of the group. They use group repositories to
persist and recover the location of nodes and their 
availability.
To control process execution, there are one or more 
process coordinators per group. They are responsible to 
instantiate and execute processes and controllers, select resources,
and schedule and dispatch activities to workers. In order
to persist and recover process execution and data, and also
load process specification, they use group repositories.
Finally, in several group nodes there is an activity 
manager. It is responsible to execute activities in the hosted
machine on behalf of the group process coordinators, and to
inform the current availability of the associated machine to
group managers. They also have pendent activity queues,
containing activities to be executed.
3.2 Inter-group Relationships
In order to model real grid architecture, the infrastructure
must comprise several, potentially all, local networks, like
Internet does. Aiming to satisfy this intent, local groups are
89 Middleware 2004 Companion
GM
GM
GM
GM
Figure 3: Inter-group relationships
connected to others, directly or indirectly, through its group
managers (see Figure 3).
Each group manager deals with requests of its group 
(represented by dashed ellipses), in order to register local 
machines and maintain correspondent availability. 
Additionally, group managers communicate to group managers of
other groups. Each group manager exports coarse 
availability information to group managers of adjacent groups and
also receives requests from other external services to 
furnish detailed availability information. In this way, if there
are resources available in external groups, it is possible to
send processes, controllers and activities to these groups in
order to execute them in external process coordinators and
activity managers, respectively.
4. PROCESS EXECUTION
In the proposed grid architecture, a process is specified
in XML, using controllers to determine control flow; 
referencing other processes and activities; and passing objects to
their parameters in order to define data flow. After specified,
the process is compiled in a set of classes, which represent
specific process, activity and controller types. At this time,
it can be instantiated and executed by a process coordinator.
4.1 Dynamic Model
To execute a specified process, it must be instantiated by
referencing its type on a process coordinator service of a
specific group. Also, the initial parameters must be passed
to it, and then it can be started.
The process coordinator carries out the process by 
executing the process elements included in its body sequentially.
If the element is a process or a controller, the process 
coordinator can choose to execute it in the same machine or to
pass it to another process coordinator in a remote machine,
if available. Else, if the element is an activity, it passes to
an activity manager of an available machine.
Process coordinators request the local group manager to
find available machines that contain the required service,
process coordinator or activity manager, in order to 
execute a process element. Then, it can return a local 
machine, a machine in another group or none, depending on
the availability of such resource in the grid. It returns an
external worker (activity manager machine) if there are no
available workers in the local group; and, it returns an 
external process server (process coordinator machine), if there
are no available process servers or workers in the local group.
Obeying this rule, group managers try to find process servers
in the same group of the available workers.
Such procedure is followed recursively by all process 
coGM
FindPrimes
Activity
AM
FindPrimes
Activity
AM
FindPrimes
Activity
AM
FindPrimes
Process
PC
Figure 4: FindPrimes process execution
ordinators that execute subprocesses or controllers of a 
process. Therefore, because processes are structured by 
nesting process elements, the process execution is automatically
distributed hierarchically through one or more grid groups
according to the availability and locality of computing 
resources.
The advantage of this distribution model is wide area 
execution, which takes advantage of potentially all grid 
resources; and localized communication of process elements,
because strong dependent elements, which are under the
same controller, are placed in the same or near groups. 
Besides, it supports easy monitoring and steering, due to its
structured controllers, which maintain state and control over
its inner elements.
4.2 Process Execution Example
Revisiting the example shown in Section 2.2, a process
type is specified to find prime numbers in a certain range of
numbers. In order to solve this problem, it creates a number
of activities using the parfor controller. Each activity, then,
finds primes in a determined part of the range of numbers.
Figure 4 shows an instance of this process type executing
over the proposed infrastructure. A FindPrimes process 
instance is created in an available process coordinator (PC),
which begins executing the parfor controller. In each 
iteration of this controller, the process coordinator requests
to the group manager (GM) an available activity manager
(AM) in order to execute a new instance of the FindPrimes
activity. If there is any AM available in this group or in an
external one, the process coordinator sends the activity class
and initial parameters to this activity manager and requests
its execution. Else, if no activity manager is available, then
the controller enters in a wait state until an activity manager
is made available, or is created.
In parallel, whenever an activity finishes, its result is sent
back to the process coordinator, which records it in the
parfor controller. Then, the controller waits until all 
activities that have been started are finished, and it ends. At
this point, the process coordinator verifies that there is no
other process element to execute and finishes the process.
5. RELATED WORK
There are several academic and commercial products that
promise to support grid computing, aiming to provide 
interfaces, protocols and services to leverage the use of widely
Middleware for Grid Computing 90
distributed resources in heterogeneous and autonomous 
networks. Among them, Globus [6], Condor-G [9] and Legion
[10] are widely known. Aiming to standardize interfaces
and services to grid, the Open Grid Services Architecture
(OGSA) [7] has been defined.
The grid architectures generally have services that 
manage computing resources and distribute the execution of 
independent tasks on available ones. However, emerging 
architectures maintain task dependencies and automatically
execute tasks in a correct order. They take advantage of
these dependencies to provide automatic recovery, and 
better distribution and scheduling algorithms.
Following such model, WebFlow [1] is a process 
specification tool and execution environment constructed over
CORBA that allows graphical composition of activities and
their distributed execution in a grid environment. Opera-G
[3], like WebFlow, uses a process specification language 
similar to the data flow diagram and workflow languages, but
furnishes automatic execution recovery and limited steering
of process execution.
The previously referred architectures and others that 
enact processes over the grid have a centralized coordination.
In order to surpass this limitation, systems like SwinDew [13]
proposed a widely distributed process execution, in which
each node knows where to execute the next activity or join
activities in a peer-to-peer environment.
In the specific area of activity distribution and scheduling,
emphasized in this work, GridFlow [5] is remarkable. It uses
a two-level scheduling: global and local. In the local level,
it has services that predict computing resource utilization
and activity duration. Based on this information, GridFlow
employs a PERT-like technique that tries to forecast the
activity execution start time and duration in order to better
schedule them to the available resources.
The architecture proposed in this paper, which 
encompasses a programming model and an execution support 
infrastructure, is widely decentralized, differently from WebFlow
and Opera-G, being more scalable and fault-tolerant. But,
like the latter, it is designed to support execution recovery.
Comparing to SwinDew, the proposed architecture 
contains widely distributed process coordinators, which 
coordinate processes or parts of them, differently from SwinDew
where each node has a limited view of the process: only the
activity that starts next. This makes easier to monitor and
control processes.
Finally, the support infrastructure breaks the process and
its subprocesses for grid execution, allowing a group to 
require another group for the coordination and execution of
process elements on behalf of the first one. This is 
different from GridFlow, which can execute a process in at most
two levels, having the global level as the only responsible to
schedule subprocesses in other groups. This can limit the
overall performance of processes, and make the system less
scalable.
6. CONCLUSION AND FUTURE WORK
Grid computing is an emerging research field that intends
to promote distributed and parallel computing over the wide
area network of heterogeneous and autonomous 
administrative domains in a seamless way, similar to what Internet
does to the data sharing. There are several products that
support execution of independent tasks over grid, but only a
few supports the execution of processes with interdependent
tasks.
In order to address such subject, this paper proposes a
programming model and a support infrastructure that 
allow the execution of structured processes in a widely 
distributed and hierarchical manner. This support 
infrastructure provides automatic, structured and recursive 
distribution of process elements over groups of available machines;
better resource use, due to its on demand creation of 
process elements; easy process monitoring and steering, due to
its structured nature; and localized communication among
strong dependent process elements, which are placed under
the same controller. These features contribute to better 
scalability, fault-tolerance and control for processes execution
over the grid. Moreover, it opens doors for better scheduling
algorithms, recovery mechanisms, and also, dynamic 
modification schemes.
The next work will be the implementation of a recovery
mechanism that uses the execution and data state of 
processes and controllers to recover process execution. After
that, it is desirable to advance the scheduling algorithm to
forecast machine use in the same or other groups and to
foresee start time of process elements, in order to use this
information to pre-allocate resources and, then, obtain a
better process execution performance. Finally, it is 
interesting to investigate schemes of dynamic modification of 
processes over the grid, in order to evolve and adapt long-term
processes to the continuously changing grid environment.
7. ACKNOWLEDGMENTS
We would like to thank Paulo C. Oliveira, from the State
Treasury Department of Sao Paulo, for its deeply revision
and insightful comments.
8. REFERENCES
[1] E. Akarsu, G. C. Fox, W. Furmanski, and T. Haupt.
WebFlow: High-Level Programming Environment and
Visual Authoring Toolkit for High Performance
Distributed Computing. In Proceedings of
Supercom puting (SC98), 1998.
[2] T. Andrews and F. Curbera. Specification: Business
Process Execution Language for W eb Services V ersion
1.1. IBM DeveloperWorks, 2003. Available at

http://www-106.ibm.com/developerworks/library/wsbpel.
[3] W. Bausch. O PERA -G :A M icrokernelfor
Com putationalG rids. PhD thesis, Swiss Federal
Institute of Technology, Zurich, 2004.
[4] T. Bray and J. Paoli. Extensible M arkup Language
(X M L) 1.0. XML Core WG, W3C, 2004. Available at
http://www.w3.org/TR/2004/REC-xml-20040204.
[5] J. Cao, S. A. Jarvis, S. Saini, and G. R. Nudd.
GridFlow: Workflow Management for Grid
Computing. In Proceedings ofthe International
Sym posium on Cluster Com puting and the G rid
(CCG rid 2003), 2003.
[6] I. Foster and C. Kesselman. Globus: A
Metacomputing Infrastructure Toolkit. Intl.J.
Supercom puter A pplications, 11(2):115-128, 1997.
[7] I. Foster, C. Kesselman, J. M. Nick, and S. Tuecke.
The Physiology ofthe G rid: A n O pen G rid Services
A rchitecture for D istributed System s Integration.
91 Middleware 2004 Companion
Open Grid Service Infrastructure WG, Global Grid
Forum, 2002.
[8] I. Foster, C. Kesselman, and S. Tuecke. The Anatomy
of the Grid: Enabling Scalable Virtual Organization.
The Intl.JournalofH igh Perform ance Com puting
A pplications, 15(3):200-222, 2001.
[9] J. Frey, T. Tannenbaum, M. Livny, I. Foster, and
S. Tuecke. Condor-G: A Computational Management
Agent for Multi-institutional Grids. In Proceedings of
the Tenth Intl.Sym posium on H igh Perform ance
D istributed Com puting (H PD C-10). IEEE, 2001.
[10] A. S. Grimshaw and W. A. Wulf. Legion - A View
from 50,000 Feet. In Proceedings ofthe Fifth Intl.
Sym posium on H igh Perform ance D istributed
Com puting. IEEE, 1996.
[11] T. Lindholm and F. Yellin. The Java V irtualM achine
Specification. Sun Microsystems, Second Edition
edition, 1999.
[12] B. R. Schulze and E. R. M. Madeira. Grid Computing
with Active Services. Concurrency and Com putation:
Practice and Experience Journal, 5(16):535-542, 2004.
[13] J. Yan, Y. Yang, and G. K. Raikundalia. Enacting
Business Processes in a Decentralised Environment
with P2P-Based Workflow Support. In Proceedings of
the Fourth Intl.Conference on W eb-Age Inform ation
M anagem ent(W A IM 2003), 2003.
Middleware for Grid Computing 92
Demonstration of Grid-Enabled Ensemble Kalman Filter
Data Assimilation Methodology for Reservoir
Characterization
Ravi Vadapalli
High Performance Computing Center
Texas Tech University
Lubbock, TX 79409
001-806-742-4350
Ravi.Vadapalli@ttu.edu
Ajitabh Kumar
Department of Petroleum Engineering
Texas A&M University
College Station, TX 77843
001-979-847-8735
akumar@tamu.edu
Ping Luo
Supercomputing Facility
Texas A&M University
College Station, TX 77843
001-979-862-3107
pingluo@sc.tamu.edu
Shameem Siddiqui
Department of Petroleum Engineering
Texas Tech University
Lubbock, TX 79409
001-806-742-3573
Shameem.Siddiqui@ttu.edu
Taesung Kim
Supercomputing Facility
Texas A&M University
College Station, TX 77843
001-979-204-5076
tskim@sc.tamu.edu
ABSTRACT
Ensemble Kalman filter data assimilation methodology is a
popular approach for hydrocarbon reservoir simulations in energy
exploration. In this approach, an ensemble of geological models
and production data of oil fields is used to forecast the dynamic
response of oil wells. The Schlumberger ECLIPSE software is
used for these simulations. Since models in the ensemble do not
communicate, message-passing implementation is a good choice.
Each model checks out an ECLIPSE license and therefore,
parallelizability of reservoir simulations depends on the number
licenses available. We have Grid-enabled the ensemble Kalman
filter data assimilation methodology for the TIGRE Grid
computing environment. By pooling the licenses and computing
resources across the collaborating institutions using GridWay
metascheduler and TIGRE environment, the computational
accuracy can be increased while reducing the simulation runtime.
In this paper, we provide an account of our efforts in 
Gridenabling the ensemble Kalman Filter data assimilation
methodology. Potential benefits of this approach, observations
and lessons learned will be discussed.
Categories and Subject Descriptors
C 2.4 [Distributed Systems]: Distributed applications
General Terms
Algorithms, Design, Performance
1. INTRODUCTION
Grid computing [1] is an emerging collaborative
computing paradigm to extend institution/organization
specific high performance computing (HPC) capabilities
greatly beyond local resources. Its importance stems from
the fact that ground breaking research in strategic
application areas such as bioscience and medicine, energy
exploration and environmental modeling involve strong
interdisciplinary components and often require intercampus
collaborations and computational capabilities beyond
institutional limitations.
The Texas Internet Grid for Research and Education
(TIGRE) [2,3] is a state funded cyberinfrastructure
development project carried out by five (Rice, A&M, TTU,
UH and UT Austin) major university systems - collectively
called TIGRE Institutions. The purpose of TIGRE is to
create a higher education Grid to sustain and extend
research and educational opportunities across Texas.
TIGRE is a project of the High Performance Computing
across Texas (HiPCAT) [4] consortium. The goal of
HiPCAT is to support advanced computational technologies
to enhance research, development, and educational
activities.
The primary goal of TIGRE is to design and deploy
state-of-the-art Grid middleware that enables integration of
computing systems, storage systems and databases,
visualization laboratories and displays, and even
instruments and sensors across Texas. The secondary goal
is to demonstrate the TIGRE capabilities to enhance
research and educational opportunities in strategic
application areas of interest to the State of Texas. These are
bioscience and medicine, energy exploration and air quality
modeling. Vision of the TIGRE project is to foster
interdisciplinary and intercampus collaborations, identify
novel approaches to extend academic-government-private
partnerships, and become a competitive model for external
funding opportunities. The overall goal of TIGRE is to
support local, campus and regional user interests and offer
avenues to connect with national Grid projects such as
Open Science Grid [5], and TeraGrid [6].
Within the energy exploration strategic application area,
we have Grid-enabled the ensemble Kalman Filter (EnKF)
[7] approach for data assimilation in reservoir modeling and
demonstrated the extensibility of the application using the
TIGRE environment and the GridWay [8] metascheduler.
Section 2 provides an overview of the TIGRE environment
and capabilities. Application description and the need for
Grid-enabling EnKF methodology is provided in Section 3.
The implementation details and merits of our approach are
discussed in Section 4. Conclusions are provided in Section
5. Finally, observations and lessons learned are documented
in Section 6.
2. TIGRE ENVIRONMENT
The TIGRE Grid middleware consists of minimal set of
components derived from a subset of the Virtual Data
Toolkit (VDT) [9] which supports a variety of operating
systems. The purpose of choosing a minimal software stack
is to support applications at hand, and to simplify
installation and distribution of client/server stacks across
TIGRE sites. Additional components will be added as they
become necessary. The PacMan [10] packaging and
distribution mechanism is employed for TIGRE
client/server installation and management. The PacMan
distribution mechanism involves retrieval, installation, and
often configuration of the packaged software. This
approach allows the clients to keep current, consistent
versions of TIGRE software. It also helps TIGRE sites to
install the needed components on resources distributed
throughout the participating sites. The TIGRE client/server
stack consists of an authentication and authorization layer,
Globus GRAM4-based job submission via web services
(pre-web services installations are available up on request).
The tools for handling Grid proxy generation, Grid-enabled
file transfer and Grid-enabled remote login are supported.
The pertinent details of TIGRE services and tools for job
scheduling and management are provided below.
2.1. Certificate Authority
The TIGRE security infrastructure includes a certificate
authority (CA) accredited by the International Grid Trust
Federation (IGTF) for issuing X. 509 user and resource
Grid certificates [11]. The Texas Advanced Computing
Center (TACC), University of Texas at Austin is the
TIGRE"s shared CA. The TIGRE Institutions serve as
Registration Authorities (RA) for their respective local user
base. For up-to-date information on securing user and
resource certificates and their installation instructions see
ref [2]. The users and hosts on TIGRE are identified by
their distinguished name (DN) in their X.509 certificate
provided by the CA. A native Grid-mapfile that contains a
list of authorized DNs is used to authenticate and authorize
user job scheduling and management on TIGRE site
resources. At Texas Tech University, the users are
dynamically allocated one of the many generic pool
accounts. This is accomplished through the Grid User
Management System (GUMS) [12].
2.2. Job Scheduling and Management
The TIGRE environment supports GRAM4-based job
submission via web services. The job submission scripts are
generated using XML. The web services GRAM translates
the XML scripts into target cluster specific batch schedulers
such as LSF, PBS, or SGE. The high bandwidth file transfer
protocols such as GridFTP are utilized for staging files in
and out of the target machine. The login to remote hosts for
compilation and debugging is only through GSISSH service
which requires resource authentication through X.509
certificates. The authentication and authorization of Grid
jobs are managed by issuing Grid certificates to both users
and hosts. The certificate revocation lists (CRL) are
updated on a daily basis to maintain high security standards
of the TIGRE Grid services. The TIGRE portal [2]
documentation area provides a quick start tutorial on
running jobs on TIGRE.
2.3. Metascheduler
The metascheduler interoperates with the cluster level
batch schedulers (such as LSF, PBS) in the overall Grid
workflow management. In the present work, we have
employed GridWay [8] metascheduler - a Globus incubator
project - to schedule and manage jobs across TIGRE.
The GridWay is a light-weight metascheduler that fully
utilizes Globus functionalities. It is designed to provide
efficient use of dynamic Grid resources by multiple users
for Grid infrastructures built on top of Globus services. The
TIGRE site administrator can control the resource sharing
through a powerful built-in scheduler provided by GridWay
or by extending GridWay"s external scheduling module to
provide their own scheduling policies. Application users
can write job descriptions using GridWay"s simple and
direct job template format (see Section 4 for details) or
standard Job Submission Description Language (JSDL).
See section 4 for implementation details.
2.4. Customer Service Management System
A TIGRE portal [2] was designed and deployed to interface
users and resource providers. It was designed using
GridPort [13] and is maintained by TACC. The TIGRE
environment is supported by open source tools such as the
Open Ticket Request System (OTRS) [14] for servicing
trouble tickets, and MoinMoin [15] Wiki for TIGRE
content and knowledge management for education, outreach
and training. The links for OTRS and Wiki are consumed
by the TIGRE portal [2] - the gateway for users and
resource providers. The TIGRE resource status and loads
are monitored by the Grid Port Information Repository
(GPIR) service of the GridPort toolkit [13] which interfaces
with local cluster load monitoring service such as Ganglia.
The GPIR utilizes cron jobs on each resource to gather
site specific resource characteristics such as jobs that are
running, queued and waiting for resource allocation.
3. ENSEMBLE KALMAN FILTER
APPLICATION
The main goal of hydrocarbon reservoir simulations is to
forecast the production behavior of oil and gas field
(denoted as field hereafter) for its development and optimal
management. In reservoir modeling, the field is divided into
several geological models as shown in Figure 1. For
accurate performance forecasting of the field, it is necessary
to reconcile several geological models to the dynamic
response of the field through history matching [16-20].
Figure 1. Cross-sectional view of the Field. Vertical
layers correspond to different geological models and the
nails are oil wells whose historical information will be
used for forecasting the production behavior.
(Figure Ref:http://faculty.smu.edu/zchen/research.html).
The EnKF is a Monte Carlo method that works with an
ensemble of reservoir models. This method utilizes 
crosscovariances [21] between the field measurements and the
reservoir model parameters (derived from several models)
to estimate prediction uncertainties. The geological model
parameters in the ensemble are sequentially updated with a
goal to minimize the prediction uncertainties. Historical
production response of the field for over 50 years is used in
these simulations. The main advantage of EnKF is that it
can be readily linked to any reservoir simulator, and can
assimilate latest production data without the need to re-run
the simulator from initial conditions. Researchers in Texas
are large subscribers of the Schlumberger ECLIPSE [22]
package for reservoir simulations. In the reservoir
modeling, each geological model checks out an ECLIPSE
license. The simulation runtime of the EnKF methodology
depends on the number of geological models used, number
of ECLIPSE licenses available, production history of the
field, and propagated uncertainties in history matching.
The overall EnKF workflow is shown Figure 2.
Figure 2. Ensemble Kaman Filter Data Assimilation
Workflow. Each site has L licenses.
At START, the master/control process (EnKF main
program) reads the simulation configuration file for number
(N) of models, and model-specific input files. Then, N
working directories are created to store the output files. At
the end of iteration, the master/control process collects the
output files from N models and post processes 
crosscovariances [21] to estimate the prediction uncertainties.
This information will be used to update models (or input
files) for the next iteration. The simulation continues until
the production histories are exhausted.
Typical EnKF simulation with N=50 and field histories
of 50-60 years, in time steps ranging from three months to a
year, takes about three weeks on a serial computing
environment.
In parallel computing environment, there is no
interprocess communication between the geological models
in the ensemble. However, at the end of each simulation
time-step, model-specific output files are to be collected for
analyzing cross covariances [21] and to prepare next set of
input files. Therefore, master-slave model in 
messagepassing (MPI) environment is a suitable paradigm. In this
approach, the geological models are treated as slaves and
are distributed across the available processors. The master
Cluster or (TIGRE/GridWay)
START
Read Configuration File
Create N Working Directories
Create N Input files
Model l Model 2 Model N. . .
ECLIPSE
on site A
ECLIPSE
on Site B
ECLIPSE
on Site Z
Collect N Model Outputs,
Post-process Output files
END
. . .
process collects model-specific output files, analyzes and
prepares next set of input files for the simulation. Since
each geological model checks out an ECLIPSE license,
parallelizability of the simulation depends on the number of
licenses available. When the available number of licenses is
less than the number of models in the ensemble, one or
more of the nodes in the MPI group have to handle more
than one model in a serial fashion and therefore, it takes
longer to complete the simulation.
A Petroleum Engineering Department usually procures
10-15 ECLIPSE licenses while at least ten-fold increase in
the number of licenses would be necessary for industry
standard simulations. The number of licenses can be
increased by involving several Petroleum Engineering
Departments that support ECLIPSE package.
Since MPI does not scale very well for applications that
involve remote compute clusters, and to get around the
firewall issues with license servers across administrative
domains, Grid-enabling the EnKF workflow seems to be
necessary. With this motivation, we have implemented
Grid-enabled EnKF workflow for the TIGRE environment
and demonstrated parallelizability of the application across
TIGRE using GridWay metascheduler. Further details are
provided in the next section.
4. IMPLEMENTATION DETAILS
To Grid-enable the EnKF approach, we have eliminated
the MPI code for parallel processing and replaced with N
single processor jobs (or sub-jobs) where, N is the number
of geological models in the ensemble. These model-specific
sub-jobs were distributed across TIGRE sites that support
ECLIPSE package using the GridWay [8] metascheduler.
For each sub-job, we have constructed a GridWay job
template that specifies the executable, input and output
files, and resource requirements. Since the TIGRE compute
resources are not expected to change frequently, we have
used static resource discovery policy for GridWay and the
sub-jobs were scheduled dynamically across the TIGRE
resources using GridWay. Figure 3 represents the sub-job
template file for the GridWay metascheduler.
Figure 3. GridWay Sub-Job Template
In Figure 3, REQUIREMENTS flag is set to choose the
resources that satisfy the application requirements. In the
case of EnKF application, for example, we need resources
that support ECLIPSE package. ARGUMENTS flag
specifies the model in the ensemble that will invoke
ECLIPSE at a remote site. INPUT_FILES is prepared by
the EnKF main program (or master/control process) and is
transferred by GridWay to the remote site where it is 
untared and is prepared for execution. Finally,
OUTPUT_FILES specifies the name and location where the
output files are to be written.
The command-line features of GridWay were used to
collect and process the model-specific outputs to prepare
new set of input files. This step mimics MPI process
synchronization in master-slave model. At the end of each
iteration, the compute resources and licenses are committed
back to the pool. Table 1 shows the sub-jobs in TIGRE
Grid via GridWay using gwps command and for clarity,
only selected columns were shown
.
USER JID DM EM NAME HOST
pingluo 88 wrap pend enkf.jt antaeus.hpcc.ttu.edu/LSF
pingluo 89 wrap pend enkf.jt antaeus.hpcc.ttu.edu/LSF
pingluo 90 wrap actv enkf.jt minigar.hpcc.ttu.edu/LSF
pingluo 91 wrap pend enkf.jt minigar.hpcc.ttu.edu/LSF
pingluo 92 wrap done enkf.jt cosmos.tamu.edu/PBS
pingluo 93 wrap epil enkf.jt cosmos.tamu.edu/PBS
Table 1. Job scheduling across TIGRE using GridWay
Metascheduler. DM: Dispatch state, EM: Execution state,
JID is the job id and HOST corresponds to site specific
cluster and its local batch scheduler.
When a job is submitted to GridWay, it will go through a
series of dispatch (DM) and execution (EM) states. For
DM, the states include pend(ing), prol(og), wrap(per),
epil(og), and done. DM=prol means the job has been
scheduled to a resource and the remote working directory is
in preparation. DM=warp implies that GridWay is
executing the wrapper which in turn executes the
application. DM=epil implies the job has finished
running at the remote site and results are being transferred
back to the GridWay server. Similarly, when EM=pend
implies the job is waiting in the queue for resource and the
job is running when EM=actv. For complete list of
message flags and their descriptions, see the documentation
in ref [8].
We have demonstrated the Grid-enabled EnKF runs
using GridWay for TIGRE environment. The jobs are so
chosen that the runtime doesn"t exceed more than a half
hour. The simulation runs involved up to 20 jobs between
A&M and TTU sites with TTU serving 10 licenses. For
resource information, see Table I.
One of the main advantages of Grid-enabled EnKF
simulation is that both the resources and licenses are
released back to the pool at the end of each simulation time
step unlike in the case of MPI implementation where
licenses and nodes are locked until the completion of entire
simulation. However, the fact that each sub-job gets
scheduled independently via GridWay could possibly incur
another time delay caused by waiting in queue for execution
in each simulation time step. Such delays are not expected
EXECUTABLE=runFORWARD
REQUIREMENTS=HOSTNAME=cosmos.tamu.edu |
HOSTNAME=antaeus.hpcc.ttu.edu |
HOSTNAME=minigar.hpcc.ttu.edu |
ARGUMENTS=001
INPUT_FILES=001.in.tar
OUTPUT_FILES=001.out.tar
in MPI implementation where the node is blocked for
processing sub-jobs (model-specific calculation) until the
end of the simulation. There are two main scenarios for
comparing Grid and cluster computing approaches.
Scenario I: The cluster is heavily loaded. The conceived
average waiting time of job requesting large number of
CPUs is usually longer than waiting time of jobs requesting
single CPU. Therefore, overall waiting time could be
shorter in Grid approach which requests single CPU for
each sub-job many times compared to MPI implementation
that requests large number of CPUs at a single time. It is
apparent that Grid scheduling is beneficial especially when
cluster is heavily loaded and requested number of CPUs for
the MPI job is not readily available.
Scenario II: The cluster is relatively less loaded or
largely available. It appears the MPI implementation is
favorable compared to the Grid scheduling. However,
parallelizability of the EnKF application depends on the
number of ECLIPSE licenses and ideally, the number of
licenses should be equal to the number of models in the
ensemble. Therefore, if a single institution does not have
sufficient number of licenses, the cluster availability doesn"t
help as much as it is expected.
Since the collaborative environment such as TIGRE can
address both compute and software resource requirements
for the EnKF application, Grid-enabled approach is still
advantageous over the conventional MPI implementation in
any of the above scenarios.
5. CONCLUSIONS AND FUTURE WORK
TIGRE is a higher education Grid development project
and its purpose is to sustain and extend research and
educational opportunities across Texas. Within the energy
exploration application area, we have Grid-enabled the MPI
implementation of the ensemble Kalman filter data
assimilation methodology for reservoir characterization.
This task was accomplished by removing MPI code for
parallel processing and replacing with single processor jobs
one for each geological model in the ensemble. These
single processor jobs were scheduled across TIGRE via
GridWay metascheduler. We have demonstrated that by
pooling licenses across TIGRE sites, more geological
models can be handled in parallel and therefore conceivably
better simulation accuracy. This approach has several
advantages over MPI implementation especially when a site
specific cluster is heavily loaded and/or the number licenses
required for the simulation is more than those available at a
single site.
Towards the future work, it would be interesting to
compare the runtime between MPI, and Grid
implementations for the EnKF application. This effort could
shed light on quality of service (QoS) of Grid environments
in comparison with cluster computing.
Another aspect of interest in the near future would be
managing both compute and license resources to address
the job (or processor)-to-license ratio management.
6. OBSERVATIONS AND LESSIONS
LEARNED
The Grid-enabling efforts for EnKF application have
provided ample opportunities to gather insights on the
visibility and promise of Grid computing environments for
application development and support. The main issues are
industry standard data security and QoS comparable to
cluster computing.
Since the reservoir modeling research involves
proprietary data of the field, we had to invest substantial
efforts initially in educating the application researchers on
the ability of Grid services in supporting the industry
standard data security through role- and privilege-based
access using X.509 standard.
With respect to QoS, application researchers expect
cluster level QoS with Grid environments. Also, there is a
steep learning curve in Grid computing compared to the
conventional cluster computing. Since Grid computing is
still an emerging technology, and it spans over several
administrative domains, Grid computing is still premature
especially in terms of the level of QoS although, it offers
better data security standards compared to commodity
clusters.
It is our observation that training and outreach programs
that compare and contrast the Grid and cluster computing
environments would be a suitable approach for enhancing
user participation in Grid computing. This approach also
helps users to match their applications and abilities Grids
can offer.
In summary, our efforts through TIGRE in Grid-enabling
the EnKF data assimilation methodology showed
substantial promise in engaging Petroleum Engineering
researchers through intercampus collaborations. Efforts are
under way to involve more schools in this effort. These
efforts may result in increased collaborative research,
educational opportunities, and workforce development
through graduate/faculty research programs across TIGRE
Institutions.
7. ACKNOWLEDGMENTS
The authors acknowledge the State of Texas for supporting
the TIGRE project through the Texas Enterprise Fund, and
TIGRE Institutions for providing the mechanism, in which
the authors (Ravi Vadapalli, Taesung Kim, and Ping Luo)
are also participating. The authors thank the application
researchers Prof. Akhil Datta-Gupta of Texas A&M
University and Prof. Lloyd Heinze of Texas Tech
University for their discussions and interest to exploit the
TIGRE environment to extend opportunities in research and
development.
8. REFERENCES
[1] Foster, I. and Kesselman, C. (eds.) 2004. The Grid: Blueprint
for a new computing infrastructure (The Elsevier series in
Grid computing)
[2] TIGRE Portal: http://tigreportal.hipcat.net
[3] Vadapalli, R. Sill, A., Dooley, R., Murray, M., Luo, P., Kim,
T., Huang, M., Thyagaraja, K., and Chaffin, D. 2007.
Demonstration of TIGRE environment for Grid
enabled/suitable applications. 8th
IEEE/ACM Int. Conf. on
Grid Computing, Sept 19-21, Austin
[4] The High Performance Computing across Texas Consortium
http://www.hipcat.net
[5] Pordes, R. Petravick, D. Kramer, B. Olson, D. Livny, M.
Roy, A. Avery, P. Blackburn, K. Wenaus, T. Würthwein, F.
Foster, I. Gardner, R. Wilde, M. Blatecky, A. McGee, J. and
Quick, R. 2007. The Open Science Grid, J. Phys Conf Series
http://www.iop.org/EJ/abstract/1742-6596/78/1/012057 and
http://www.opensciencegrid.org
[6] Reed, D.A. 2003. Grids, the TeraGrid and Beyond,
Computer, vol 30, no. 1 and http://www.teragrid.org
[7] Evensen, G. 2006. Data Assimilation: The Ensemble Kalman
Filter, Springer
[8] Herrera, J. Huedo, E. Montero, R. S. and Llorente, I. M.
2005. Scientific Programming, vol 12, No. 4. pp 317-331
[9] Avery, P. and Foster, I. 2001. The GriPhyN project: Towards
petascale virtual data grids, technical report 
GriPhyN-200115 and http://vdt.cs.wisc.edu
[10] The PacMan documentation and installation guide
http://physics.bu.edu/pacman/htmls
[11] Caskey, P. Murray, M. Perez, J. and Sill, A. 2007. Case
studies in identify management for virtual organizations,
EDUCAUSE Southwest Reg. Conf., Feb 21-23, Austin, TX.
http://www.educause.edu/ir/library/pdf/SWR07058.pdf
[12] The Grid User Management System (GUMS)
https://www.racf.bnl.gov/Facility/GUMS/index.html
[13] Thomas, M. and Boisseau, J. 2003. Building grid computing
portals: The NPACI grid portal toolkit, Grid computing:
making the global infrastructure a reality, Chapter 28,
Berman, F. Fox, G. Thomas, M. Boisseau, J. and Hey, T.
(eds), John Wiley and Sons, Ltd, Chichester
[14] Open Ticket Request System http://otrs.org
[15] The MoinMoin Wiki Engine
http://moinmoin.wikiwikiweb.de
[16] Vasco, D.W. Yoon, S. and Datta-Gupta, A. 1999. Integrating
dynamic data into high resolution reservoir models using
streamline-based analytic sensitivity coefficients, Society of
Petroleum Engineers (SPE) Journal, 4 (4).
[17] Emanuel, A. S. and Milliken, W. J. 1998. History matching
finite difference models with 3D streamlines, SPE 49000,
Proc of the Annual Technical Conf and Exhibition, Sept 
2730, New Orleans, LA.
[18] Nævdal, G. Johnsen, L.M. Aanonsen, S.I. and Vefring, E.H.
2003. Reservoir monitoring and Continuous Model Updating
using Ensemble Kalman Filter, SPE 84372, Proc of the
Annual Technical Conf and Exhibition, Oct 5-8, Denver,
CO.
[19] Jafarpour B. and McLaughlin, D.B. 2007. History matching
with an ensemble Kalman filter and discrete cosine
parameterization, SPE 108761, Proc of the Annual Technical
Conf and Exhibition, Nov 11-14, Anaheim, CA
[20] Li, G. and Reynolds, A. C. 2007. An iterative ensemble
Kalman filter for data assimilation, SPE 109808, Proc of the
SPE Annual Technical Conf and Exhibition, Nov 11-14,
Anaheim, CA
[21] Arroyo-Negrete, E. Devagowda, D. Datta-Gupta, A. 2006.
Streamline assisted ensemble Kalman filter for rapid and
continuous reservoir model updating. Proc of the Int. Oil &
Gas Conf and Exhibition, SPE 104255, Dec 5-7, China
[22] ECLIPSE Reservoir Engineering Software
http://www.slb.com/content/services/software/reseng/index.a
sp
Tracking Immediate Predecessors
in Distributed Computations
Emmanuelle Anceaume Jean-Michel H´elary Michel Raynal
IRISA, Campus Beaulieu
35042 Rennes Cedex, France
FirstName.LastName@irisa.fr
ABSTRACT
A distributed computation is usually modeled as a partially
ordered set of relevant events (the relevant events are a 
subset of the primitive events produced by the computation).
An important causality-related distributed computing 
problem, that we call the Immediate Predecessors Tracking (IPT)
problem, consists in associating with each relevant event, on
the fly and without using additional control messages, the
set of relevant events that are its immediate predecessors in
the partial order. So, IPT is the on-the-fly computation of
the transitive reduction (i.e., Hasse diagram) of the causality
relation defined by a distributed computation. This paper
addresses the IPT problem: it presents a family of 
protocols that provides each relevant event with a timestamp that
exactly identifies its immediate predecessors. The family is
defined by a general condition that allows application 
messages to piggyback control information whose size can be
smaller than n (the number of processes). In that sense,
this family defines message size-efficient IPT protocols. 
According to the way the general condition is implemented,
different IPT protocols can be obtained. Two of them are
exhibited.
Categories and Subject Descriptors
C.2.4 [Distributed Systems]:
General Terms
Asynchronous Distributed Computations
1. INTRODUCTION
A distributed computation consists of a set of processes
that cooperate to achieve a common goal. A main 
characteristic of these computations lies in the fact that the
processes do not share a common global memory, and 
communicate only by exchanging messages over a 
communication network. Moreover, message transfer delays are finite
but unpredictable. This computation model defines what
is known as the asynchronous distributed system model. It
is particularly important as it includes systems that span
large geographic areas, and systems that are subject to 
unpredictable loads. Consequently, the concepts, tools and
mechanisms developed for asynchronous distributed systems
reveal to be both important and general.
Causality is a key concept to understand and master the
behavior of asynchronous distributed systems [18]. More
precisely, given two events e and f of a distributed 
computation, a crucial problem that has to be solved in a lot of
distributed applications is to know whether they are causally
related, i.e., if the occurrence of one of them is a consequence
of the occurrence of the other. The causal past of an event
e is the set of events from which e is causally dependent.
Events that are not causally dependent are said to be 
concurrent. Vector clocks [5, 16] have been introduced to allow
processes to track causality (and concurrency) between the
events they produce. The timestamp of an event produced
by a process is the current value of the vector clock of the
corresponding process. In that way, by associating vector
timestamps with events it becomes possible to safely decide
whether two events are causally related or not.
Usually, according to the problem he focuses on, a 
designer is interested only in a subset of the events produced by
a distributed execution (e.g., only the checkpoint events are
meaningful when one is interested in determining 
consistent global checkpoints [12]). It follows that detecting causal
dependencies (or concurrency) on all the events of the 
distributed computation is not desirable in all applications [7,
15]. In other words, among all the events that may occur
in a distributed computation, only a subset of them are 
relevant. In this paper, we are interested in the restriction of
the causality relation to the subset of events defined as being
the relevant events of the computation.
Being a strict partial order, the causality relation is 
transitive. As a consequence, among all the relevant events that
causally precede a given relevant event e, only a subset are
its immediate predecessors: those are the events f such that
there is no relevant event on any causal path from f to e.
Unfortunately, given only the vector timestamp associated
with an event it is not possible to determine which events of
its causal past are its immediate predecessors. This comes
from the fact that the vector timestamp associated with e
determines, for each process, the last relevant event 
belong210
ing to the causal past of e, but such an event is not 
necessarily an immediate predecessor of e. However, some 
applications [4, 6] require to associate with each relevant event only
the set of its immediate predecessors. Those applications are
mainly related to the analysis of distributed computations.
Some of those analyses require the construction of the 
lattice of consistent cuts produced by the computation [15, 16].
It is shown in [4] that the tracking of immediate 
predecessors allows an efficient on the fly construction of this lattice.
More generally, these applications are interested in the very
structure of the causal past. In this context, the 
determination of the immediate predecessors becomes a major issue
[6]. Additionally, in some circumstances, this determination
has to satisfy behavior constraints. If the communication
pattern of the distributed computation cannot be modified,
the determination has to be done without adding control
messages. When the immediate predecessors are used to
monitor the computation, it has to be done on the fly.
We call Immediate Predecessor Tracking (IPT) the 
problem that consists in determining on the fly and without
additional messages the immediate predecessors of relevant
events. This problem consists actually in determining the
transitive reduction (Hasse diagram) of the causality graph
generated by the relevant events of the computation. 
Solving this problem requires tracking causality, hence using 
vector clocks. Previous works have addressed the efficient 
implementation of vector clocks to track causal dependence on
relevant events. Their aim was to reduce the size of 
timestamps attached to messages. An efficient vector clock 
implementation suited to systems with fifo channels is proposed
in [19]. Another efficient implementation that does not 
depend on channel ordering property is described in [11]. The
notion of causal barrier is introduced in [2, 17] to reduce
the size of control information required to implement causal
multicast. However, none of these papers considers the 
IPT problem. This problem has been addressed for the first
time (to our knowledge) in [4, 6] where an IPT protocol
is described, but without correctness proof. Moreover, in
this protocol, timestamps attached to messages are of size
n. This raises the following question which, to our 
knowledge, has never been answered: Are there efficient vector
clock implementation techniques that are suitable for the IPT
problem?.
This paper has three main contributions: (1) a positive
answer to the previous open question, (2) the design of a
family of efficient IPT protocols, and (3) a formal 
correctness proof of the associated protocols. From a 
methodological point of view the paper uses a top-down approach. It
states abstract properties from which more concrete 
properties and protocols are derived. The family of IPT 
protocols is defined by a general condition that allows 
application messages to piggyback control information whose size
can be smaller than the system size (i.e., smaller than the
number of processes composing the system). In that sense,
this family defines low cost IPT protocols when we 
consider the message size. In addition to efficiency, the proposed
approach has an interesting design property. Namely, the
family is incrementally built in three steps. The basic 
vector clock protocol is first enriched by adding to each process
a boolean vector whose management allows the processes
to track the immediate predecessor events. Then, a general
condition is stated to reduce the size of the control 
information carried by messages. Finally, according to the way this
condition is implemented, three IPT protocols are obtained.
The paper is composed of seven sections. Sections 2 
introduces the computation model, vector clocks and the notion
of relevant events. Section 3 presents the first step of the
construction that results in an IPT protocol in which each
message carries a vector clock and a boolean array, both
of size n (the number of processes). Section 4 improves
this protocol by providing the general condition that allows
a message to carry control information whose size can be
smaller than n. Section 5 provides instantiations of this
condition. Section 6 provides a simulation study comparing
the behaviors of the proposed protocols. Finally, Section 7
concludes the paper. (Due to space limitations, proofs of
lemmas and theorems are omitted. They can be found in
[1].)
2. MODEL AND VECTOR CLOCK
2.1 Distributed Computation
A distributed program is made up of sequential local 
programs which communicate and synchronize only by 
exchanging messages. A distributed computation describes the 
execution of a distributed program. The execution of a local
program gives rise to a sequential process. Let {P1, P2, . . . ,
Pn} be the finite set of sequential processes of the distributed
computation. Each ordered pair of communicating processes
(Pi, Pj ) is connected by a reliable channel cij through which
Pi can send messages to Pj. We assume that each message
is unique and a process does not send messages to itself1
.
Message transmission delays are finite but unpredictable.
Moreover, channels are not necessarily fifo. Process speeds
are positive but arbitrary. In other words, the underlying
computation model is asynchronous.
The local program associated with Pi can include send,
receive and internal statements. The execution of such a
statement produces a corresponding send/receive/internal
event. These events are called primitive events. Let ex
i
be the x-th event produced by process Pi. The sequence
hi = e1
i e2
i . . . ex
i . . . constitutes the history of Pi, denoted
Hi. Let H = ∪n
i=1Hi be the set of events produced by a
distributed computation. This set is structured as a partial
order by Lamport"s happened before relation [14] (denoted
hb
→) and defined as follows: ex
i
hb
→ ey
j if and only if
(i = j ∧ x + 1 = y) (local precedence) ∨
(∃m : ex
i = send(m) ∧ ey
j = receive(m)) (msg prec.) ∨
(∃ ez
k : ex
i
hb
→ ez
k ∧ e z
k
hb
→ ey
j ) (transitive closure).
max(ex
i , ey
j ) is a partial function defined only when ex
i and
ey
j are ordered. It is defined as follows: max(ex
i , ey
j ) = ex
i if
ey
j
hb
→ ex
i , max(ex
i , ey
j ) = ey
i if ex
i
hb
→ ey
j .
Clearly the restriction of
hb
→ to Hi, for a given i, is a total
order. Thus we will use the notation ex
i < ey
i iff x < y.
Throughout the paper, we will use the following notation:
if e ∈ Hi is not the first event produced by Pi, then pred(e)
denotes the event immediately preceding e in the sequence
Hi. If e is the first event produced by Pi, then pred(e) is
denoted by ⊥ (meaning that there is no such event), and
∀e ∈ Hi : ⊥ < e. The partial order bH = (H,
hb
→) 
constitutes a formal model of the distributed computation it is
associated with.
1
This assumption is only in order to get simple protocols.
211
P1
P2
P3
[1, 1, 2]
[1, 0, 0] [3, 2, 1]
[1, 1, 0]
(2, 1)
[0, 0, 1]
(3, 1)
[2, 0, 1]
(1, 1) (1, 3)(1, 2)
(2, 2) (2, 3)
(3, 2)
[2, 2, 1] [2, 3, 1]
(1, 1) (1, 2) (1, 3)
(2, 1)
(2, 2)
(2, 3)
(3, 1)
(3, 2)
Figure 1: Timestamped Relevant Events and Immediate Predecessors Graph (Hasse Diagram)
2.2 Relevant Events
For a given observer of a distributed computation, only
some events are relevant2
[7, 9, 15]. An interesting example
of what an observation is, is the detection of predicates
on consistent global states of a distributed computation [3,
6, 8, 9, 13, 15]. In that case, a relevant event corresponds
to the modification of a local variable involved in the global
predicate. Another example is the checkpointing problem
where a relevant event is the definition of a local checkpoint
[10, 12, 20].
The left part of Figure 1 depicts a distributed computation
using the classical space-time diagram. In this figure, only
relevant events are represented. The sequence of relevant
events produced by process Pi is denoted by Ri, and R =
∪n
i=1Ri ⊆ H denotes the set of all relevant events. Let →
be the relation on R defined in the following way:
∀ (e, f) ∈ R × R : (e → f) ⇔ (e
hb
→ f).
The poset (R, →) constitutes an abstraction of the 
distributed computation [7]. In the following we consider a
distributed computation at such an abstraction level. 
Moreover, without loss of generality we consider that the set of
relevant events is a subset of the internal events (if a 
communication event has to be observed, a relevant internal event
can be generated just before a send and just after a receive
communication event occurred). Each relevant event is 
identified by a pair (process id, sequence number) (see Figure 1).
Definition 1. The relevant causal past of an event e ∈
H is the (partially ordered) subset of relevant events f such
that f
hb
→ e. It is denoted ↑ (e). We have ↑ (e) = {f ∈
R | f
hb
→ e}.
Note that, if e ∈ R then ↑ (e) = {f ∈ R | f → e}. In
the computation described in Figure 1, we have, for the
event e identified (2, 2): ↑ (e) = {(1, 1), (1, 2), (2, 1), (3, 1)}.
The following properties are immediate consequences of the
previous definitions. Let e ∈ H.
CP1 If e is not a receive event then
↑ (e) =
8
<
:
∅ if pred(e) = ⊥,
↑ (pred(e)) ∪ {pred(e)} if pred(e) ∈ R,
↑ (pred(e)) if pred(e) ∈ R.
CP2 If e is a receive event (of a message m) then
↑ (e) =
8
>><
>>:
↑ (send(m)) if pred(e) = ⊥,
↑ (pred(e))∪ ↑ (send(m)) ∪ {pred(e)}
if pred(e) ∈ R,
↑ (pred(e))∪ ↑ (send(m)) if pred(e) ∈ R.
2
Those events are sometimes called observable events.
Definition 2. Let e ∈ Hi. For every j such that ↑ (e) ∩
Rj = ∅, the last relevant event of Pj with respect to e is:
lastr(e, j) = max{f | f ∈↑ (e) ∩ Rj}. When ↑ (e) ∩ Rj = ∅,
lastr(e, j) is denoted by ⊥ (meaning that there is no such
event).
Let us consider the event e identified (2,2) in Figure 1. We
have lastr(e, 1) = (1, 2), lastr(e, 2) = (2, 1), lastr(e, 3) =
(3, 1). The following properties relate the events lastr(e, j)
and lastr(f, j) for all the predecessors f of e in the relation
hb
→. These properties follow directly from the definitions.
Let e ∈ Hi.
LR0 ∀e ∈ Hi:
lastr(e, i) =
8
<
:
⊥ if pred(e) = ⊥,
pred(e) if pred(e) ∈ R,
lastr(pred(e),i) if pred(e) ∈ R.
LR1 If e is not a receipt event: ∀j = i :
lastr(e, j) = lastr(pred(e),j).
LR2 If e is a receive event of m: ∀j = i :
lastr(e, j) = max(lastr(pred(e),j), lastr(send(m),j)).
2.3 Vector Clock System
Definition As a fundamental concept associated with the
causality theory, vector clocks have been introduced in 1988,
simultaneously and independently by Fidge [5] and Mattern
[16]. A vector clock system is a mechanism that associates
timestamps with events in such a way that the 
comparison of their timestamps indicates whether the 
corresponding events are or are not causally related (and, if they are,
which one is the first). More precisely, each process Pi has a
vector of integers V Ci[1..n] such that V Ci[j] is the number
of relevant events produced by Pj, that belong to the 
current relevant causal past of Pi. Note that V Ci[i] counts the
number of relevant events produced so far by Pi. When a
process Pi produces a (relevant) event e, it associates with
e a vector timestamp whose value (denoted e.V C) is equal
to the current value of V Ci.
Vector Clock Implementation The following 
implementation of vector clocks [5, 16] is based on the observation
that ∀i, ∀e ∈ Hi, ∀j : e.V Ci[j] = y ⇔ lastr(e, j) = ey
j
where e.V Ci is the value of V Ci just after the occurrence
of e (this relation results directly from the properties LR0,
LR1, and LR2). Each process Pi manages its vector clock
V Ci[1..n] according to the following rules:
VC0 V Ci[1..n] is initialized to [0, . . . , 0].
VC1 Each time it produces a relevant event e, Pi increments
its vector clock entry V Ci[i] (V Ci[i] := V Ci[i] + 1) to
212
indicate it has produced one more relevant event, then
Pi associates with e the timestamp e.V C = V Ci.
VC2 When a process Pi sends a message m, it attaches to
m the current value of V Ci. Let m.V C denote this
value.
VC3 When Pi receives a message m, it updates its vector
clock as follows: ∀k : V Ci[k] := max(V Ci[k], m.V C[k]).
3. IMMEDIATE PREDECESSORS
In this section, the Immediate Predecessor Tracking 
(IPT) problem is stated (Section 3.1). Then, some technical
properties of immediate predecessors are stated and proved
(Section 3.2). These properties are used to design the basic
IPT protocol and prove its correctness (Section 3.3). This
IPT protocol, previously presented in [4] without proof, is
built from a vector clock protocol by adding the 
management of a local boolean array at each process.
3.1 The IPT Problem
As indicated in the introduction, some applications (e.g.,
analysis of distributed executions [6], detection of 
distributed properties [7]) require to determine (on-the-fly and 
without additional messages) the transitive reduction of the 
relation → (i.e., we must not consider transitive causal 
dependency). Given two relevant events f and e, we say that f
is an immediate predecessor of e if f → e and there is no
relevant event g such that f → g → e.
Definition 3. The Immediate Predecessor Tracking 
(IPT) problem consists in associating with each relevant event
e the set of relevant events that are its immediate 
predecessors. Moreover, this has to be done on the fly and without
additional control message (i.e., without modifying the 
communication pattern of the computation).
As noted in the Introduction, the IPT problem is the 
computation of the Hasse diagram associated with the partially
ordered set of the relevant events produced by a distributed
computation.
3.2 Formal Properties of IPT
In order to design a protocol solving the IPT problem, it
is useful to consider the notion of immediate relevant 
predecessor of any event, whether relevant or not. First, we
observe that, by definition, the immediate predecessor on
Pj of an event e is necessarily the lastr(e, j) event. 
Second, for lastr(e, j) to be immediate predecessor of e, there
must not be another lastr(e, k) event on a path between
lastr(e, j) and e. These observations are formalized in the
following definition:
Definition 4. Let e ∈ Hi. The set of immediate 
relevant predecessors of e (denoted IP(e)), is the set of the relevant
events lastr(e, j) (j = 1, . . . , n) such that ∀k : lastr(e, j) ∈↑
(lastr(e, k)).
It follows from this definition that IP(e) ⊆ {lastr(e, j)|j =
1, . . . , n} ⊂↑ (e). When we consider Figure 1, The graph 
depicted in its right part describes the immediate predecessors
of the relevant events of the computation defined in its left
part, more precisely, a directed edge (e, f) means that the
relevant event e is an immediate predecessor of the relevant
event f (3
).
The following lemmas show how the set of immediate 
predecessors of an event is related to those of its predecessors
in the relation
hb
→. They will be used to design and prove
the protocols solving the IPT problem. To ease the reading
of the paper, their proofs are presented in Appendix A.
The intuitive meaning of the first lemma is the following:
if e is not a receive event, all the causal paths arriving at e
have pred(e) as next-to-last event (see CP1). So, if pred(e)
is a relevant event, all the relevant events belonging to its
relevant causal past are separated from e by pred(e), and
pred(e) becomes the only immediate predecessor of e. In
other words, the event pred(e) constitutes a reset w.r.t.
the set of immediate predecessors of e. On the other hand,
if pred(e) is not relevant, it does not separate its relevant
causal past from e.
Lemma 1. If e is not a receive event, IP(e) is equal to:
∅ if pred(e) = ⊥,
{pred(e)} if pred(e) ∈ R,
IP(pred(e)) if pred(e) ∈ R.
The intuitive meaning of the next lemma is as follows: if
e is a receive event receive(m), the causal paths arriving
at e have either pred(e) or send(m) as next-to-last events.
If pred(e) is relevant, as explained in the previous lemma,
this event hides from e all its relevant causal past and
becomes an immediate predecessor of e. Concerning the
last relevant predecessors of send(m), only those that are
not predecessors of pred(e) remain immediate predecessors
of e.
Lemma 2. Let e ∈ Hi be the receive event of a message
m. If pred(e) ∈ Ri, then, ∀j, IP(e) ∩ Rj is equal to:
{pred(e)} if j = i,
∅ if lastr(pred(e),j) ≥ lastr(send(m),j),
IP(send(m)) ∩ Rj if lastr(pred(e),j) < lastr(send(m),j).
The intuitive meaning of the next lemma is the following:
if e is a receive event receive(m), and pred(e) is not 
relevant, the last relevant events in the relevant causal past of e are
obtained by merging those of pred(e) and those of send(m)
and by taking the latest on each process. So, the 
immediate predecessors of e are either those of pred(e) or those
of send(m). On a process where the last relevant events
of pred(e) and of send(m) are the same event f, none of
the paths from f to e must contain another relevant event,
and thus, f must be immediate predecessor of both events
pred(e) and send(m).
Lemma 3. Let e ∈ Hi be the receive event of a message
m. If pred(e) ∈ Ri, then, ∀j, IP(e) ∩ Rj is equal to:
IP(pred(e)) ∩ Rj if lastr(pred(e),j) > lastr(send(m),j),
IP(send(m)) ∩ Rj if lastr(pred(e),j) < lastr(send(m),j)
IP(pred(e))∩IP(send(m))∩Rj if lastr(pred(e),j) = lastr
(send(m), j).
3.3 A Basic IPT Protocol
The basic protocol proposed here associates with each 
relevant event e, an attribute encoding the set IP(e) of its
immediate predecessors. From the previous lemmas, the set
3
Actually, this graph is the Hasse diagram of the partial
order associated with the distributed computation.
213
IP(e) of any event e depends on the sets IP of the events
pred(e) and/or send(m) (when e = receive(m)). Hence the
idea to introduce a data structure allowing to manage the
sets IPs inductively on the poset (H,
hb
→). To take into 
account the information from pred(e), each process manages
a boolean array IPi such that, ∀e ∈ Hi the value of IPi
when e occurs (denoted e.IPi) is the boolean array 
representation of the set IP(e). More precisely, ∀j : IPi[j] =
1 ⇔ lastr(e, j) ∈ IP(e). As recalled in Section 2.3, the
knowledge of lastr(e,j) (for every e and every j) is based
on the management of vectors V Ci. Thus, the set IP(e) is
determined in the following way:
IP(e) = {ey
j | e.V Ci[j] = y ∧ e.IPi[j] = 1, j = 1, . . . , n}
Each process Pi updates IPi according to the Lemmas 1,
2, and 3:
1. It results from Lemma 1 that, if e is not a receive event,
the current value of IPi is sufficient to determine e.IPi.
It results from Lemmas 2 and 3 that, if e is a receive
event (e = receive(m)), then determining e.IPi 
involves information related to the event send(m). More
precisely, this information involves IP(send(m)) and
the timestamp of send(m) (needed to compare the
events lastr(send(m),j) and lastr(pred(e),j), for 
every j). So, both vectors send(m).V Cj and send(m).IPj
(assuming send(m) produced by Pj ) are attached to
message m.
2. Moreover, IPi must be updated upon the occurrence
of each event. In fact, the value of IPi just after an
event e is used to determine the value succ(e).IPi. In
particular, as stated in the Lemmas, the determination
of succ(e).IPi depends on whether e is relevant or not.
Thus, the value of IPi just after the occurrence of event
e must  keep track of this event.
The following protocol, previously presented in [4] without
proof, ensures the correct management of arrays V Ci (as in
Section 2.3) and IPi (according to the Lemmas of Section
3.2). The timestamp associated with a relevant event e is
denoted e.TS.
R0 Initialization: Both V Ci[1..n] and IPi[1..n] are 
initialized to [0, . . . , 0].
R1 Each time it produces a relevant event e:
- Pi associates with e the timestamp e.TS defined
as follows e.TS = {(k, V Ci[k]) | IPi[k] = 1},
- Pi increments its vector clock entry V Ci[i] 
(namely it executes V Ci[i] := V Ci[i] + 1),
- Pi resets IPi: ∀ = i : IPi[ ] := 0; IPi[i] := 1.
R2 When Pi sends a message m to Pj, it attaches to m
the current values of V Ci (denoted m.V C) and the
boolean array IPi (denoted m.IP).
R3 When it receives a message m from Pj , Pi executes the
following updates:
∀k ∈ [1..n] : case
V Ci[k] < m.V C[k] thenV Ci[k] := m.V C[k];
IPi[k] := m.IP[k]
V Ci[k] = m.V C[k] then IPi[k] := min(IPi[k], m.IP[k])
V Ci[k] > m.V C[k] then skip
endcase
The proof of the following theorem directly follows from
Lemmas 1, 2 and 3.
Theorem 1. The protocol described in Section 3.3 solves
the IPT problem: for any relevant event e, the timestamp
e.TS contains the identifiers of all its immediate 
predecessors and no other event identifier.
4. A GENERAL CONDITION
This section addresses a previously open problem, 
namely, How to solve the IPT problem without requiring each
application message to piggyback a whole vector clock and
a whole boolean array?. First, a general condition that
characterizes which entries of vectors V Ci and IPi can be
omitted from the control information attached to a message
sent in the computation, is defined (Section 4.1). It is then
shown (Section 4.2) that this condition is both sufficient and
necessary.
However, this general condition cannot be locally 
evaluated by a process that is about to send a message. Thus,
locally evaluable approximations of this general condition
must be defined. To each approximation corresponds a 
protocol, implemented with additional local data structures. In
that sense, the general condition defines a family of IPT 
protocols, that solve the previously open problem. This issue
is addressed in Section 5.
4.1 To Transmit or Not to Transmit Control
Information
Let us consider the previous IPT protocol (Section 3.3).
Rule R3 shows that a process Pj does not systematically
update each entry V Cj[k] each time it receives a message
m from a process Pi: there is no update of V Cj[k] when
V Cj[k] ≥ m.V C[k]. In such a case, the value m.V C[k] is
useless, and could be omitted from the control information
transmitted with m by Pi to Pj.
Similarly, some entries IPj[k] are not updated when a
message m from Pi is received by Pj. This occurs when
0 < V Cj[k] = m.V C[k] ∧ m.IP[k] = 1, or when V Cj [k] >
m.V C[k], or when m.V C[k] = 0 (in the latest case, as
m.IP[k] = IPi[k] = 0 then no update of IPj[k] is necessary).
Differently, some other entries are systematically reset to 0
(this occurs when 0 < V Cj [k] = m.V C[k] ∧ m.IP[k] = 0).
These observations lead to the definition of the condition
K(m, k) that characterizes which entries of vectors V Ci and
IPi can be omitted from the control information attached
to a message m sent by a process Pi to a process Pj:
Definition 5. K(m, k) ≡
(send(m).V Ci[k] = 0)
∨ (send(m).V Ci[k] < pred(receive(m)).V Cj[k])
∨
;
(send(m).V Ci[k] = pred(receive(m)).V Cj[k])
∧(send(m).IPi[k] = 1) .
4.2 A Necessary and Sufficient Condition
We show here that the condition K(m, k) is both 
necessary and sufficient to decide which triples of the form
(k, send(m).V Ci[k], send(m).IPi[k]) can be omitted in an
outgoing message m sent by Pi to Pj. A triple attached to
m will also be denoted (k, m.V C[k], m.IP[k]). Due to space
limitations, the proofs of Lemma 4 and Lemma 5 are given
in [1]. (The proof of Theorem 2 follows directly from these
lemmas.)
214
Lemma 4. (Sufficiency) If K(m, k) is true, then the triple
(k, m.V C[k], m.IP[k]) is useless with respect to the correct
management of IPj[k] and V Cj [k].
Lemma 5. (Necessity) If K(m, k) is false, then the triple
(k, m.V C[k], m.IP[k]) is necessary to ensure the correct 
management of IPj[k] and V Cj [k].
Theorem 2. When a process Pi sends m to a process Pj,
the condition K(m, k) is both necessary and sufficient not to
transmit the triple (k, send(m).V Ci[k], send(m).IPi[k]).
5. A FAMILY OF IPT PROTOCOLS BASED
ON EVALUABLE CONDITIONS
It results from the previous theorem that, if Pi could
evaluate K(m, k) when it sends m to Pj, this would 
allow us improve the previous IPT protocol in the following
way: in rule R2, the triple (k, V Ci[k], IPi[k]) is 
transmitted with m only if ¬K(m, k). Moreover, rule R3 is 
appropriately modified to consider only triples carried by m.
However, as previously mentioned, Pi cannot locally 
evaluate K(m, k) when it is about to send m. More 
precisely, when Pi sends m to Pj , Pi knows the exact values of
send(m).V Ci[k] and send(m).IPi[k] (they are the current
values of V Ci[k] and IPi[k]). But, as far as the value of
pred(receive(m)).V Cj[k] is concerned, two cases are 
possible. Case (i): If pred(receive(m))
hb
→ send(m), then Pi can
know the value of pred(receive(m)).V Cj[k] and 
consequently can evaluate K(m, k). Case (ii): If pred(receive(m))
and send(m) are concurrent, Pi cannot know the value of
pred(receive(m)).V Cj[k] and consequently cannot evaluate
K(m, k). Moreover, when it sends m to Pj , whatever the
case (i or ii) that actually occurs, Pi has no way to know
which case does occur. Hence the idea to define evaluable
approximations of the general condition. Let K (m, k) be
an approximation of K(m, k), that can be evaluated by a
process Pi when it sends a message m. To be correct, the
condition K must ensure that, every time Pi should 
transmit a triple (k, V Ci[k], IPi[k]) according to Theorem 2 (i.e.,
each time ¬K(m, k)), then Pi transmits this triple when it
uses condition K . Hence, the definition of a correct 
evaluable approximation:
Definition 6. A condition K , locally evaluable by a 
process when it sends a message m to another process, is 
correct if ∀(m, k) : ¬K(m, k) ⇒ ¬K (m, k) or, equivalently,
∀(m, k) : K (m, k) ⇒ K(m, k).
This definition means that a protocol evaluating K to 
decide which triples must be attached to messages, does not
miss triples whose transmission is required by Theorem 2.
Let us consider the constant condition (denoted K1),
that is always false, i.e., ∀(m, k) : K1(m, k) = false. This
trivially correct approximation of K actually corresponds
to the particular IPT protocol described in Section 3 (in
which each message carries a whole vector clock and a 
whole boolean vector). The next section presents a better
approximation of K (denoted K2).
5.1 A Boolean Matrix-Based Evaluable 
Condition
Condition K2 is based on the observation that condition
K is composed of sub-conditions. Some of them can be
Pj
send(m)
Pi
V Ci[k] = x
IPi[k] = 1
V Cj[k] ≥ x receive(m)
Figure 2: The Evaluable Condition K2
locally evaluated while the others cannot. More 
precisely, K ≡ a ∨ α ∨ (β ∧ b), where a ≡ send(m).V Ci[k] = 0
and b ≡ send(m).IPi[k] = 1 are locally evaluable, 
whereas α ≡ send(m).V Ci[k] < pred(receive(m)).V Cj[k] and
β ≡ send(m).V Ci[k] = pred(receive(m)).V Cj[k] are not.
But, from easy boolean calculus, a∨((α∨β)∧b) =⇒ a∨α∨
(β ∧ b) ≡ K. This leads to condition K ≡ a ∨ (γ ∧ b), where
γ = α ∨ β ≡ send(m).V Ci[k] ≤ pred(receive(m)).V Cj[k] ,
i.e., K ≡ (send(m).V Ci[k] ≤ pred(receive(m)).V Cj[k] ∧
send(m).IPi[k] = 1) ∨ send(m).V Ci[k] = 0.
So, Pi needs to approximate the predicate send(m).V Ci[k]
≤ pred(receive(m)).V Cj[k]. To be correct, this 
approximation has to be a locally evaluable predicate ci(j, k) such that,
when Pi is about to send a message m to Pj, ci(j, k) ⇒
(send(m).V Ci[k] ≤ pred(receive(m)).V Cj[k]). Informally,
that means that, when ci(j, k) holds, the local context of
Pi allows to deduce that the receipt of m by Pj will not
lead to V Cj[k] update (Pj knows as much as Pi about
Pk). Hence, the concrete condition K2 is the following:
K2 ≡ send(m).V Ci[k] = 0 ∨ (ci(j, k) ∧ send(m).IPi[k] = 1).
Let us now examine the design of such a predicate 
(denoted ci). First, the case j = i can be ignored, since it is
assumed (Section 2.1) that a process never sends a 
message to itself. Second, in the case j = k, the relation
send(m).V Ci[j] ≤ pred(receive(m)).V Cj [j] is always true,
because the receipt of m by Pj cannot update V Cj[j]. Thus,
∀j = i : ci(j, j) must be true. Now, let us consider the case
where j = i and j = k (Figure 2). Suppose that there exists
an event e = receive(m ) with e < send(m), m sent by
Pj and piggybacking the triple (k, m .V C[k], m .IP[k]), and
m .V C[k] ≥ V Ci[k] (hence m .V C[k] = receive(m ).V Ci[k]).
As V Cj[k] cannot decrease this means that, as long as V Ci[k]
does not increase, for every message m sent by Pi to Pj we
have the following: send(m).V Ci[k] = receive(m ).V Ci[k] =
send(m ).V Cj[k] ≤ receive(m).V Cj [k], i.e., ci(j, k) must 
remain true. In other words, once ci(j, k) is true, the only
event of Pi that could reset it to false is either the receipt
of a message that increases V Ci[k] or, if k = i, the 
occurrence of a relevant event (that increases V Ci[i]). Similarly,
once ci(j, k) is false, the only event that can set it to true is
the receipt of a message m from Pj, piggybacking the triple
(k, m .V C[k], m .IP[k]) with m .V C[k] ≥ V Ci[k].
In order to implement the local predicates ci(j, k), each
process Pi is equipped with a boolean matrix Mi (as in [11])
such that M[j, k] = 1 ⇔ ci(j, k). It follows from the 
previous discussion that this matrix is managed according to the
following rules (note that its i-th line is not significant (case
j = i), and that its diagonal is always equal to 1):
M0 Initialization: ∀ (j, k) : Mi[j, k] is initialized to 1.
215
M1 Each time it produces a relevant event e: Pi resets4
the ith column of its matrix: ∀j = i : Mi[j, i] := 0.
M2 When Pi sends a message: no update of Mi occurs.
M3 When it receives a message m from Pj , Pi executes the
following updates:
∀ k ∈ [1..n] : case
V Ci[k] < m.V C[k] then ∀ = i, j, k : Mi[ , k] := 0;
Mi[j, k] := 1
V Ci[k] = m.V C[k] then Mi[j, k] := 1
V Ci[k] > m.V C[k] then skip
endcase
The following lemma results from rules M0-M3. The 
theorem that follows shows that condition K2(m, k) is correct.
(Both are proved in [1].)
Lemma 6. ∀i, ∀m sent by Pi to Pj, ∀k, we have:
send(m).Mi[j, k] = 1 ⇒
send(m).V Ci[k] ≤ pred(receive(m)).V Cj [k].
Theorem 3. Let m be a message sent by Pi to Pj . Let
K2(m, k) ≡ ((send(m).Mi[j, k] = 1) ∧ (send(m).IPi[k] =
1)∨(send(m).V Ci[k] = 0)). We have: K2(m, k) ⇒ K(m, k).
5.2 Resulting IPT Protocol
The complete text of the IPT protocol based on the 
previous discussion follows.
RM0 Initialization:
- Both V Ci[1..n] and IPi[1..n] are set to [0, . . . , 0],
and ∀ (j, k) : Mi[j, k] is set to 1.
RM1 Each time it produces a relevant event e:
- Pi associates with e the timestamp e.TS defined
as follows: e.TS = {(k, V Ci[k]) | IPi[k] = 1},
- Pi increments its vector clock entry V Ci[i] 
(namely, it executes V Ci[i] := V Ci[i] + 1),
- Pi resets IPi: ∀ = i : IPi[ ] := 0; IPi[i] := 1.
- Pi resets the ith column of its boolean matrix:
∀j = i : Mi[j, i] := 0.
RM2 When Pi sends a message m to Pj, it attaches to m the
set of triples (each made up of a process id, an integer
and a boolean): {(k, V Ci[k], IPi[k]) | (Mi[j, k] = 0 ∨
IPi[k] = 0) ∧ (V Ci[k] > 0)}.
RM3 When Pi receives a message m from Pj , it executes the
following updates:
∀(k,m.V C[k], m.IP[k]) carried by m:
case
V Ci[k] < m.V C[k] then V Ci[k] := m.V C[k];
IPi[k] := m.IP[k];
∀ = i, j, k : Mi[ , k] := 0;
4
Actually, the value of this column remains constant after
its first update. In fact, ∀j, Mi[j, i] can be set to 1 only upon
the receipt of a message from Pj, carrying the value V Cj[i]
(see R3). But, as Mj [i, i] = 1, Pj does not send V Cj[i] to
Pi. So, it is possible to improve the protocol by executing
this reset of the column Mi[∗, i] only when Pi produces
its first relevant event.
Mi[j, k] := 1
V Ci[k] = m.V C[k] then IPi[k] := min(IPi[k], m.IP[k]);
Mi[j, k] := 1
V Ci[k] > m.V C[k] then skip
endcase
5.3 A Tradeoff
The condition K2(m, k) shows that a triple has not to be
transmitted when (Mi[j, k] = 1 ∧ IPi[k] = 1) ∨ (V Ci[k] >
0). Let us first observe that the management of IPi[k]
is governed by the application program. More precisely,
the IPT protocol does not define which are the 
relevant events, it has only to guarantee a correct 
management of IPi[k]. Differently, the matrix Mi does not belong
to the problem specification, it is an auxiliary variable of
the IPT protocol, which manages it so as to satisfy the
following implication when Pi sends m to Pj : (Mi[j, k] =
1) ⇒ (pred(receive(m)).V Cj [k] ≥ send(m).V Ci[k]). The
fact that the management of Mi is governed by the protocol
and not by the application program leaves open the 
possibility to design a protocol where more entries of Mi are equal
to 1. This can make the condition K2(m, k) more often 
satisfied5
and can consequently allow the protocol to transmit
less triples.
We show here that it is possible to transmit less triples
at the price of transmitting a few additional boolean 
vectors. The previous IPT matrix-based protocol (Section 5.2)
is modified in the following way. The rules RM2 and 
RM3 are replaced with the modified rules RM2" and RM3"
(Mi[∗, k] denotes the kth column of Mi).
RM2" When Pi sends a message m to Pj, it attaches to m
the following set of 4-uples (each made up of a 
process id, an integer, a boolean and a boolean vector):
{(k, V Ci[k], IPi[k], Mi[∗, k]) | (Mi[j, k] = 0 ∨ IPi[k] =
0) ∧ V Ci[k] > 0}.
RM3" When Pi receives a message m from Pj , it executes the
following updates:
∀(k,m.V C[k], m.IP[k], m.M[1..n, k]) carried by m:
case
V Ci[k] < m.V C[k] then V Ci[k] := m.V C[k];
IPi[k] := m.IP[k];
∀ = i : Mi[ , k] := m.M[ , k]
V Ci[k] = m.V C[k] then IPi[k] := min(IPi[k], m.IP[k]);
∀ =i : Mi[ , k] :=
max(Mi[ , k], m.M[ , k])
V Ci[k] > m.V C[k] then skip
endcase
Similarly to the proofs described in [1], it is possible to
prove that the previous protocol still satisfies the 
property proved in Lemma 6, namely, ∀i, ∀m sent by Pi to Pj,
∀k we have (send(m).Mi[j, k] = 1) ⇒ (send(m).V Ci[k] ≤
pred(receive(m)).V Cj[k]).
5
Let us consider the previously described protocol (Section
5.2) where the value of each matrix entry Mi[j, k] is always
equal to 0. The reader can easily verify that this setting 
correctly implements the matrix. Moreover, K2(m, k) is then
always false: it actually coincides with K1(k, m) (which 
corresponds to the case where whole vectors have to be 
transmitted with each message).
216
Intuitively, the fact that some columns of matrices M are
attached to application messages allows a transitive 
transmission of information. More precisely, the relevant history
of Pk known by Pj is transmitted to a process Pi via a causal
sequence of messages from Pj to Pi. In contrast, the 
protocol described in Section 5.2 used only a direct transmission of
this information. In fact, as explained Section 5.1, the 
predicate c (locally implemented by the matrix M) was based on
the existence of a message m sent by Pj to Pi, piggybacking
the triple (k, m .V C[k], m .IP[k]), and m .V C[k] ≥ V Ci[k],
i.e., on the existence of a direct transmission of information
(by the message m ).
The resulting IPT protocol (defined by the rules RM0,
RM1, RM2" and RM3") uses the same condition K2(m, k)
as the previous one. It shows an interesting tradeoff between
the number of triples (k, V Ci[k], IPi[k]) whose transmission
is saved and the number of boolean vectors that have to
be additionally piggybacked. It is interesting to notice that
the size of this additional information is bounded while each
triple includes a non-bounded integer (namely a vector clock
value).
6. EXPERIMENTAL STUDY
This section compares the behaviors of the previous 
protocols. This comparison is done with a simulation study.
IPT1 denotes the protocol presented in Section 3.3 that 
uses the condition K1(m, k) (which is always equal to false).
IPT2 denotes the protocol presented in Section 5.2 that uses
the condition K2(m, k) where messages carry triples. 
Finally, IPT3 denotes the protocol presented in Section 5.3 that
also uses the condition K2(m, k) but where messages carry
additional boolean vectors.
This section does not aim to provide an in-depth 
simulation study of the protocols, but rather presents a general
view on the protocol behaviors. To this end, it compares
IPT2 and IPT3 with regard to IPT1. More precisely, for
IPT2 the aim was to evaluate the gain in terms of triples
(k, V Ci[k], IPi[k]) not transmitted with respect to the 
systematic transmission of whole vectors as done in IPT1. For
IPT3, the aim was to evaluate the tradeoff between the 
additional boolean vectors transmitted and the number of saved
triples. The behavior of each protocol was analyzed on a set
of programs.
6.1 Simulation Parameters
The simulator provides different parameters enabling to
tune both the communication and the processes features.
These parameters allow to set the number of processes for
the simulated computation, to vary the rate of 
communication (send/receive) events, and to alter the time duration
between two consecutive relevant events. Moreover, to be
independent of a particular topology of the underlying 
network, a fully connected network is assumed. Internal events
have not been considered.
Since the presence of the triples (k, V Ci[k], IPi[k]) 
piggybacked by a message strongly depends on the frequency at
which relevant events are produced by a process, different
time distributions between two consecutive relevant events
have been implemented (e.g., normal, uniform, and Poisson
distributions). The senders of messages are chosen 
according to a random law. To exhibit particular configurations
of a distributed computation a given scenario can be 
provided to the simulator. Message transmission delays follow
a standard normal distribution. Finally, the last parameter
of the simulator is the number of send events that occurred
during a simulation.
6.2 Parameter Settings
To compare the behavior of the three IPT protocols, we
performed a large number of simulations using different 
parameters setting. We set to 10 the number of processes
participating to a distributed computation. The number of
communication events during the simulation has been set to
10 000. The parameter λ of the Poisson time distribution (λ
is the average number of relevant events in a given time 
interval) has been set so that the relevant events are generated
at the beginning of the simulation. With the uniform time
distribution, a relevant event is generated (in the average)
every 10 communication events. The location parameter of
the standard normal time distribution has been set so that
the occurrence of relevant events is shifted around the third
part of the simulation experiment.
As noted previously, the simulator can be fed with a 
given scenario. This allows to analyze the worst case scenarios
for IPT2 and IPT3. These scenarios correspond to the case
where the relevant events are generated at the maximal 
frequency (i.e., each time a process sends or receives a message,
it produces a relevant event).
Finally, the three IPT protocols are analyzed with the
same simulation parameters.
6.3 Simulation Results
The results are displayed on the Figures 3.a-3.d. These
figures plot the gain of the protocols in terms of the number
of triples that are not transmitted (y axis) with respect to
the number of communication events (x axis). From these
figures, we observe that, whatever the time distribution 
followed by the relevant events, both IPT2 and IPT3 exhibit
a behavior better than IPT1 (i.e., the total number of 
piggybacked triples is lower in IPT2 and IPT3 than in IPT1),
even in the worst case (see Figure 3.d).
Let us consider the worst scenario. In that case, the gain
is obtained at the very beginning of the simulation and lasts
as long as it exists a process Pj for which ∀k : V Cj[k] = 0.
In that case, the condition ∀k : K(m, k) is satisfied. As soon
as ∃k : V Cj[k] = 0, both IPT2 and IPT3 behave as IPT1
(the shape of the curve becomes flat) since the condition
K(m, k) is no longer satisfied.
Figure 3.a shows that during the first events of the 
simulation, the slope of curves IPT2 and IPT3 are steep. The
same occurs in Figure 3.d (that depicts the worst case 
scenario). Then the slope of these curves decreases and remains
constant until the end of the simulation. In fact, as soon as
V Cj[k] becomes greater than 0, the condition ¬K(m, k) 
reduces to (Mi[j, k] = 0 ∨ IPi[k] = 0).
Figure 3.b displays an interesting feature. It considers λ =
100. As the relevant events are taken only during the very
beginning of the simulation, this figure exhibits a very steep
slope as the other figures. The figure shows that, as soon as
no more relevant events are taken, on average, 45% of the
triples are not piggybacked by the messages. This shows
the importance of matrix Mi. Furthermore, IPT3 benefits
from transmitting additional boolean vectors to save triple
transmissions. The Figures 3.a-3.c show that the average
gain of IPT3 with respect to IPT2 is close to 10%.
Finally, Figure 3.c underlines even more the importance
217
of matrix Mi. When very few relevant events are taken,
IPT2 and IPT3 turn out to be very efficient. Indeed, this
figure shows that, very quickly, the gain in number of triples
that are saved is very high (actually, 92% of the triples are
saved).
6.4 Lessons Learned from the Simulation
Of course, all simulation results are consistent with the
theoretical results. IPT3 is always better than or equal to
IPT2, and IPT2 is always better than IPT1. The simulation
results teach us more:
• The first lesson we have learnt concerns the matrix Mi.
Its use is quite significant but mainly depends on the time
distribution followed by the relevant events. On the one
hand, when observing Figure 3.b where a large number of
relevant events are taken in a very short time, IPT2 can save
up to 45% of the triples. However, we could have 
expected a more sensitive gain of IPT2 since the boolean vector
IP tends to stabilize to [1, ..., 1] when no relevant events
are taken. In fact, as discussed in Section 5.3, the 
management of matrix Mi within IPT2 does not allow a transitive
transmission of information but only a direct transmission
of this information. This explains why some columns of Mi
may remain equal to 0 while they could potentially be equal
to 1. Differently, as IPT3 benefits from transmitting 
additional boolean vectors (providing a transitive transmission
information) it reaches a gain of 50%.
On the other hand, when very few relevant events are 
taken in a large period of time (see Figure 3.c), the behavior of
IPT2 and IPT3 turns out to be very efficient since the 
transmission of up to 92% of the triples is saved. This comes from
the fact that very quickly the boolean vector IPi tends to
stabilize to [1, ..., 1] and that matrix Mi contains very few
0 since very few relevant events have been taken. Thus, a
direct transmission of the information is sufficient to quickly
get matrices Mi equal to [1, ..., 1], . . . , [1, ..., 1].
• The second lesson concerns IPT3, more precisely, the
tradeoff between the additional piggybacking of boolean 
vectors and the number of triples whose transmission is saved.
With n = 10, adding 10 booleans to a triple does not 
substantially increases its size. The Figures 3.a-3.c exhibit the
number of triples whose transmission is saved: the average
gain (in number of triples) of IPT3 with respect to IPT2 is
about 10%.
7. CONCLUSION
This paper has addressed an important causality-related
distributed computing problem, namely, the Immediate 
Predecessors Tracking problem. It has presented a family of
protocols that provide each relevant event with a timestamp
that exactly identify its immediate predecessors. The 
family is defined by a general condition that allows application
messages to piggyback control information whose size can
be smaller than n (the number of processes). In that sense,
this family defines message size-efficient IPT protocols. 
According to the way the general condition is implemented, 
different IPT protocols can be obtained. Three of them have
been described and analyzed with simulation experiments.
Interestingly, it has also been shown that the efficiency of
the protocols (measured in terms of the size of the control
information that is not piggybacked by an application 
message) depends on the pattern defined by the communication
events and the relevant events.
Last but not least, it is interesting to note that if one is not
interested in tracking the immediate predecessor events, the
protocols presented in the paper can be simplified by 
suppressing the IPi booleans vectors (but keeping the boolean
matrices Mi). The resulting protocols, that implement a
vector clock system, are particularly efficient as far as the
size of the timestamp carried by each message is concerned.
Interestingly, this efficiency is not obtained at the price of
additional assumptions (such as fifo channels).
8. REFERENCES
[1] Anceaume E., H´elary J.-M. and Raynal M., Tracking
Immediate Predecessors in Distributed Computations. Res.
Report #1344, IRISA, Univ. Rennes (France), 2001.
[2] Baldoni R., Prakash R., Raynal M. and Singhal M.,
Efficient ∆-Causal Broadcasting. Journal of Computer
Systems Science and Engineering, 13(5):263-270, 1998.
[3] Chandy K.M. and Lamport L., Distributed Snapshots:
Determining Global States of Distributed Systems, ACM
Transactions on Computer Systems, 3(1):63-75, 1985.
[4] Diehl C., Jard C. and Rampon J.-X., Reachability Analysis
of Distributed Executions, Proc. TAPSOFT"93,
Springer-Verlag LNCS 668, pp. 629-643, 1993.
[5] Fidge C.J., Timestamps in Message-Passing Systems that
Preserve Partial Ordering, Proc. 11th Australian
Computing Conference, pp. 56-66, 1988.
[6] Fromentin E., Jard C., Jourdan G.-V. and Raynal M.,
On-the-fly Analysis of Distributed Computations, IPL,
54:267-274, 1995.
[7] Fromentin E. and Raynal M., Shared Global States in
Distributed Computations, JCSS, 55(3):522-528, 1997.
[8] Fromentin E., Raynal M., Garg V.K. and Tomlinson A.,
On-the-Fly Testing of Regular Patterns in Distributed
Computations. Proc. ICPP"94, Vol. 2:73-76, 1994.
[9] Garg V.K., Principles of Distributed Systems, Kluwer
Academic Press, 274 pages, 1996.
[10] H´elary J.-M., Most´efaoui A., Netzer R.H.B. and Raynal
M., Communication-Based Prevention of Useless
Ckeckpoints in Distributed Computations. Distributed
Computing, 13(1):29-43, 2000.
[11] H´elary J.-M., Melideo G. and Raynal M., Tracking
Causality in Distributed Systems: a Suite of Efficient
Protocols. Proc. SIROCCO"00, Carleton University Press,
pp. 181-195, L"Aquila (Italy), June 2000.
[12] H´elary J.-M., Netzer R. and Raynal M., Consistency Issues
in Distributed Checkpoints. IEEE TSE,
25(4):274-281, 1999.
[13] Hurfin M., Mizuno M., Raynal M. and Singhal M., Efficient
Distributed Detection of Conjunction of Local Predicates
in Asynch Computations. IEEE TSE, 24(8):664-677, 1998.
[14] Lamport L., Time, Clocks and the Ordering of Events in a
Distributed System. Comm. ACM, 21(7):558-565, 1978.
[15] Marzullo K. and Sabel L., Efficient Detection of a Class of
Stable Properties. Distributed Computing, 8(2):81-91, 1994.
[16] Mattern F., Virtual Time and Global States of Distributed
Systems. Proc. Int. Conf. Parallel and Distributed
Algorithms, (Cosnard, Quinton, Raynal, Robert Eds),
North-Holland, pp. 215-226, 1988.
[17] Prakash R., Raynal M. and Singhal M., An Adaptive
Causal Ordering Algorithm Suited to Mobile Computing
Environment. JPDC, 41:190-204, 1997.
[18] Raynal M. and Singhal S., Logical Time: Capturing
Causality in Distributed Systems. IEEE Computer,
29(2):49-57, 1996.
[19] Singhal M. and Kshemkalyani A., An Efficient
Implementation of Vector Clocks. IPL, 43:47-52, 1992.
[20] Wang Y.M., Consistent Global Checkpoints That Contain
a Given Set of Local Checkpoints. IEEE TOC,
46(4):456-468, 1997.
218
0
1000
2000
3000
4000
5000
6000
0 2000 4000 6000 8000 10000
gaininnumberoftriples
communication events number
IPT1
IPT2
IPT3
relevant events
(a) The relevant events follow a uniform distribution
(ratio=1/10)
-5000
0
5000
10000
15000
20000
25000
30000
35000
40000
45000
50000
0 2000 4000 6000 8000 10000
gaininnumberoftriples
communication events number
IPT1
IPT2
IPT3
relevant events
(b) The relevant events follow a Poisson distribution
(λ = 100)
0
10000
20000
30000
40000
50000
60000
70000
80000
90000
100000
0 2000 4000 6000 8000 10000
gaininnumberoftriples
communication events number
IPT1
IPT2
IPT3
relevant events
(c) The relevant events follow a normal distribution
0
50
100
150
200
250
300
350
400
450
1 10 100 1000 10000
gaininnumberoftriples
communication events number
IPT1
IPT2
IPT3
relevant events
(d) For each pi, pi takes a relevant event and 
broadcast to all processes
Figure 3: Experimental Results
219
Shooter Localization and Weapon Classification with
Soldier-Wearable Networked Sensors
Peter Volgyesi, Gyorgy Balogh, Andras Nadas, Christopher B. Nash, Akos Ledeczi
Institute for Software Integrated Systems, Vanderbilt University
Nashville, TN, USA
akos.ledeczi@vanderbilt.edu
ABSTRACT
The paper presents a wireless sensor network-based mobile
countersniper system. A sensor node consists of a 
helmetmounted microphone array, a COTS MICAz mote for 
internode communication and a custom sensorboard that 
implements the acoustic detection and Time of Arrival (ToA) 
estimation algorithms on an FPGA. A 3-axis compass provides
self orientation and Bluetooth is used for communication
with the soldier"s PDA running the data fusion and the user
interface. The heterogeneous sensor fusion algorithm can
work with data from a single sensor or it can fuse ToA or
Angle of Arrival (AoA) observations of muzzle blasts and
ballistic shockwaves from multiple sensors. The system 
estimates the trajectory, the range, the caliber and the weapon
type. The paper presents the system design and the results
from an independent evaluation at the US Army Aberdeen
Test Center. The system performance is characterized by 
1degree trajectory precision and over 95% caliber estimation
accuracy for all shots, and close to 100% weapon estimation
accuracy for 4 out of 6 guns tested.
Categories and Subject Descriptors
C.2.4 [Computer-Communications Networks]: 
Distributed Systems; J.7 [Computers in Other Systems]: 
Military
General Terms: Design, Measurement, Performance
1. INTRODUCTION
The importance of countersniper systems is underscored
by the constant stream of news reports coming from the
Middle East. In October 2006 CNN reported on a new 
tactic employed by insurgents. A mobile sniper team moves
around busy city streets in a car, positions itself at a good
standoff distance from dismounted US military personnel,
takes a single well-aimed shot and immediately melts in the
city traffic. By the time the soldiers can react, they are
gone. A countersniper system that provides almost 
immediate shooter location to every soldier in the vicinity would
provide clear benefits to the warfigthers.
Our team introduced PinPtr, the first sensor 
networkbased countersniper system [17, 8] in 2003. The system is
based on potentially hundreds of inexpensive sensor nodes
deployed in the area of interest forming an ad-hoc multihop
network. The acoustic sensors measure the Time of Arrival
(ToA) of muzzle blasts and ballistic shockwaves, pressure
waves induced by the supersonic projectile, send the data to
a base station where a sensor fusion algorithm determines
the origin of the shot. PinPtr is characterized by high 
precision: 1m average 3D accuracy for shots originating within
or near the sensor network and 1 degree bearing precision
for both azimuth and elevation and 10% accuracy in range
estimation for longer range shots. The truly unique 
characteristic of the system is that it works in such reverberant
environments as cluttered urban terrain and that it can 
resolve multiple simultaneous shots at the same time. This
capability is due to the widely distributed sensing and the
unique sensor fusion approach [8]. The system has been
tested several times in US Army MOUT (Military 
Operations in Urban Terrain) facilities.
The obvious disadvantage of such a system is its static
nature. Once the sensors are distributed, they cover a 
certain area. Depending on the operation, the deployment may
be needed for an hour or a month, but eventually the area
looses its importance. It is not practical to gather and reuse
the sensors, especially under combat conditions. Even if the
sensors are cheap, it is still a waste and a logistical problem
to provide a continuous stream of sensors as the operations
move from place to place. As it is primarily the soldiers that
the system protects, a natural extension is to mount the
sensors on the soldiers themselves. While there are 
vehiclemounted countersniper systems [1] available commercially,
we are not aware of a deployed system that protects 
dismounted soldiers. A helmet-mounted system was developed
in the mid 90s by BBN [3], but it was not continued beyond
the Darpa program that funded it.
113
To move from a static sensor network-based solution to a
highly mobile one presents significant challenges. The sensor
positions and orientation need to be constantly monitored.
As soldiers may work in groups of as little as four people,
the number of sensors measuring the acoustic phenomena
may be an order of magnitude smaller than before. 
Moreover, the system should be useful to even a single soldier.
Finally, additional requirements called for caliber estimation
and weapon classification in addition to source localization.
The paper presents the design and evaluation of our 
soldierwearable mobile countersniper system. It describes the 
hardware and software architecture including the custom sensor
board equipped with a small microphone array and 
connected to a COTS MICAz mote [12]. Special emphasis is
paid to the sensor fusion technique that estimates the 
trajectory, range, caliber and weapon type simultaneously. The
results and analysis of an independent evaluation of the 
system at the US Army Aberdeen Test Center are also 
presented.
2. APPROACH
The firing of a typical military rifle, such as the AK47
or M16, produces two distinct acoustic phenomena. The
muzzle blast is generated at the muzzle of the gun and 
travels at the speed sound. The supersonic projectile generates
an acoustic shockwave, a kind of sonic boom. The 
wavefront has a conical shape, the angle of which depends on the
Mach number, the speed of the bullet relative to the speed
of sound.
The shockwave has a characteristic shape resembling a
capital N. The rise time at both the start and end of the
signal is very fast, under 1 μsec. The length is determined by
the caliber and the miss distance, the distance between the
trajectory and the sensor. It is typically a few hundred μsec.
Once a trajectory estimate is available, the shockwave length
can be used for caliber estimation.
Our system is based on four microphones connected to
a sensorboard. The board detects shockwaves and muzzle
blasts and measures their ToA. If at least three acoustic
channels detect the same event, its AoA is also computed.
If both the shockwave and muzzle blast AoA are available,
a simple analytical solution gives the shooter location as
shown in Section 6. As the microphones are close to each
other, typically 2-4, we cannot expect very high precision.
Also, this method does not estimate a trajectory. In fact, an
infinite number of trajectory-bullet speed pairs satisfy the
observations. However, the sensorboards are also connected
to COTS MICAz motes and they share their AoA and ToA
measurements, as well as their own location and orientation,
with each other using a multihop routing service [9]. A
hybrid sensor fusion algorithm then estimates the trajectory,
the range, the caliber and the weapon type based on all
available observations.
The sensorboard is also Bluetooth capable for 
communication with the soldier"s PDA or laptop computer. A wired
USB connection is also available. The sensorfusion 
algorithm and the user interface get their data through one of
these channels.
The orientation of the microphone array at the time of
detection is provided by a 3-axis digital compass. Currently
the system assumes that the soldier"s PDA is GPS-capable
and it does not provide self localization service itself. 
However, the accuracy of GPS is a few meters degrading the
Figure 1: Acoustic sensorboard/mote assembly
.
overall accuracy of the system. Refer to Section 7 for an
analysis. The latest generation sensorboard features a Texas
Instruments CC-1000 radio enabling the high-precision radio
interferometric self localization approach we have developed
separately [7]. However, we leave the integration of the two
technologies for future work.
3. HARDWARE
Since the first static version of our system in 2003, the 
sensor nodes have been built upon the UC Berkeley/Crossbow
MICA product line [11]. Although rudimentary acoustic 
signal processing can be done on these microcontroller-based
boards, they do not provide the required computational
performance for shockwave detection and angle of arrival
measurements, where multiple signals from different 
microphones need to be processed in parallel at a high sampling
rate. Our 3rd generation sensorboard is designed to be used
with MICAz motes-in fact it has almost the same size as
the mote itself (see Figure 1).
The board utilizes a powerful Xilinx XC3S1000 FPGA
chip with various standard peripheral IP cores, multiple soft
processor cores and custom logic for the acoustic detectors
(Figure 2). The onboard Flash (4MB) and PSRAM (8MB)
modules allow storing raw samples of several acoustic events,
which can be used to build libraries of various acoustic 
signatures and for refining the detection cores off-line. Also, the
external memory blocks can store program code and data
used by the soft processor cores on the FPGA.
The board supports four independent analog channels 
sampled at up to 1 MS/s (million samples per seconds). These
channels, featuring an electret microphone (Panasonic 
WM64PNT), amplifiers with controllable gain (30-60 dB) and
a 12-bit serial ADC (Analog Devices AD7476), reside on
separate tiny boards which are connected to the main 
sensorboard with ribbon cables. This partitioning enables the
use of truly different audio channels (eg.: slower sampling
frequency, different gain or dynamic range) and also results
in less noisy measurements by avoiding long analog signal
paths.
The sensor platform offers a rich set of interfaces and can
be integrated with existing systems in diverse ways. An
RS232 port and a Bluetooth (BlueGiga WT12) wireless link
with virtual UART emulation are directly available on the
board and provide simple means to connect the sensor to
PCs and PDAs. The mote interface consists of an I2
C bus
along with an interrupt and GPIO line (the latter one is used
114
Figure 2: Block diagram of the sensorboard.
for precise time synchronization between the board and the
mote). The motes are equipped with IEEE 802.15.4 
compliant radio transceivers and support ad-hoc wireless 
networking among the nodes and to/from the base station. The
sensorboard also supports full-speed USB transfers (with
custom USB dongles) for uploading recorded audio samples
to the PC. The on-board JTAG chain-directly accessible
through a dedicated connector-contains the FPGA part
and configuration memory and provides in-system 
programming and debugging facilities.
The integrated Honeywell HMR3300 digital compass 
module provides heading, pitch and roll information with 1◦
accuracy, which is essential for calculating and combining
directional estimates of the detected events.
Due to the complex voltage requirements of the FPGA,
the power supply circuitry is implemented on the 
sensorboard and provides power both locally and to the mote. We
used a quad pack of rechargeable AA batteries as the power
source (although any other configuration is viable that meets
the voltage requirements). The FPGA core (1.2 V) and I/O
(3.3 V) voltages are generated by a highly efficient buck
switching regulator. The FPGA configuration (2.5 V) and a
separate 3.3 V power net are fed by low current LDOs, the
latter one is used to provide independent power to the mote
and to the Bluetooth radio. The regulators-except the last
one-can be turned on/off from the mote or through the
Bluetooth radio (via GPIO lines) to save power.
The first prototype of our system employed 10 sensor
nodes. Some of these nodes were mounted on military kevlar
helmets with the microphones directly attached to the 
surface at about 20 cm separation as shown in Figure 3(a). The
rest of the nodes were mounted in plastic enclosures 
(Figure 3(b)) with the microphones placed near the corners of
the boxes to form approximately 5 cm×10 cm rectangles.
4. SOFTWARE ARCHITECTURE
The sensor application relies on three subsystems 
exploiting three different computing paradigms as they are shown
in Figure 4. Although each of these execution models suit
their domain specific tasks extremely well, this diversity
(a) (b)
Figure 3: Sensor prototypes mounted on a kevlar
helmet (a) and in a plastic box on a tripod (b).
presents a challenge for software development and system
integration. The sensor fusion and user interface 
subsystem is running on PDAs and were implemented in Java.
The sensing and signal processing tasks are executed by an
FPGA, which also acts as a bridge between various wired
and wireless communication channels. The ad-hoc internode
communication, time synchronization and data sharing are
the responsibilities of a microcontroller based radio module.
Similarly, the application employs a wide variety of 
communication protocols such as Bluetooth and IEEE 802.14.5
wireless links, as well as optional UARTs, I2
C and/or USB
buses.
Soldier
Operated Device
(PDA/Laptop)
FPGA
Sensor Board
Mica Radio
Module
2.4 GHz Wireless Link
Radio Control
Message Routing
Acoustic Event Encoder
Sensor Time Synch.
Network Time Synch.Remote Control
Time
stamping
Interrupts
Virtual
Register
Interface
C
O
O
R
D
I
N
A
T
O
R
A
n
a
l
o
g
c
h
a
n
n
e
l
s Compass
PicoBlaze
Comm.
Interface
PicoBlaze
WT12 Bluetooth Radio
MOTE IF:I2C,Interrupts
USB PSRAM
U
A
R
T
U
A
R
T
MB
det
SW
det
REC
Bluetooth Link
User
Interface
Sensor
Fusion
Location
Engine GPS
Message (Dis-)AssemblerSensor
Control
Figure 4: Software architecture diagram.
The sensor fusion module receives and unpacks raw 
measurements (time stamps and feature vectors) from the 
sensorboard through the Bluetooth link. Also, it fine tunes
the execution of the signal processing cores by setting 
parameters through the same link. Note that measurements
from other nodes along with their location and orientation
information also arrive from the sensorboard which acts as
a gateway between the PDA and the sensor network. The
handheld device obtains its own GPS location data and 
di115
rectly receives orientation information through the 
sensorboard. The results of the sensor fusion are displayed on the
PDA screen with low latency. Since, the application is 
implemented in pure Java, it is portable across different PDA
platforms.
The border between software and hardware is 
considerably blurred on the sensor board. The IP 
cores-implemented in hardware description languages (HDL) on the 
reconfigurable FPGA fabric-closely resemble hardware 
building blocks. However, some of them-most notably the soft
processor cores-execute true software programs. The 
primary tasks of the sensor board software are 1) acquiring
data samples from the analog channels, 2) processing 
acoustic data (detection), and 3) providing access to the results
and run-time parameters through different interfaces.
As it is shown in Figure 4, a centralized virtual register
file contains the address decoding logic, the registers for
storing parameter values and results and the point to point
data buses to and from the peripherals. Thus, it effectively
integrates the building blocks within the sensorboard and
decouples the various communication interfaces. This 
architecture enabled us to deploy the same set of sensors in a
centralized scenario, where the ad-hoc mote network (using
the I2
C interface) collected and forwarded the results to a
base station or to build a decentralized system where the
local PDAs execute the sensor fusion on the data obtained
through the Bluetooth interface (and optionally from other
sensors through the mote interface). The same set of 
registers are also accessible through a UART link with a terminal
emulation program. Also, because the low-level interfaces
are hidden by the register file, one can easily add/replace
these with new ones (eg.: the first generation of motes 
supported a standard μP interface bus on the sensor connector,
which was dropped in later designs).
The most important results are the time stamps of the
detected events. These time stamps and all other timing
information (parameters, acoustic event features) are based
on a 1 MHz clock and an internal timer on the FPGA. The
time conversion and synchronization between the sensor 
network and the board is done by the mote by periodically 
requesting the capture of the current timer value through a
dedicated GPIO line and reading the captured value from
the register file through the I2
C interface. Based on the the
current and previous readings and the corresponding mote
local time stamps, the mote can calculate and maintain the
scaling factor and offset between the two time domains.
The mote interface is implemented by the I2
C slave IP
core and a thin adaptation layer which provides a data and
address bus abstraction on top of it. The maximum 
effective bandwidth is 100 Kbps through this interface. The
FPGA contains several UART cores as well: for 
communicating with the on-board Bluetooth module, for 
controlling the digital compass and for providing a wired RS232
link through a dedicated connector. The control, status and
data registers of the UART modules are available through
the register file. The higher level protocols on these lines are
implemented by Xilinx PicoBlaze microcontroller cores [13]
and corresponding software programs. One of them provides
a command line interface for test and debug purposes, while
the other is responsible for parsing compass readings. By
default, they are connected to the RS232 port and to the
on-board digital compass line respectively, however, they
can be rewired to any communication interface by changing
the register file base address in the programs (e.g. the 
command line interface can be provided through the Bluetooth
channel).
Two of the external interfaces are not accessible through
the register file: a high speed USB link and the SRAM 
interface are tied to the recorder block. The USB module 
implements a simple FIFO with parallel data lines connected to an
external FT245R USB device controller. The RAM driver
implements data read/write cycles with correct timing and
is connected to the on-board pseudo SRAM. These 
interfaces provide 1 MB/s effective bandwidth for downloading
recorded audio samples, for example.
The data acquisition and signal processing paths exhibit
clear symmetry: the same set of IP cores are instantiated
four times (i.e. the number of acoustic channels) and run
independently. The signal paths meet only just before
the register file. Each of the analog channels is driven by
a serial A/D core for providing a 20 MHz serial clock and
shifting in 8-bit data samples at 1 MS/s and a digital 
potentiometer driver for setting the required gain. Each channel
has its own shockwave and muzzle blast detector, which are
described in Section 5. The detectors fetch run-time 
parameter values from the register file and store their results there
as well. The coordinator core constantly monitors the 
detection results and generates a mote interrupt promptly upon
full detection or after a reasonable timeout after partial
detection.
The recorder component is not used in the final 
deployment, however, it is essential for development purposes for
refining parameter values for new types of weapons or for
other acoustic sources. This component receives the 
samples from all channels and stores them in circular buffers in
the PSRAM device. If the signal amplitude on one of the
channels crosses a predefined threshold, the recorder 
component suspends the sample collection with a predefined delay
and dumps the contents of the buffers through the USB link.
The length of these buffers and delays, the sampling rate,
the threshold level and the set of recorded channels can be
(re)configured run-time through the register file. Note that
the core operates independently from the other signal 
processing modules, therefore, it can be used to validate the
detection results off-line.
The FPGA cores are implemented in VHDL, the PicoBlaze
programs are written in assembly. The complete 
configuration occupies 40% of the resources (slices) of the FPGA and
the maximum clock speed is 30 MHz, which is safely higher
than the speed used with the actual device (20MHz).
The MICAz motes are responsible for distributing 
measurement data across the network, which drastically 
improves the localization and classification results at each node.
Besides a robust radio (MAC) layer, the motes require two
essential middleware services to achieve this goal. The 
messages need to be propagated in the ad-hoc multihop network
using a routing service. We successfully integrated the 
Directed Flood-Routing Framework (DFRF) [9] in our 
application. Apart from automatic message aggregation and 
efficient buffer management, the most unique feature of DFRF
is its plug-in architecture, which accepts custom routing
policies. Routing policies are state machines that govern
how received messages are stored, resent or discarded. 
Example policies include spanning tree routing, broadcast, 
geographic routing, etc. Different policies can be used for 
different messages concurrently, and the application is able to
116
change the underlying policies at run-time (eg.: because of
the changing RF environment or power budget). In fact, we
switched several times between a simple but lavish broadcast
policy and a more efficient gradient routing on the field.
Correlating ToA measurements requires a common time
base and precise time synchronization in the sensor network.
The Routing Integrated Time Synchronization (RITS) [15]
protocol relies on very accurate MAC-layer time-stamping
to embed the cumulative delay that a data message accrued
since the time of the detection in the message itself. That
is, at every node it measures the time the message spent
there and adds this to the number in the time delay slot of
the message, right before it leaves the current node. Every
receiving node can subtract the delay from its current time
to obtain the detection time in its local time reference. The
service provides very accurate time conversion (few μs per
hop error), which is more than adequate for this application.
Note, that the motes also need to convert the sensorboard
time stamps to mote time as it is described earlier.
The mote application is implemented in nesC [5] and is
running on top of TinyOS [6]. With its 3 KB RAM and
28 KB program space (ROM) requirement, it easily fits on
the MICAz motes.
5. DETECTION ALGORITHM
There are several characteristics of acoustic shockwaves
and muzzle blasts which distinguish their detection and 
signal processing algorithms from regular audio applications.
Both events are transient by their nature and present very
intense stimuli to the microphones. This is increasingly
problematic with low cost electret microphones-designed
for picking up regular speech or music. Although 
mechanical damping of the microphone membranes can mitigate
the problem, this approach is not without side effects. The
detection algorithms have to be robust enough to handle 
severe nonlinear distortion and transitory oscillations. Since
the muzzle blast signature closely follows the shockwave 
signal and because of potential automatic weapon bursts, it is
extremely important to settle the audio channels and the
detection logic as soon as possible after an event. Also, 
precise angle of arrival estimation necessitates high sampling
frequency (in the MHz range) and accurate event detection.
Moreover, the detection logic needs to process multiple 
channels in parallel (4 channels on our existing hardware).
These requirements dictated simple and robust algorithms
both for muzzle blast and shockwave detections. Instead of
using mundane energy detectors-which might not be able
to distinguish the two different events-the applied 
detectors strive to find the most important characteristics of the
two signals in the time-domain using simple state machine
logic. The detectors are implemented as independent IP
cores within the FPGA-one pair for each channel. The
cores are run-time configurable and provide detection event
signals with high precision time stamps and event specific
feature vectors. Although the cores are running 
independently and in parallel, a crude local fusion module integrates
them by shutting down those cores which missed their events
after a reasonable timeout and by generating a single 
detection message towards the mote. At this point, the mote can
read and forward the detection times and features and is
responsible to restart the cores afterwards.
The most conspicuous characteristics of an acoustic 
shockwave (see Figure 5(a)) are the steep rising edges at the 
be0 200 400 600 800 1000 1200 1400 1600
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Shockwave (M16)
Time (µs)
Amplitude
1
3
5
2
4
len
(a)
s[t] - s[t-D] > E
tstart
:= t
s[t] - s[t-D] < E
s[t] - s[t-D] > E &
t - t_start > Lmin
s[t] - s[t-D] < E
len := t - tstart
IDLE
1
FIRST EDGE DONE
3
SECOND EDGE
4
FIRST EDGE
2
FOUND
5
t - tstart
≥ Lmax
t - tstart
≥ Lmax
(b)
Figure 5: Shockwave signal generated by a 5.56 ×
45 mm NATO projectile (a) and the state machine
of the detection algorithm (b).
ginning and end of the signal. Also, the length of the N-wave
is fairly predictable-as it is described in Section 6.5-and is
relatively short (200-300 μs). The shockwave detection core
is continuously looking for two rising edges within a given
interval. The state machine of the algorithm is shown in
Figure 5(b). The input parameters are the minimum 
steepness of the edges (D, E), and the bounds on the length of
the wave (Lmin, Lmax). The only feature calculated by the
core is the length of the observed shockwave signal.
In contrast to shockwaves, the muzzle blast signatures are
characterized by a long initial period (1-5 ms) where the first
half period is significantly shorter than the second half [4].
Due to the physical limitations of the analog circuitry 
described at the beginning of this section, irregular oscillations
and glitches might show up within this longer time window
as they can be clearly seen in Figure 6(a). Therefore, the real
challenge for the matching detection core is to identify the
first and second half periods properly. The state machine
(Figure 6(b)) does not work on the raw samples directly
but is fed by a zero crossing (ZC) encoder. After the initial
triggering, the detector attempts to collect those ZC 
segments which belong to the first period (positive amplitude)
while discarding too short (in our terminology: garbage)
segments-effectively implementing a rudimentary low-pass
filter in the ZC domain. After it encounters a sufficiently
long negative segment, it runs the same collection logic for
the second half period. If too much garbage is discarded
in the collection phases, the core resets itself to prevent the
(false) detection of the halves from completely different 
periods separated by rapid oscillation or noise. Finally, if the
constraints on the total length and on the length ratio hold,
the core generates a detection event along with the actual
length, amplitude and energy of the period calculated 
concurrently. The initial triggering mechanism is based on two
amplitude thresholds: one static (but configurable) 
amplitude level and a dynamically computed one. The latter one
is essential to adapt the sensor to different ambient noise
environments and to temporarily suspend the muzzle blast
detector after a shock wave event (oscillations in the analog
section or reverberations in the sensor enclosure might 
otherwise trigger false muzzle blast detections). The dynamic
noise level is estimated by a single pole recursive low-pass
filter (cutoff @ 0.5 kHz ) on the FPGA.
117
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Time (µs)
Amplitude
Muzzle blast (M16)
1
2
3
4 5
len2
+
len1
(a)
IDLE
1
SECOND ZC
3
PENDING ZC
4
FIRST ZC
2
FOUND
5
amplitude
threshold
long
positive ZC
long
negative ZC
valid
full period
max
garbage
wrong sign
garbage
collect
first period
garbage
collect
first period
garbage
(b)
Figure 6: Muzzle blast signature (a) produced by an
M16 assault rifle and the corresponding detection
logic (b).
The detection cores were originally implemented in Java
and evaluated on pre-recorded signals because of much faster
test runs and more convenient debugging facilities. Later
on, they were ported to VHDL and synthesized using the
Xilinx ISE tool suite. The functional equivalence between
the two implementations were tested by VHDL test benches
and Python scripts which provided an automated way to
exercise the detection cores on the same set of pre-recorded
signals and to compare the results.
6. SENSOR FUSION
The sensor fusion algorithm receives detection messages
from the sensor network and estimates the bullet trajectory,
the shooter position, the caliber of the projectile and the
type of the weapon. The algorithm consists of well separated
computational tasks outlined below:
1. Compute muzzle blast and shockwave directions of 
arrivals for each individual sensor (see 6.1).
2. Compute range estimates. This algorithm can 
analytically fuse a pair of shockwave and muzzle blast AoA
estimates. (see 6.2).
3. Compute a single trajectory from all shockwave 
measurements (see 6.3).
4. If trajectory available compute range (see 6.4).
else compute shooter position first and then trajectory
based on it. (see 6.4)
5. If trajectory available compute caliber (see 6.5).
6. If caliber available compute weapon type (see 6.6).
We describe each step in the following sections in detail.
6.1 Direction of arrival
The first step of the sensor fusion is to calculate the 
muzzle blast and shockwave AoA-s for each sensorboard. Each
sensorboard has four microphones that measure the ToA-s.
Since the microphone spacing is orders of magnitude smaller
than the distance to the sound source, we can approximate
the approaching sound wave front with a plane (far field
assumption).
Let us formalize the problem for 3 microphones first. Let
P1, P2 and P3 be the position of the microphones ordered by
time of arrival t1 < t2 < t3. First we apply a simple 
geometry validation step. The measured time difference between
two microphones cannot be larger than the sound 
propagation time between the two microphones:
|ti − tj| <= |Pi − Pj |/c + ε
Where c is the speed of sound and ε is the maximum
measurement error. If this condition does not hold, the 
corresponding detections are discarded. Let v(x, y, z) be the
normal vector of the unknown direction of arrival. We also
use r1(x1, y1, z1), the vector from P1 to P2 and r2(x2, y2, z2),
the vector from P1 to P3. Let"s consider the projection of
the direction of the motion of the wave front (v) to r1 
divided by the speed of sound (c). This gives us how long it
takes the wave front to propagate form P1 to P2:
vr1 = c(t2 − t1)
The same relationship holds for r2 and v:
vr2 = c(t3 − t1)
We also know that v is a normal vector:
vv = 1
Moving from vectors to coordinates using the dot product
definition leads to a quadratic system:
xx1 + yy1 + zz1 = c(t2 − t1)
xx2 + yy2 + zz2 = c(t3 − t1)
x2
+ y2
+ z2
= 1
We omit the solution steps here, as they are 
straightforward, but long. There are two solutions (if the source is on
the P1P2P3 plane the two solutions coincide). We use the
fourth microphone"s measurement-if there is one-to 
eliminate one of them. Otherwise, both solutions are considered
for further processing.
6.2 Muzzle-shock fusion
u
v
11,tP
22,tP
tP,
2P′
Bullet trajectory
Figure 7: Section plane of a shot (at P) and two
sensors (at P1 and at P2). One sensor detects the
muzzle blast"s, the other the shockwave"s time and
direction of arrivals.
Consider the situation in Figure 7. A shot was fired from
P at time t. Both P and t are unknown. We have one muzzle
blast and one shockwave detections by two different sensors
118
with AoA and hence, ToA information available. The 
muzzle blast detection is at position P1 with time t1 and AoA
u. The shockwave detection is at P2 with time t2 and AoA
v. u and v are normal vectors. It is shown below that these
measurements are sufficient to compute the position of the
shooter (P).
Let P2 be the point on the extended shockwave cone 
surface where PP2 is perpendicular to the surface. Note that
PP2 is parallel with v. Since P2 is on the cone surface which
hits P2, a sensor at P2 would detect the same shockwave
time of arrival (t2). The cone surface travels at the speed of
sound (c), so we can express P using P2:
P = P2 + cv(t2 − t).
P can also be expressed from P1:
P = P1 + cu(t1 − t)
yielding
P1 + cu(t1 − t) = P2 + cv(t2 − t).
P2P2 is perpendicular to v:
(P2 − P2)v = 0
yielding
(P1 + cu(t1 − t) − cv(t2 − t) − P2)v = 0
containing only one unknown t. One obtains:
t =
(P1−P2)v
c
+uvt1−t2
uv−1
.
From here we can calculate the shoter position P.
Let"s consider the special single sensor case where P1 = P2
(one sensor detects both shockwave and muzzle blast AoA).
In this case:
t = uvt1−t2
uv−1
.
Since u and v are not used separately only uv, the absolute
orientation of the sensor can be arbitrary, we still get t which
gives us the range.
Here we assumed that the shockwave is a cone which is
only true for constant projectile speeds. In reality, the angle
of the cone slowly grows; the surface resembles one half of
an American football. The decelerating bullet results in a
smaller time difference between the shockwave and the 
muzzle blast detections because the shockwave generation slows
down with the bullet. A smaller time difference results in a
smaller range, so the above formula underestimates the true
range. However, it can still be used with a proper 
deceleration correction function. We leave this for future work.
6.3 Trajectory estimation
Danicki showed that the bullet trajectory and speed can
be computed analytically from two independent shockwave
measurements where both ToA and AoA are measured [2].
The method gets more sensitive to measurement errors as
the two shockwave directions get closer to each other. In
the special case when both directions are the same, the 
trajectory cannot be computed. In a real world application,
the sensors are typically deployed on a plane approximately.
In this case, all sensors located on one side of the 
trajectory measure almost the same shockwave AoA. To avoid
this error sensitivity problem, we consider shockwave 
measurement pairs only if the direction of arrival difference is
larger than a certain threshold.
We have multiple sensors and one sensor can report two
different directions (when only three microphones detect the
shockwave). Hence, we typically have several trajectory 
candidates, i.e. one for each AoA pair over the threshold. We
applied an outlier filtering and averaging method to fuse 
together the shockwave direction and time information and
come up with a single trajectory. Assume that we have
N individual shockwave AoA measurements. Let"s take all
possible unordered pairs where the direction difference is
above the mentioned threshold and compute the trajectory
for each. This gives us at most N(N−1)
2
trajectories. A 
trajectory is represented by one point pi and the normal vector
vi (where i is the trajectory index). We define the distance
of two trajectories as the dot product of their normal 
vectors:
D(i, j) = vivj
For each trajectory a neighbor set is defined:
N(i) := {j|D(i, j) < R}
where R is a radius parameter. The largest neighbor set is
considered to be the core set C, all other trajectories are
outliers. The core set can be found in O(N2
) time. The
trajectories in the core set are then averaged to get the final
trajectory.
It can happen that we cannot form any sensor pairs 
because of the direction difference threshold. It means all 
sensors are on the same side of the trajectory. In this case,
we first compute the shooter position (described in the next
section) that fixes p making v the only unknown. To find
v in this case, we use a simple high resolution grid search
and minimize an error function based on the shockwave 
directions.
We have made experiments to utilize the measured 
shockwave length in the trajectory estimation. There are some
promising results, but it needs further research.
6.4 Shooter position estimation
The shooter position estimation algorithm aggregates the
following heterogenous information generated by earlier 
computational steps:
1. trajectory,
2. muzzle blast ToA at a sensor,
3. muzzle blast AoA at a sensor, which is effectively a
bearing estimate to the shooter, and
4. range estimate at a sensor (when both shockwave and
muzzle blast AoA are available).
Some sensors report only ToA, some has bearing 
estimate(s) also and some has range estimate(s) as well, 
depending on the number of successful muzzle blast and shockwave
detections by the sensor. For an example, refer to Figure 8.
Note that a sensor may have two different bearing and range
estimates. 3 detections gives two possible AoA-s for 
muzzle blast (i.e. bearing) and/or shockwave. Furthermore, the
combination of two different muzzle blast and shockwave
AoA-s may result in two different ranges.
119
11111 ,,,, rrvvt ′′
22 ,vt
333 ,, vvt ′
4t
5t
6t
bullet trajectory
shooter position
Figure 8: Example of heterogenous input data for
the shooter position estimation algorithm. All 
sensors have ToA measurements (t1, t2, t3, t4, t5), one 
sensor has a single bearing estimate (v2), one sensor has
two possible bearings (v3, v3) and one sensor has two
bearing and two range estimates (v1, v1,r1, r1)
In a multipath environment, these detections will not only
contain gaussian noise, but also possibly large errors due to
echoes. It has been showed in our earlier work that a similar
problem can be solved efficiently with an interval arithmetic
based bisection search algorithm [8]. The basic idea is to
define a discrete consistency function over the area of 
interest and subdivide the space into 3D boxes. For any given
3D box, this function gives the number of measurements
supporting the hypothesis that the shooter was within that
box. The search starts with a box large enough to contain
the whole area of interest, then zooms in by dividing and
evaluating boxes. The box with the maximum consistency
is divided until the desired precision is reached. 
Backtracking is possible to avoid getting stuck in a local maximum.
This approach has been shown to be fast enough for 
online processing. Note, however, that when the trajectory
has already been calculated in previous steps, the search
needs to be done only on the trajectory making it orders of
magnitude faster.
Next let us describe how the consistency function is 
calculated in detail. Consider B, a three dimensional box, we
would like to compute the consistency value of. First we
consider only the ToA information. If one sensor has 
multiple ToA detections, we use the average of those times, so
one sensor supplies at most one ToA estimate. For each
ToA, we can calculate the corresponding time of the shot,
since the origin is assumed to be in box B. Since it is a box
and not a single point, this gives us an interval for the shot
time. The maximum number of overlapping time intervals
gives us the value of the consistency function for B. For a
detailed description of the consistency function and search
algorithm, refer to [8].
Here we extend the approach the following way. We 
modify the consistency function based on the bearing and range
data from individual sensors. A bearing estimate supports
B if the line segment starting from the sensor with the 
measured direction intersects the B box. A range supports B,
if the sphere with the radius of the range and origin of the
sensor intersects B. Instead of simply checking whether the
position specified by the corresponding bearing-range pairs
falls within B, this eliminates the sensor"s possible 
orientation error. The value of the consistency function is 
incremented by one for each bearing and range estimate that is
consistent with B.
6.5 Caliber estimation
The shockwave signal characteristics has been studied 
before by Whitham [20]. He showed that the shockwave period
T is related to the projectile diameter d, the length l, the
perpendicular miss distance b from the bullet trajectory to
the sensor, the Mach number M and the speed of sound c.
T = 1.82Mb1/4
c(M2−1)3/8
d
l1/4 ≈ 1.82d
c
(Mb
l
)1/4
0
100
200
300
400
500
600
0 10 20 30
miss distance (m)shockwavelength(microseconds)
.50 cal
5.56 mm
7.62 mm
Figure 9: Shockwave length and miss distance 
relationship. Each data point represents one 
sensorboard after an aggregation of the individual 
measurements of the four acoustic channels. Three
different caliber projectiles have been tested (196
shots, 10 sensors).
To illustrate the relationship between miss distance and
shockwave length, here we use all 196 shots with three 
different caliber projectiles fired during the evaluation. (During
the evaluation we used data obtained previously using a few
practice shots per weapon.) 10 sensors (4 microphones by
sensor) measured the shockwave length. For each sensor,
we considered the shockwave length estimation valid if at
least three out of four microphones agreed on a value with
at most 5 microsecond variance. This filtering leads to a
86% report rate per sensor and gets rid of large 
measurement errors. The experimental data is shown in Figure 9.
Whitham"s formula suggests that the shockwave length for a
given caliber can be approximated with a power function of
the miss distance (with a 1/4 exponent). Best fit functions
on our data are:
.50 cal: T = 237.75b0.2059
7.62 mm: T = 178.11b0.1996
5.56 mm: T = 144.39b0.1757
To evaluate a shot, we take the caliber whose 
approximation function results in the smallest RMS error of the
filtered sensor readings. This method has less than 1% 
caliber estimation error when an accurate trajectory estimate
is available. In other words, caliber estimation only works
if enough shockwave detections are made by the system to
compute a trajectory.
120
6.6 Weapon estimation
We analyzed all measured signal characteristics to find
weapon specific information. Unfortunately, we concluded
that the observed muzzle blast signature is not characteristic
enough of the weapon for classification purposes. The 
reflections of the high energy muzzle blast from the environment
have much higher impact on the muzzle blast signal shape
than the weapon itself. Shooting the same weapon from 
different places caused larger differences on the recorded signal
than shooting different weapons from the same place.
0
100
200
300
400
500
600
700
800
900
0 100 200 300 400
range (m)
speed(m/s)
AK-47
M240
Figure 10: AK47 and M240 bullet deceleration 
measurements. Both weapons have the same caliber.
Data is approximated using simple linear regression.
0
100
200
300
400
500
600
700
800
900
1000
0 50 100 150 200 250 300 350
range (m)
speed(m/s)
M16
M249
M4
Figure 11: M16, M249 and M4 bullet deceleration
measurements. All weapons have the same caliber.
Data is approximated using simple linear regression.
However, the measured speed of the projectile and its 
caliber showed good correlation with the weapon type. This
is because for a given weapon type and ammunition pair,
the muzzle velocity is nearly constant. In Figures 10 and
11 we can see the relationship between the range and the
measured bullet speed for different calibers and weapons.
In the supersonic speed range, the bullet deceleration can
be approximated with a linear function. In case of the
7.62 mm caliber, the two tested weapons (AK47, M240) can
be clearly separated (Figure 10). Unfortunately, this is not
necessarily true for the 5.56 mm caliber. The M16 with its
higher muzzle speed can still be well classified, but the M4
and M249 weapons seem practically undistinguishable 
(Figure 11). However, this may be partially due to the limited
number of practice shots we were able to take before the
actual testing began. More training data may reveal better
separation between the two weapons since their published
muzzle velocities do differ somewhat.
The system carries out weapon classification in the 
following manner. Once the trajectory is known, the speed can be
calculated for each sensor based on the shockwave geometry.
To evaluate a shot, we choose the weapon type whose 
deceleration function results in the smallest RMS error of the
estimated range-speed pairs for the estimated caliber class.
7. RESULTS
An independent evaluation of the system was carried out
by a team from NIST at the US Army Aberdeen Test Center
in April 2006 [19]. The experiment was setup on a shooting
range with mock-up wooden buildings and walls for 
supporting elevated shooter positions and generating multipath
effects. Figure 12 shows the user interface with an aerial
photograph of the site. 10 sensor nodes were deployed on
surveyed points in an approximately 30×30 m area. There
were five fixed targets behind the sensor network. Several
firing positions were located at each of the firing lines at
50, 100, 200 and 300 meters. These positions were known
to the evaluators, but not to the operators of the system.
Six different weapons were utilized: AK47 and M240 
firing 7.62 mm projectiles, M16, M4 and M249 with 5.56mm
ammunition and the .50 caliber M107.
Note that the sensors remained static during the test. The
primary reason for this is that nobody is allowed downrange
during live fire tests. Utilizing some kind of remote 
control platform would have been too involved for the limited
time the range was available for the test. The experiment,
therefore, did not test the mobility aspect of the system.
During the one day test, there were 196 shots fired. The
results are summarized in Table 1. The system detected all
shots successfully. Since a ballistic shockwave is a unique
acoustic phenomenon, it makes the detection very robust.
There were no false positives for shockwaves, but there were
a handful of false muzzle blast detections due to parallel
tests of artillery at a nearby range.
Shooter Local- Caliber Trajectory Trajectory Distance No.
Range ization Accu- Azimuth Distance Error of
(m) Rate racy Error (deg) Error (m) (m) Shots
50 93% 100% 0.86 0.91 2.2 54
100 100% 100% 0.66 1.34 8.7 54
200 96% 100% 0.74 2.71 32.8 54
300 97% 97% 1.49 6.29 70.6 34
All 96% 99.5% 0.88 2.47 23.0 196
Table 1: Summary of results fusing all available 
sensor observations. All shots were successfully 
detected, so the detection rate is omitted. Localization
rate means the percentage of shots that the sensor
fusion was able to estimate the trajectory of. The
caliber accuracy rate is relative to the shots localized
and not all the shots because caliber estimation 
requires the trajectory. The trajectory error is broken
down to azimuth in degrees and the actual distance
of the shooter from the trajectory. The distance 
error shows the distance between the real shooter 
position and the estimated shooter position. As such,
it includes the error caused by both the trajectory
and that of the range estimation. Note that the 
traditional bearing and range measures are not good
ones for a distributed system such as ours because
of the lack of a single reference point.
121
Figure 12: The user interface of the system 
showing the experimental setup. The 10 sensor nodes
are labeled by their ID and marked by dark circles.
The targets are black squares marked T-1 through
T-5. The long white arrows point to the shooter 
position estimated by each sensor. Where it is 
missing, the corresponding sensor did not have enough
detections to measure the AoA of either the 
muzzle blast, the shockwave or both. The thick black
line and large circle indicate the estimated 
trajectory and the shooter position as estimated by fusing
all available detections from the network. This shot
from the 100-meter line at target T-3 was localized
almost perfectly by the sensor network. The caliber
and weapon were also identified correctly. 6 out of
10 nodes were able to estimate the location alone.
Their bearing accuracy is within a degree, while the
range is off by less than 10% in the worst case.
The localization rate characterizes the system"s ability to
successfully estimate the trajectory of shots. Since caliber
estimation and weapon classification relies on the trajectory,
non-localized shots are not classified either. There were 7
shots out of 196 that were not localized. The reason for
missed shots is the trajectory ambiguity problem that occurs
when the projectile passes on one side of all the sensors. In
this case, two significantly different trajectories can generate
the same set of observations (see [8] and also Section 6.3).
Instead of estimating which one is more likely or displaying
both possibilities, we decided not to provide a trajectory at
all. It is better not to give an answer other than a shot
alarm than misleading the soldier.
Localization accuracy is broken down to trajectory 
accuracy and range estimation precision. The angle of the 
estimated trajectory was better than 1 degree except for the
300 m range. Since the range should not affect trajectory
estimation as long as the projectile passes over the network,
we suspect that the slightly worse angle precision for 300 m
is due to the hurried shots we witnessed the soldiers took
near the end of the day. This is also indicated by another
datapoint: the estimated trajectory distance from the 
actual targets has an average error of 1.3 m for 300 m shots,
0.75 m for 200 m shots and 0.6 m for all but 300 m shots.
As the distance between the targets and the sensor network
was fixed, this number should not show a 2× improvement
just because the shooter is closer.
Since the angle of the trajectory itself does not 
characterize the overall error-there can be a translation 
alsoTable 1 also gives the distance of the shooter from the 
estimated trajectory. These indicate an error which is about
1-2% of the range. To put this into perspective, a 
trajectory estimate for a 100 m shot will very likely go through or
very near the window the shooter is located at. Again, we
believe that the disproportionally larger errors at 300 m are
due to human errors in aiming. As the ground truth was
obtained by knowing the precise location of the shooter and
the target, any inaccuracy in the actual trajectory directly
adds to the perceived error of the system.
We call the estimation of the shooter"s position on the
calculated trajectory range estimation due to the lack of a
better term. The range estimates are better than 5% 
accurate from 50 m and 10% for 100 m. However, this goes
to 20% or worse for longer distances. We did not have a
facility to test system before the evaluation for ranges 
beyond 100 m. During the evaluation, we ran into the 
problem of mistaking shockwave echoes for muzzle blasts. These
echoes reached the sensors before the real muzzle blast for
long range shots only, since the projectile travels 2-3× faster
than the speed of sound, so the time between the shockwave
(and its possible echo from nearby objects) and the muzzle
blast increases with increasing ranges. This resulted in 
underestimating the range, since the system measured shorter
times than the real ones. Since the evaluation we finetuned
the muzzle blast detection algorithm to avoid this problem.
Distance M16 AK47 M240 M107 M4 M249 M4-M249
50m 100% 100% 100% 100% 11% 25% 94%
100m 100% 100% 100% 100% 22% 33% 100%
200m 100% 100% 100% 100% 50% 22% 100%
300m 67% 100% 83% 100% 33% 0% 57%
All 96% 100% 97% 100% 23% 23% 93%
Table 2: Weapon classification results. The 
percentages are relative to the number of shots localized and
not all shots, as the classification algorithm needs to
know the trajectory and the range. Note that the
difference is small; there were 189 shots localized
out of the total 196.
The caliber and weapon estimation accuracy rates are
based on the 189 shots that were successfully localized. Note
that there was a single shot that was falsely classified by the
caliber estimator. The 73% overall weapon classification 
accuracy does not seem impressive. But if we break it down
to the six different weapons tested, the picture changes 
dramatically as shown in Table 2. For four of the weapons
(AK14, M16, M240 and M107), the classification rate is 
almost 100%. There were only two shots out of approximately
140 that were missed. The M4 and M249 proved to be too
similar and they were mistaken for each other most of the
time. One possible explanation is that we had only a limited
number of test shots taken with these weapons right before
the evaluation and used the wrong deceleration 
approximation function. Either this or a similar mistake was made
122
since if we simply used the opposite of the system"s answer
where one of these weapons were indicated, the accuracy
would have improved 3x. If we consider these two weapons
a single weapon class, then the classification accuracy for it
becomes 93%.
Note that the AK47 and M240 have the same caliber
(7.62 mm), just as the M16, M4 and M249 do (5.56 mm).
That is, the system is able to differentiate between weapons
of the same caliber. We are not aware of any system that
classifies weapons this accurately.
7.1 Single sensor performance
As was shown previously, a single sensor alone is able
to localize the shooter if it can determine both the muzzle
blast and the shockwave AoA, that is, it needs to measure
the ToA of both on at least three acoustic channels. While
shockwave detection is independent of the range-unless the
projectile becomes subsonic-, the likelihood of muzzle blast
detection beyond 150 meters is not enough for consistently
getting at least three per sensor node for AoA estimation.
Hence, we only evaluate the single sensor performance for
the 104 shots that were taken from 50 and 100 m. Note that
we use the same test data as in the previous section, but we
evaluate individually for each sensor.
Table 3 summarizes the results broken down by the ten
sensors utilized. Since this is now not a distributed system,
the results are given relative to the position of the given 
sensor, that is, a bearing and range estimate is provided. Note
that many of the common error sources of the networked
system do not play a role here. Time synchronization is
not applicable. The sensor"s absolute location is irrelevant
(just as the relative location of multiple sensors). The 
sensor"s orientation is still important though. There are several
disadvantages of the single sensor case compared to the 
networked system: there is no redundancy to compensate for
other errors and to perform outlier rejection, the 
localization rate is markedly lower, and a single sensor alone is not
able to estimate the caliber or classify the weapon.
Sensor id 1 2 3 5 7 8 9 10 11 12
Loc. rate 44% 37% 53% 52% 19% 63% 51% 31% 23% 44%
Bearing (deg) 0.80 1.25 0.60 0.85 1.02 0.92 0.73 0.71 1.28 1.44
Range (m) 3.2 6.1 4.4 4.7 4.6 4.6 4.1 5.2 4.8 8.2
Table 3: Single sensor accuracy for 108 shots fired
from 50 and 100 meters. Localization rate refers to
the percentage of shots the given sensor alone was
able to localize. The bearing and range values are
average errors. They characterize the accuracy of
localization from the given sensor"s perspective.
The data indicates that the performance of the sensors
varied significantly especially considering the localization
rate. One factor has to be the location of the given 
sensor including how far it was from the firing lines and how
obstructed its view was. Also, the sensors were hand-built
prototypes utilizing nowhere near production quality 
packaging/mounting. In light of these factors, the overall 
average bearing error of 0.9 degrees and range error of 5 m
with a microphone spacing of less than 10 cm are excellent.
We believe that professional manufacturing and better 
microphones could easily achieve better performance than the
best sensor in our experiment (>60% localization rate and
3 m range error).
Interestingly, the largest error in range was a huge 90 m
clearly due to some erroneous detection, yet the largest 
bearing error was less than 12 degrees which is still a good 
indication for the soldier where to look.
The overall localization rate over all single sensors was
42%, while for 50 m shots only, this jumped to 61%. Note
that the firing range was prepared to simulate an urban
area to some extent: there were a few single- and two-storey
wooden structures built both in and around the sensor 
deployment area and the firing lines. Hence, not all sensors had
line-of-sight to all shooting positions. We estimate that 10%
of the sensors had obstructed view to the shooter on 
average. Hence, we can claim that a given sensor had about 50%
chance of localizing a shot within 130 m. (Since the sensor
deployment area was 30 m deep, 100 m shots correspond to
actual distances between 100 and 130 m.) Again, we 
emphasize that localization needs at least three muzzle blast and
three shockwave detections out of a possible four for each per
sensor. The detection rate for single sensors-corresponding
to at least one shockwave detection per sensor-was 
practically 100%.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 1 2 3 4 5 6 7 8 9 10
number of sensors
percentageofshots
Figure 13: Histogram showing what fraction of the
104 shots taken from 50 and 100 meters were 
localized by at most how many individual sensors alone.
13% of the shots were missed by every single 
sensor, i.e., none of them had both muzzle blast and
shockwave AoA detections. Note that almost all
of these shots were still accurately localized by the
networked system, i.e. the sensor fusion using all
available observations in the sensor network.
It would be misleading to interpret these results as the
system missing half the shots. As soldiers never work alone
and the sensor node is relatively cheap to afford having 
every soldier equipped with one, we also need to look at the
overall detection rates for every shot. Figure 13 shows the
histogram of the percentage of shots vs. the number of 
individual sensors that localized it. 13% of shots were not
localized by any sensor alone, but 87% was localized by at
least one sensor out of ten.
7.2 Error sources
In this section, we analyze the most significant sources of
error that affect the performance of the networked shooter
localization and weapon classification system. In order to
correlate the distributed observations of the acoustic events,
the nodes need to have a common time and space reference.
Hence, errors in the time synchronization, node localization
and node orientation all degrade the overall accuracy of the
system.
123
Our time synchronization approach yields errors 
significantly less than 100 microseconds. As the sound travels
about 3 cm in that time, time synchronization errors have a
negligible effect on the system.
On the other hand, node location and orientation can have
a direct effect on the overall system performance. Notice
that to analyze this, we do not have to resort to 
simulation, instead we can utilize the real test data gathered at
Aberdeen. But instead of using the real sensor locations
known very accurately and the measured and calibrated 
almost perfect node orientations, we can add error terms to
them and run the sensor fusion. This exactly replicates how
the system would have performed during the test using the
imprecisely known locations and orientations.
Another aspect of the system performance that can be
evaluated this way is the effect of the number of available
sensors. Instead of using all ten sensors in the data fusion,
we can pick any subset of the nodes to see how the accuracy
degrades as we decrease the number of nodes.
The following experiment was carried out. The number
of sensors were varied from 2 to 10 in increments of 2. Each
run picked the sensors randomly using a uniform 
distribution. At each run each node was randomly moved to a
new location within a circle around its true position with
a radius determined by a zero-mean Gaussian distribution.
Finally, the node orientations were perturbed using a 
zeromean Gaussian distribution. Each combination of 
parameters were generated 100 times and utilized for all 196 shots.
The results are summarized in Figure 14. There is one 3D
barchart for each of the experiment sets with the given fixed
number of sensors. The x-axis shows the node location error,
that is, the standard deviation of the corresponding 
Gaussian distribution that was varied between 0 and 6 meters.
The y-axis shows the standard deviation of the node 
orientation error that was varied between 0 and 6 degrees. The
z-axis is the resulting trajectory azimuth error. Note that
the elevation angles showed somewhat larger errors than the
azimuth. Since all the sensors were in approximately a 
horizontal plane and only a few shooter positions were out of the
same plane and only by 2 m or so, the test was not sufficient
to evaluate this aspect of the system.
There are many interesting observation one can make by
analyzing these charts. Node location errors in this range
have a small effect on accuracy. Node orientation errors, on
the other hand, noticeably degrade the performance. Still
the largest errors in this experiment of 3.5 degrees for 6
sensors and 5 degrees for 2 sensors are still very good.
Note that as the location and orientation errors increase
and the number of sensors decrease, the most significantly
affected performance metric is the localization rate. See
Table 4 for a summary. Successful localization goes down
from almost 100% to 50% when we go from 10 sensors to
2 even without additional errors. This is primarily caused
by geometry: for a successful localization, the bullet needs
to pass over the sensor network, that is, at least one sensor
should be on the side of the trajectory other than the rest
of the nodes. (This is a simplification for illustrative 
purposes. If all the sensors and the trajectory are not coplanar,
localization may be successful even if the projectile passes
on one side of the network. See Section 6.3.) As the 
numbers of sensors decreased in the experiment by randomly
selecting a subset, the probability of trajectories abiding by
this rule decreased. This also means that even if there are
0
2
4
6
0
2
4
6
0
1
2
3
4
5
6
azimutherror(degree)
position error (m)
orientation error
(degree)
2 sensors
0
2
4
6
0
2
4
6
0
1
2
3
4
5
6
azimutherror(degree)
position error (m)
orientation error
(degree)
4 sensors
0
2
4
6
0
2
4
6
0
1
2
3
4
5
6
azimutherror(degree)
position error (m)
orientation error
(degree)
6 sensors
0
2
4
6
0
2
4
6
0
1
2
3
4
5
6
azimutherror(degree)
position error (m)
orientation error
(degree)
8 sensors
Figure 14: The effect of node localization and 
orientation errors on azimuth accuracy with 2, 4, 6 and
8 nodes. Note that the chart for 10 nodes is almost
identical for the 8-node case, hence, it is omitted.
124
many sensors (i.e. soldiers), but all of them are right next to
each other, the localization rate will suffer. However, when
the sensor fusion does provide a result, it is still accurate
even with few available sensors and relatively large 
individual errors. A very few consistent observation lead to good
accuracy as the inconsistent ones are discarded by the 
algorithm. This is also supported by the observation that for
the cases with the higher number of sensors (8 or 10), the
localization rate is hardly affected by even large errors.
Errors/Sensors 2 4 6 8 10
0 m, 0 deg 54% 87% 94% 95% 96%
2 m, 2 deg 53% 80% 91% 96% 96%
6 m, 0 deg 43% 79% 88% 94% 94%
0 m, 6 deg 44% 78% 90% 93% 94%
6 m, 6 deg 41% 73% 85% 89% 92%
Table 4: Localization rate as a function of the 
number of sensors used, the sensor node location and
orientation errors.
One of the most significant observations on Figure 14 and
Table 4 is that there is hardly any difference in the data for
6, 8 and 10 sensors. This means that there is little advantage
of adding more nodes beyond 6 sensors as far as the accuracy
is concerned.
The speed of sound depends on the ambient temperature.
The current prototype considers it constant that is typically
set before a test. It would be straightforward to employ
a temperature sensor to update the value of the speed of
sound periodically during operation. Note also that wind
may adversely affect the accuracy of the system. The sensor
fusion, however, could incorporate wind speed into its 
calculations. It would be more complicated than temperature
compensation, but could be done.
Other practical issues also need to be looked at before a
real world deployment. Silencers reduce the muzzle blast
energy and hence, the effective range the system can 
detect it at. However, silencers do not effect the shockwave
and the system would still detect the trajectory and caliber
accurately. The range and weapon type could not be 
estimated without muzzle blast detections. Subsonic weapons
do not produce a shockwave. However, this is not of great
significance, since they have shorter range, lower accuracy
and much less lethality. Hence, their use is not widespread
and they pose less danger in any case.
Another issue is the type of ammunition used. Irregular
armies may use substandard, even hand manufactured 
bullets. This effects the muzzle velocity of the weapon. For
weapon classification to work accurately, the system would
need to be calibrated with the typical ammunition used by
the given adversary.
8. RELATED WORK
Acoustic detection and recognition has been under 
research since the early fifties. The area has a close 
relevance to the topic of supersonic flow mechanics [20]. Fansler
analyzed the complex near-field pressure waves that occur
within a foot of the muzzle blast. Fansler"s work gives a
good idea of the ideal muzzle blast pressure wave without
contamination from echoes or propagation effects [4]. 
Experiments with greater distances from the muzzle were 
conducted by Stoughton [18]. The measurements of the ballistic
shockwaves using calibrated pressure transducers at known
locations, measured bullet speeds, and miss distances of 
355 meters for 5.56 mm and 7.62 mm projectiles were made.
Results indicate that ground interaction becomes a problem
for miss distances of 30 meters or larger.
Another area of research is the signal processing of gunfire
acoustics. The focus is on the robust detection and length
estimation of small caliber acoustic shockwaves and 
muzzle blasts. Possible techniques for classifying signals as 
either shockwaves or muzzle blasts includes short-time Fourier
Transform (STFT), the Smoothed Pseudo Wigner-Ville 
distribution (SPWVD), and a discrete wavelet transformation
(DWT). Joint time-frequency (JTF) spectrograms are used
to analyze the typical separation of the shockwave and 
muzzle blast transients in both time and frequency. Mays 
concludes that the DWT is the best method for classifying 
signals as either shockwaves or muzzle blasts because it works
well and is less expensive to compute than the SPWVD [10].
The edges of the shockwave are typically well defined and
the shockwave length is directly related to the bullet 
characteristics. A paper by Sadler [14] compares two shockwave
edge detection methods: a simple gradient-based detector,
and a multi-scale wavelet detector. It also demonstrates how
the length of the shockwave, as determined by the edge 
detectors, can be used along with Whithams equations [20] to
estimate the caliber of a projectile. Note that the available
computational performance on the sensor nodes, the limited
wireless bandwidth and real-time requirements render these
approaches infeasible on our platform.
A related topic is the research and development of 
experimental and prototype shooter location systems. Researchers
at BBN have developed the Bullet Ears system [3] which has
the capability to be installed in a fixed position or worn by
soldiers. The fixed system has tetrahedron shaped 
microphone arrays with 1.5 meter spacing. The overall system
consists of two to three of these arrays spaced 20 to 100
meters from each other. The soldier-worn system has 12
microphones as well as a GPS antenna and orientation 
sensors mounted on a helmet. There is a low speed RF 
connection from the helmet to the processing body. An extensive
test has been conducted to measure the performance of both
type of systems. The fixed systems performance was one 
order of magnitude better in the angle calculations while their
range performance where matched. The angle accuracy of
the fixed system was dominantly less than one degree while
it was around five degrees for the helmet mounted one. The
range accuracy was around 5 percent for both of the 
systems. The problem with this and similar centralized 
systems is the need of the one or handful of microphone arrays
to be in line-of-sight of the shooter. A sensor networked
based solution has the advantage of widely distributed 
sensing for better coverage, multipath effect compensation and
multiple simultaneous shot resolution [8]. This is especially
important for operation in acoustically reverberant urban
areas. Note that BBN"s current vehicle-mounted system
called BOOMERANG, a modified version of Bullet Ears,
is currently used in Iraq [1].
The company ShotSpotter specializes in law enforcement
systems that report the location of gunfire to police within
seconds. The goal of the system is significantly different
than that of military systems. Shotspotter reports 25 m
typical accuracy which is more than enough for police to
125
respond. They are also manufacturing experimental soldier
wearable and UAV mounted systems for military use [16],
but no specifications or evaluation results are publicly 
available.
9. CONCLUSIONS
The main contribution of this work is twofold. First, the
performance of the overall distributed networked system is
excellent. Most noteworthy are the trajectory accuracy of
one degree, the correct caliber estimation rate of well over
90% and the close to 100% weapon classification rate for 4 of
the 6 weapons tested. The system proved to be very robust
when increasing the node location and orientation errors and
decreasing the number of available sensors all the way down
to a couple. The key factor behind this is the sensor fusion
algorithm"s ability to reject erroneous measurements. It is
also worth mentioning that the results presented here 
correspond to the first and only test of the system beyond 100 m
and with six different weapons. We believe that with the
lessons learned in the test, a consecutive field experiment
could have showed significantly improved results especially
in range estimation beyond 100 m and weapon classification
for the remaining two weapons that were mistaken for each
other the majority of the times during the test.
Second, the performance of the system when used in 
standalone mode, that is, when single sensors alone provided
localization, was also very good. While the overall 
localization rate of 42% per sensor for shots up to 130 m could be
improved, the bearing accuracy of less than a degree and
the average 5% range error are remarkable using the 
handmade prototypes of the low-cost nodes. Note that 87% of
the shots were successfully localized by at least one of the
ten sensors utilized in standalone mode.
We believe that the technology is mature enough that
a next revision of the system could be a commercial one.
However, important aspects of the system would still need
to be worked on. We have not addresses power 
management yet. A current node runs on 4 AA batteries for about
12 hours of continuous operation. A deployable version of
the sensor node would need to be asleep during normal 
operation and only wake up when an interesting event occurs.
An analog trigger circuit could solve this problem, however,
the system would miss the first shot. Instead, the acoustic
channels would need to be sampled and stored in a circular
buffer. The rest of the board could be turned off. When
a trigger wakes up the board, the acoustic data would be
immediately available. Experiments with a previous 
generation sensor board indicated that this could provide a 10x
increase in battery life. Other outstanding issues include
weatherproof packaging and ruggedization, as well as 
integration with current military infrastructure.
10. REFERENCES
[1] BBN technologies website. http://www.bbn.com.
[2] E. Danicki. Acoustic sniper localization. Archives of
Acoustics, 30(2):233-245, 2005.
[3] G. L. Duckworth et al. Fixed and wearable acoustic
counter-sniper systems for law enforcement. In E. M.
Carapezza and D. B. Law, editors, Proc. SPIE Vol.
3577, p. 210-230, pages 210-230, Jan. 1999.
[4] K. Fansler. Description of muzzle blast by modified
scaling models. Shock and Vibration, 5(1):1-12, 1998.
[5] D. Gay, P. Levis, R. von Behren, M. Welsh,
E. Brewer, and D. Culler. The nesC language: a
holistic approach to networked embedded systems.
Proceedings of Programming Language Design and
Implementation (PLDI), June 2003.
[6] J. Hill, R. Szewczyk, A. Woo, S. Hollar, D. Culler, and
K. Pister. System architecture directions for networked
sensors. in Proc. of ASPLOS 2000, Nov. 2000.
[7] B. Kus´y, G. Balogh, P. V¨olgyesi, J. Sallai, A. N´adas,
A. L´edeczi, M. Mar´oti, and L. Meertens. Node-density
independent localization. Information Processing in
Sensor Networks (IPSN 06) SPOTS Track, Apr. 2006.
[8] A. L´edeczi, A. N´adas, P. V¨olgyesi, G. Balogh,
B. Kus´y, J. Sallai, G. Pap, S. D´ora, K. Moln´ar,
M. Mar´oti, and G. Simon. Countersniper system for
urban warfare. ACM Transactions on Sensor
Networks, 1(1):153-177, Nov. 2005.
[9] M. Mar´oti. Directed flood-routing framework for
wireless sensor networks. In Proceedings of the 5th
ACM/IFIP/USENIX International Conference on
Middleware, pages 99-114, New York, NY, USA, 2004.
Springer-Verlag New York, Inc.
[10] B. Mays. Shockwave and muzzle blast classification
via joint time frequency and wavelet analysis.
Technical report, Army Research Lab Adelphi MD
20783-1197, Sept. 2001.
[11] TinyOS Hardware Platforms.
http://tinyos.net/scoop/special/hardware.
[12] Crossbow MICAz (MPR2400) Radio Module.
http://www.xbow.com/Products/productsdetails.
aspx?sid=101.
[13] PicoBlaze User Resources.
http://www.xilinx.com/ipcenter/processor_
central/picoblaze/picoblaze_user_resources.htm.
[14] B. M. Sadler, T. Pham, and L. C. Sadler. Optimal
and wavelet-based shock wave detection and
estimation. Acoustical Society of America Journal,
104:955-963, Aug. 1998.
[15] J. Sallai, B. Kus´y, A. L´edeczi, and P. Dutta. On the
scalability of routing-integrated time synchronization.
3rd European Workshop on Wireless Sensor Networks
(EWSN 2006), Feb. 2006.
[16] ShotSpotter website. http:
//www.shotspotter.com/products/military.html.
[17] G. Simon, M. Mar´oti, A. L´edeczi, G. Balogh, B. Kus´y,
A. N´adas, G. Pap, J. Sallai, and K. Frampton. Sensor
network-based countersniper system. In SenSys "04:
Proceedings of the 2nd international conference on
Embedded networked sensor systems, pages 1-12, New
York, NY, USA, 2004. ACM Press.
[18] R. Stoughton. Measurements of small-caliber ballistic
shock waves in air. Acoustical Society of America
Journal, 102:781-787, Aug. 1997.
[19] B. A. Weiss, C. Schlenoff, M. Shneier, and A. Virts.
Technology evaluations and performance metrics for
soldier-worn sensors for assist. In Performance Metrics
for Intelligent Systems Workshop, Aug. 2006.
[20] G. Whitham. Flow pattern of a supersonic projectile.
Communications on pure and applied mathematics,
5(3):301, 1952.
126
A Point-Distribution Index and Its Application to
Sensor-Grouping in Wireless Sensor Networks
Yangfan Zhou Haixuan Yang Michael R. Lyu Edith C.-H. Ngai
Department of Computer Science and Engineering
The Chinese University of Hong Kong
Hong Kong, China
{yfzhou, hxyang, lyu, chngai}@cse.cuhk.edu.hk
ABSTRACT
We propose ι, a novel index for evaluation of point-distribution.
ι is the minimum distance between each pair of points 
normalized by the average distance between each pair of points. We find
that a set of points that achieve a maximum value of ι result in
a honeycomb structure. We propose that ι can serve as a good
index to evaluate the distribution of the points, which can be 
employed in coverage-related problems in wireless sensor networks
(WSNs). To validate this idea, we formulate a general 
sensorgrouping problem for WSNs and provide a general sensing model.
We show that locally maximizing ι at sensor nodes is a good 
approach to solve this problem with an algorithm called 
Maximizingι Node-Deduction (MIND). Simulation results verify that MIND
outperforms a greedy algorithm that exploits sensor-redundancy we
design. This demonstrates a good application of employing ι in
coverage-related problems for WSNs.
Categories and Subject Descriptors
C.2.4 [Computer - Communication Networks]: Network 
Architecture and Design; C.3 [Special-Purpose and Application-Based
Systems]: Realtime and Embedded Systems
General Terms
Theory, Algorithms, Design, Verification, Performance
1. INTRODUCTION
A wireless sensor network (WSN) consists of a large number of
in-situ battery-powered sensor nodes. A WSN can collect the data
about physical phenomena of interest [1]. There are many 
potential applications of WSNs, including environmental monitoring and
surveillance, etc. [1][11].
In many application scenarios, WSNs are employed to conduct
surveillance tasks in adverse, or even worse, in hostile working 
environments. One major problem caused is that sensor nodes are
subjected to failures. Therefore, fault tolerance of a WSN is 
critical.
One way to achieve fault tolerance is that a WSN should contain
a large number of redundant nodes in order to tolerate node 
failures. It is vital to provide a mechanism that redundant nodes can be
working in sleeping mode (i.e., major power-consuming units such
as the transceiver of a redundant sensor node can be shut off) to
save energy, and thus to prolong the network lifetime. Redundancy
should be exploited as much as possible for the set of sensors that
are currently taking charge in the surveillance work of the network
area [6].
We find that the minimum distance between each pair of points
normalized by the average distance between each pair of points
serves as a good index to evaluate the distribution of the points. We
call this index, denoted by ι, the normalized minimum distance. If
points are moveable, we find that maximizing ι results in a 
honeycomb structure. The honeycomb structure poses that the coverage
efficiency is the best if each point represents a sensor node that
is providing surveillance work. Employing ι in coverage-related
problems is thus deemed promising.
This enlightens us that maximizing ι is a good approach to 
select a set of sensors that are currently taking charge in the 
surveillance work of the network area. To explore the effectiveness of
employing ι in coverage-related problems, we formulate a 
sensorgrouping problem for high-redundancy WSNs. An algorithm called
Maximizing-ι Node-Deduction (MIND) is proposed in which 
redundant sensor nodes are removed to obtain a large ι for each set of
sensors that are currently taking charge in the surveillance work of
the network area. We also introduce another greedy solution called
Incremental Coverage Quality Algorithm (ICQA) for this problem,
which serves as a benchmark to evaluate MIND.
The main contribution of this paper is twofold. First, we 
introduce a novel index ι for evaluation of point-distribution. We show
that maximizing ι of a WSN results in low redundancy of the 
network. Second, we formulate a general sensor-grouping problem
for WSNs and provide a general sensing model. With the MIND
algorithm we show that locally maximizing ι among each sensor
node and its neighbors is a good approach to solve this problem.
This demonstrates a good application of employing ι in 
coveragerelated problems.
The rest of the paper is organized as follows. In Section 2, we
introduce our point-distribution index ι. We survey related work
and formulate a sensor-grouping problem together with a general
sensing model in Section 3. Section 4 investigates the application
of ι in this grouping problem. We propose MIND for this problem
1171
and introduce ICQA as a benchmark. In Section 5, we present
our simulation results in which MIND and ICQA are compared.
Section 6 provides conclusion remarks.
2. THE NORMALIZED MINIMUM DISTANCE
ι: A POINT-DISTRIBUTION INDEX
Suppose there are n points in a Euclidean space Ω. The 
coordinates of these points are denoted by xi (i = 1, ..., n).
It may be necessary to evaluate how the distribution of these
points is. There are many metrics to achieve this goal. For 
example, the Mean Square Error from these points to their mean value
can be employed to calculate how these points deviate from their
mean (i.e., their central). In resource-sharing evaluation, the Global
Fairness Index (GFI) is often employed to measure how even the
resource distributes among these points [8], when xi represents the
amount of resource that belong to point i. In WSNs, GFI is usually
used to calculate how even the remaining energy of sensor nodes
is.
When n is larger than 2 and the points do not all overlap (That
points all overlap means xi = xj, ∀ i, j = 1, 2, ..., n). We propose
a novel index called the normalized minimum distance, namely ι,
to evaluate the distribution of the points. ι is the minimum distance
between each pair of points normalized by the average distance
between each pair of points. It is calculated by:
ι =
min(||xi − xj||)
µ
(∀ i, j = 1, 2, ..., n; and i = j) (1)
where ||xi − xj|| denotes the Euclidean distance between point
i and point j in Ω, the min(·) function calculates the minimum
distance between each pair of points, and µ is the average distance
between each pair of points, which is:
µ =
(
Pn
i=1
Pn
j=1,j=i ||xi − xj||)
n(n − 1)
(2)
ι measures how well the points separate from one another. 
Obviously, ι is in interval [0, 1]. ι is equal to 1 if and only if n is equal
to 3 and these three points forms an equilateral triangle. ι is equal
to zero if any two points overlap. ι is a very interesting value of a
set of points. If we consider each xi (∀i = 1, ..., n) is a variable in
Ω, how these n points would look like if ι is maximized?
An algorithm is implemented to generate the topology in which
ι is locally maximized (The algorithm can be found in [19]). We
consider a 2-dimensional space. We select n = 10, 20, 30, ..., 100
and perform this algorithm. In order to avoid that the algorithm
converge to local optimum, we select different random seeds to
generate the initial points for 1000 time and obtain the best one
that results in the largest ι when the algorithm converges. Figure 1
demonstrates what the resulting topology looks like when n = 20
as an example.
Suppose each point represents a sensor node. If the sensor 
coverage model is the Boolean coverage model [15][17][18][14] and
the coverage radius of each node is the same. It is exciting to see
that this topology results in lowest redundancy because the Vonoroi
diagram [2] formed by these nodes (A Vonoroi diagram formed by
a set of nodes partitions a space into a set of convex polygons such
that points inside a polygon are closest to only one particular node)
is a honeycomb-like structure1
.
This enlightens us that ι may be employed to solve problems
related to sensor-coverage of an area. In WSNs, it is desirable
1
This is how base stations of a wireless cellular network are 
deployed and why such a network is called a cellular one.
0 20 40 60 80 100 120 140 160
0
20
40
60
80
100
120
140
160
X
Y
Figure 1: Node Number = 20, ι = 0.435376
that the active sensor nodes that are performing surveillance task
should separate from one another. Under the constraint that the
sensing area should be covered, the more each node separates from
the others, the less the redundancy of the coverage is. ι indicates
the quality of such separation. It should be useful for approaches
on sensor-coverage related problems.
In our following discussions, we will show the effectiveness of
employing ι in sensor-grouping problem.
3. THE SENSOR-GROUPING PROBLEM
In many application scenarios, to achieve fault tolerance, a WSN
contains a large number of redundant nodes in order to tolerate
node failures. A node sleeping-working schedule scheme is 
therefore highly desired to exploit the redundancy of working sensors
and let as many nodes as possible sleep.
Much work in the literature is on this issue [6]. Yan et al 
introduced a differentiated service in which a sensor node finds out
its responsible working duration with cooperation of its neighbors
to ensure the coverage of sampled points [17]. Ye et al developed
PEAS in which sensor nodes wake up randomly over time, probe
their neighboring nodes, and decide whether they should begin to
take charge of surveillance work [18]. Xing et al exploited a 
probabilistic distributed detection model with a protocol called 
Coordinating Grid (Co-Grid) [16]. Wang et al designed an approach called
Coverage Configuration Protocol (CCP) which introduced the 
notion that the coverage degree of intersection-points of the 
neighboring nodes" sensing-perimeters indicates the coverage of a convex
region [15]. In our recent work [7], we also provided a sleeping
configuration protocol, namely SSCP, in which sleeping eligibility
of a sensor node is determined by a local Voronoi diagram. SSCP
can provide different levels of redundancy to maintain different 
requirements of fault tolerance.
The major feature of the aforementioned protocols is that they
employ online distributed and localized algorithms in which a 
sensor node determines its sleeping eligibility and/or sleeping time
based on the coverage requirement of its sensing area with some
information provided by its neighbors.
Another major approach for sensor node sleeping-working 
scheduling issue is to group sensor nodes. Sensor nodes in a network are
divided into several disjoint sets. Each set of sensor nodes are able
to maintain the required area surveillance work. The sensor nodes
are scheduled according to which set they belong to. These sets
work successively. Only one set of sensor nodes work at any time.
We call the issue sensor-grouping problem.
The major advantage of this approach is that it avoids the 
overhead caused by the processes of coordination of sensor nodes to
make decision on whether a sensor node is a candidate to sleep or
1172
work and how long it should sleep or work. Such processes should
be performed from time to time during the lifetime of a network in
many online distributed and localized algorithms. The large 
overhead caused by such processes is the main drawback of the 
online distributed and localized algorithms. On the contrary, roughly
speaking, this approach groups sensor nodes in one time and 
schedules when each set of sensor nodes should be on duty. It does not
require frequent decision-making on working/sleeping eligibility2
.
In [13] by Slijepcevic et al, the sensing area is divided into 
regions. Sensor nodes are grouped with the most-constrained 
leastconstraining algorithm. It is a greedy algorithm in which the 
priority of selecting a given sensor is determined by how many 
uncovered regions this sensor covers and the redundancy caused by
this sensor. In [5] by Cardei et al, disjoint sensor sets are 
modeled as disjoint dominating sets. Although maximum dominating
sets computation is NP-complete, the authors proposed a 
graphcoloring based algorithm. Cardei et al also studied similar problem
in the domain of covering target points in [4]. The NP-completeness
of the problem is proved and a heuristic that computes the sets are
proposed. These algorithms are centralized solutions of 
sensorgrouping problem.
However, global information (e.g., the location of each in-network
sensor node) of a large scale WSN is also very expensive to 
obtained online. Also it is usually infeasible to obtain such 
information before sensor nodes are deployed. For example, sensor nodes
are usually deployed in a random manner and the location of each
in-network sensor node is determined only after a node is deployed.
The solution of sensor-grouping problem should only base on 
locally obtainable information of a sensor node. That is to say, nodes
should determine which group they should join in a fully 
distributed way. Here locally obtainable information refers to a node"s
local information and the information that can be directly obtained
from its adjacent nodes, i.e., nodes within its communication range.
In Subsection 3.1, we provide a general problem formulation of
the sensor-grouping problem. Distributed-solution requirement is
formulated in this problem. It is followed by discussion in 
Subsection 3.2 on a general sensing model, which serves as a given
condition of the sensor-grouping problem formulation.
To facilitate our discussions, the notations in our following 
discussions are described as follows.
• n: The number in-network sensor nodes.
• S(j) (j = 1, 2, ..., m): The jth set of sensor nodes where m
is the number of sets.
• L(i) (i = 1, 2, ..., n): The physical location of node i.
• φ: The area monitored by the network: i.e., the sensing area
of the network.
• R: The sensing radius of a sensor node. We assume that
a sensor node can only be responsible to monitor a circular
area centered at the node with a radius equal to R. This is
a usual assumption in work that addresses sensor-coverage
related problems. We call this circular area the sensing area
of a node.
3.1 Problem Formulation
We assume that each sensor node can know its approximate 
physical location. The approximate location information is obtainable
if each sensor node carries a GPS receiver or if some localization
algorithms are employed (e.g., [3]).
2
Note that if some nodes die, a re-grouping process might also be
performed to exploit the remaining nodes in a set of sensor nodes.
How to provide this mechanism is beyond the scope of this paper
and yet to be explored.
Problem 1. Given:
• The set of each sensor node i"s sensing neighbors N(i) and
the location of each member in N(i);
• A sensing model which quantitatively describes how a point
P in area φ is covered by sensor nodes that are responsible to
monitor this point. We call this quantity the coverage quality
of P.
• The coverage quality requirement in φ, denoted by s. When
the coverage of a point is larger than this threshold, we say
this point is covered.
For each sensor node i, make a decision on which group S(j) it
should join so that:
• Area φ can be covered by sensor nodes in each set S(j)
• m, the number of sets S(j) is maximized.
In this formulation, we call sensor nodes within a circular area
centered at a sensor node i with a radius equal to 2 · R the sensing
neighbors of node i. This is because sensors nodes in this area,
together with node i, may be cooperative to ensure the coverage of
a point inside node i"s sensing area.
We assume that the communication range of a sensor node is
larger than 2 · R, which is also a general assumption in work that
addresses sensor-coverage related problems. That is to say, the first
given condition in Problem 1 is the information that can be obtained
directly from a node"s adjacent nodes. It is therefore locally 
obtainable information. The last two given conditions in this problem
formulation can be programmed into a node before it is deployed
or by a node-programming protocol (e.g., [9]) during network 
runtime. Therefore, the given conditions can all be easily obtained by
a sensor-grouping scheme with fully distributed implementation.
We reify this problem with a realistic sensing model in next 
subsection.
3.2 A General Sensing Model
As WSNs are usually employed to monitor possible events in a
given area, it is therefore a design requirement that an event 
occurring in the network area must/may be successfully detected by
sensors.
This issue is usually formulated as how to ensure that an event
signal omitted in an arbitrary point in the network area can be 
detected by sensor nodes. Obviously, a sensing model is required to
address this problem so that how a point in the network area is 
covered can be modeled and quantified. Thus the coverage quality of
a WSN can be evaluated.
Different applications of WSNs employ different types of 
sensors, which surely have widely different theoretical and physical
characteristics. Therefore, to fulfill different application 
requirements, different sensing models should be constructed based on the
characteristics of the sensors employed.
A simple theoretical sensing model is the Boolean sensing model
[15][18][17][14]. Boolean sensing model assumes that a sensor
node can always detect an event occurring in its responsible 
sensing area. But most sensors detect events according to the signal
strength sensed. Event signals usually fade in relation to the 
physical distance between an event and the sensor. The larger the 
distance, the weaker the event signals that can be sensed by the sensor,
which results in a reduction of the probability that the event can be
successfully detected by the sensor.
As in WSNs, event signals are usually electromagnetic, acoustic,
or photic signals, they fade exponentially with the increasing of
1173
their transmit distance. Specifically, the signal strength E(d) of an
event that is received by a sensor node satisfies:
E(d) =
α
dβ
(3)
where d is the physical distance from the event to the sensor node;
α is related to the signal strength omitted by the event; and β is
signal fading factor which is typically a positive number larger than
or equal to 2. Usually, α and β are considered as constants.
Based on this notion, to be more reasonable, researchers propose
collaborative sensing model to capture application requirements:
Area coverage can be maintained by a set of collaborative sensor
nodes: For a point with physical location L, the point is considered
covered by the collaboration of i sensors (denoted by k1, ..., ki) if
and only if the following two equations holds [7][10][12].
∀j = 1, ..., i; L(kj) − L < R. (4)
C(L) =
iX
j=1
(E( L(kj) − L ) > s. (5)
C(L) is regarded as the coverage quality of location L in the
network area [7][10][12].
However, we notice that defining the sensibility as the sum of the
sensed signal strength by each collaborative sensor implies a very
special application: Applications must employ the sum of the 
signal strength to achieve decision-making. To capture generally 
realistic application requirement, we modify the definition described
in Equation (5). The model we adopt in this paper is described in
details as follows.
We consider the probability P(L, kj ) that an event located at L
can be detected by sensor kj is related to the signal strength sensed
by kj. Formally,
P(L, kj) = γE(d) =
δ
( L(kj) − L / + 1)β
, (6)
where γ is a constant and δ = γα is a constant too. normalizes
the distance to a proper scale and the +1 item is to avoid infinite
value of P(L, kj).
The probability that an event located at L can be detected by any
collaborative sensors that satisfied Equation (4) is:
P (L) = 1 −
iY
j=1
(1 − P(L, kj )). (7)
As the detection probability P (L) reasonably determines how
an event occurring at location L can be detected by the networks, it
is a good measure of the coverage quality of location L in a WSN.
Specifically, Equation (5) is modified to:
C(L) = P (L)
= 1 −
iY
j=1
[1 −
δ
( L(kj) − L / + 1)β
] > s. (8)
To sum it up, we consider a point at location L is covered if
Equation (4) and (8) hold.
4. MAXIMIZING-ι NODE-DEDUCTION
ALGORITHM FOR SENSOR-GROUPING
PROBLEM
Before we process to introduce algorithms to solve the sensor
grouping problem, let us define the margin (denoted by θ) of an
area φ monitored by the network as the band-like marginal area
of φ and all the points on the outer perimeter of θ is ρ distance
away from all the points on the inner perimeter of θ. ρ is called the
margin length.
In a practical network, sensor nodes are usually evenly deployed
in the network area. Obviously, the number of sensor nodes that
can sense an event occurring in the margin of the network is smaller
than the number of sensor nodes that can sense an event occurring
in other area of the network. Based on this consideration, in our
algorithm design, we ensure the coverage quality of the network
area except the margin. The information on φ and ρ is 
networkbased. Each in-network sensor node can be pre-programmed or
on-line informed about φ and ρ, and thus calculate whether a point
in its sensing area is in the margin or not.
4.1 Maximizing-ι Node-Deduction Algorithm
The node-deduction process of our Maximizing-ι Node-Deduction
Algorithm (MIND) is simple. A node i greedily maximizes ι of the
sub-network composed by itself, its ungrouped sensing neighbors,
and the neighbors that are in the same group of itself. Under the
constraint that the coverage quality of its sensing area should be
ensured, node i deletes nodes in this sub-network one by one. The
candidate to be pruned satisfies that:
• It is an ungrouped node.
• The deletion of the node will not result in uncovered-points
inside the sensing area of i.
A candidate is deleted if the deletion of the candidate results in
largest ι of the sub-network compared to the deletion of other 
candidates. This node-deduction process continues until no candidate
can be found. Then all the ungrouped sensing neighbors that are
not deleted are grouped into the same group of node i. We call the
sensing neighbors that are in the same group of node i the group
sensing neighbors of node i. We then call node i a finished node,
meaning that it has finished the above procedure and the sensing
area of the node is covered. Those nodes that have not yet finished
this procedure are called unfinished nodes.
The above procedure initiates at a random-selected node that is
not in the margin. The node is grouped to the first group. It 
calculates the resulting group sensing neighbors of it based on the above
procedure. It informs these group sensing neighbors that they are
selected in the group. Then it hands over the above procedure to
an unfinished group sensing neighbors that is the farthest from 
itself. This group sensing neighbor continues this procedure until no
unfinished neighbor can be found. Then the first group is formed
(Algorithmic description of this procedure can be found at [19]).
After a group is formed, another random-selected ungrouped
node begins to group itself to the second group and initiates the
above procedure. In this way, groups are formed one by one. When
a node that involves in this algorithm found out that the coverage
quality if its sensing area, except what overlaps the network margin,
cannot be ensured even if all its ungrouped sensing neighbors are
grouped into the same group as itself, the algorithm stops. MIND
is based on locally obtainable information of sensor nodes. It is
a distributed algorithm that serves as an approximate solution of
Problem 1.
4.2 Incremental Coverage Quality Algorithm:
A Benchmark for MIND
To evaluate the effectiveness of introducing ι in the sensor-group
problem, another algorithm for sensor-group problem called 
Incremental Coverage Quality Algorithm (ICQA) is designed. Our aim
1174
is to evaluate how an idea, i.e., MIND, based on locally maximize
ι performs.
In ICQA, a node-selecting process is as follows. A node i 
greedily selects an ungrouped sensing neighbor in the same group as 
itself one by one, and informs the neighbor it is selected in the group.
The criterion is:
• The selected neighbor is responsible to provide surveillance
work for some uncovered parts of node i"s sensing area. (i.e.,
the coverage quality requirement of the parts is not fulfilled
if this neighbor is not selected.)
• The selected neighbor results in highest improvement of the
coverage quality of the neighbor"s sensing area.
The improvement of the coverage quality, mathematically, should
be the integral of the the improvements of all points inside the
neighbor"s sensing area. A numerical approximation is employed
to calculate this improvement. Details are presented in our 
simulation study.
This node-selecting process continues until the sensing area of
node i is entirely covered. In this way, node i"s group sensing
neighbors are found. The above procedure is handed over as what
MIND does and new groups are thus formed one by one. And
the condition that ICQA stops is the same as MIND. ICQA is also
based on locally obtainable information of sensor nodes. ICQA is
also a distributed algorithm that serves as an approximate solution
of Problem 1.
5. SIMULATION RESULTS
To evaluate the effectiveness of employing ι in sensor-grouping
problem, we build simulation surveillance networks. We employ
MIND and ICQA to group the in-network sensor nodes. We 
compare the grouping results with respect to how many groups both
algorithms find and how the performance of the resulting groups
are.
Detailed settings of the simulation networks are shown in Table
1. In simulation networks, sensor nodes are randomly deployed in
a uniform manner in the network area.
Table 1: The settings of the simulation networks
Area of sensor field 400m*400m
ρ 20m
R 80m
α, β, γ and 1.0, 2.0, 1.0 and 100.0
s 0.6
For evaluating the coverage quality of the sensing area of a node,
we divide the sensing area of a node into several regions and regard
the coverage quality of the central point in each region as a 
representative of the coverage quality of the region. This is a numerical
approximation. Larger number of such regions results in better 
approximation. As sensor nodes are with low computational 
capacity, there is a tradeoff between the number of such regions and the
precision of the resulting coverage quality of the sensing area of a
node. In our simulation study, we set this number 12. For 
evaluating the improvement of coverage quality in ICQA, we sum up all
the improvements at each region-center as the total improvement.
5.1 Number of Groups Formed by MIND and
ICQA
We set the total in-network node number to different values and
let the networks perform MIND and ICQA. For each n, 
simulations run with several random seeds to generate different networks.
Results are averaged. Figure 2 shows the group numbers found in
networks with different n"s.
500 1000 1500 2000
0
5
10
15
20
25
30
35
40
45
50
Total in−network node number
Totalnumberofgroupsfound
ICQA
MMNP
Figure 2: The number of groups found by MIND and ICQA
We can see that MIND always outperforms ICQA in terms of
the number of groups formed. Obviously, the larger the number of
groups can be formed, the more the redundancy of each group is
exploited. This output shows that an approach like MIND that aim
to maximize ι of the resulting topology can exploits redundancy
well.
As an example, in case that n = 1500, the results of five 
networks are listed in Table 2.
Table 2: The grouping results of five networks with n = 1500
Net MIND ICQA MIND ICQA
Group Number Group Number Average ι Average ι
1 34 31 0.145514 0.031702
2 33 30 0.145036 0.036649
3 33 31 0.156483 0.033578
4 32 31 0.152671 0.029030
5 33 32 0.146560 0.033109
The difference between the average ι of the groups in each 
network shows that groups formed by MIND result in topologies with
larger ι"s. It demonstrates that ι is good indicator of redundancy in
different networks.
5.2 The Performance of the Resulting Groups
Although MIND forms more groups than ICQA does, which 
implies longer lifetime of the networks, another importance 
consideration is how these groups formed by MIND and ICQA perform.
We let 10000 events randomly occur in the network area except
the margin. We compare how many events happen at the locations
where the quality is less than the requirement s = 0.6 when each
resulting group is conducting surveillance work (We call the 
number of such events the failure number of group). Figure 3 shows
the average failure numbers of the resulting groups when different
node numbers are set.
We can see that the groups formed by MIND outperform those
formed by ICQA because the groups formed by MIND result in
lower failure numbers. This further demonstrates that MIND is a
good approach for sensor-grouping problem.
1175
500 1000 1500 2000
0
10
20
30
40
50
60
Total in−network node number
averagefailurenumbers
ICQA
MMNP
Figure 3: The failure numbers of MIND and ICQA
6. CONCLUSION
This paper proposes ι, a novel index for evaluation of 
pointdistribution. ι is the minimum distance between each pair of points
normalized by the average distance between each pair of points.
We find that a set of points that achieve a maximum value of ι 
result in a honeycomb structure. We propose that ι can serve as a
good index to evaluate the distribution of the points, which can be
employed in coverage-related problems in wireless sensor networks
(WSNs). We set out to validate this idea by employing ι to a 
sensorgrouping problem. We formulate a general sensor-grouping 
problem for WSNs and provide a general sensing model. With an 
algorithm called Maximizing-ι Node-Deduction (MIND), we show that
maximizing ι at sensor nodes is a good approach to solve this 
problem. Simulation results verify that MIND outperforms a greedy 
algorithm that exploits sensor-redundancy we design in terms of the
number and the performance of the groups formed. This 
demonstrates a good application of employing ι in coverage-related 
problems.
7. ACKNOWLEDGEMENT
The work described in this paper was substantially supported by
two grants, RGC Project No. CUHK4205/04E and UGC Project
No. AoE/E-01/99, of the Hong Kong Special Administrative 
Region, China.
8. REFERENCES
[1] I. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci.
A survey on wireless sensor networks. IEEE
Communications Magazine, 40(8):102-114, 2002.
[2] F. Aurenhammer. Vononoi diagram - a survey of a
fundamental geometric data structure. ACM Computing
Surveys, 23(2):345-405, September 1991.
[3] N. Bulusu, J. Heidemann, and D. Estrin. GPS-less low-cost
outdoor localization for very small devices. IEEE Personal
Communication, October 2000.
[4] M. Cardei and D.-Z. Du. Improving wireless sensor network
lifetime through power aware organization. ACM Wireless
Networks, 11(3), May 2005.
[5] M. Cardei, D. MacCallum, X. Cheng, M. Min, X. Jia, D. Li,
and D.-Z. Du. Wireless sensor networks with energy efficient
organization. Journal of Interconnection Networks, 3(3-4),
December 2002.
[6] M. Cardei and J. Wu. Coverage in wireless sensor networks.
In Handbook of Sensor Networks, (eds. M. Ilyas and I.
Magboub), CRC Press, 2004.
[7] X. Chen and M. R. Lyu. A sensibility-based sleeping
configuration protocol for dependable wireless sensor
networks. CSE Technical Report, The Chinese University of
Hong Kong, 2005.
[8] R. Jain, W. Hawe, and D. Chiu. A quantitative measure of
fairness and discrimination for resource allocation in shared
computer systems. Technical Report DEC-TR-301,
September 1984.
[9] S. S. Kulkarni and L. Wang. MNP: Multihop network
reprogramming service for sensor networks. In Proc. of the
25th International Conference on Distributed Computing
Systems (ICDCS), June 2005.
[10] B. Liu and D. Towsley. A study on the coverage of
large-scale sensor networks. In Proc. of the 1st IEEE
International Conference on Mobile ad-hoc and Sensor
Systems, Fort Lauderdale, FL, October 2004.
[11] A. Mainwaring, J. Polastre, R. Szewczyk, D. Culler, and
J. Anderson. Wireless sensor networks for habitat
monitoring. In Proc. of the ACM International Workshop on
Wireless Sensor Networks and Applications, 2002.
[12] S. Megerian, F. Koushanfar, G. Qu, G. Veltri, and
M. Potkonjak. Explosure in wirless sensor networks: Theory
and pratical solutions. Wireless Networks, 8, 2002.
[13] S. Slijepcevic and M. Potkonjak. Power efficient
organization of wireless sensor networks. In Proc. of the
IEEE International Conference on Communications (ICC),
volume 2, Helsinki, Finland, June 2001.
[14] D. Tian and N. D. Georganas. A node scheduling scheme for
energy conservation in large wireless sensor networks.
Wireless Communications and Mobile Computing,
3:272-290, May 2003.
[15] X. Wang, G. Xing, Y. Zhang, C. Lu, R. Pless, and C. Gill.
Integrated coverage and connectivity configuration in
wireless sensor networks. In Proc. of the 1st ACM
International Conference on Embedded Networked Sensor
Systems (SenSys), Los Angeles, CA, November 2003.
[16] G. Xing, C. Lu, R. Pless, and J. A. O´ Sullivan. Co-Grid: an
efficient converage maintenance protocol for distributed
sensor networks. In Proc. of the 3rd International
Symposium on Information Processing in Sensor Networks
(IPSN), Berkeley, CA, April 2004.
[17] T. Yan, T. He, and J. A. Stankovic. Differentiated
surveillance for sensor networks. In Proc. of the 1st ACM
International Conference on Embedded Networked Sensor
Systems (SenSys), Los Angeles, CA, November 2003.
[18] F. Ye, G. Zhong, J. Cheng, S. Lu, and L. Zhang. PEAS: A
robust energy conserving protocol for long-lived sensor
networks. In Proc. of the 23rd International Conference on
Distributed Computing Systems (ICDCS), Providence, Rhode
Island, May 2003.
[19] Y. Zhou, H. Yang, and M. R. Lyu. A point-distribution index
and its application in coverage-related problems. CSE
Technical Report, The Chinese University of Hong Kong,
2006.
1176
CenWits: A Sensor-Based Loosely Coupled Search and
Rescue System Using Witnesses
Jyh-How Huang
Department of Computer
Science
University of Colorado,
Campus Box 0430
Boulder, CO 80309-0430
huangjh@cs.colorado.edu
Saqib Amjad
Department of Computer
Science
University of Colorado,
Campus Box 0430
Boulder, CO 80309-0430
Saqib.Amjad@colorado.edu
Shivakant Mishra
Department of Computer
Science
University of Colorado,
Campus Box 0430
Boulder, CO 80309-0430
mishras@cs.colorado.edu
ABSTRACT
This paper describes the design, implementation and 
evaluation of a search and rescue system called CenWits. CenWits
uses several small, commonly-available RF-based sensors,
and a small number of storage and processing devices. It is
designed for search and rescue of people in emergency 
situations in wilderness areas. A key feature of CenWits is that
it does not require a continuously connected sensor network
for its operation. It is designed for an intermittently 
connected network that provides only occasional connectivity.
It makes a judicious use of the combined storage capability
of sensors to filter, organize and store important 
information, combined battery power of sensors to ensure that the
system remains operational for longer time periods, and 
intermittent network connectivity to propagate information
to a processing center. A prototype of CenWits has been
implemented using Berkeley Mica2 motes. The paper 
describes this implementation and reports on the performance
measured from it.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems
General Terms
Algorithms, Design, Experimentation
1. INTRODUCTION
Search and rescue of people in emergency situation in a
timely manner is an extremely important service. It has
been difficult to provide such a service due to lack of timely
information needed to determine the current location of a
person who may be in an emergency situation. With the
emergence of pervasive computing, several systems [12, 19,
1, 5, 6, 4, 11] have been developed over the last few years
that make use of small devices such as cell phones, 
sensors, etc. All these systems require a connected network
via satellites, GSM base stations, or mobile devices. This
requirement severely limits their applicability, particularly
in remote wilderness areas where maintaining a connected
network is very difficult.
For example, a GSM transmitter has to be in the range
of a base station to transmit. As a result, it cannot operate
in most wilderness areas. While a satellite transmitter is
the only viable solution in wilderness areas, it is typically
expensive and cumbersome. Furthermore, a line of sight is
required to transmit to satellite, and that makes it 
infeasible to stay connected in narrow canyons, large cities with
skyscrapers, rain forests, or even when there is a roof or some
other obstruction above the transmitter, e.g. in a car. An
RF transmitter has a relatively smaller range of 
transmission. So, while an in-situ sensor is cheap as a single unit, it is
expensive to build a large network that can provide 
connectivity over a large wilderness area. In a mobile environment
where sensors are carried by moving people, power-efficient
routing is difficult to implement and maintain over a large
wilderness area. In fact, building an adhoc sensor network
using only the sensors worn by hikers is nearly impossible
due to a relatively small number of sensors spread over a
large wilderness area.
In this paper, we describe the design, implementation
and evaluation of a search and rescue system called 
CenWits (Connection-less Sensor-Based Tracking System 
Using Witnesses). CenWits is comprised of mobile, in-situ
sensors that are worn by subjects (people, wild animals, or
in-animate objects), access points (AP) that collect 
information from these sensors, and GPS receivers and location
points (LP) that provide location information to the 
sensors. A subject uses GPS receivers (when it can connect to
a satellite) and LPs to determine its current location. The
key idea of CenWits is that it uses a concept of witnesses
to convey a subject"s movement and location information
to the outside world. This averts a need for maintaining a
connected network to transmit location information to the
outside world. In particular, there is no need for 
expensive GSM or satellite transmitters, or maintaining an adhoc
network of in-situ sensors in CenWits.
180
CenWits employs several important mechanisms to 
address the key problem of resource constraints (low signal
strength, low power and limited memory) in sensors. In
particular, it makes a judicious use of the combined 
storage capability of sensors to filter, organize and store 
important information, combined battery power of sensors to
ensure that the system remains operational for longer time
periods, and intermittent network connectivity to propagate
information to a processing center.
The problem of low signal strengths (short range RF 
communication) is addressed by avoiding a need for maintaining
a connected network. Instead, CenWits propagates the 
location information of sensors using the concept of witnesses
through an intermittently connected network. As a result,
this system can be deployed in remote wilderness areas, as
well as in large urban areas with skyscrapers and other tall
structures. Also, this makes CenWits cost-effective. A 
subject only needs to wear light-weight and low-cost sensors
that have GPS receivers but no expensive GSM or satellite
transmitters. Furthermore, since there is no need for a 
connected sensor network, there is no need to deploy sensors in
very large numbers.
The problem of limited battery life and limited memory of
a sensor is addressed by incorporating the concepts of groups
and partitions. Groups and partitions allow sensors to stay
in sleep or receive modes most of the time. Using groups and
partitions, the location information collected by a sensor can
be distributed among several sensors, thereby reducing the
amount of memory needed in one sensor to store that 
information. In fact, CenWits provides an adaptive tradeoff
between memory and power consumption of sensors. Each
sensor can dynamically adjust its power and memory 
consumption based on its remaining power or available memory.
It has amply been noted that the strength of sensor 
networks comes from the fact that several sensor nodes can
be distributed over a relatively large area to construct a
multihop network. This paper demonstrates that important
large-scale applications can be built using sensors by 
judiciously integrating the storage, communication and 
computation capabilities of sensors. The paper describes 
important techniques to combine memory, transmission and 
battery power of many sensors to address resource constraints
in the context of a search and rescue application. However,
these techniques are quite general. We discuss several other
sensor-based applications that can employ these techniques.
While CenWits addresses the general location tracking
and reporting problem in a wide-area network, there are
two important differences from the earlier work done in this
area. First, unlike earlier location tracking solutions, 
CenWits does not require a connected network. Second, unlike
earlier location tracking solutions, CenWits does not aim for
a very high accuracy of localization. Instead, the main goal
is to provide an approximate, small area where search and
rescue efforts can be concentrated.
The rest of this paper is organized as follows. In Section
2, we overview some of the recent projects and technologies
related to movement and location tracking, and search and
rescue systems. In Section 3, we describe the overall 
architecture of CenWits, and provide a high-level description of
its functionality. In the next section, Section 4, we discuss
power and memory management in CenWits. To simplify
our presentation, we will focus on a specific application of
tracking lost/injured hikers in all these sections. In 
Section 6, we describe a prototype implementation of CenWits
and present performance measured from this 
implementation. We discuss how the ideas of CenWits can be used
to build several other applications in Section 7. Finally, in
Section 8, we discuss some related issues and conclude the
paper.
2. RELATED WORK
A survey of location systems for ubiquitous computing
is provided in [11]. A location tracking system for adhoc
sensor networks using anchor sensors as reference to gain
location information and spread it out to outer node is 
proposed in [17]. Most location tracking systems in adhoc 
sensor networks are for benefiting geographic-aware routing.
They don"t fit well for our purposes. The well-known 
active badge system [19] lets a user carry a badge around.
An infrared sensor in the room can detect the presence of
a badge and determine the location and identification of
the person. This is a useful system for indoor environment,
where GPS doesn"t work. Locationing using 802.11 devices
is probably the cheapest solution for indoor position 
tracking [8]. Because of the popularity and low cost of 802.11
devices, several business solutions based on this technology
have been developed[1].
A system that combines two mature technologies and is
viable in suburban area where a user can see clear sky and
has GSM cellular reception at the same time is currently
available[5]. This system receives GPS signal from a 
satellite and locates itself, draws location on a map, and sends
location information through GSM network to the others
who are interested in the user"s location.
A very simple system to monitor children consists an RF
transmitter and a receiver. The system alarms the holder
of the receiver when the transmitter is about to run out of
range [6].
Personal Locater Beacons (PLB) has been used for avalanche
rescuing for years. A skier carries an RF transmitter that
emits beacons periodically, so that a rescue team can find
his/her location based on the strength of the RF signal. 
Luxury version of PLB combines a GPS receiver and a 
COSPASSARSAT satellite transmitter that can transmit user"s 
location in latitude and longitude to the rescue team whenever
an accident happens [4]. However, the device either is turned
on all the time resulting in fast battery drain, or must be
turned on after the accident to function.
Another related technology in widespread use today is the
ONSTAR system [3], typically used in several luxury cars.
In this system, a GPS unit provides position information,
and a powerful transmitter relays that information via 
satellite to a customer service center. Designed for emergencies,
the system can be triggered either by the user with the push
of a button, or by a catastrophic accident. Once the system
has been triggered, a human representative attempts to gain
communication with the user via a cell phone built as an 
incar device. If contact cannot be made, emergency services
are dispatched to the location provided by GPS. Like PLBs,
this system has several limitations. First, it is heavy and 
expensive. It requires a satellite transmitter and a connected
network. If connectivity with either the GPS network or a
communication satellite cannot be maintained, the system
fails. Unfortunately, these are common obstacles 
encountered in deep canyons, narrow streets in large cities, parking
garages, and a number of other places.
181
The Lifetch system uses GPS receiver board combined
with a GSM/GPRS transmitter and an RF transmitter in
one wireless sensor node called Intelligent Communication
Unit (ICU). An ICU first attempts to transmit its location
to a control center through GSM/GPRS network. If that
fails, it connects with other ICUs (adhoc network) to 
forward its location information until the information reaches
an ICU that has GSM/GPRS reception. This ICU then
transmits the location information of the original ICU via
the GSM/GPRS network.
ZebraNet is a system designed to study the moving 
patterns of zebras [13]. It utilizes two protocols: History-based
protocol and flooding protocol. History-based protocol is
used when the zebras are grazing and not moving around
too much. While this might be useful for tracking zebras,
it"s not suitable for tracking hikers because two hikers are
most likely to meet each other only once on a trail. In
the flooding protocol, a node dumps its data to a neighbor
whenever it finds one and doesn"t delete its own copy until
it finds a base station. Without considering routing loops,
packet filtering and grouping, the size of data on a node will
grow exponentially and drain the power and memory of a
sensor node with in a short time. Instead, Cenwits uses a
four-phase hand-shake protocol to ensure that a node 
transmits only as much information as the other node is willing
to receive. While ZebraNet is designed for a big group of
sensors moving together in the same direction with same
speed, Cenwits is designed to be used in the scenario where
sensors move in different directions at different speeds.
Delay tolerant network architecture addresses some 
important problems in challenged (resource-constrained) 
networks [9]. While this work is mainly concerned with 
interoperability of challenged networks, some problems related
to occasionally-connected networks are similar to the ones
we have addressed in CenWits.
Among all these systems, luxury PLB and Lifetch are 
designed for location tracking in wilderness areas. However,
both of these systems require a connected network. Luxury
PLB requires the user to transmit a signal to a satellite,
while Lifetch requires connection to GSM/GPRS network.
Luxury PLB transmits location information, only when an
accident happens. However, if the user is buried in the snow
or falls into a deep canyon, there is almost no chance for the
signal to go through and be relayed to the rescue team. This
is because satellite transmission needs line of sight. 
Furthermore, since there is no known history of user"s location, it is
not possible for the rescue team to infer the current location
of the user. Another disadvantage of luxury PLB is that a
satellite transmitter is very expensive, costing in the range
of $750. Lifetch attempts to transmit the location 
information by GSM/GPRS and adhoc sensor network that uses
AODV as the routing protocol. However, having a cellular
reception in remote areas in wilderness areas, e.g. 
American national parks is unlikely. Furthermore, it is extremely
unlikely that ICUs worn by hikers will be able to form an
adhoc network in a large wilderness area. This is because
the hikers are mobile and it is very unlikely to have several
ICUs placed dense enough to forward packets even on a very
popular hike route.
CenWits is designed to address the limitations of systems
such as luxury PLB and Lifetch. It is designed to 
provide hikers, skiers, and climbers who have their activities
mainly in wilderness areas a much higher chance to convey
their location information to a control center. It is not 
reliant upon constant connectivity with any communication
medium. Rather, it communicates information along from
user to user, finally arriving at a control center. Unlike 
several of the systems discussed so far, it does not require that
a user"s unit is constantly turned on. In fact, it can discover
a victim"s location, even if the victim"s sensor was off at the
time of accident and has remained off since then. CenWits
solves one of the greatest problems plaguing modern search
and rescue systems: it has an inherent on-site storage 
capability. This means someone within the network will have
access to the last-known-location information of a victim,
and perhaps his bearing and speed information as well.
Figure 1: Hiker A and Hiker B are are not in the
range of each other
3. CENWITS
We describe CenWits in the context of locating lost/injured
hikers in wilderness areas. Each hiker wears a sensor (MICA2
motes in our prototype) equipped with a GPS receiver and
an RF transmitter. Each sensor is assigned a unique ID and
maintains its current location based on the signal received by
its GPS receiver. It also emits beacons periodically. When
any two sensors are in the range of one another, they record
the presence of each other (witness information), and also
exchange the witness information they recorded earlier. The
key idea here is that if two sensors come with in range of
each other at any time, they become each other"s witnesses.
Later on, if the hiker wearing one of these sensors is lost, the
other sensor can convey the last known (witnessed) location
of the lost hiker. Furthermore, by exchanging the witness
information that each sensor recorded earlier, the witness
information is propagated beyond a direct contact between
two sensors.
To convey witness information to a processing center or to
a rescue team, access points are established at well-known
locations that the hikers are expected to pass through, e.g.
at the trail heads, trail ends, intersection of different trails,
scenic view points, resting areas, and so on. Whenever a
sensor node is in the vicinity of an access point, all witness
information stored in that sensor is automatically dumped
to the access point. Access points are connected to a 
processing center via satellite or some other network1
. The
witness information is downloaded to the processing center
from various access points at regular intervals. In case, 
connection to an access point is lost, the information from that
1
A connection is needed only between access points and a
processing center. There is no need for any connection 
between different access points.
182
access point can be downloaded manually, e.g. by UAVs.
To estimate the speed, location and direction of a hiker at
any point in time, all witness information of that hiker that
has been collected from various access points is processed.
Figure 2: Hiker A and Hiker B are in the range
of each other. A records the presence of B and B
records the presence of A. A and B become each
other"s witnesses.
Figure 3: Hiker A is in the range of an access
point. It uploads its recorded witness information
and clears its memory.
An example of how CenWits operates is illustrated in 
Figures 1, 2 and 3. First, hikers A and B are on two close
trails, but out of range of each other (Figure 1). This is
a very common scenario during a hike. For example, on a
popular four-hour hike, a hiker might run into as many as
20 other hikers. This accounts for one encounter every 12
minutes on average. A slow hiker can go 1 mile (5,280 feet)
per hour. Thus in 12 minutes a slow hiker can go as far as
1056 feet. This implies that if we were to put 20 hikers on a
4-hour, one-way hike evenly, the range of each sensor node
should be at least 1056 feet for them to communicate with
one another continuously. The signal strength starts 
dropping rapidly for two Mica2 nodes to communicate with each
other when they are 180 feet away, and is completely lost
when they are 230 feet away from each other[7]. So, for the
sensors to form a sensor network on a 4-hour hiking trail,
there should be at least 120 hikers scattered evenly. Clearly,
this is extremely unlikely. In fact, in a 4-hour, less-popular
hiking trail, one might only run into say five other hikers.
CenWits takes advantage of the fact that sensors can 
communicate with one another and record their presence. Given
a walking speed of one mile per hour (88 feet per minute)
and Mica2 range of about 150 feet for non-line-of-sight radio
transmission, two hikers have about 150/88 = 1.7 minutes to
discover the presence of each other and exchange their 
witness information. We therefore design our system to have
each sensor emit a beacon every one-and-a-half minute. In
Figure 2, hiker B"s sensor emits a beacon when A is in range,
this triggers A to exchange data with B. A communicates
the following information to B: My ID is A; I saw C at 1:23
PM at (39◦
49.3277655", 105◦
39.1126776"), I saw E at 3:09
PM at (40◦
49.2234879", 105◦
20.3290168"). B then replies
with My ID is B; I saw K at 11:20 AM at (39◦
51.4531655",
105◦
41.6776223"). In addition, A records I saw B at 4:17
PM at (41◦
29.3177354", 105◦
04.9106211") and B records I
saw A at 4:17 PM at (41◦
29.3177354", 105◦
04.9106211").
B goes on his way to overnight camping while A heads
back to trail head where there is an AP, which emits beacon
every 5 seconds to avoid missing any hiker. A dumps all
witness information it has collected to the access point. This
is shown in Figure 3.
3.1 Witness Information: Storage
A critical concern is that there is limited amount of 
memory available on motes (4 KB SDRAM memory, 128 KB
flash memory, and 4-512 KB EEPROM). So, it is important
to organize witness information efficiently. CenWits stores
witness information at each node as a set of witness records
(Format is shown in Figure 4.
1 B
Node ID Record Time X, Y Location Time Hop Count
1 B 3 B 8 B 3 B
Figure 4: Format of a witness record.
When two nodes i and j encounter each other, each node
generates a new witness record. In the witness record 
generated by i, Node ID is j, Record Time is the current time
in i"s clock, (X,Y) are the coordinates of the location of i
that i recorded most recently (either from satellite or an
LP), Location Time is the time when the this location was
recorded, and Hop Count is 0.
Each node is assigned a unique Node Id when it enters a
trail. In our current prototype, we have allocated one byte
for Node Id, although this can be increased to two or more
bytes if a large number of hikers are expected to be present
at the same time. We can represent time in 17 bits to a 
second precision. So, we have allocated 3 bytes each for Record
Time and Location Time. The circumference of the Earth
is approximately 40,075 KM. If we use a 32-bit number to
represent both longitude and latitude, the precision we get
is 40,075,000/232
= 0.0093 meter = 0.37 inches, which is
quite precise for our needs. So, we have allocated 4 bytes
each for X and Y coordinates of the location of a node. In
fact, a foot precision can be achieved by using only 27 bits.
3.2 Location Point and Location Inference
Although a GPS receiver provides an accurate location
information, it has it"s limitation. In canyons and rainy
forests, a GPS receiver does not work. When there is a
heavy cloud cover, GPS users have experienced inaccuracy
in the reported location as well. Unfortunately, a lot of 
hiking trails are in dense forests and canyons, and it"s not that
uncommon to rain after hikers start hiking. To address this,
CenWits incorporates the idea of location points (LP). A
location point can update a sensor node with its current 
location whenever the node is near that LP. LPs are placed at
different locations in a wilderness area where GPS receivers
don"t work. An LP is a very simple device that emits 
prerecorded location information at some regular time interval.
It can be placed in difficult-to-reach places such as deep
canyons and dense rain forests by simply dropping them
from an airplane. LPs allow a sensor node to determine
its current location more accurately. However, they are not
183
an essential requirement of CenWits. If an LP runs out of
power, the CenWits will continue to work correctly.
Figure 5: GPS receiver not working correctly. 
Sensors then have to rely on LP to provide coordination
In Figure 5, B cannot get GPS reception due to bad
weather. It then runs into A on the trail who doesn"t have
GPS reception either. Their sensors record the presence of
each other. After 10 minutes, A is in range of an LP that
provides an accurate location information to A. When A
returns to trail head and uploads its data (Figure 6), the
system can draw a circle centered at the LP from which A
fetched location information for the range of encounter 
location of A and B. By Overlapping this circle with the trail
map, two or three possible location of encounter can be 
inferred. Thus when a rescue is required, the possible location
of B can be better inferred (See Figures 7 and 8).
Figure 6: A is back to trail head, It reports the
time of encounter with B to AP, but no location
information to AP
Figure 7: B is still missing after sunset. CenWits
infers the last contact point and draws the circle of
possible current locations based on average hiking
speed
CenWits requires that the clocks of different sensor nodes
be loosely synchronized with one another. Such a 
synchronization is trivial when GPS coverage is available. In 
addition, sensor nodes in CenWits synchronize their clocks
whenever they are in the range of an AP or an LP. The
Figure 8: Based on overlapping landscape, B might
have hiked to wrong branch and fallen off a cliff. Hot
rescue areas can thus be determined
synchronization accuracy Cenwits needs is of the order of a
second or so. As long as the clocks are synchronized with in
one second range, whether A met B at 12:37"45 or 12:37"46
doesn"t matter in the ordering of witness events and 
inferring the path.
4. MEMORY AND POWER MANAGEMENT
CenWits employs several important mechanisms to 
conserve power and memory. It is important to note while
current sensor nodes have limited amount of memory, 
future sensor nodes are expected to have much more memory.
With this in mind, the main focus in our design is to 
provide a tradeoff between the amount of memory available and
amount of power consumption.
4.1 Memory Management
The size of witness information stored at a node can get
very large. This is because the node may come across several
other nodes during a hike, and may end up accumulating a
large amount of witness information over time. To address
this problem, CenWits allows a node to pro-actively free up
some parts of its memory periodically. This raises an 
interesting question of when and which witness record should be
deleted from the memory of a node? CenWits uses three 
criteria to determine this: record count, hop count, and record
gap.
Record count refers to the number of witness records with
same node id that a node has stored in its memory. A node
maintains an integer parameter MAX RECORD COUNT.
It stores at most MAX RECORD COUNT witness records
of any node.
Every witness record has a hop count field that stores the
number times (hops) this record has been transferred since
being created. Initially this field is set to 0. Whenever a
node receives a witness record from another node, it 
increments the hop count of that record by 1. A node maintains
an integer parameter called MAX HOP COUNT. It keeps
only those witness records in its memory, whose hop count
is less than MAX HOP COUNT. The MAX HOP COUNT
parameter provides a balance between two conflicting goals:
(1) To ensure that a witness record has been propagated to
and thus stored at as many nodes as possible, so that it has
a high probability of being dumped at some AP as quickly
as possible; and (2) To ensure that a witness record is stored
only at a few nodes, so that it does not clog up too much of
the combined memory of all sensor nodes. We chose to use
hop count instead of time-to-live to decide when to drop a
packet. The main reason for this is that the probability of
a packet reaching an AP goes up as the hop count adds up.
For example, when the hop count if 5 for a specific record,
184
the record is in at least 5 sensor nodes. On the other hand,
if we discard old records, without considering hop count,
there is no guarantee that the record is present in any other
sensor node.
Record gap refers to the time difference between the record
times of two witness records with the same node id. To
save memory, a node n ensures the the record gap between
any two witness records with the same node id is at least
MIN RECORD GAP. For each node id i, n stores the 
witness record with the most recent record time rti, the witness
with most recent record time that is at least MIN RECORD GAP
time units before rti, and so on until the record count limit
(MAX RECORD COUNT) is reached.
When a node is tight in memory, it adjusts the three 
parameters, MAX RECORD COUNT, MAX HOP COUNT and
MIN RECORD GAP to free up some memory. It 
decrements MAX RECORD COUNT and MAX HOP COUNT,
and increments MIN RECORD GAP. It then first erases
all witness records whose hop count exceeds the reduced
MAX HOP COUNT value, and then erases witness records
to satisfy the record gap criteria. Also, when a node has 
extra memory space available, e.g. after dumping its witness
information at an access point, it resets MAX RECORD COUNT,
MAX HOP COUNT and MIN RECORD GAP to some 
predefined values.
4.2 Power Management
An important advantage of using sensors for tracking 
purposes is that we can regulate the behavior of a sensor node
based on current conditions. For example, we mentioned
earlier that a sensor should emit a beacon every 1.7 minute,
given a hiking speed of 1 mile/hour. However, if a user is
moving 10 feet/sec, a beacon should be emitted every 10
seconds. If a user is not moving at all, a beacon can be
emitted every 10 minutes. In the night, a sensor can be put
into sleep mode to save energy, when a user is not likely to
move at all for a relatively longer period of time. If a user
is active for only eight hours in a day, we can put the sensor
into sleep mode for the other 16 hours and thus save 2/3rd
of the energy.
In addition, a sensor node can choose to not send any
beacons during some time intervals. For example, suppose
hiker A has communicated its witness information to three
other hikers in the last five minutes. If it is running low
on power, it can go to receive mode or sleep mode for the
next ten minutes. It goes to receive mode if it is still willing
to receive additional witness information from hikers that it
encounters in the next ten minutes. It goes to sleep mode if
it is extremely low on power.
The bandwidth and energy limitations of sensor nodes
require that the amount of data transferred among the nodes
be reduced to minimum. It has been observed that in some
scenarios 3000 instructions could be executed for the same
energy cost of sending a bit 100m by radio [15]. To reduce
the amount of data transfer, CenWits employs a handshake
protocol that two nodes execute when they encounter one
another. The goal of this protocol is to ensure that a node
transmits only as much witness information as the other
node is willing to receive. This protocol is initiated when
a node i receives a beacon containing the node ID of the
sender node j and i has not exchanged witness information
with j in the last δ time units. Assume that i < j. The
protocol consists of four phases (See Figure 9):
1. Phase I: Node i sends its receive constraints and the
number of witness records it has in its memory.
2. Phase II: On receiving this message from i, j sends its
receive constraints and the number of witness records
it has in its memory.
3. Phase III: On receiving the above message from j, i
sends its witness information (filtered based on receive
constraints received in phase II).
4. Phase IV: After receiving the witness records from
i, j sends its witness information (filtered based on
receive constraints received in phase I).
j
<Constaints, Witness info size>
<Constaints, Witness info size>
<Filtered Witness info>
<Filtered Witness info>
i j
j
j
i
i
i
Figure 9: Four-Phase Hand Shake Protocol (i < j)
Receive constraints are a function of memory and power.
In the most general case, they are comprised of the three
parameters (record count, hop count and record gap) used
for memory management. If i is low on memory, it specifies
the maximum number of records it is willing to accept from
j. Similarly, i can ask j to send only those records that
have hop count value less than MAX HOP COUNT − 1.
Finally, i can include its MIN RECORD GAP value in its
receive constraints. Note that the handshake protocol is
beneficial to both i and j. They save memory by receiving
only as much information as they are willing to accept and
conserve energy by sending only as many witness records as
needed.
It turns out that filtering witness records based on
MIN RECORD GAP is complex. It requires that the 
witness records of any given node be arranged in an order sorted
by their record time values. Maintaining this sorted order is
complex in memory, because new witness records with the
same node id can arrive later that may have to be inserted
in between to preserve the sorted order. For this reason, the
receive constraints in the current CenWits prototype do not
include record gap.
Suppose i specifies a hop count value of 3. In this case,
j checks the hop count field of every witness record before
sending them. If the hop count value is greater than 3, the
record is not transmitted.
4.3 Groups and Partitions
To further reduce communication and increase the 
lifetime of our system, we introduce the notion of groups. The
idea is based on the concept of abstract regions presented
in [20]. A group is a set of n nodes that can be defined
in terms of radio connectivity, geographic location, or other
properties of nodes. All nodes within a group can 
communicate directly with one another and they share information
to maintain their view of the external world. At any point
in time, a group has exactly one leader that communicates
185
with external nodes on behalf of the entire group. A group
can be static, meaning that the group membership does not
change over the period of time, or it could be dynamic in
which case nodes can leave or join the group. To make our
analysis simple and to explain the advantages of group, we
first discuss static groups.
A static group is formed at the start of a hiking trail or ski
slope. Suppose there are five family members who want to
go for a hike in the Rocky Mountain National Park. Before
these members start their hike, each one of them is given
a sensor node and the information is entered in the system
that the five nodes form a group. Each group member is
given a unique id and every group member knows about
other members of the group. The group, as a whole, is also
assigned an id to distinguish it from other groups in the
system.
Figure 10: A group of five people. Node 2 is the
group leader and it is communicating on behalf of
the group with an external node 17. All other
(shown in a lighter shade) are in sleep mode.
As the group moves through the trail, it exchanges 
information with other nodes or groups that it comes across. At
any point in time, only one group member, called the leader,
sends and receives information on behalf of the group and
all other n − 1 group members are put in the sleep mode
(See Figure 10). It is this property of groups that saves
us energy. The group leadership is time-multiplexed among
the group members. This is done to make sure that a single
node does not run out of battery due to continuous exchange
of information. Thus after every t seconds, the leadership
is passed on to another node, called the successor, and the
leader (now an ordinary member) is put to sleep. Since
energy is dear, we do not implement an extensive election
algorithm for choosing the successor. Instead, we choose the
successor on the basis of node id. The node with the next
highest id in the group is chosen as the successor. The last
node, of course, chooses the node with the lowest id as its
successor.
We now discuss the data storage schemes for groups. 
Memory is a scarce resource in sensor nodes and it is therefore 
important that witness information be stored efficiently among
group members. Efficient data storage is not a trivial task
when it comes to groups. The tradeoff is between simplicity
of the scheme and memory savings. A simpler scheme 
incurs lesser energy cost as compared to a more sophisticated
scheme, but offers lesser memory savings as well. This is
because in a more complicated scheme, the group members
have to coordinate to update and store information. After
considering a number of different schemes, we have come
to a conclusion that there is no optimal storage scheme for
groups. The system should be able to adapt according to
its requirements. If group members are low on battery, then
the group can adapt a scheme that is more energy efficient.
Similarly, if the group members are running out of memory,
they can adapt a scheme that is more memory efficient. We
first present a simple scheme that is very energy efficient but
does not offer significant memory savings. We then present
an alternate scheme that is much more memory efficient.
As already mentioned a group can receive information
only through the group leader. Whenever the leader comes
across an external node e, it receives information from that
node and saves it. In our first scheme, when the timeslot for
the leader expires, the leader passes this new information it
received from e to its successor. This is important because
during the next time slot, if the new leader comes across
another external node, it should be able to pass 
information about all the external nodes this group has witnessed
so far. Thus the information is fully replicated on all nodes
to maintain the correct view of the world.
Our first scheme does not offer any memory savings but is
highly energy efficient and may be a good choice when the
group members are running low on battery. Except for the
time when the leadership is switched, all n − 1 members are
asleep at any given time. This means that a single member
is up for t seconds once every n∗t seconds and therefore has
to spend approximately only 1/nth
of its energy. Thus, if
there are 5 members in a group, we save 80% energy, which
is huge. More energy can be saved by increasing the group
size.
We now present an alternate data storage scheme that
aims at saving memory at the cost of energy. In this scheme
we divide the group into what we call partitions. Partitions
can be thought of as subgroups within a group. Each 
partition must have at least two nodes in it. The nodes within a
partition are called peers. Each partition has one peer 
designated as partition leader. The partition leader stays in 
receive mode at all times, while all others peers a partition stay
in the sleep mode. Partition leadership is time-multiplexed
among the peers to make sure that a single node does not
run out of battery. Like before, a group has exactly one
leader and the leadership is time-multiplexed among 
partitions. The group leader also serves as the partition leader
for the partition it belongs to (See Figure 11).
In this scheme, all partition leaders participate in 
information exchange. Whenever a group comes across an external
node e, every partition leader receives all witness 
information, but it only stores a subset of that information after
filtering. Information is filtered in such a way that each
partition leader has to store only B/K bytes of data, where
K is the number of partitions and B is the total number
of bytes received from e. Similarly when a group wants to
send witness information to e, each partition leader sends
only B/K bytes that are stored in the partition it belongs
to. However, before a partition leader can send information,
it must switch from receive mode to send mode. Also, 
partition leaders must coordinate with one another to ensure
that they do not send their witness information at the same
time, i.e. their message do not collide. All this is achieved
by having the group leader send a signal to every partition
leader in turn.
186
Figure 11: The figure shows a group of eight nodes
divided into four partitions of 2 nodes each. Node
1 is the group leader whereas nodes 2, 9, and 7 are
partition leaders. All other nodes are in the sleep
mode.
Since the partition leadership is time-multiplexed, it is
important that any information received by the partition
leader, p1, be passed on to the next leader, p2. This has
to be done to make sure that p2 has all the information
that it might need to send when it comes across another
external node during its timeslot. One way of achieving this
is to wake p2 up just before p1"s timeslot expires and then
have p1 transfer information only to p2. An alternate is to
wake all the peers up at the time of leadership change, and
then have p1 broadcast the information to all peers. Each
peer saves the information sent by p1 and then goes back
to sleep. In both cases, the peers send acknowledgement to
the partition leader after receiving the information. In the
former method, only one node needs to wake up at the time
of leadership change, but the amount of information that
has to be transmitted between the nodes increases as time
passes. In the latter case, all nodes have to be woken up at
the time of leadership change, but small piece of information
has to be transmitted each time among the peers. Since
communication is much more expensive than bringing the
nodes up, we prefer the second method over the first one.
A group can be divided into partitions in more than one
way. For example, suppose we have a group of six members.
We can divide this group into three partitions of two peers
each, or two partitions with three peers each. The choice
once again depends on the requirements of the system. A
few big partitions will make the system more energy efficient.
This is because in this configuration, a greater number of
nodes will stay in sleep mode at any given point in time.
On the other hand, several small partitions will make the
system memory efficient, since each node will have to store
lesser information (See Figure 12).
A group that is divided into partitions must be able to
readjust itself when a node leaves or runs out of battery.
This is crucial because a partition must have at least two
nodes at any point in time to tolerate failure of one node.
For example, in figure 3 (a), if node 2 or node 5 dies, the
partition is left with only one node. Later on, if that single
node in the partition dies, all witness information stored in
that partition will be lost. We have devised a very simple
protocol to solve this problem. We first explain how 
partiFigure 12: The figure shows two different ways of
partitioning a group of six nodes. In (a), a group
is divided into three partitions of two nodes. Node
1 is the group leader, nodes 9 and 5 are partition
leaders, and nodes 2, 3, and 6 are in sleep mode. In
(b) the group is divided into two partitions of three
nodes. Node 1 is the group leader, node 9 is the
partition leader and nodes 2, 3, 5, and 6 are in sleep
mode.
tions are adjusted when a peer dies, and then explain what
happens if a partition leader dies.
Suppose node 2 in figure 3 (a) dies. When node 5, the
partition leader, sends information to node 2, it does not 
receive an acknowledgement from it and concludes that node
2 has died2
. At this point, node 5 contacts other partition
leaders (nodes 1 ... 9) using a broadcast message and 
informs them that one of its peers has died. Upon hearing
this, each partition leader informs node 5 (i) the number of
nodes in its partition, (ii) a candidate node that node 5 can
take if the number of nodes in its partition is greater than
2, and (iii) the amount of witness information stored in its
partition. Upon hearing from every leader, node 5 chooses
the candidate node from the partition with maximum 
number (must be greater than 2) of peers, and sends a message
back to all leaders. Node 5 then sends data to its new peer
to make sure that the information is replicated within the
partition.
However, if all partitions have exactly two nodes, then
node 5 must join another partition. It chooses the partition
that has the least amount of witness information to join. It
sends its witness information to the new partition leader.
Witness information and membership update is propagated
to all peers during the next partition leadership change.
We now consider the case where the partition leader dies.
If this happens, then we wait for the partition leadership to
change and for the new partition leader to eventually find
out that a peer has died. Once the new partition leader finds
out that it needs more peers, it proceeds with the protocol
explained above. However, in this case, we do lose 
information that the previous partition leader might have received
just before it died. This problem can be solved by 
implementing a more rigorous protocol, but we have decided to
give up on accuracy to save energy.
Our current design uses time-division multiplexing to 
schedule wakeup and sleep modes in the sensor nodes. However,
recent work on radio wakeup sensors [10] can be used to do
this scheduling more efficiently. we plan to incorporate radio
wakeup sensors in CenWits when the hardware is mature.
2
The algorithm to conclude that a node has died can be
made more rigorous by having the partition leader query
the suspected node a few times.
187
5. SYSTEM EVALUATION
A sensor is constrained in the amount of memory and
power. In general, the amount of memory needed and power
consumption depends on a variety of factors such as node
density, number of hiker encounters, and the number of 
access points. In this Section, we provide an estimate of how
long the power of a MICA2 mote will last under certain
assumtions.
First, we assume that each sensor node carries about 100
witness records. On encountering another hiker, a sensor
node transmits 50 witness records and receives 50 new 
witness records. Since, each record is 16 bytes long, it will take
0.34 seconds to transmit 50 records and another 0.34 
seconds to receive 50 records over a 19200 bps link. The power
consumption of MICA2 due to CPU processing, 
transmission and reception are approximately 8.0 mA, 7.0 mA and
8.5 mA per hour respectively [18], and the capacity of an
alkaline battery is 2500mAh.
Since the radio module of Mica2 is half-duplex and 
assuming that the CPU is always active when a node is awake,
power consumption due to transmission is 8 + 8.5 = 16.5
mA per hour and due to reception is 8 + 7 = 15mA per
hour. So, average power consumtion due to transmission
and reception is (16.5 + 15)/2 = 15.75 mA per hour.
Given that the capacity of an alkaline battery is 2500
mAh, a battery should last for 2500/15.75 = 159 hours of
transmission and reception. An encounter between two 
hikers results in exchange of about 50 witness records that takes
about 0.68 seconds as calculated above. Thus, a single 
alkaline battery can last for (159 ∗ 60 ∗ 60)/0.68 = 841764 hiker
encounters.
Assuming that a node emits a beacon every 90 seconds
and a hiker encounter occurs everytime a beacon is emitted
(worst-case scenario), a single alkaline battery will last for
(841764 ∗ 90)/(30 ∗ 24 ∗ 60 ∗ 60) = 29 days. Since, a Mica2
is equipped with two batteries, a Mica2 sensor can remain
operation for about two months. Notice that this 
calculation is preliminary, because it assumes that hikers are active
24 hours of the day and a hiker encounter occurs every 90
seconds. In a more realistic scenario, power is expected to
last for a much longer time period. Also, this time period
will significantly increase when groups of hikers are moving
together.
Finally, the lifetime of a sensor running on two 
batteries can definitely be increased significantly by using energy
scavenging techniques and energy harvesting techniques [16,
14].
6. PROTOTYPE IMPLEMENTATION
We have implemented a prototype of CenWits on MICA2
sensor 900MHz running Mantis OS 0.9.1b. One of the sensor
is equipped with MTS420CA GPS module, which is capable
of barometric pressure and two-axis acceleration sensing in
addition to GPS location tracking. We use SiRF, the serial
communication protocol, to control GPS module. SiRF has
a rich command set, but we record only X and Y coordinates.
A witness record is 16 bytes long. When a node starts up, it
stores its current location and emits a beacon 
periodicallyin the prototype, a node emits a beacon every minute.
We have conducted a number of experiments with this
prototype. A detailed report on these experiments with the
raw data collected and photographs of hikers, access points
etc. is available at http://csel.cs.colorado.edu/∼huangjh/
Cenwits.index.htm. Here we report results from three of
them. In all these experiments, there are three access points
(A, B and C) where nodes dump their witness information.
These access points also provide location information to the
nodes that come with in their range. We first show how 
CenWits can be used to determine the hiking trail a hiker is most
likely on and the speed at which he is hiking, and identify
hot search areas in case he is reported missing. Next, we
show the results of power and memory management 
techniques of CenWits in conserving power and memory of a
sensor node in one of our experiments.
6.1 Locating Lost Hikers
The first experiment is called Direct Contact. It is a very
simple experiment in which a single hiker starts from A,
goes to B and then C, and finally returns to A (See Figure
13). The goal of this experiment is to illustrate that 
CenWits can deduce the trail a hiker takes by processing witness
information.
Figure 13: Direct Contact Experiment
Node Id Record (X,Y) Location Hop
Time Time Count
1 15 (12,7) 15 0
1 33 (31,17) 33 0
1 46 (12,23) 46 0
1 10 (12,7) 10 0
1 48 (12,23) 48 0
1 16 (12,7) 16 0
1 34 (31,17) 34 0
Table 1: Witness information collected in the direct
contact experiment.
The witness information dumped at the three access points
was then collected and processed at a control center. Part
of the witness information collected at the control center is
shown in Table 1. The X,Y locations in this table 
correspond to the location information provided by access points
A, B, and C. A is located at (12,7), B is located at (31,17)
and C is located at (12,23). Three encounter points 
(between hiker 1 and the three access points) extracted from
188
this witness information are shown in Figure 13 (shown in
rectangular boxes). For example, A,1 at 16 means 1 came in
contact with A at time 16. Using this information, we can
infer the direction in which hiker 1 was moving and speed at
which he was moving. Furthermore, given a map of hiking
trails in this area, it is clearly possible to identify the hiking
trail that hiker 1 took.
The second experiment is called Indirect Inference. This
experiment is designed to illustrate that the location, 
direction and speed of a hiker can be inferred by CenWits, even
if the hiker never comes in the range of any access point. It
illustrates the importance of witness information in search
and rescue applications. In this experiment, there are three
hikers, 1, 2 and 3. Hiker 1 takes a trail that goes along 
access points A and B, while hiker 3 takes trail that goes along
access points C and B. Hiker 2 takes a trail that does not
come in the range of any access points. However, this hiker
meets hiker 1 and 3 during his hike. This is illustrated in
Figure 14.
Figure 14: Indirect Inference Experiment
Node Id Record (X,Y) Location Hop
Time Time Count
2 16 (12,7) 6 0
2 15 (12,7) 6 0
1 4 (12,7) 4 0
1 6 (12,7) 6 0
1 29 (31,17) 29 0
1 31 (31,17) 31 0
Table 2: Witness information collected from hiker 1
in indirect inference experiment.
Part of the witness information collected at the control
center from access points A, B and C is shown in Tables
2 and 3. There are some interesting data in these tables.
For example, the location time in some witness records is
not the same as the record time. This means that the node
that generated that record did not have its most up-to-date
location at the encounter time. For example, when hikers
1 and 2 meet at time 16, the last recorded location time of
Node Id Record (X,Y) Location Hop
Time Time Count
3 78 (12,23) 78 0
3 107 (31,17) 107 0
3 106 (31,17) 106 0
3 76 (12,23) 76 0
3 79 (12,23) 79 0
2 94 (12,23) 79 0
1 16 (?,?) ? 1
1 15 (?,?) ? 1
Table 3: Witness information collected from hiker 3
in indirect inference experiment.
hiker 1 is (12,7) recorded at time 6. So, node 1 generates
a witness record with record time 16, location (12,7) and
location time 6. In fact, the last two records in Table 3
have (?,?) as their location. This has happened because
these witness records were generate by hiker 2 during his
encounter with 1 at time 15 and 16. Until this time, hiker
2 hadn"t come in contact with any location points.
Interestingly, a more accurate location information of 1
and 2 encounter or 2 and 3 encounter can be computed by
process the witness information at the control center. It
took 25 units of time for hiker 1 to go from A (12,7) to B
(31,17). Assuming a constant hiking speed and a relatively
straight-line hike, it can be computed that at time 16, hiker
1 must have been at location (18,10). Thus (18,10) is a more
accurate location of encounter between 1 and 2.
Finally, our third experiment called Identifying Hot Search
Areas is designed to determine the trail a hiker has taken
and identify hot search areas for rescue after he is reported
missing. There are six hikers (1, 2, 3, 4, 5 and 6) in this
experiment. Figure 15 shows the trails that hikers 1, 2,
3, 4 and 5 took, along with the encounter points obtained
from witness records collected at the control center. For
brevity, we have not shown the entire witness information
collected at the control center. This information is available
at http://csel.cs.colorado.edu/∼huangjh/Cenwits/index.htm.
Figure 15: Identifying Hot Search Area Experiment
(without hiker 6)
189
Now suppose hiker 6 is reported missing at time 260. To
determine the hot search areas, the witness records of hiker
6 are processed to determine the trail he is most likely on,
the speed at which he had been moving, direction in which
he had been moving, and his last known location. Based on
this information and the hiking trail map, hot search areas
are identified. The hiking trail taken by hiker 6 as inferred
by CenWits is shown by a dotted line and the hot search
areas identified by CenWits are shown by dark lines inside
the dotted circle in Figure 16.
Figure 16: Identifying Hot Search Area Experiment
(with hiker 6)
6.2 Results of Power and Memory 
Management
The witness information shown in Tables 1, 2 and 3 has
not been filtered using the three criteria described in 
Section 4.1. For example, the witness records generated by 3 at
record times 76, 78 and 79 (see Table 3) have all been 
generated due a single contact between access point C and node
3. By applying the record gap criteria, two of these three
records will be erased. Similarly, the witness records 
generated by 1 at record times 10, 15 and 16 (see Table 1) have
all been generated due a single contact between access point
A and node 1. Again, by applying the record gap criteria,
two of these three records will be erased. Our experiments
did not generate enough data to test the impact of record
count or hop count criteria.
To evaluate the impact of these criteria, we simulated 
CenWits to generate a significantly large number of records for a
given number of hikers and access points. We generated 
witness records by having the hikers walk randomly. We applied
the three criteria to measure the amount of memory savings
in a sensor node. The results are shown in Table 4. The
number of hikers in this simulation was 10 and the number
of access points was 5. The number of witness records 
reported in this table is an average number of witness records
a sensor node stored at the time of dump to an access point.
These results show that the three memory management
criteria significantly reduces the memory consumption of
sensor nodes in CenWits. For example, they can reduce
MAX MIN MAX # of
RECORD RECORD HOP Witness
COUNT GAP COUNT Records
5 5 5 628
4 5 5 421
3 5 5 316
5 10 5 311
5 20 5 207
5 5 4 462
5 5 3 341
3 20 3 161
Table 4: Impact of memory management techniques.
the memory consumption by up to 75%. However, these
results are premature at present for two reasons: (1) They
are generated via simulation of hikers walking at random;
and (2) It is not clear what impact the erasing of witness
records has on the accuracy of inferred location/hot search
areas of lost hikers. In our future work, we plan to undertake
a major study to address these two concerns.
7. OTHER APPLICATIONS
In addition to the hiking in wilderness areas, CenWits can
be used in several other applications, e.g. skiing, climbing,
wild life monitoring, and person tracking. Since CenWits 
relies only on intermittent connectivity, it can take advantage
of the existing cheap and mature technologies, and thereby
make tracking cheaper and fairly accurate. Since CenWits
doesn"t rely on keeping track of a sensor holder all time,
but relies on maintaining witnesses, the system is relatively
cheaper and widely applicable. For example, there are some
dangerous cliffs in most ski resorts. But it is too expensive
for a ski resort to deploy a connected wireless sensor network
through out the mountain. Using CenWits, we can deploy
some sensors at the cliff boundaries. These boundary 
sensors emit beacons quite frequently, e.g. every second, and
so can record presence of skiers who cross the boundary and
fall off the cliff. Ski patrols can cruise the mountains every
hour, and automatically query the boundary sensor when in
range using PDAs. If a PDA shows that a skier has been
close to the boundary sensor, the ski patrol can use a long
range walkie-talkie to query control center at the resort base
to check the witness record of the skier. If there is no 
witness record after the recorded time in the boundary sensor,
there is a high chance that a rescue is needed.
In wildlife monitoring, a very popular method is to attach
a GPS receiver on the animals. To collect data, either a
satellite transmitter is used, or the data collector has to
wait until the GPS receiver brace falls off (after a year or so)
and then search for the GPS receiver. GPS transmitters are
very expensive, e.g. the one used in geese tracking is $3,000
each [2]. Also, it is not yet known if continuous radio signal
is harmful to the birds. Furthermore, a GPS transmitter is
quite bulky and uncomfortable, and as a result, birds always
try to get rid of it. Using CenWits, not only can we record
the presence of wildlife, we can also record the behavior
of wild animals, e.g. lions might follow the migration of
deers. CenWits does nor require any bulky and expensive
satellite transmitters, nor is there a need to wait for a year
and search for the braces. CenWits provides a very simple
and cost-effective solution in this case. Also, access points
190
can be strategically located, e.g. near a water source, to
increase chances of collecting up-to-date data. In fact, the
access points need not be statically located. They can be
placed in a low-altitude plane (e.g a UAV) and be flown over
a wilderness area to collect data from wildlife.
In large cities, CenWits can be used to complement GPS,
since GPS doesn"t work indoor and near skyscrapers. If a
person A is reported missing, and from the witness records
we find that his last contacts were C and D, we can trace
an approximate location quickly and quite efficiently.
8. DISCUSSION AND FUTURE WORK
This paper presents a new search and rescue system called
CenWits that has several advantages over the current search
and rescue systems. These advantages include a 
looselycoupled system that relies only on intermittent network 
connectivity, power and storage efficiency, and low cost. It
solves one of the greatest problems plaguing modern search
and rescue systems: it has an inherent on-site storage 
capability. This means someone within the network will have
access to the last-known-location information of a victim,
and perhaps his bearing and speed information as well. It
utilizes the concept of witnesses to propagate information,
infer current possible location and speed of a subject, and
identify hot search and rescue areas in case of emergencies.
A large part of CenWits design focuses on addressing the
power and memory limitations of current sensor nodes. In
fact, power and memory constraints depend on how much
weight (of sensor node) a hiker is willing to carry and the
cost of these sensors. An important goal of CenWits is build
small chips that can be implanted in hiking boots or ski
jackets. This goal is similar to the avalanche beacons that
are currently implanted in ski jackets. We anticipate that
power and memory will continue to be constrained in such
an environment.
While the paper focuses on the development of a search
and rescue system, it also provides some innovative, 
systemlevel ideas for information processing in a sensor network
system.
We have developed and experimented with a basic 
prototype of CenWits at present. Future work includes 
developing a more mature prototype addressing important issues
such as security, privacy, and high availability. There are
several pressing concerns regarding security, privacy, and
high availability in CenWits. For example, an adversary
can sniff the witness information to locate endangered 
animals, females, children, etc. He may inject false information
in the system. An individual may not be comfortable with
providing his/her location and movement information, even
though he/she is definitely interested in being located in a
timely manner at the time of emergency. In general, people
in hiking community are friendly and usually trustworthy.
So, a bullet-proof security is not really required. However,
when CenWits is used in the context of other applications,
security requirements may change. Since the sensor nodes
used in CenWits are fragile, they can fail. In fact, the nature
and level of security, privacy and high availability support
needed in CenWits strongly depends on the application for
which it is being used and the individual subjects involved.
Accordingly, we plan to design a multi-level support for 
security, privacy and high availability in CenWits.
So far, we have experimented with CenWits in a very
restricted environment with a small number of sensors. Our
next goal is to deploy this system in a much larger and more
realistic environment. In particular, discussions are currenly
underway to deploy CenWits in the Rocky Mountain and
Yosemite National Parks.
9. REFERENCES
[1] 802.11-based tracking system.
http://www.pangonetworks.com/locator.htm.
[2] Brent geese 2002. http://www.wwt.org.uk/brent/.
[3] The onstar system. http://www.onstar.com.
[4] Personal locator beacons with GPS receiver and
satellite transmitter. http://www.aeromedix.com/.
[5] Personal tracking using GPS and GSM system.
http://www.ulocate.com/trimtrac.html.
[6] Rf based kid tracking system.
http://www.ion-kids.com/.
[7] F. Alessio. Performance measurements with motes
technology. MSWiM"04, 2004.
[8] P. Bahl and V. N. Padmanabhan. RADAR: An
in-building RF-based user location and tracking
system. IEEE Infocom, 2000.
[9] K. Fall. A delay-tolerant network architecture for
challenged internets. In SIGCOMM, 2003.
[10] L. Gu and J. Stankovic. Radio triggered wake-up
capability for sensor networks. In Real-Time
Applications Symposium, 2004.
[11] J. Hightower and G. Borriello. Location systems for
ubiquitous computing. IEEE Computer, 2001.
[12] W. Jaskowski, K. Jedrzejek, B. Nyczkowski, and
S. Skowronek. Lifetch life saving system. CSIDC, 2004.
[13] P. Juang, H. Oki, Y. Wang, M. Martonosi, L. Peh,
and D. Rubenstein. Energy-efficient computing for
wildlife tracking: design tradeoffs and early
experiences with ZebraNet. In ASPLOS, 2002.
[14] K. Kansal and M. Srivastava. Energy harvesting aware
power management. In Wireless Sensor Networks: A
Systems Perspective, 2005.
[15] G. J. Pottie and W. J. Kaiser. Embedding the
internet: wireless integrated network sensors.
Communications of the ACM, 43(5), May 2000.
[16] S. Roundy, P. K. Wright, and J. Rabaey. A study of
low-level vibrations as a power source for wireless
sensor networks. Computer Communications, 26(11),
2003.
[17] C. Savarese, J. M. Rabaey, and J. Beutel. Locationing
in distributed ad-hoc wireless sensor networks.
ICASSP, 2001.
[18] V. Shnayder, M. Hempstead, B. Chen, G. Allen, and
M. Welsh. Simulating the power consumption of
large-scale sensor network applications. In Sensys,
2004.
[19] R. Want and A. Hopper. Active badges and personal
interactive computing objects. IEEE Transactions of
Consumer Electronics, 1992.
[20] M. Welsh and G. Mainland. Programming sensor
networks using abstract regions. First USENIX/ACM
Symposium on Networked Systems Design and
Implementation (NSDI "04), 2004.
191
MSP: Multi-Sequence Positioning of Wireless Sensor Nodes∗
Ziguo Zhong
Computer Science and Engineering
University of Minnesota
zhong@cs.umn.edu
Tian He
Computer Science and Engineering
University of Minnesota
tianhe@cs.umn.edu
Abstract
Wireless Sensor Networks have been proposed for use in
many location-dependent applications. Most of these need to
identify the locations of wireless sensor nodes, a challenging
task because of the severe constraints on cost, energy and 
effective range of sensor devices. To overcome limitations in 
existing solutions, we present a Multi-Sequence Positioning (MSP)
method for large-scale stationary sensor node localization in
outdoor environments. The novel idea behind MSP is to 
reconstruct and estimate two-dimensional location information for
each sensor node by processing multiple one-dimensional node
sequences, easily obtained through loosely guided event 
distribution. Starting from a basic MSP design, we propose four
optimizations, which work together to increase the localization
accuracy. We address several interesting issues, such as 
incomplete (partial) node sequences and sequence flip, found in the
Mirage test-bed we built. We have evaluated the MSP system
through theoretical analysis, extensive simulation as well as
two physical systems (an indoor version with 46 MICAz motes
and an outdoor version with 20 MICAz motes). This 
evaluation demonstrates that MSP can achieve an accuracy within
one foot, requiring neither additional costly hardware on 
sensor nodes nor precise event distribution. It also provides a nice
tradeoff between physical cost (anchors) and soft cost (events),
while maintaining localization accuracy.
Categories and Subject Descriptors
C.2.4 [Computer Communications Networks]: 
Distributed Systems
General Terms
Algorithms, Measurement, Design, Performance, 
Experimentation
1 Introduction
Although Wireless Sensor Networks (WSN) have shown
promising prospects in various applications [5], researchers
still face several challenges for massive deployment of such
networks. One of these is to identify the location of 
individual sensor nodes in outdoor environments. Because of 
unpredictable flow dynamics in airborne scenarios, it is not currently
feasible to localize sensor nodes during massive UVA-based
deployment. On the other hand, geometric information is 
indispensable in these networks, since users need to know where
events of interest occur (e.g., the location of intruders or of a
bomb explosion).
Previous research on node localization falls into two 
categories: range-based approaches and range-free approaches.
Range-based approaches [13, 17, 19, 24] compute per-node
location information iteratively or recursively based on 
measured distances among target nodes and a few anchors which
precisely know their locations. These approaches generally
require costly hardware (e.g., GPS) and have limited 
effective range due to energy constraints (e.g., ultrasound-based
TDOA [3, 17]). Although range-based solutions can be 
suitably used in small-scale indoor environments, they are 
considered less cost-effective for large-scale deployments. On the
other hand, range-free approaches [4, 8, 10, 13, 14, 15] do not
require accurate distance measurements, but localize the node
based on network connectivity (proximity) information. 
Unfortunately, since wireless connectivity is highly influenced by the
environment and hardware calibration, existing solutions fail
to deliver encouraging empirical results, or require substantial
survey [2] and calibration [24] on a case-by-case basis.
Realizing the impracticality of existing solutions for the
large-scale outdoor environment, researchers have recently
proposed solutions (e.g., Spotlight [20] and Lighthouse [18])
for sensor node localization using the spatiotemporal 
correlation of controlled events (i.e., inferring nodes" locations based
on the detection time of controlled events). These solutions
demonstrate that long range and high accuracy localization can
be achieved simultaneously with little additional cost at 
sensor nodes. These benefits, however, come along with an 
implicit assumption that the controlled events can be precisely
distributed to a specified location at a specified time. We argue
that precise event distribution is difficult to achieve, especially
at large scale when terrain is uneven, the event distribution 
device is not well calibrated and its position is difficult to 
maintain (e.g., the helicopter-mounted scenario in [20]).
To address these limitations in current approaches, in this
paper we present a multi-sequence positioning (MSP) method
15
for large-scale stationary sensor node localization, in 
deployments where an event source has line-of-sight to all sensors.
The novel idea behind MSP is to estimate each sensor node"s
two-dimensional location by processing multiple easy-to-get
one-dimensional node sequences (e.g., event detection order)
obtained through loosely-guided event distribution.
This design offers several benefits. First, compared to a
range-based approach, MSP does not require additional costly
hardware. It works using sensors typically used by sensor 
network applications, such as light and acoustic sensors, both of
which we specifically consider in this work. Second, compared
to a range-free approach, MSP needs only a small number of
anchors (theoretically, as few as two), so high accuracy can be
achieved economically by introducing more events instead of
more anchors. And third, compared to Spotlight, MSP does not
require precise and sophisticated event distribution, an 
advantage that significantly simplifies the system design and reduces
calibration cost.
This paper offers the following additional intellectual 
contributions:
• We are the first to localize sensor nodes using the concept
of node sequence, an ordered list of sensor nodes, sorted
by the detection time of a disseminated event. We 
demonstrate that making full use of the information embedded
in one-dimensional node sequences can significantly 
improve localization accuracy. Interestingly, we discover
that repeated reprocessing of one-dimensional node 
sequences can further increase localization accuracy.
• We propose a distribution-based location estimation 
strategy that obtains the final location of sensor nodes using
the marginal probability of joint distribution among 
adjacent nodes within the sequence. This new algorithm 
outperforms the widely adopted Centroid estimation [4, 8].
• To the best of our knowledge, this is the first work to
improve the localization accuracy of nodes by adaptive
events. The generation of later events is guided by 
localization results from previous events.
• We evaluate line-based MSP on our new Mirage test-bed,
and wave-based MSP in outdoor environments. Through
system implementation, we discover and address several
interesting issues such as partial sequence and sequence
flips. To reveal MSP performance at scale, we provide
analytic results as well as a complete simulation study.
All the simulation and implementation code is available
online at http://www.cs.umn.edu/∼zhong/MSP.
The rest of the paper is organized as follows. Section 2
briefly surveys the related work. Section 3 presents an
overview of the MSP localization system. In sections 4 and 5,
basic MSP and four advanced processing methods are 
introduced. Section 6 describes how MSP can be applied in a wave
propagation scenario. Section 7 discusses several 
implementation issues. Section 8 presents simulation results, and Section 9
reports an evaluation of MSP on the Mirage test-bed and an
outdoor test-bed. Section 10 concludes the paper.
2 Related Work
Many methods have been proposed to localize wireless 
sensor devices in the open air. Most of these can be 
classified into two categories: range-based and range-free 
localization. Range-based localization systems, such as GPS [23],
Cricket [17], AHLoS [19], AOA [16], Robust 
Quadrilaterals [13] and Sweeps [7], are based on fine-grained 
point-topoint distance estimation or angle estimation to identify 
pernode location. Constraints on the cost, energy and hardware
footprint of each sensor node make these range-based 
methods undesirable for massive outdoor deployment. In addition,
ranging signals generated by sensor nodes have a very limited
effective range because of energy and form factor concerns.
For example, ultrasound signals usually effectively propagate
20-30 feet using an on-board transmitter [17]. Consequently,
these range-based solutions require an undesirably high 
deployment density. Although the received signal strength 
indicator (RSSI) related [2, 24] methods were once considered
an ideal low-cost solution, the irregularity of radio 
propagation [26] seriously limits the accuracy of such systems. The
recently proposed RIPS localization system [11] superimposes
two RF waves together, creating a low-frequency envelope that
can be accurately measured. This ranging technique performs
very well as long as antennas are well oriented and 
environmental factors such as multi-path effects and background noise
are sufficiently addressed.
Range-free methods don"t need to estimate or measure 
accurate distances or angles. Instead, anchors or controlled-event
distributions are used for node localization. Range-free 
methods can be generally classified into two types: anchor-based
and anchor-free solutions.
• For anchor-based solutions such as Centroid [4], APIT
[8], SeRLoc [10], Gradient [13] , and APS [15], the main
idea is that the location of each node is estimated based on
the known locations of the anchor nodes. Different anchor
combinations narrow the areas in which the target nodes
can possibly be located. Anchor-based solutions normally
require a high density of anchor nodes so as to achieve
good accuracy. In practice, it is desirable to have as few
anchor nodes as possible so as to lower the system cost.
• Anchor-free solutions require no anchor nodes. Instead,
external event generators and data processing platforms
are used. The main idea is to correlate the event detection
time at a sensor node with the known space-time 
relationship of controlled events at the generator so that detection
time-stamps can be mapped into the locations of sensors.
Spotlight [20] and Lighthouse [18] work in this fashion.
In Spotlight [20], the event distribution needs to be 
precise in both time and space. Precise event distribution
is difficult to achieve without careful calibration, 
especially when the event-generating devices require certain
mechanical maneuvers (e.g., the telescope mount used in
Spotlight). All these increase system cost and reduce 
localization speed. StarDust [21], which works much faster,
uses label relaxation algorithms to match light spots 
reflected by corner-cube retro-reflectors (CCR) with sensor
nodes using various constraints. Label relaxation 
algorithms converge only when a sufficient number of robust
constraints are obtained. Due to the environmental impact
on RF connectivity constraints, however, StarDust is less
accurate than Spotlight.
In this paper, we propose a balanced solution that avoids
the limitations of both anchor-based and anchor-free solutions.
Unlike anchor-based solutions [4, 8], MSP allows a flexible
tradeoff between the physical cost (anchor nodes) with the soft
16
1
A
B
2
3
4
5
Target nodeAnchor node
1A 5 3 B2 4
1 B2 5A 43
1A25B4 3
1 52 AB 4 3
1
2
3
5
4
(b)
(c)(d)
(a)
Event 1
Node Sequence generated by event 1
Event 3
Node Sequence generated by event 2
Node Sequence generated by event 3
Node Sequence generated by event 4
Event 2 Event 4
Figure 1. The MSP System Overview
cost (localization events). MSP uses only a small number of
anchors (theoretically, as few as two). Unlike anchor-free 
solutions, MSP doesn"t need to maintain rigid time-space 
relationships while distributing events, which makes system design
simpler, more flexible and more robust to calibration errors.
3 System Overview
MSP works by extracting relative location information from
multiple simple one-dimensional orderings of nodes. 
Figure 1(a) shows a layout of a sensor network with anchor nodes
and target nodes. Target nodes are defined as the nodes to be
localized. Briefly, the MSP system works as follows. First,
events are generated one at a time in the network area (e.g.,
ultrasound propagations from different locations, laser scans
with diverse angles). As each event propagates, as shown in
Figure 1(a), each node detects it at some particular time 
instance. For a single event, we call the ordering of nodes, which
is based on the sequential detection of the event, a node 
sequence. Each node sequence includes both the targets and the
anchors as shown in Figure 1(b). Second, a multi-sequence
processing algorithm helps to narrow the possible location of
each node to a small area (Figure 1(c)). Finally, a 
distributionbased estimation method estimates the exact location of each
sensor node, as shown in Figure 1(d).
Figure 1 shows that the node sequences can be obtained
much more economically than accurate pair-wise distance
measurements between target nodes and anchor nodes via 
ranging methods. In addition, this system does not require a rigid
time-space relationship for the localization events, which is
critical but hard to achieve in controlled event distribution 
scenarios (e.g., Spotlight [20]).
For the sake of clarity in presentation, we present our system
in two cases:
• Ideal Case, in which all the node sequences obtained
from the network are complete and correct, and nodes are
time-synchronized [12, 9].
• Realistic Deployment, in which (i) node sequences can
be partial (incomplete), (ii) elements in sequences could
flip (i.e., the order obtained is reversed from reality), and
(iii) nodes are not time-synchronized.
To introduce the MSP algorithm, we first consider a simple
straight-line scan scenario. Then, we describe how to 
implement straight-line scans as well as other event types, such as
sound wave propagation.
1
A
2
3
4
5
B
C
6
7
8
9
Straight-line Scan 1
Straight-lineScan2
8
1
5 A
6
C
4
3
7
2
B
9
3
1
C 5
9
2 A 4 6
B
7 8
Target node
Anchor node
Figure 2. Obtaining Multiple Node Sequences
4 Basic MSP
Let us consider a sensor network with N target nodes and
M anchor nodes randomly deployed in an area of size S. The
top-level idea for basic MSP is to split the whole sensor 
network area into small pieces by processing node sequences. 
Because the exact locations of all the anchors in a node sequence
are known, all the nodes in this sequence can be divided into
O(M +1) parts in the area.
In Figure 2, we use numbered circles to denote target nodes
and numbered hexagons to denote anchor nodes. Basic MSP
uses two straight lines to scan the area from different directions,
treating each scan as an event. All the nodes react to the event
sequentially generating two node sequences. For vertical scan
1, the node sequence is (8,1,5,A,6,C,4,3,7,2,B,9), as shown
outside the right boundary of the area in Figure 2; for 
horizontal scan 2, the node sequence is (3,1,C,5,9,2,A,4,6,B,7,8),
as shown under the bottom boundary of the area in Figure 2.
Since the locations of the anchor nodes are available, the
anchor nodes in the two node sequences actually split the area
vertically and horizontally into 16 parts, as shown in Figure 2.
To extend this process, suppose we have M anchor nodes and
perform d scans from different angles, obtaining d node 
sequences and dividing the area into many small parts. 
Obviously, the number of parts is a function of the number of 
anchors M, the number of scans d, the anchors" location as well as
the slop k for each scan line. According to the pie-cutting 
theorem [22], the area can be divided into O(M2d2) parts. When
M and d are appropriately large, the polygon for each target
node may become sufficiently small so that accurate 
estimation can be achieved. We emphasize that accuracy is affected
not only by the number of anchors M, but also by the number
of events d. In other words, MSP provides a tradeoff between
the physical cost of anchors and the soft cost of events.
Algorithm 1 depicts the computing architecture of basic
MSP. Each node sequence is processed within line 1 to 8. For
each node, GetBoundaries() in line 5 searches for the 
predecessor and successor anchors in the sequence so as to 
determine the boundaries of this node. Then in line 6 UpdateMap()
shrinks the location area of this node according to the newly
obtained boundaries. After processing all sequences, Centroid
Estimation (line 11) set the center of gravity of the final 
polygon as the estimated location of the target node.
Basic MSP only makes use of the order information 
between a target node and the anchor nodes in each sequence.
Actually, we can extract much more location information from
17
Algorithm 1 Basic MSP Process
Output: The estimated location of each node.
1: repeat
2: GetOneUnprocessedSeqence();
3: repeat
4: GetOneNodeFromSequenceInOrder();
5: GetBoundaries();
6: UpdateMap();
7: until All the target nodes are updated;
8: until All the node sequences are processed;
9: repeat
10: GetOneUnestimatedNode();
11: CentroidEstimation();
12: until All the target nodes are estimated;
each sequence. Section 5 will introduce advanced MSP, in
which four novel optimizations are proposed to improve the
performance of MSP significantly.
5 Advanced MSP
Four improvements to basic MSP are proposed in this 
section. The first three improvements do not need additional
sensing and communication in the networks but require only
slightly more off-line computation. The objective of all these
improvements is to make full use of the information embedded
in the node sequences. The results we have obtained 
empirically indicate that the implementation of the first two methods
can dramatically reduce the localization error, and that the third
and fourth methods are helpful for some system deployments.
5.1 Sequence-Based MSP
As shown in Figure 2, each scan line and M anchors, splits
the whole area into M + 1 parts. Each target node falls into
one polygon shaped by scan lines. We noted that in basic MSP,
only the anchors are used to narrow down the polygon of each
target node, but actually there is more information in the node
sequence that we can made use of.
Let"s first look at a simple example shown in Figure 3. The
previous scans narrow the locations of target node 1 and node
2 into two dashed rectangles shown in the left part of Figure 3.
Then a new scan generates a new sequence (1, 2). With 
knowledge of the scan"s direction, it is easy to tell that node 1 is
located to the left of node 2. Thus, we can further narrow the
location area of node 2 by eliminating the shaded part of node
2"s rectangle. This is because node 2 is located on the right of
node 1 while the shaded area is outside the lower boundary of
node 1. Similarly, the location area of node 1 can be narrowed
by eliminating the shaded part out of node 2"s right boundary.
We call this procedure sequence-based MSP which means that
the whole node sequence needs to be processed node by node
in order. Specifically, sequence-based MSP follows this exact
processing rule:
1
2
1 2
1
2
Lower boundary of 1 Upper boundary of 1
Lower boundary of 2 Upper boundary of 2
New sequence
New upper boundary of 1
New Lower boundary of 2
EventPropagation
Figure 3. Rule Illustration in Sequence Based MSP
Algorithm 2 Sequence-Based MSP Process
Output: The estimated location of each node.
1: repeat
2: GetOneUnprocessedSeqence();
3: repeat
4: GetOneNodeByIncreasingOrder();
5: ComputeLowbound();
6: UpdateMap();
7: until The last target node in the sequence;
8: repeat
9: GetOneNodeByDecreasingOrder();
10: ComputeUpbound();
11: UpdateMap();
12: until The last target node in the sequence;
13: until All the node sequences are processed;
14: repeat
15: GetOneUnestimatedNode();
16: CentroidEstimation();
17: until All the target nodes are estimated;
Elimination Rule: Along a scanning direction, the lower
boundary of the successor"s area must be equal to or larger
than the lower boundary of the predecessor"s area, and the 
upper boundary of the predecessor"s area must be equal to or
smaller than the upper boundary of the successor"s area.
In the case of Figure 3, node 2 is the successor of node 1,
and node 1 is the predecessor of node 2. According to the
elimination rule, node 2"s lower boundary cannot be smaller
than that of node 1 and node 1"s upper boundary cannot exceed
node 2"s upper boundary.
Algorithm 2 illustrates the pseudo code of sequence-based
MSP. Each node sequence is processed within line 3 to 13. The
sequence processing contains two steps:
Step 1 (line 3 to 7): Compute and modify the lower 
boundary for each target node by increasing order in the node 
sequence. Each node"s lower boundary is determined by the
lower boundary of its predecessor node in the sequence, thus
the processing must start from the first node in the sequence
and by increasing order. Then update the map according to the
new lower boundary.
Step 2 (line 8 to 12): Compute and modify the upper 
boundary for each node by decreasing order in the node sequence.
Each node"s upper boundary is determined by the upper 
boundary of its successor node in the sequence, thus the processing
must start from the last node in the sequence and by 
decreasing order. Then update the map according to the new upper
boundary.
After processing all the sequences, for each node, a polygon
bounding its possible location has been found. Then, 
center-ofgravity-based estimation is applied to compute the exact 
location of each node (line 14 to 17).
An example of this process is shown in Figure 4. The third
scan generates the node sequence (B,9,2,7,4,6,3,8,C,A,5,1). In
addition to the anchor split lines, because nodes 4 and 7 come
after node 2 in the sequence, node 4 and 7"s polygons could
be narrowed according to node 2"s lower boundary (the lower
right-shaded area); similarly, the shaded area in node 2"s 
rectangle could be eliminated since this part is beyond node 7"s
upper boundary indicated by the dotted line. Similar 
eliminating can be performed for node 3 as shown in the figure.
18
1
A
2
3
4
5
B
C
6
7
8
9
Straight-line Scan 1
Straight-lineScan2
Straight-line Scan 3
Target node
Anchor node
Figure 4. Sequence-Based MSP Example
1
A
2
3
4
5
B
C
6
7
8
9
Straight-line Scan 1
Straight-lineScan2
Straight-line Scan 3
Reprocessing Scan 1
Target node
Anchor node
Figure 5. Iterative MSP: Reprocessing Scan 1
From above, we can see that the sequence-based MSP
makes use of the information embedded in every sequential
node pair in the node sequence. The polygon boundaries of
the target nodes obtained in prior could be used to further split
other target nodes" areas. Our evaluation in Sections 8 and 9
shows that sequence-based MSP considerably enhances system
accuracy.
5.2 Iterative MSP
Sequence-based MSP is preferable to basic MSP because it
extracts more information from the node sequence. In fact, 
further useful information still remains! In sequence-based MSP,
a sequence processed later benefits from information produced
by previously processed sequences (e.g., the third scan in 
Figure 5). However, the first several sequences can hardly benefit
from other scans in this way. Inspired by this phenomenon,
we propose iterative MSP. The basic idea of iterative MSP is
to process all the sequences iteratively several times so that the
processing of each single sequence can benefit from the results
of other sequences.
To illustrate the idea more clearly, Figure 4 shows the results
of three scans that have provided three sequences. Now if we
process the sequence (8,1,5,A,6,C,4,3,7,2,B,9) obtained from
scan 1 again, we can make progress, as shown in Figure 5.
The reprocessing of the node sequence 1 provides information
in the way an additional vertical scan would. From 
sequencebased MSP, we know that the upper boundaries of nodes 3 and
4 along the scan direction must not extend beyond the upper
boundary of node 7, therefore the grid parts can be eliminated
(a) Central of Gravity (b) Joint Distribution
1 2
2
1 1
2
1
2 2
1
1 2
2
1 1
2
Figure 6. Example of Joint Distribution Estimation
…...
vm
ap[0]
vm
ap[1]
vm
ap[2]
vm
ap[3]
Combine
m
ap
Figure 7. Idea of DBE MSP for Each Node
for the nodes 3 and node 4, respectively, as shown in Figure 5.
From this example, we can see that iterative processing of the
sequence could help further shrink the polygon of each target
node, and thus enhance the accuracy of the system.
The implementation of iterative MSP is straightforward:
process all the sequences multiple times using sequence-based
MSP. Like sequence-based MSP, iterative MSP introduces no
additional event cost. In other words, reprocessing does not
actually repeat the scan physically. Evaluation results in 
Section 8 will show that iterative MSP contributes noticeably to
a lower localization error. Empirical results show that after 5
iterations, improvements become less significant. In summary,
iterative processing can achieve better performance with only
a small computation overhead.
5.3 Distribution-Based Estimation
After determining the location area polygon for each node,
estimation is needed for a final decision. Previous research
mostly applied the Center of Gravity (COG) method [4] [8]
[10] which minimizes average error. If every node is 
independent of all others, COG is the statistically best solution. In
MSP, however, each node may not be independent. For 
example, two neighboring nodes in a certain sequence could have
overlapping polygon areas. In this case, if the marginal 
probability of joint distribution is used for estimation, better 
statistical results are achieved.
Figure 6 shows an example in which node 1 and node 2 are
located in the same polygon. If COG is used, both nodes are
localized at the same position (Figure 6(a)). However, the node
sequences obtained from two scans indicate that node 1 should
be to the left of and above node 2, as shown in Figure 6(b).
The high-level idea of distribution-based estimation 
proposed for MSP, which we call DBE MSP, is illustrated in 
Figure 7. The distributions of each node under the ith scan (for the
ith node sequence) are estimated in node.vmap[i], which is a
data structure for remembering the marginal distribution over
scan i. Then all the vmaps are combined to get a single map
and weighted estimation is used to obtain the final location.
For each scan, all the nodes are sorted according to the gap,
which is the diameter of the polygon along the direction of the
scan, to produce a second, gap-based node sequence. Then,
the estimation starts from the node with the smallest gap. This
is because it is statistically more accurate to assume a uniform
distribution of the node with smaller gap. For each node 
processed in order from the gap-based node sequence, either if
19
Pred. node"s area
Predecessor node exists:
conditional distribution
based on pred. node"s area
Alone: Uniformly Distributed
Succ. node"s area
Successor node exists:
conditional distribution
based on succ. node"s area
Succ. node"s area
Both predecessor and successor
nodes exist: conditional distribution
based on both of them
Pred. node"s area
Figure 8. Four Cases in DBE Process
no neighbor node in the original event-based node sequence
shares an overlapping area, or if the neighbors have not been
processed due to bigger gaps, a uniform distribution Uniform()
is applied to this isolated node (the Alone case in Figure 8).
If the distribution of its neighbors sharing overlapped areas has
been processed, we calculate the joint distribution for the node.
As shown in Figure 8, there are three possible cases 
depending on whether the distribution of the overlapping predecessor
and/or successor nodes have/has already been estimated.
The estimation"s strategy of starting from the most accurate
node (smallest gap node) reduces the problem of estimation 
error propagation. The results in the evaluation section indicate
that applying distribution-based estimation could give 
statistically better results.
5.4 Adaptive MSP
So far, all the enhancements to basic MSP focus on 
improving the multi-sequence processing algorithm given a fixed set
of scan directions. All these enhancements require only more
computing time without any overhead to the sensor nodes. 
Obviously, it is possible to have some choice and optimization on
how events are generated. For example, in military situations,
artillery or rocket-launched mini-ultrasound bombs can be used
for event generation at some selected locations. In adaptive
MSP, we carefully generate each new localization event so as
to maximize the contribution of the new event to the refinement
of localization, based on feedback from previous events.
Figure 9 depicts the basic architecture of adaptive MSP.
Through previous localization events, the whole map has been
partitioned into many small location areas. The idea of 
adaptive MSP is to generate the next localization event to achieve
best-effort elimination, which ideally could shrink the location
area of individual node as much as possible.
We use a weighted voting mechanism to evaluate candidate
localization events. Every node wants the next event to split its
area evenly, which would shrink the area fast. Therefore, every
node votes for the parameters of the next event (e.g., the scan
angle k of the straight-line scan). Since the area map is 
maintained centrally, the vote is virtually done and there is no need
for the real sensor nodes to participate in it. After gathering all
the voting results, the event parameters with the most votes win
the election. There are two factors that determine the weight of
each vote:
• The vote for each candidate event is weighted according
to the diameter D of the node"s location area. Nodes with
bigger location areas speak louder in the voting, because
Map Partitioned by the Localization Events
Diameter of Each
Area
Candidate
Localization Events
Evaluation
Trigger Next
Localization Evet
Figure 9. Basic Architecture of Adaptive MSP
2
3
Diameter D3
1
1
3k
2
3k
3
3k
4
3k
5
3k
6
3k
1
3k 2
3k 3
3k
6
3k4
3k 5
3k
Weight
el
small
i
opt
i
j
ii
j
i
S
S
DkkDfkWeight
arg
),(,()( ⋅=∆=
1
3
opt
k
Target node
Anchor node
Center of Gravity
Node 3's area
Figure 10. Candidate Slops for Node 3 at Anchor 1
overall system error is reduced mostly by splitting the
larger areas.
• The vote for each candidate event is also weighted 
according to its elimination efficiency for a location area, which
is defined as how equally in size (or in diameter) an event
can cut an area. In other words, an optimal scan event
cuts an area in the middle, since this cut shrinks the area
quickly and thus reduces localization uncertainty quickly.
Combining the above two aspects, the weight for each vote
is computed according to the following equation (1):
Weight(k
j
i ) = f(Di,△(k
j
i ,k
opt
i )) (1)
k
j
i is node i"s jth supporting parameter for next event 
generation; Di is diameter of node i"s location area; △(k
j
i ,k
opt
i ) is the
distance between k
j
i and the optimal parameter k
opt
i for node i,
which should be defined to fit the specific application.
Figure 10 presents an example for node 1"s voting for the
slopes of the next straight-line scan. In the system, there
are a fixed number of candidate slopes for each scan (e.g.,
k1,k2,k3,k4...). The location area of target node 3 is shown
in the figure. The candidate events k1
3,k2
3,k3
3,k4
3,k5
3,k6
3 are 
evaluated according to their effectiveness compared to the optimal
ideal event which is shown as a dotted line with appropriate
weights computed according to equation (1). For this 
specific example, as is illustrated in the right part of Figure 10,
f(Di,△(k
j
i ,kopt
i )) is defined as the following equation (2):
Weight(kj
i ) = f(Di,△(kj
i ,kopt
i )) = Di ·
Ssmall
Slarge
(2)
Ssmall and Slarge are the sizes of the smaller part and larger
part of the area cut by the candidate line respectively. In this
case, node 3 votes 0 for the candidate lines that do not cross its
area since Ssmall = 0.
We show later that adaptive MSP improves localization 
accuracy in WSNs with irregularly shaped deployment areas.
20
5.5 Overhead and MSP Complexity Analysis
This section provides a complexity analysis of the MSP 
design. We emphasize that MSP adopts an asymmetric design in
which sensor nodes need only to detect and report the events.
They are blissfully oblivious to the processing methods 
proposed in previous sections. In this section, we analyze the 
computational cost on the node sequence processing side, where
resources are plentiful.
According to Algorithm 1, the computational complexity of
Basic MSP is O(d · N · S), and the storage space required is
O(N · S), where d is the number of events, N is the number of
target nodes, and S is the area size.
According to Algorithm 2, the computational complexity of
both sequence-based MSP and iterative MSP is O(c·d ·N ·S),
where c is the number of iterations and c = 1 for 
sequencebased MSP, and the storage space required is O(N ·S). Both the
computational complexity and storage space are equal within a
constant factor to those of basic MSP.
The computational complexity of the distribution-based 
estimation (DBE MSP) is greater. The major overhead comes
from the computation of joint distributions when both 
predecessor and successor nodes exit. In order to compute the
marginal probability, MSP needs to enumerate the locations of
the predecessor node and the successor node. For example,
if node A has predecessor node B and successor node C, then
the marginal probability PA(x,y) of node A"s being at location
(x,y) is:
PA(x,y) = ∑
i
∑
j
∑
m
∑
n
1
NB,A,C
·PB(i, j)·PC(m,n) (3)
NB,A,C is the number of valid locations for A satisfying the
sequence (B, A, C) when B is at (i, j) and C is at (m,n);
PB(i, j) is the available probability of node B"s being located
at (i, j); PC(m,n) is the available probability of node C"s 
being located at (m,n). A naive algorithm to compute equation
(3) has complexity O(d · N · S3). However, since the marginal
probability indeed comes from only one dimension along the
scanning direction (e.g., a line), the complexity can be reduced
to O(d · N · S1.5) after algorithm optimization. In addition, the
final location areas for every node are much smaller than the
original field S; therefore, in practice, DBE MSP can be 
computed much faster than O(d ·N ·S1.5).
6 Wave Propagation Example
So far, the description of MSP has been solely in the 
context of straight-line scan. However, we note that MSP is 
conceptually independent of how the event is propagated as long
as node sequences can be obtained. Clearly, we can also 
support wave-propagation-based events (e.g., ultrasound 
propagation, air blast propagation), which are polar coordinate 
equivalences of the line scans in the Cartesian coordinate system.
This section illustrates the effects of MSP"s implementation in
the wave propagation-based situation. For easy modelling, we
have made the following assumptions:
• The wave propagates uniformly in all directions, 
therefore the propagation has a circle frontier surface. Since
MSP does not rely on an accurate space-time relationship,
a certain distortion in wave propagation is tolerable. If any
directional wave is used, the propagation frontier surface
can be modified accordingly.
1
3
5
9
Target node
Anchor node
Previous Event location
A
2
Center of Gravity
4
8
7
B
6
C
A line of preferred locations for next event
Figure 11. Example of Wave Propagation Situation
• Under the situation of line-of-sight, we allow obstacles to
reflect or deflect the wave. Reflection and deflection are
not problems because each node reacts only to the first
detected event. Those reflected or deflected waves come
later than the line-of-sight waves. The only thing the 
system needs to maintain is an appropriate time interval 
between two successive localization events.
• We assume that background noise exists, and therefore we
run a band-pass filter to listen to a particular wave 
frequency. This reduces the chances of false detection.
The parameter that affects the localization event generation
here is the source location of the event. The different 
distances between each node and the event source determine the
rank of each node in the node sequence. Using the node 
sequences, the MSP algorithm divides the whole area into many
non-rectangular areas as shown in Figure 11. In this figure,
the stars represent two previous event sources. The previous
two propagations split the whole map into many areas by those
dashed circles that pass one of the anchors. Each node is 
located in one of the small areas. Since sequence-based MSP,
iterative MSP and DBE MSP make no assumptions about the
type of localization events and the shape of the area, all three
optimization algorithms can be applied for the wave 
propagation scenario.
However, adaptive MSP needs more explanation. Figure 11
illustrates an example of nodes" voting for next event source
locations. Unlike the straight-line scan, the critical parameter
now is the location of the event source, because the distance
between each node and the event source determines the rank of
the node in the sequence. In Figure 11, if the next event breaks
out along/near the solid thick gray line, which perpendicularly
bisects the solid dark line between anchor C and the center of
gravity of node 9"s area (the gray area), the wave would reach
anchor C and the center of gravity of node 9"s area at roughly
the same time, which would relatively equally divide node 9"s
area. Therefore, node 9 prefers to vote for the positions around
the thick gray line.
7 Practical Deployment Issues
For the sake of presentation, until now we have described
MSP in an ideal case where a complete node sequence can be
obtained with accurate time synchronization. In this section
we describe how to make MSP work well under more realistic
conditions.
21
7.1 Incomplete Node Sequence
For diverse reasons, such as sensor malfunction or natural
obstacles, the nodes in the network could fail to detect 
localization events. In such cases, the node sequence will not be
complete. This problem has two versions:
• Anchor nodes are missing in the node sequence
If some anchor nodes fail to respond to the localization
events, then the system has fewer anchors. In this case,
the solution is to generate more events to compensate for
the loss of anchors so as to achieve the desired accuracy
requirements.
• Target nodes are missing in the node sequence
There are two consequences when target nodes are 
missing. First, if these nodes are still be useful to sensing 
applications, they need to use other backup localization 
approaches (e.g., Centroid) to localize themselves with help
from their neighbors who have already learned their own
locations from MSP. Secondly, since in advanced MSP
each node in the sequence may contribute to the overall
system accuracy, dropping of target nodes from sequences
could also reduce the accuracy of the localization. Thus,
proper compensation procedures such as adding more 
localization events need to be launched.
7.2 Localization without Time Synchronization
In a sensor network without time synchronization support,
nodes cannot be ordered into a sequence using timestamps. For
such cases, we propose a listen-detect-assemble-report 
protocol, which is able to function independently without time 
synchronization.
listen-detect-assemble-report requires that every node 
listens to the channel for the node sequence transmitted from its
neighbors. Then, when the node detects the localization event,
it assembles itself into the newest node sequence it has heard
and reports the updated sequence to other nodes. Figure 12
(a) illustrates an example for the listen-detect-assemble-report
protocol. For simplicity, in this figure we did not differentiate
the target nodes from anchor nodes. A solid line between two
nodes stands for a communication link. Suppose a straight line
scans from left to right. Node 1 detects the event, and then it
broadcasts the sequence (1) into the network. Node 2 and node
3 receive this sequence. When node 2 detects the event, node
2 adds itself into the sequence and broadcasts (1, 2). The 
sequence propagates in the same direction with the scan as shown
in Figure 12 (a). Finally, node 6 obtains a complete sequence
(1,2,3,5,7,4,6).
In the case of ultrasound propagation, because the event
propagation speed is much slower than that of radio, the 
listendetect-assemble-report protocol can work well in a situation
where the node density is not very high. For instance, if the
distance between two nodes along one direction is 10 meters,
the 340m/s sound needs 29.4ms to propagate from one node
to the other. While normally the communication data rate is
250Kbps in the WSN (e.g., CC2420 [1]), it takes only about
2 ∼ 3 ms to transmit an assembled packet for one hop.
One problem that may occur using the 
listen-detectassemble-report protocol is multiple partial sequences as
shown in Figure 12 (b). Two separate paths in the network may
result in two sequences that could not be further combined. In
this case, since the two sequences can only be processed as 
separate sequences, some order information is lost. Therefore the
1,2,5,4
1,3,7,4
1,2,3,5 1,2,3,5,7,4
1,2,3,5,7
1,2,3,5
1,3
1,2
1
2
3
5
7
4
6
1
1
1,3 1,2,3,5,7,4,6
1,2,5
1,3,7
1,3
1,2
1
2
3
5
7
4
6
1
1
1,3,7,4,6
1,2,5,4,6
(a)
(b)
(c)
1,3,2,5 1,3,2,5,7,4
1,3,2,5,7
1,3,2,5
1,3
1,2
1
2
3
5
7
4
6
1
1
1,3 1,3,2,5,7,4,6
Event Propagation
Event Propagation
Event Propagation
Figure 12. Node Sequence without Time Synchronization
accuracy of the system would decrease.
The other problem is the sequence flip problem. As shown
in Figure 12 (c), because node 2 and node 3 are too close to
each other along the scan direction, they detect the scan 
almost simultaneously. Due to the uncertainty such as media
access delay, two messages could be transmitted out of order.
For example, if node 3 sends out its report first, then the order
of node 2 and node 3 gets flipped in the final node sequence.
The sequence flip problem would appear even in an accurately
synchronized system due to random jitter in node detection if
an event arrives at multiple nodes almost simultaneously. A
method addressing the sequence flip is presented in the next
section.
7.3 Sequence Flip and Protection Band
Sequence flip problems can be solved with and without
time synchronization. We firstly start with a scenario 
applying time synchronization. Existing solutions for time 
synchronization [12, 6] can easily achieve sub-millisecond-level 
accuracy. For example, FTSP [12] achieves 16.9µs (microsecond)
average error for a two-node single-hop case. Therefore, we
can comfortably assume that the network is synchronized with
maximum error of 1000µs. However, when multiple nodes are
located very near to each other along the event propagation 
direction, even when time synchronization with less than 1ms
error is achieved in the network, sequence flip may still occur.
For example, in the sound wave propagation case, if two nodes
are less than 0.34 meters apart, the difference between their
detection timestamp would be smaller than 1 millisecond.
We find that sequence flip could not only damage system 
accuracy, but also might cause a fatal error in the MSP algorithm.
Figure 13 illustrates both detrimental results. In the left side of
Figure 13(a), suppose node 1 and node 2 are so close to each
other that it takes less than 0.5ms for the localization event to
propagate from node 1 to node 2. Now unfortunately, the node
sequence is mistaken to be (2,1). So node 1 is expected to be
located to the right of node 2, such as at the position of the
dashed node 1. According to the elimination rule in 
sequencebased MSP, the left part of node 1"s area is cut off as shown in
the right part of Figure 13(a). This is a potentially fatal error,
because node 1 is actually located in the dashed area which has
been eliminated by mistake. During the subsequent 
eliminations introduced by other events, node 1"s area might be cut off
completely, thus node 1 could consequently be erased from the
map! Even in cases where node 1 still survives, its area actually
does not cover its real location.
22
1
2
12
2
Lower boundary of 1 Upper boundary of 1
Flipped Sequence Fatal Elimination Error
EventPropagation
1 1
Fatal Error
1
1
2
12
2
Lower boundary of 1 Upper boundary of 1
Flipped Sequence Safe Elimination
EventPropagation
1 1
New lower boundary of 1
1
B
(a)
(b)
B: Protection band
Figure 13. Sequence Flip and Protection Band
Another problem is not fatal but lowers the localization 
accuracy. If we get the right node sequence (1,2), node 1 has a
new upper boundary which can narrow the area of node 1 as in
Figure 3. Due to the sequence flip, node 1 loses this new upper
boundary.
In order to address the sequence flip problem, especially to
prevent nodes from being erased from the map, we propose
a protection band compensation approach. The basic idea of
protection band is to extend the boundary of the location area
a little bit so as to make sure that the node will never be erased
from the map. This solution is based on the fact that nodes
have a high probability of flipping in the sequence if they are
near to each other along the event propagation direction. If
two nodes are apart from each other more than some distance,
say, B, they rarely flip unless the nodes are faulty. The width
of a protection band B, is largely determined by the maximum
error in system time synchronization and the localization event
propagation speed.
Figure 13(b) presents the application of the protection band.
Instead of eliminating the dashed part in Figure 13(a) for node
1, the new lower boundary of node 1 is set by shifting the 
original lower boundary of node 2 to the left by distance B. In this
case, the location area still covers node 1 and protects it from
being erased. In a practical implementation, supposing that the
ultrasound event is used, if the maximum error of system time
synchronization is 1ms, two nodes might flip with high 
probability if the timestamp difference between the two nodes is
smaller than or equal to 1ms. Accordingly, we set the 
protection band B as 0.34m (the distance sound can propagate within
1 millisecond). By adding the protection band, we reduce the
chances of fatal errors, although at the cost of localization 
accuracy. Empirical results obtained from our physical test-bed
verified this conclusion.
In the case of using the listen-detect-assemble-report 
protocol, the only change we need to make is to select the protection
band according to the maximum delay uncertainty introduced
by the MAC operation and the event propagation speed. To
bound MAC delay at the node side, a node can drop its report
message if it experiences excessive MAC delay. This converts
the sequence flip problem to the incomplete sequence problem,
which can be more easily addressed by the method proposed in
Section 7.1.
8 Simulation Evaluation
Our evaluation of MSP was conducted on three platforms:
(i) an indoor system with 46 MICAz motes using straight-line
scan, (ii) an outdoor system with 20 MICAz motes using sound
wave propagation, and (iii) an extensive simulation under 
various kinds of physical settings.
In order to understand the behavior of MSP under 
numerous settings, we start our evaluation with simulations.
Then, we implemented basic MSP and all the advanced
MSP methods for the case where time synchronization is
available in the network. The simulation and 
implementation details are omitted in this paper due to space 
constraints, but related documents [25] are provided online at
http://www.cs.umn.edu/∼zhong/MSP. Full implementation and
evaluation of system without time synchronization are yet to be
completed in the near future.
In simulation, we assume all the node sequences are perfect
so as to reveal the performance of MSP achievable in the 
absence of incomplete node sequences or sequence flips. In our
simulations, all the anchor nodes and target nodes are assumed
to be deployed uniformly. The mean and maximum errors are
averaged over 50 runs to obtain high confidence. For legibility
reasons, we do not plot the confidence intervals in this paper.
All the simulations are based on the straight-line scan example.
We implement three scan strategies:
• Random Scan: The slope of the scan line is randomly 
chosen at each time.
• Regular Scan: The slope is predetermined to rotate 
uniformly from 0 degree to 180 degrees. For example, if the
system scans 6 times, then the scan angles would be: 0,
30, 60, 90, 120, and 150.
• Adaptive Scan: The slope of each scan is determined
based on the localization results from previous scans.
We start with basic MSP and then demonstrate the 
performance improvements one step at a time by adding (i) 
sequencebased MSP, (ii) iterative MSP, (iii) DBE MSP and (iv) adaptive
MSP.
8.1 Performance of Basic MSP
The evaluation starts with basic MSP, where we compare the
performance of random scan and regular scan under different
configurations. We intend to illustrate the impact of the number
of anchors M, the number of scans d, and target node density
(number of target nodes N in a fixed-size region) on the 
localization error. Table 1 shows the default simulation parameters.
The error of each node is defined as the distance between the
estimated location and the real position. We note that by 
default we only use three anchors, which is considerably fewer
than existing range-free solutions [8, 4].
Impact of the Number of Scans: In this experiment, we 
compare regular scan with random scan under a different number
of scans from 3 to 30 in steps of 3. The number of anchors
Table 1. Default Configuration Parameters
Parameter Description
Field Area 200×200 (Grid Unit)
Scan Type Regular (Default)/Random Scan
Anchor Number 3 (Default)
Scan Times 6 (Default)
Target Node Number 100 (Default)
Statistics Error Mean/Max
Random Seeds 50 runs
23
0 5 10 15 20 25 30
0
10
20
30
40
50
60
70
80
90
Mean Error and Max Error VS Scan Time
Scan Time
Error Max Error of Random Scan
Max Error of Regular Scan
Mean Error of Random Scan
Mean Error of Regular Scan
(a) Error vs. Number of Scans
0 5 10 15 20 25 30
0
10
20
30
40
50
60
Mean Error and Max Error VS Anchor Number
Anchor Number
Error
Max Error of Random Scan
Max Error of Regular Scan
Mean Error of Random Scan
Mean Error of Regular Scan
(b) Error vs. Anchor Number
0 50 100 150 200
10
20
30
40
50
60
70
Mean Error and Max Error VS Target Node Number
Target Node Number
Error
Max Error of Random Scan
Max Error of Regular Scan
Mean Error of Random Scan
Mean Error of Regular Scan
(c) Error vs. Number of Target Nodes
Figure 14. Evaluation of Basic MSP under Random and Regular Scans
0 5 10 15 20 25 30
0
10
20
30
40
50
60
70
Basic MSP VS Sequence Based MSP II
Scan Time
Error
Max Error of Basic MSP
Max Error of Seq MSP
Mean Error of Basic MSP
Mean Error of Seq MSP
(a) Error vs. Number of Scans
0 5 10 15 20 25 30
0
5
10
15
20
25
30
35
40
45
50
Basic MSP VS Sequence Based MSP I
Anchor Number
Error
Max Error of Basic MSP
Max Error of Seq MSP
Mean Error of Basic MSP
Mean Error of Seq MSP
(b) Error vs. Anchor Number
0 50 100 150 200
5
10
15
20
25
30
35
40
45
50
55
Basic MSP VS Sequence Based MSP III
Target Node Number
Error
Max Error of Basic MSP
Max Error of Seq MSP
Mean Error of Basic MSP
Mean Error of Seq MSP
(c) Error vs. Number of Target Nodes
Figure 15. Improvements of Sequence-Based MSP over Basic MSP
is 3 by default. Figure 14(a) indicates the following: (i) as
the number of scans increases, the localization error decreases
significantly; for example, localization errors drop more than
60% from 3 scans to 30 scans; (ii) statistically, regular scan
achieves better performance than random scan under identical
number of scans. However, the performance gap reduces as
the number of scans increases. This is expected since a large
number of random numbers converges to a uniform 
distribution. Figure 14(a) also demonstrates that MSP requires only
a small number of anchors to perform very well, compared to
existing range-free solutions [8, 4].
Impact of the Number of Anchors: In this experiment, we
compare regular scan with random scan under different 
number of anchors from 3 to 30 in steps of 3. The results shown in
Figure 14(b) indicate that (i) as the number of anchor nodes
increases, the localization error decreases, and (ii) 
statistically, regular scan obtains better results than random scan with
identical number of anchors. By combining Figures 14(a)
and 14(b), we can conclude that MSP allows a flexible tradeoff
between physical cost (anchor nodes) and soft cost 
(localization events).
Impact of the Target Node Density: In this experiment, we
confirm that the density of target nodes has no impact on the
accuracy, which motivated the design of sequence-based MSP.
In this experiment, we compare regular scan with random scan
under different number of target nodes from 10 to 190 in steps
of 20. Results in Figure 14(c) show that mean localization 
errors remain constant across different node densities. However,
when the number of target nodes increases, the average 
maximum error increases.
Summary: From the above experiments, we can conclude that
in basic MSP, regular scan are better than random scan under
different numbers of anchors and scan events. This is because
regular scans uniformly eliminate the map from different 
directions, while random scans would obtain sequences with 
redundant overlapping information, if two scans choose two similar
scanning slopes.
8.2 Improvements of Sequence-Based MSP
This section evaluates the benefits of exploiting the order
information among target nodes by comparing sequence-based
MSP with basic MSP. In this and the following sections, 
regular scan is used for straight-line scan event generation. The
purpose of using regular scan is to keep the scan events and
the node sequences identical for both sequence-based MSP and
basic MSP, so that the only difference between them is the 
sequence processing procedure.
Impact of the Number of Scans: In this experiment, we
compare sequence-based MSP with basic MSP under different
number of scans from 3 to 30 in steps of 3. Figure 15(a) 
indicates significant performance improvement in sequence-based
MSP over basic MSP across all scan settings, especially when
the number of scans is large. For example, when the number
of scans is 30, errors in sequence-based MSP are about 20%
of that of basic MSP. We conclude that sequence-based MSP
performs extremely well when there are many scan events.
Impact of the Number of Anchors: In this experiment, we
use different number of anchors from 3 to 30 in steps of 3. As
seen in Figure 15(b), the mean error and maximum error of
sequence-based MSP is much smaller than that of basic MSP.
Especially when there is limited number of anchors in the 
system, e.g., 3 anchors, the error rate was almost halved by 
using sequence-based MSP. This phenomenon has an interesting
explanation: the cutting lines created by anchor nodes are 
exploited by both basic MSP and sequence-based MSP, so as the
24
0 2 4 6 8 10
0
5
10
15
20
25
30
35
40
45
50
Basic MSP VS Iterative MSP
Iterative Times
Error
Max Error of Iterative Seq MSP
Mean Error of Iterative Seq MSP
Max Error of Basic MSP
Mean Error of Basic MSP
Figure 16. Improvements of Iterative MSP
0 2 4 6 8 10 12 14 16
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
DBE VS Non−DBE
Error
CumulativeDistrubutioinFunctions(CDF)
Mean Error CDF of DBE MSP
Mean Error CDF of Non−DBE MSP
Max Error CDF of DBE MSP
Max Error CDF of Non−DBE MSP
Figure 17. Improvements of DBE MSP
0 20 40 60 80 100
0
10
20
30
40
50
60
70
Adaptive MSP for 500by80
Target Node Number
Error
Max Error of Regualr Scan
Max Error of Adaptive Scan
Mean Error of Regualr Scan
Mean Error of Adaptive Scan
(a) Adaptive MSP for 500 by 80 field
0 10 20 30 40 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Mean Error CDF at Different Angle Steps in Adaptive Scan
Mean Error
CumulativeDistrubutioinFunctions(CDF)
5 Degree Angle Step Adaptive
10 Degree Angle Step Adaptive
20 Degree Angle Step Adaptive
30 Degree Step Regular Scan
(b) Impact of the Number of Candidate Events
Figure 18. The Improvements of Adaptive MSP
number of anchor nodes increases, anchors tend to dominate
the contribution. Therefore the performance gaps lessens.
Impact of the Target Node Density: Figure 15(c) 
demonstrates the benefits of exploiting order information among 
target nodes. Since sequence-based MSP makes use of the 
information among the target nodes, having more target nodes
contributes to the overall system accuracy. As the number of
target nodes increases, the mean error and maximum error of
sequence-based MSP decreases. Clearly the mean error in 
basic MSP is not affected by the number of target nodes, as shown
in Figure 15(c).
Summary: From the above experiments, we can conclude that
exploiting order information among target nodes can improve
accuracy significantly, especially when the number of events is
large but with few anchors.
8.3 Iterative MSP over Sequence-Based MSP
In this experiment, the same node sequences were processed
iteratively multiple times. In Figure 16, the two single marks
are results from basic MSP, since basic MSP doesn"t perform
iterations. The two curves present the performance of 
iterative MSP under different numbers of iterations c. We note that
when only a single iteration is used, this method degrades to
sequence-based MSP. Therefore, Figure 16 compares the three
methods to one another.
Figure 16 shows that the second iteration can reduce the
mean error and maximum error dramatically. After that, the
performance gain gradually reduces, especially when c > 5.
This is because the second iteration allows earlier scans to 
exploit the new boundaries created by later scans in the first 
iteration. Such exploitation decays quickly over iterations.
8.4 DBE MSP over Iterative MSP
Figure 17, in which we augment iterative MSP with
distribution-based estimation (DBE MSP), shows that DBE
MSP could bring about statistically better performance. 
Figure 17 presents cumulative distribution localization errors. In
general, the two curves of the DBE MSP lay slightly to the left
of that of non-DBE MSP, which indicates that DBE MSP has
a smaller statistical mean error and averaged maximum error
than non-DBE MSP. We note that because DBE is augmented
on top of the best solution so far, the performance 
improvement is not significant. When we apply DBE on basic MSP
methods, the improvement is much more significant. We omit
these results because of space constraints.
8.5 Improvements of Adaptive MSP
This section illustrates the performance of adaptive MSP
over non-adaptive MSP. We note that feedback-based 
adaptation can be applied to all MSP methods, since it affects only
the scanning angles but not the sequence processing. In this
experiment, we evaluated how adaptive MSP can improve the
best solution so far. The default angle granularity (step) for
adaptive searching is 5 degrees.
Impact of Area Shape: First, if system settings are regular,
the adaptive method hardly contributes to the results. For a
square area (regular), the performance of adaptive MSP and
regular scans are very close. However, if the shape of the area
is not regular, adaptive MSP helps to choose the appropriate
localization events to compensate. Therefore, adaptive MSP
can achieve a better mean error and maximum error as shown
in Figure 18(a). For example, adaptive MSP improves 
localization accuracy by 30% when the number of target nodes is
10.
Impact of the Target Node Density: Figure 18(a) shows that
when the node density is low, adaptive MSP brings more 
benefit than when node density is high. This phenomenon makes
statistical sense, because the law of large numbers tells us that
node placement approaches a truly uniform distribution when
the number of nodes is increased. Adaptive MSP has an edge
25
Figure 19. The Mirage Test-bed (Line Scan) Figure 20. The 20-node Outdoor Experiments (Wave)
when layout is not uniform.
Impact of Candidate Angle Density: Figure 18(b) shows that
the smaller the candidate scan angle step, the better the 
statistical performance in terms of mean error. The rationale is clear,
as wider candidate scan angles provide adaptive MSP more 
opportunity to choose the one approaching the optimal angle.
8.6 Simulation Summary
Starting from basic MSP, we have demonstrated 
step-bystep how four optimizations can be applied on top of each other
to improve localization performance. In other words, these 
optimizations are compatible with each other and can jointly 
improve the overall performance. We note that our simulations
were done under assumption that the complete node sequence
can be obtained without sequence flips. In the next section, we
present two real-system implementations that reveal and 
address these practical issues.
9 System Evaluation
In this section, we present a system implementation of MSP
on two physical test-beds. The first one is called Mirage, a
large indoor test-bed composed of six 4-foot by 8-foot boards,
illustrated in Figure 19. Each board in the system can be used
as an individual sub-system, which is powered, controlled and
metered separately. Three Hitachi CP-X1250 projectors, 
connected through a Matorx Triplehead2go graphics expansion
box, are used to create an ultra-wide integrated display on six
boards. Figure 19 shows that a long tilted line is generated by
the projectors. We have implemented all five versions of MSP
on the Mirage test-bed, running 46 MICAz motes. Unless 
mentioned otherwise, the default setting is 3 anchors and 6 scans at
the scanning line speed of 8.6 feet/s. In all of our graphs, each
data point represents the average value of 50 trials. In the 
outdoor system, a Dell A525 speaker is used to generate 4.7KHz
sound as shown in Figure 20. We place 20 MICAz motes in the
backyard of a house. Since the location is not completely open,
sound waves are reflected, scattered and absorbed by various
objects in the vicinity, causing a multi-path effect. In the 
system evaluation, simple time synchronization mechanisms are
applied on each node.
9.1 Indoor System Evaluation
During indoor experiments, we encountered several 
realworld problems that are not revealed in the simulation. First,
sequences obtained were partial due to misdetection and 
message losses. Second, elements in the sequences could flip due
to detection delay, uncertainty in media access, or error in time
synchronization. We show that these issues can be addressed
by using the protection band method described in Section 7.3.
9.1.1 On Scanning Speed and Protection Band
In this experiment, we studied the impact of the scanning
speed and the length of protection band on the performance of
the system. In general, with increasing scanning speed, nodes
have less time to respond to the event and the time gap between
two adjacent nodes shrinks, leading to an increasing number of
partial sequences and sequence flips.
Figure 21 shows the node flip situations for six scans with
distinct angles under different scan speeds. The x-axis shows
the distance between the flipped nodes in the correct node 
sequence. y-axis shows the total number of flips in the six scans.
This figure tells us that faster scan brings in not only 
increasing number of flips, but also longer-distance flips that require
wider protection band to prevent from fatal errors.
Figure 22(a) shows the effectiveness of the protection band
in terms of reducing the number of unlocalized nodes. When
we use a moderate scan speed (4.3feet/s), the chance of flipping
is rare, therefore we can achieve 0.45 feet mean accuracy 
(Figure 22(b)) with 1.6 feet maximum error (Figure 22(c)). With
increasing speeds, the protection band needs to be set to a larger
value to deal with flipping. Interesting phenomena can be 
observed in Figures 22: on one hand, the protection band can
sharply reduce the number of unlocalized nodes; on the other
hand, protection bands enlarge the area in which a target would
potentially reside, introducing more uncertainty. Thus there is
a concave curve for both mean and maximum error when the
scan speed is at 8.6 feet/s.
9.1.2 On MSP Methods and Protection Band
In this experiment, we show the improvements resulting
from three different methods. Figure 23(a) shows that a 
protection band of 0.35 feet is sufficient for the scan speed of
8.57feet/s. Figures 23(b) and 23(c) show clearly that iterative
MSP (with adaptation) achieves best performance. For 
example, Figures 23(b) shows that when we set the protection band
at 0.05 feet, iterative MSP achieves 0.7 feet accuracy, which
is 42% more accurate than the basic design. Similarly, 
Figures 23(b) and 23(c) show the double-edged effects of 
protection band on the localization accuracy.
0 5 10 15 20
0
20
40
(3) Flip Distribution for 6 Scans at Line Speed of 14.6feet/s
Flips
Node Distance in the Ideal Node Sequence
0 5 10 15 20
0
20
40
(2) Flip Distribution for 6 Scans at Line Speed of 8.6feet/s
Flips
0 5 10 15 20
0
20
40
(1) Flip Distribution for 6 Scans at Line Speed of 4.3feet/s
Flips
Figure 21. Number of Flips for Different Scan Speeds
26
0 0.2 0.4 0.6 0.8 1
0
2
4
6
8
10
12
14
16
18
20
Unlocalized Node Number(Line Scan at Different Speed)
Protection Band (in feet)
UnlocalizedNodeNumber
Scan Line Speed: 14.6feet/s
Scan Line Speed: 8.6feet/s
Scan Line Speed: 4.3feet/s
(a) Number of Unlocalized Nodes
0 0.2 0.4 0.6 0.8 1
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
Mean Error(Line Scan at Different Speed)
Protection Band (in feet)
Error(infeet)
Scan Line Speed:14.6feet/s
Scan Line Speed: 8.6feet/s
Scan Line Speed: 4.3feet/s
(b) Mean Localization Error
0 0.2 0.4 0.6 0.8 1
1.5
2
2.5
3
3.5
4
Max Error(Line Scan at Different Speed)
Protection Band (in feet)
Error(infeet)
Scan Line Speed: 14.6feet/s
Scan Line Speed: 8.6feet/s
Scan Line Speed: 4.3feet/s
(c) Max Localization Error
Figure 22. Impact of Protection Band and Scanning Speed
0 0.2 0.4 0.6 0.8 1
0
2
4
6
8
10
12
14
16
18
20
Unlocalized Node Number(Scan Line Speed 8.57feet/s)
Protection Band (in feet)
Numberofunlocalizednodeoutof46
Unlocalized node of Basic MSP
Unlocalized node of Sequence Based MSP
Unlocalized node of Iterative MSP
(a) Number of Unlocalized Nodes
0 0.2 0.4 0.6 0.8 1
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
Mean Error(Scan Line Speed 8.57feet/s)
Protection Band (in feet)
Error(infeet)
Mean Error of Basic MSP
Mean Error of Sequence Based MSP
Mean Error of Iterative MSP
(b) Mean Localization Error
0 0.2 0.4 0.6 0.8 1
1.5
2
2.5
3
3.5
4
Max Error(Scan Line Speed 8.57feet/s)
Protection Band (in feet)
Error(infeet)
Max Error of Basic MSP
Max Error of Sequence Based MSP
Max Error of Iterative MSP
(c) Max Localization Error
Figure 23. Impact of Protection Band under Different MSP Methods
3 4 5 6 7 8 9 10 11
0
0.5
1
1.5
2
2.5
Unlocalized Node Number(Protection Band: 0.35 feet)
Anchor Number
UnlocalizedNodeNumber
4 Scan Events at Speed 8.75feet/s
6 Scan Events at Speed 8.75feet/s
8 Scan Events at Speed 8.75feet/s
(a) Number of Unlocalized Nodes
3 4 5 6 7 8 9 10 11
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Mean Error(Protection Band: 0.35 feet)
Anchor Number
Error(infeet)
Mean Error of 4 Scan Events at Speed 8.75feet/s
Mean Error of 6 Scan Events at Speed 8.75feet/s
Mean Error of 8 Scan Events at Speed 8.75feet/s
(b) Mean Localization Error
3 4 5 6 7 8 9 10 11
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
Max Error(Protection Band: 0.35 feet)
Anchor Number
Error(infeet)
Max Error of 4 Scan Events at Speed 8.75feet/s
Max Error of 6 Scan Events at Speed 8.75feet/s
Max Error of 8 Scan Events at Speed 8.75feet/s
(c) Max Localization Error
Figure 24. Impact of the Number of Anchors and Scans
9.1.3 On Number of Anchors and Scans
In this experiment, we show a tradeoff between hardware
cost (anchors) with soft cost (events). Figure 24(a) shows that
with more cutting lines created by anchors, the chance of 
unlocalized nodes increases slightly. We note that with a 0.35 feet
protection band, the percentage of unlocalized nodes is very
small, e.g., in the worst-case with 11 anchors, only 2 out of 46
nodes are not localized due to flipping. Figures 24(b) and 24(c)
show the tradeoff between number of anchors and the number
of scans. Obviously, with the number of anchors increases, the
error drops significantly. With 11 anchors we can achieve a 
localization accuracy as low as 0.25 ∼ 0.35 feet, which is nearly a
60% improvement. Similarly, with increasing number of scans,
the accuracy drops significantly as well. We can observe about
30% across all anchor settings when we increase the number of
scans from 4 to 8. For example, with only 3 anchors, we can
achieve 0.6-foot accuracy with 8 scans.
9.2 Outdoor System Evaluation
The outdoor system evaluation contains two parts: (i) 
effective detection distance evaluation, which shows that the
node sequence can be readily obtained, and (ii) sound 
propagation based localization, which shows the results of 
wavepropagation-based localization.
9.2.1 Effective Detection Distance Evaluation
We firstly evaluate the sequence flip phenomenon in wave
propagation. As shown in Figure 25, 20 motes were placed as
five groups in front of the speaker, four nodes in each group
at roughly the same distances to the speaker. The gap between
each group is set to be 2, 3, 4 and 5 feet respectively in four 
experiments. Figure 26 shows the results. The x-axis in each 
subgraph indicates the group index. There are four nodes in each
group (4 bars). The y-axis shows the detection rank (order)
of each node in the node sequence. As distance between each
group increases, number of flips in the resulting node sequence
27
Figure 25. Wave Detection
1 2 3 4 5
0
5
10
15
20
2 feet group distance
Rank
Group Index
1 2 3 4 5
0
5
10
15
20
3 feet group distance
Rank
Group Index
1 2 3 4 5
0
5
10
15
20
4 feet group distance
Rank
Group Index
1 2 3 4 5
0
5
10
15
20
5 feet group distance
Rank
Group Index
Figure 26. Ranks vs. Distances
0
2
4
6
8
10
12
14
16
18
20
22
24
0 2 4 6 8 10 12 14
Y-Dimension(feet)
X-Dimension (feet)
Node
0
2
4
6
8
10
12
14
16
18
20
22
24
0 2 4 6 8 10 12 14
Y-Dimension(feet)
X-Dimension (feet)
Anchor
Figure 27. Localization Error (Sound)
decreases. For example, in the 2-foot distance subgraph, there
are quite a few flips between nodes in adjacent and even 
nonadjacent groups, while in the 5-foot subgraph, flips between
different groups disappeared in the test.
9.2.2 Sound Propagation Based Localization
As shown in Figure 20, 20 motes are placed as a grid 
including 5 rows with 5 feet between each row and 4 columns with
4 feet between each column. Six 4KHz acoustic wave 
propagation events are generated around the mote grid by a speaker.
Figure 27 shows the localization results using iterative MSP
(3 times iterative processing) with a protection band of 3 feet.
The average error of the localization results is 3 feet and the
maximum error is 5 feet with one un-localized node.
We found that sequence flip in wave propagation is more
severe than that in the indoor, line-based test. This is expected
due to the high propagation speed of sound. Currently we use
MICAz mote, which is equipped with a low quality 
microphone. We believe that using a better speaker and more events,
the system can yield better accuracy. Despite the hardware 
constrains, the MSP algorithm still successfully localized most of
the nodes with good accuracy.
10 Conclusions
In this paper, we present the first work that exploits the 
concept of node sequence processing to localize sensor nodes. We
demonstrated that we could significantly improve localization
accuracy by making full use of the information embedded in
multiple easy-to-get one-dimensional node sequences. We 
proposed four novel optimization methods, exploiting order and
marginal distribution among non-anchor nodes as well as the
feedback information from early localization results. 
Importantly, these optimization methods can be used together, and
improve accuracy additively. The practical issues of partial
node sequence and sequence flip were identified and addressed
in two physical system test-beds. We also evaluated 
performance at scale through analysis as well as extensive 
simulations. Results demonstrate that requiring neither costly 
hardware on sensor nodes nor precise event distribution, MSP can
achieve a sub-foot accuracy with very few anchor nodes 
provided sufficient events.
11 References
[1] CC2420 Data Sheet. Avaiable at http://www.chipcon.com/.
[2] P. Bahl and V. N. Padmanabhan. Radar: An In-Building RF-Based User
Location and Tracking System. In IEEE Infocom "00.
[3] M. Broxton, J. Lifton, and J. Paradiso. Localizing A Sensor Network via
Collaborative Processing of Global Stimuli. In EWSN "05.
[4] N. Bulusu, J. Heidemann, and D. Estrin. GPS-Less Low Cost Outdoor
Localization for Very Small Devices. IEEE Personal Communications
Magazine, 7(4), 2000.
[5] D. Culler, D. Estrin, and M. Srivastava. Overview of Sensor Networks.
IEEE Computer Magazine, 2004.
[6] J. Elson, L. Girod, and D. Estrin. Fine-Grained Network Time 
Synchronization Using Reference Broadcasts. In OSDI "02.
[7] D. K. Goldenberg, P. Bihler, M. Gao, J. Fang, B. D. Anderson, A. Morse,
and Y. Yang. Localization in Sparse Networks Using Sweeps. In 
MobiCom "06.
[8] T. He, C. Huang, B. M. Blum, J. A. Stankovic, and T. Abdelzaher. 
RangeFree Localization Schemes in Large-Scale Sensor Networks. In 
MobiCom "03.
[9] B. Kusy, P. Dutta, P. Levis, M. Mar, A. Ledeczi, and D. Culler. Elapsed
Time on Arrival: A Simple and Versatile Primitive for Canonical Time
Synchronization Services. International Journal of ad-hoc and 
Ubiquitous Computing, 2(1), 2006.
[10] L. Lazos and R. Poovendran. SeRLoc: Secure Range-Independent 
Localization for Wireless Sensor Networks. In WiSe "04.
[11] M. Maroti, B. Kusy, G. Balogh, P. Volgyesi, A. Nadas, K. Molnar,
S. Dora, and A. Ledeczi. Radio Interferometric Geolocation. In 
SenSys "05.
[12] M. Maroti, B. Kusy, G. Simon, and A. Ledeczi. The Flooding Time
Synchronization Protocol. In SenSys "04.
[13] D. Moore, J. Leonard, D. Rus, and S. Teller. Robust Distributed Network
Localization with Noise Range Measurements. In SenSys "04.
[14] R. Nagpal and D. Coore. An Algorithm for Group Formation in an 
Amorphous Computer. In PDCS "98.
[15] D. Niculescu and B. Nath. ad-hoc Positioning System. In GlobeCom
"01.
[16] D. Niculescu and B. Nath. ad-hoc Positioning System (APS) Using
AOA. In InfoCom "03.
[17] N. B. Priyantha, A. Chakraborty, and H. Balakrishnan. The Cricket
Location-Support System. In MobiCom "00.
[18] K. R¨omer. The Lighthouse Location System for Smart Dust. In MobiSys
"03.
[19] A. Savvides, C. C. Han, and M. B. Srivastava. Dynamic Fine-Grained
Localization in ad-hoc Networks of Sensors. In MobiCom "01.
[20] R. Stoleru, T. He, J. A. Stankovic, and D. Luebke. A High-Accuracy,
Low-Cost Localization System for Wireless Sensor Networks. In SenSys
"05.
[21] R. Stoleru, P. Vicaire, T. He, and J. A. Stankovic. StarDust: a Flexible
Architecture for Passive Localization in Wireless Sensor Networks. In
SenSys "06.
[22] E. W. Weisstein. Plane Division by Lines. mathworld.wolfram.com.
[23] B. H. Wellenhoff, H. Lichtenegger, and J. Collins. Global Positions 
System: Theory and Practice,Fourth Edition. Springer Verlag, 1997.
[24] K. Whitehouse. The Design of Calamari: an ad-hoc Localization System
for Sensor Networks. In University of California at Berkeley, 2002.
[25] Z. Zhong. MSP Evaluation and Implementation Report. Avaiable at
http://www.cs.umn.edu/∼zhong/MSP.
[26] G. Zhou, T. He, and J. A. Stankovic. Impact of Radio Irregularity on
Wireless Sensor Networks. In MobiSys "04.
28
An Evaluation of Availability Latency in Carrier-based
Vehicular ad-hoc Networks
Shahram
Ghandeharizadeh
Dept of Computer Science
Univ of Southern California
Los Angeles, CA 90089, USA
shahram@usc.edu
Shyam Kapadia
Dept of Computer Science
Univ of Southern California
Los Angeles, CA 90089, USA
kapadia@usc.edu
Bhaskar Krishnamachari
Dept of Computer Science
Dept of Electrical Engineering
Univ of Southern California
Los Angeles, CA 90089, USA
bkrishna@usc.edu
ABSTRACT
On-demand delivery of audio and video clips in peer-to-peer 
vehicular ad-hoc networks is an emerging area of research. Our target
environment uses data carriers, termed zebroids, where a mobile
device carries a data item on behalf of a server to a client thereby
minimizing its availability latency. In this study, we quantify the
variation in availability latency with zebroids as a function of a rich
set of parameters such as car density, storage per device, repository
size, and replacement policies employed by zebroids. Using 
analysis and extensive simulations, we gain novel insights into the 
design of carrier-based systems. Significant improvements in latency
can be obtained with zebroids at the cost of a minimal overhead.
These improvements occur even in scenarios with lower accuracy
in the predictions of the car routes. Two particularly surprising
findings are: (1) a naive random replacement policy employed by
the zebroids shows competitive performance, and (2) latency 
improvements obtained with a simplified instantiation of zebroids are
found to be robust to changes in the popularity distribution of the
data items.
Categories and Subject Descriptors: C.2.4 [Distributed Systems]:
Client/Server
General Terms: Algorithms, Performance, Design, 
Experimentation.
1. INTRODUCTION
Technological advances in areas of storage and wireless 
communications have now made it feasible to envision on-demand delivery
of data items, for e.g., video and audio clips, in vehicular 
peer-topeer networks. In prior work, Ghandeharizadeh et al. [10] 
introduce the concept of vehicles equipped with a 
Car-to-Car-Peer-toPeer device, termed AutoMata, for in-vehicle entertainment. The
notable features of an AutoMata include a mass storage device 
offering hundreds of gigabytes (GB) of storage, a fast processor, and
several types of networking cards. Even with today"s 500 GB disk
drives, a repository of diverse entertainment content may exceed
the storage capacity of a single AutoMata. Such repositories 
constitute the focus of this study. To exchange data, we assume each
AutoMata is configured with two types of networking cards: 1) a
low-bandwidth networking card with a long radio-range in the 
order of miles that enables an AutoMata device to communicate with
a nearby cellular or WiMax station, 2) a high-bandwidth 
networking card with a limited radio-range in the order of hundreds of feet.
The high bandwidth connection supports data rates in the 
order of tens to hundreds of Megabits per second and represents the
ad-hoc peer to peer network between the vehicles. This is 
labelled as the data plane and is employed to exchange data items
between devices. The low-bandwidth connection serves as the 
control plane, enabling AutoMata devices to exchange meta-data with
one or more centralized servers. This connection offers bandwidths
in the order of tens to hundreds of Kilobits per second. The 
centralized servers, termed dispatchers, compute schedules of data 
delivery along the data plane using the provided meta-data. These
schedules are transmitted to the participating vehicles using the
control plane. The technical feasibility of such a two-tier 
architecture is presented in [7], with preliminary results to demonstrate the
bandwidth of the control plane is sufficient for exchange of control
information needed for realizing such an application.
In a typical scenario, an AutoMata device presents a passenger
with a list of data items1
, showing both the name of each data item
and its availability latency. The latter, denoted as δ, is defined as
the earliest time at which the client encounters a copy of its 
requested data item. A data item is available immediately when it
resides in the local storage of the AutoMata device serving the 
request. Due to storage constraints, an AutoMata may not store the
entire repository. In this case, availability latency is the time from
when the user issues a request until when the AutoMata encounters
another car containing the referenced data item. (The terms car and
AutoMata are used interchangeably in this study.)
The availability latency for an item is a function of the current
location of the client, its destination and travel path, the mobility
model of the AutoMata equipped vehicles, the number of replicas
constructed for the different data items, and the placement of data
item replicas across the vehicles. A method to improve the 
availability latency is to employ data carriers which transport a replica
of the requested data item from a server car containing it to a client
that requested it. These data carriers are termed ‘zebroids".
Selection of zebroids is facilitated by the two-tiered architecture.
The control plane enables centralized information gathering at a
dispatcher present at a base-station.2
Some examples of control 
in1
Without loss of generality, the term data item might be either 
traditional media such as text or continuous media such as an audio or
video clip.
2
There may be dispatchers deployed at a subset of the base-stations
for fault-tolerance and robustness. Dispatchers between 
basestations may communicate via the wired infrastructure.
75
formation are currently active requests, travel path of the clients and
their destinations, and paths of the other cars. For each client 
request, the dispatcher may choose a set of z carriers that collaborate
to transfer a data item from a server to a client (z-relay zebroids).
Here, z is the number of zebroids such that 0 ≤ z < N, where
N is the total number of cars. When z = 0 there are no carriers,
requiring a server to deliver the data item directly to the client. 
Otherwise, the chosen relay team of z zebroids hand over the data item
transitively to one another to arrive at the client, thereby reducing
availability latency (see Section 3.1 for details). To increase 
robustness, the dispatcher may employ multiple relay teams of z-carriers
for every request. This may be useful in scenarios where the 
dispatcher has lower prediction accuracy in the information about the
routes of the cars. Finally, storage constraints may require a zebroid
to evict existing data items from its local storage to accommodate
the client requested item.
In this study, we quantify the following main factors that affect
availability latency in the presence of zebroids: (i) data item 
repository size, (ii) car density, (iii) storage capacity per car, (iv) client
trip duration, (v) replacement scheme employed by the zebroids,
and (vi) accuracy of the car route predictions. For a significant 
subset of these factors, we address some key questions pertaining to
use of zebroids both via analysis and extensive simulations.
Our main findings are as follows. A naive random replacement
policy employed by the zebroids shows competitive performance
in terms of availability latency. With such a policy, substantial 
improvements in latency can be obtained with zebroids at a minimal
replacement overhead. In more practical scenarios, where the 
dispatcher has inaccurate information about the car routes, zebroids
continue to provide latency improvements. A surprising result is
that changes in popularity of the data items do not impact the 
latency gains obtained with a simple instantiation of z-relay zebroids
called one-instantaneous zebroids (see Section 3.1). This study
suggests a number of interesting directions to be pursued to gain
better understanding of design of carrier-based systems that 
improve availability latency.
Related Work: Replication in mobile ad-hoc networks has been
a widely studied topic [11, 12, 15]. However, none of these 
studies employ zebroids as data carriers to reduce the latency of the
client"s requests. Several novel and important studies such as 
ZebraNet [13], DakNet [14], Data Mules [16], Message Ferries [20],
and Seek and Focus [17] have analyzed factors impacting 
intermittently connected networks consisting of data carriers similar in
spirit to zebroids. Factors considered by each study are dictated by
their assumed environment and target application. A novel 
characteristic of our study is the impact on availability latency for a
given database repository of items. A detailed description of 
related works can be obtained in [9].
The rest of this paper is organized as follows. Section 2 
provides an overview of the terminology along with the factors that
impact availability latency in the presence of zebroids. Section 3
describes how the zebroids may be employed. Section 4 provides
details of the analysis methodology employed to capture the 
performance with zebroids. Section 5 describes the details of the 
simulation environment used for evaluation. Section 6 enlists the key
questions examined in this study and answers them via analysis
and simulations. Finally, Section 7 presents brief conclusions and
future research directions.
2. OVERVIEW AND TERMINOLOGY
Table 1 summarizes the notation of the parameters used in the
paper. Below we introduce some terminology used in the paper.
Assume a network of N AutoMata-equipped cars, each with
storage capacity of α bytes. The total storage capacity of the 
system is ST =N ·α. There are T data items in the database, each with
Database Parameters
T Number of data items.
Si Size of data item i
fi Frequency of access to data item i.
Replication Parameters
Ri Normalized frequency of access to data item i
ri Number of replicas for data item i
n Characterizes a particular replication scheme.
δi Average availability latency of data item i
δagg Aggregate availability latency, δagg = T
j=1 δj · fj
AutoMata System Parameters
G Number of cells in the map (2D-torus).
N Number of AutoMata devices in the system.
α Storage capacity per AutoMata.
γ Trip duration of the client AutoMata.
ST Total storage capacity of the AutoMata system, ST = N · α.
Table 1: Terms and their definitions
size Si. The frequency of access to data item i is denoted as fi,
with T
j=1 fj = 1. Let the trip duration of the client AutoMata
under consideration be γ.
We now define the normalized frequency of access to the data
item i, denoted by Ri, as:
Ri =
(fi)n
T
j=1(fj)n
; 0 ≤ n ≤ ∞ (1)
The exponent n characterizes a particular replication technique.
A square-root replication scheme is realized when n = 0.5 [5].
This serves as the base-line for comparison with the case when 
zebroids are deployed. Ri is normalized to a value between 0 and
1. The number of replicas for data item i, denoted as ri, is: ri =
min (N, max (1, Ri·N·α
Si
)). This captures the case when at least
one copy of every data item must be present in the ad-hoc network
at all times. In cases where a data item may be lost from the ad-hoc
network, this equation becomes ri = min (N, max (0, Ri·N·α
Si
)).
In this case, a request for the lost data item may need to be satisfied
by fetching the item from a remote server.
The availability latency for a data item i, denoted as δi, is defined
as the earliest time at which a client AutoMata will find the first
replica of the item accessible to it. If this condition is not satisfied,
then we set δi to γ. This indicates that data item i was not available
to the client during its journey. Note that since there is at least one
replica in the system for every data item i, by setting γ to a large
value we ensure that the client"s request for any data item i will be
satisfied. However, in most practical circumstances γ may not be
so large as to find every data item.
We are interested in the availability latency observed across all
data items. Hence, we augment the average availability latency
for every data item i with its fi to obtain the following weighted
availability latency (δagg) metric: δagg = T
i=1 δi · fi
Next, we present our solution approach describing how zebroids
are selected.
3. SOLUTION APPROACH
3.1 Zebroids
When a client references a data item missing from its local 
storage, the dispatcher identifies all cars with a copy of the data item
as servers. Next, the dispatcher obtains the future routes of all cars
for a finite time duration equivalent to the maximum time the client
is willing to wait for its request to be serviced. Using this 
information, the dispatcher schedules the quickest delivery path from any
of the servers to the client using any other cars as intermediate 
carriers. Hence, it determines the optimal set of forwarding decisions
76
that will enable the data item to be delivered to the client in the
minimum amount of time. Note that the latency along the quickest
delivery path that employs a relay team of z zebroids is similar to
that obtained with epidemic routing [19] under the assumptions of
infinite storage and no interference.
A simple instantiation of z-relay zebroids occurs when z = 1
and the client"s request triggers a transfer of a copy of the requested
data item from a server to a zebroid in its vicinity. Such a 
zebroid is termed one-instantaneous zebroid. In some cases, the
dispatcher might have inaccurate information about the routes of
the cars. Hence, a zebroid scheduled on the basis of this inaccurate
information may not rendezvous with its target client. To minimize
the likelihood of such scenarios, the dispatcher may schedule 
multiple zebroids. This may incur additional overhead due to redundant
resource utilization to obtain the same latency improvements.
The time required to transfer a data item from a server to a 
zebroid depends on its size and the available link bandwidth. With
small data items, it is reasonable to assume that this transfer time is
small, especially in the presence of the high bandwidth data plane.
Large data items may be divided into smaller chunks enabling the
dispatcher to schedule one or more zebroids to deliver each chunk
to a client in a timely manner. This remains a future research 
direction.
Initially, number of replicas for each data item replicas might be
computed using Equation 1. This scheme computes the number
of data item replicas as a function of their popularity. It is static
because number of replicas in the system do not change and no
replacements are performed. Hence, this is referred to as the 
‘nozebroids" environment. We quantify the performance of the various
replacement policies with reference to this base-line that does not
employ zebroids.
One may assume a cold start phase, where initially only one or
few copies of every data item exist in the system. Many storage
slots of the cars may be unoccupied. When the cars encounter one
another they construct new replicas of some selected data items to
occupy the empty slots. The selection procedure may be to choose
the data items uniformly at random. New replicas are created as
long as a car has a certain threshold of its storage unoccupied.
Eventually, majority of the storage capacity of a car will be 
exhausted.
3.2 Carrier-based Replacement policies
The replacement policies considered in this paper are reactive
since a replacement occurs only in response to a request issued for a
certain data item. When the local storage of a zebroid is completely
occupied, it needs to replace one of its existing items to carry the
client requested data item. For this purpose, the zebroid must 
select an appropriate candidate for eviction. This decision process
is analogous to that encountered in operating system paging where
the goal is to maximize the cache hit ratio to prevent disk access 
delay [18]. The carrier-based replacement policies employed in our
study are Least Recently Used (LRU), Least Frequently Used
(LFU) and Random (where a eviction candidate is chosen 
uniformly at random). We have considered local and global variants
of the LRU/LFU policies which determine whether local or global
knowledge of contents of the cars known at the dispatcher is used
for the eviction decision at a zebroid (see [9] for more details).
The replacement policies incur the following overheads. First,
the complexity associated with the implementation of a policy. 
Second, the bandwidth used to transfer a copy of a data item from a
server to the zebroid. Third, the average number of replacements
incurred by the zebroids. Note that in the no-zebroids case neither
overhead is incurred.
The metrics considered in this study are aggregate availability 
latency, δagg, percentage improvement in δagg with zebroids as 
compared to the no-zebroids case and average number of replacements
incurred per client request which is an indicator of the overhead
incurred by zebroids.
Note that the dispatchers with the help of the control plane may
ensure that no data item is lost from the system. In other words,
at least one replica of every data item is maintained in the ad-hoc
network at all times. In such cases, even though a car may meet a
requesting client earlier than other servers, if its local storage 
contains data items with only a single copy in the system, then such a
car is not chosen as a zebroid.
4. ANALYSIS METHODOLOGY
Here, we present the analytical evaluation methodology and some
approximations as closed-form equations that capture the 
improvements in availability latency that can be obtained with both 
oneinstantaneous and z-relay zebroids. First, we present some 
preliminaries of our analysis methodology.
• Let N be the number of cars in the network performing a 2D
random walk on a
√
G×
√
G torus. An additional car serves
as a client yielding a total of N + 1 cars. Such a mobility
model has been used widely in the literature [17, 16] chiefly
because it is amenable to analysis and provides a baseline
against which performance of other mobility models can be
compared. Moreover, this class of Markovian mobility 
models has been used to model the movements of vehicles [3,
21].
• We assume that all cars start from the stationary distribution
and perform independent random walks. Although for sparse
density scenarios, the independence assumption does hold, it
is no longer valid when N approaches G.
• Let the size of data item repository of interest be T. Also,
data item i has ri replicas. This implies ri cars, identified as
servers, have a copy of this data item when the client requests
item i.
All analysis results presented in this section are obtained 
assuming that the client is willing to wait as long as it takes for its request
to be satisfied (unbounded trip duration γ = ∞). With the random
walk mobility model on a 2D-torus, there is a guarantee that as
long as there is at least one replica of the requested data item in the
network, the client will eventually encounter this replica [2]. 
Extensions to the analysis that also consider finite trip durations can
be obtained in [9].
Consider a scenario where no zebroids are employed. In this
case, the expected availability latency for the data item is the 
expected meeting time of the random walk undertaken by the client
with any of the random walks performed by the servers. Aldous et
al. [2] show that the the meeting time of two random walks in such
a setting can be modelled as an exponential distribution with the
mean C = c · G · log G, where the constant c 0.17 for G ≥ 25.
The meeting time, or equivalently the availability latency δi, for
the client requesting data item i is the time till it encounters any of
these ri replicas for the first time. This is also an exponential 
distribution with the following expected value (note that this formulation
is valid only for sparse cases when G >> ri): δi = cGlogG
ri
The aggregate availability latency without employing zebroids is
then this expression averaged over all data items, weighted by their
frequency of access:
δagg(no − zeb) =
T
i=1
fi · c · G · log G
ri
=
T
i=1
fi · C
ri
(2)
77
4.1 One-instantaneous zebroids
Recall that with one-instantaneous zebroids, for a given request,
a new replica is created on a car in the vicinity of the server, 
provided this car meets the client earlier than any of the ri servers.
Moreover, this replica is spawned at the time step when the client
issues the request. Let Nc
i be the expected total number of nodes
that are in the same cell as any of the ri servers. Then, we have
Nc
i = (N − ri) · (1 − (1 −
1
G
)ri
) (3)
In the analytical model, we assume that Nc
i new replicas are
created, so that the total number of replicas is increased to ri +Nc
i .
The availability latency is reduced since the client is more likely to
meet a replica earlier. The aggregated expected availability latency
in the case of one-instantaneous zebroids is then given by,
δagg(zeb) =
T
i=1
fi · c · G · log G
ri + Nc
i
=
T
i=1
fi · C
ri + Nc
i
(4)
Note that in obtaining this expression, for ease of analysis, we
have assumed that the new replicas start from random locations
in the torus (not necessarily from the same cell as the original ri
servers). It thus treats all the Nc
i carriers independently, just like
the ri original servers. As we shall show below by comparison
with simulations, this approximation provides an upper-bound on
the improvements that can be obtained because it results in a lower
expected latency at the client.
It should be noted that the procedure listed above will yield a
similar latency to that employed by a dispatcher employing 
oneinstantaneous zebroids (see Section 3.1). Since the dispatcher is
aware of all future car movements it would only transfer the 
requested data item on a single zebroid, if it determines that the 
zebroid will meet the client earlier than any other server. This selected
zebroid is included in the Nc
i new replicas.
4.2 z-relay zebroids
To calculate the expected availability latency with z-relay 
zebroids, we use a coloring problem analog similar to an approach
used by Spyropoulos et al. [17]. Details of the procedure to obtain
a closed-form expression are given in [9]. The aggregate 
availability latency (δagg) with z-relay zebroids is given by,
δagg(zeb) =
T
i=1
[fi ·
C
N + 1
·
1
N + 1 − ri
·
(N · log
N
ri
− log (N + 1 − ri))] (5)
5. SIMULATION METHODOLOGY
The simulation environment considered in this study comprises
of vehicles such as cars that carry a fraction of the data item 
repository. A prediction accuracy parameter inherently provides a certain
probabilistic guarantee on the confidence of the car route 
predictions known at the dispatcher. A value of 100% implies that the
exact routes of all cars are known at all times. A 70% value for this
parameter indicates that the routes predicted for the cars will match
the actual ones with probability 0.7. Note that this probability is
spread across the car routes for the entire trip duration. We now
provide the preliminaries of the simulation study and then describe
the parameter settings used in our experiments.
• Similar to the analysis methodology, the map used is a 2D
torus. A Markov mobility model representing a unbiased 2D
random walk on the surface of the torus describes the 
movement of the cars across this torus.
• Each grid/cell is a unique state of this Markov chain. In each
time slot, every car makes a transition from a cell to any of
its neighboring 8 cells. The transition is a function of the
current location of the car and a probability transition matrix
Q = [qij] where qij is the probability of transition from state
i to state j. Only AutoMata equipped cars within the same
cell may communicate with each other.
• The parameters γ, δ have been discretized and expressed in
terms of the number of time slots.
• An AutoMata device does not maintain more than one replica
of a data item. This is because additional replicas occupy
storage without providing benefits.
• Either one-instantaneous or z-relay zebroids may be employed
per client request for latency improvement.
• Unless otherwise mentioned, the prediction accuracy 
parameter is assumed to be 100%. This is because this study
aims to quantify the effect of a large number of parameters
individually on availability latency.
Here, we set the size of every data item, Si, to be 1. α represents
the number of storage slots per AutoMata. Each storage slot stores
one data item. γ represents the duration of the client"s journey in
terms of the number of time slots. Hence the possible values of
availability latency are between 0 and γ. δ is defined as the number
of time slots after which a client AutoMata device will encounter a
replica of the data item for the first time. If a replica for the data
item requested was encountered by the client in the first cell then
we set δ = 0. If δ > γ then we set δ = γ indicating that no copy
of the requested data item was encountered by the client during its
entire journey. In all our simulations, for illustration we consider a
5 × 5 2D-torus with γ set to 10. Our experiments indicate that the
trends in the results scale to maps of larger size.
We simulated a skewed distribution of access to the T data items
that obeys Zipf"s law with a mean of 0.27. This distribution is
shown to correspond to sale of movie theater tickets in the United
States [6]. We employ a replication scheme that allocates replicas
for a data item as a function of the square-root of the frequency of
access of that item. The square-root replication scheme is shown
to have competitive latency performance over a large parameter
space [8]. The data item replicas are distributed uniformly across
the AutoMata devices. This serves as the base-line no-zebroids
case. The square-root scheme also provides the initial replica 
distribution when zebroids are employed. Note that the replacements
performed by the zebroids will cause changes to the data item replica
distribution. Requests generated as per the Zipf distribution are 
issued one at a time. The client car that issues the request is chosen
in a round-robin manner. After a maximum period of γ, the latency
encountered by this request is recorded.
In all the simulation results, each point is an average of 200,000
requests. Moreover, the 95% confidence intervals determined for
the results are quite tight for the metrics of latency and 
replacement overhead. Hence, we only present them for the metric that
captures the percentage improvement in latency with respect to the
no-zebroids case.
6. RESULTS
In this section, we describe our evaluation results where the 
following key questions are addressed. With a wide choice of 
replacement schemes available for a zebroid, what is their effect on 
availability latency? A more central question is: Do zebroids provide
78
0 20 40 60 80 100
1.5
2
2.5
3
3.5
Number of cars
Aggregate availability latency (δ
agg
)
lru_global
lfu_global
lru_local
lfu_local
random
Figure 1: Figure 1 shows the availability latency when 
employing one-instantaneous zebroids as a function of (N,α) values,
when the total storage in the system is kept fixed, ST = 200.
significant improvements in availability latency? What is the 
associated overhead incurred in employing these zebroids? What 
happens to these improvements in scenarios where a dispatcher may
have imperfect information about the car routes? What inherent
trade-offs exist between car density and storage per car with 
regards to their combined as well as individual effect on availability
latency in the presence of zebroids? We present both simple 
analysis and detailed simulations to provide answers to these questions
as well as gain insights into design of carrier-based systems.
6.1 How does a replacement scheme employed
by a zebroid impact availability latency?
For illustration, we present ‘scale-up" experiments where 
oneinstantaneous zebroids are employed (see Figure 1). By scale-up,
we mean that α and N are changed proportionally to keep the total
system storage, ST , constant. Here, T = 50 and ST = 200. We
choose the following values of (N,α) = {(20,10), (25,8), (50,4),
(100,2)}. The figure indicates that a random replacement scheme
shows competitive performance. This is because of several reasons.
Recall that the initial replica distribution is set as per the 
squareroot replication scheme. The random replacement scheme does not
alter this distribution since it makes replacements blind to the 
popularity of a data item. However, the replacements cause dynamic
data re-organization so as to better serve the currently active 
request. Moreover, the mobility pattern of the cars is random, hence,
the locations from which the requests are issued by clients are also
random and not known a priori at the dispatcher. These findings
are significant because a random replacement policy can be 
implemented in a simple decentralized manner.
The lru-global and lfu-global schemes provide a latency 
performance that is worse than random. This is because these 
policies rapidly develop a preference for the more popular data items
thereby creating a larger number of replicas for them. During 
eviction, the more popular data items are almost never selected as a
replacement candidate. Consequently, there are fewer replicas for
the less popular items. Hence, the initial distribution of the data
item replicas changes from square-root to that resembling linear
replication. The higher number of replicas for the popular data
items provide marginal additional benefits, while the lower number
of replicas for the other data items hurts the latency performance of
these global policies. The lfu-local and lru-local schemes have 
similar performance to random since they do not have enough history
of local data item requests. We speculate that the performance of
these local policies will approach that of their global variants for a
large enough history of data item requests. On account of the 
competitive performance shown by a random policy, for the remainder
of the paper, we present the performance of zebroids that employ a
random replacement policy.
6.2 Do zebroids provide significant 
improvements in availability latency?
We find that in many scenarios employing zebroids provides 
substantial improvements in availability latency.
6.2.1 Analysis
We first consider the case of one-instantaneous zebroids. 
Figure 2.a shows the variation in δagg as a function of N for T = 10
and α = 1 with a 10 × 10 torus using Equation 4. Both the x and y
axes are drawn to a log-scale. Figure 2.b show the % improvement
in δagg obtained with one-instantaneous zebroids. In this case, only
the x-axis is drawn to a log-scale. For illustration, we assume that
the T data items are requested uniformly.
Initially, when the network is sparse the analytical approximation
for improvements in latency with zebroids, obtained from 
Equations 2 and 4, closely matches the simulation results. However, as
N increases, the sparseness assumption for which the analysis is
valid, namely N << G, is no longer true. Hence, the two curves
rapidly diverge. The point at which the two curves move away from
each other corresponds to a value of δagg ≤ 1. Moreover, as 
mentioned earlier, the analysis provides an upper bound on the latency
improvements, as it treats the newly created replicas given by Nc
i
independently. However, these Nc
i replicas start from the same cell
as one of the server replicas ri. Finally, the analysis captures a 
oneshot scenario where given an initial data item replica distribution,
the availability latency is computed. The new replicas created do
not affect future requests from the client.
On account of space limitations, here, we summarize the 
observations in the case when z-relay zebroids are employed. The 
interested reader can obtain further details in [9]. Similar observations,
like the one-instantaneous zebroid case, apply since the simulation
and analysis curves again start diverging when the analysis 
assumptions are no longer valid. However, the key observation here is that
the latency improvement with z-relay zebroids is significantly 
better than the one-instantaneous zebroids case, especially for lower
storage scenarios. This is because in sparse scenarios, the 
transitive hand-offs between the zebroids creates higher number of 
replicas for the requested data item, yielding lower availability latency.
Moreover, it is also seen that the simulation validation curve for the
improvements in δagg with z-relay zebroids approaches that of the
one-instantaneous zebroid case for higher storage (higher N 
values). This is because one-instantaneous zebroids are a special case
of z-relay zebroids.
6.2.2 Simulation
We conduct simulations to examine the entire storage spectrum
obtained by changing car density N or storage per car α to also
capture scenarios where the sparseness assumptions for which the
analysis is valid do not hold. We separate the effect of N and α
by capturing the variation of N while keeping α constant (case
1) and vice-versa (case 2) both with z-relay and one-instantaneous
zebroids. Here, we set the repository size as T = 25. Figure 3
captures case 1 mentioned above. Similar trends are observed with
case 2, a complete description of those results are available in [9].
With Figure 3.b, keeping α constant, initially increasing car 
density has higher latency benefits because increasing N introduces
more zebroids in the system. As N is further increased, ω reduces
because the total storage in the system goes up. Consequently, the
number of replicas per data item goes up thereby increasing the
79
number of servers. Hence, the replacement policy cannot find a
zebroid as often to transport the requested data item to the client
earlier than any of the servers. On the other hand, the increased
number of servers benefits the no-zebroids case in bringing δagg
down. The net effect results in reduction in ω for larger values of
N.
10
1
10
2
10
3
10
−1
10
0
10
1
10
2
Number of cars
no−zebroidsanal
no−zebroids
sim
one−instantaneous
anal
one−instantaneoussim
Aggregate Availability latency (δagg
)
2.a) δagg
10
1
10
2
10
3
0
10
20
30
40
50
60
70
80
90
100
Number of cars
% Improvement in δagg
wrt no−zebroids (ω)
analytical upper−bound
simulation
2.b) ω
Figure 2: Figure 2 shows the latency performance with 
oneinstantaneous zebroids via simulations along with the 
analytical approximation for a 10 × 10 torus with T = 10.
The trends mentioned above are similar to that obtained from the
analysis. However, somewhat counter-intuitively with relatively
higher system storage, z-relay zebroids provide slightly lower 
improvements in latency as compared to one-instantaneous zebroids.
We speculate that this is due to the different data item replica 
distributions enforced by them. Note that replacements performed by
the zebroids cause fluctuations in these replica distributions which
may effect future client requests. We are currently exploring 
suitable choices of parameters that can capture these changing replica
distributions.
6.3 What is the overhead incurred with 
improvements in latency with zebroids?
We find that the improvements in latency with zebroids are 
obtained at a minimal replacement overhead (< 1 per client request).
6.3.1 Analysis
With one-instantaneous zebroids, for each client request a 
maximum of one zebroid is employed for latency improvement. Hence,
the replacement overhead per client request can amount to a 
maximum of one. Recall that to calculate the latency with one-instantaneous
0 50 100 150 200 250 300 350 400
0
1
2
3
4
5
6
Number of cars
Aggregate availability latency (δagg
)
no−zebroids
one−instantaneous
z−relays
3.a
0 50 100 150 200 250 300 350 400
0
10
20
30
40
50
60
Number of cars
% Improvement in δagg
wrt no−zebroids (ω)
one−instantaneous
z−relays
3.b
Figure 3: Figure 3 depicts the latency performance with both
one-instantaneous and z-relay zebroids as a function of the car
density when α = 2 and T = 25.
zebroids, Nc
i new replicas are created in the same cell as the servers.
Now a replacement is only incurred if one of these Nc
i newly 
created replicas meets the client earlier than any of the ri servers.
Let Xri and XNc
i
respectively be random variables that capture
the minimum time till any of the ri and Nc
i replicas meet the client.
Since Xri and XNc
i
are assumed to be independent, by the property
of exponentially distributed random variables we have,
Overhead/request = 1 · P(XNc
i
< Xri ) + 0 · P(Xri ≤ XNc
i
)
(6)
Overhead/request =
ri
C
ri
C
+
Nc
i
C
=
ri
ri + Nc
i
(7)
Recall that the number of replicas for data item i, ri, is a function
of the total storage in the system i.e., ri = k·N ·α where k satisfies
the constraint 1 ≤ ri ≤ N. Using this along with Equation 2, we
get
Overhead/request = 1 −
G
G + N · (1 − k · α)
(8)
Now if we keep the total system storage N · α constant since
G and T are also constant, increasing N increases the replacement
overhead. However, if N ·α is constant then increasing N causes α
80
0 20 40 60 80 100
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Number of cars
one−instantaneous
zebroids
Average number of replacements per request
(N=20,α=10)
(N=25,α=8)
(N=50,α=4)
(N=100,α=2)
Figure 4: Figure 4 captures replacement overhead when 
employing one-instantaneous zebroids as a function of (N,α) 
values, when the total storage in the system is kept fixed, ST =
200.
to go down. This implies that a higher replacement overhead is 
incurred for higher N and lower α values. Moreover, when ri = N,
this means that every car has a replica of data item i. Hence, no
zebroids are employed when this item is requested, yielding an
overhead/request for this item as zero. Next, we present 
simulation results that validate our analysis hypothesis for the overhead
associated with deployment of one-instantaneous zebroids.
6.3.2 Simulation
Figure 4 shows the replacement overhead with one-instantaneous
zebroids when (N,α) are varied while keeping the total system 
storage constant. The trends shown by the simulation are in agreement
with those predicted by the analysis above. However, the total 
system storage can be changed either by varying car density (N) or
storage per car (α). On account of similar trends, here we present
the case when α is kept constant and N is varied (Figure 5). We
refer the reader to [9] for the case when α is varied and N is held
constant.
We present an intuitive argument for the behavior of the 
perrequest replacement overhead curves. When the storage is extremely
scarce so that only one replica per data item exists in the AutoMata
network, the number of replacements performed by the zebroids is
zero since any replacement will cause a data item to be lost from
the system. The dispatcher ensures that no data item is lost from
the system. At the other end of the spectrum, if storage becomes
so abundant that α = T then the entire data item repository can
be replicated on every car. The number of replacements is again
zero since each request can be satisfied locally. A similar scenario
occurs if N is increased to such a large value that another car with
the requested data item is always available in the vicinity of the
client. However, there is a storage spectrum in the middle where
replacements by the scheduled zebroids result in improvements in
δagg (see Figure 3).
Moreover, we observe that for sparse storage scenarios, the higher
improvements with z-relay zebroids are obtained at the cost of a
higher replacement overhead when compared to the one-instantaneous
zebroids case. This is because in the former case, each of these z
zebroids selected along the lowest latency path to the client needs
to perform a replacement. However, the replacement overhead is
still less than 1 indicating that on an average less than one 
replacement per client request is needed even when z-relay zebroids are
employed.
0 50 100 150 200 250 300 350 400
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of cars
z−relays
one−instantaneous
Average number of replacements per request
Figure 5: Figure 5 shows the replacement overhead with 
zebroids for the cases when N is varied keeping α = 2.
10 20 30 40 50 60 70 80 90 100
0
0.5
1
1.5
2
2.5
3
3.5
4
Prediction percentage
no−zebroids (N=50)
one−instantaneous (N=50)
z−relays (N=50)
no−zebroids (N=200)
one−instantaneous (N=200) z−relays (N=200)
Aggregate Availability Latency (δagg
)
Figure 6: Figure 6 shows δagg for different car densities as a
function of the prediction accuracy metric with α = 2 and T =
25.
6.4 What happens to the availability latency
with zebroids in scenarios with 
inaccuracies in the car route predictions?
We find that zebroids continue to provide improvements in 
availability latency even with lower accuracy in the car route 
predictions. We use a single parameter p to quantify the accuracy of the
car route predictions.
6.4.1 Analysis
Since p represents the probability that a car route predicted by the
dispatcher matches the actual one, hence, the latency with zebroids
can be approximated by,
δerr
agg = p · δagg(zeb) + (1 − p) · δagg(no − zeb) (9)
δerr
agg = p · δagg(zeb) + (1 − p) ·
C
ri
(10)
Expressions for δagg(zeb) can be obtained from Equations 4
(one-instantaneous) or 5 (z-relay zebroids).
6.4.2 Simulation
Figure 6 shows the variation in δagg as a function of this route
prediction accuracy metric. We observe a smooth reduction in the
81
improvement in δagg as the prediction accuracy metric reduces. For
zebroids that are scheduled but fail to rendezvous with the client
due to the prediction error, we tag any such replacements made by
the zebroids as failed. It is seen that failed replacements gradually
increase as the prediction accuracy reduces.
6.5 Under what conditions are the 
improvements in availability latency with zebroids
maximized?
Surprisingly, we find that the improvements in latency obtained
with one-instantaneous zebroids are independent of the input 
distribution of the popularity of the data items.
6.5.1 Analysis
The fractional difference (labelled ω) in the latency between the
no-zebroids and one-instantaneous zebroids is obtained from 
equations 2, 3, and 4 as
ω =
T
i=1
fi·C
ri
− T
i=1
fi·C
ri+(N−ri)·(1−(1− 1
G )ri
)
T
i=1
fi·C
ri
(11)
Here C = c·G·log G. This captures the fractional improvement
in the availability latency obtained by employing one-instantaneous
zebroids. Let α = 1, making the total storage in the system ST =
N. Assuming the initial replica distribution is as per the 
squareroot replication scheme, we have, ri =
√
fi·N
T
j=1
√
fj
. Hence, we get
fi =
K2
·r2
i
N2 , where K = T
j=1 fj. Using this, along with the
approximation (1 − x)n
1 − n · x for small x, we simplify the
above equation to get, ω = 1 −
T
i=1
ri
1+
N−ri
G
T
i=1 ri
In order to determine when the gains with one-instantaneous 
zebroids are maximized, we can frame an optimization problem as
follows: Maximize ω, subject to T
i=1 ri = ST
THEOREM 1. With a square-root replication scheme, 
improvements obtained with one-instantaneous zebroids are independent
of the input popularity distribution of the data items. (See [9] for
proof)
6.5.2 Simulation
We perform simulations with two different frequency 
distribution of data items: Uniform and Zipfian (with mean= 0.27). 
Similar latency improvements with one-instantaneous zebroids are 
obtained in both cases. This result has important implications. In
cases with biased popularity toward certain data items, the 
aggregate improvements in latency across all data item requests still 
remain the same. Even in scenarios where the frequency of access
to the data items changes dynamically, zebroids will continue to
provide similar latency improvements.
7. CONCLUSIONS AND
FUTURE RESEARCH DIRECTIONS
In this study, we examined the improvements in latency that can
be obtained in the presence of carriers that deliver a data item from
a server to a client. We quantified the variation in availability 
latency as a function of a rich set of parameters such as car density,
storage per car, title database size, and replacement policies 
employed by zebroids.
Below we summarize some key future research directions we 
intend to pursue. To better reflect reality we would like to validate the
observations obtained from this study with some real world 
simulation traces of vehicular movements (for example using 
CORSIM [1]). This will also serve as a validation for the utility of the
Markov mobility model used in this study. We are currently 
analyzing the performance of zebroids on a real world data set 
comprising of an ad-hoc network of buses moving around a small 
neighborhood in Amherst [4]. Zebroids may also be used for delivery
of data items that carry delay sensitive information with a certain
expiry. Extensions to zebroids that satisfy such application 
requirements presents an interesting future research direction.
8. ACKNOWLEDGMENTS
This research was supported in part by an Annenberg fellowship and NSF
grants numbered CNS-0435505 (NeTS NOSS), CNS-0347621 (CAREER),
and IIS-0307908.
9. REFERENCES
[1] Federal Highway Administration. Corridor simulation. Version 5.1,
http://www.ops.fhwa.dot.gov/trafficanalysistools/cors im.htm.
[2] D. Aldous and J. Fill. Reversible markov chains and random walks
on graphs. Under preparation.
[3] A. Bar-Noy, I. Kessler, and M. Sidi. Mobile Users: To Update or Not
to Update. In IEEE Infocom, 1994.
[4] J. Burgess, B. Gallagher, D. Jensen, and B. Levine. MaxProp:
Routing for Vehicle-Based Disruption-Tolerant Networking. In IEEE
Infocom, April 2006.
[5] E. Cohen and S. Shenker. Replication Strategies in Unstructured
Peer-to-Peer Networks. In SIGCOMM, 2002.
[6] A. Dan, D. Dias, R. Mukherjee, D. Sitaram, and R. Tewari. Buffering
and Caching in Large-Scale Video Servers. In COMPCON, 1995.
[7] S. Ghandeharizadeh, S. Kapadia, and B. Krishnamachari. PAVAN: A
Policy Framework for Content Availabilty in Vehicular ad-hoc
Networks. In VANET, New York, NY, USA, 2004. ACM Press.
[8] S. Ghandeharizadeh, S. Kapadia, and B. Krishnamachari.
Comparison of Replication Strategies for Content Availability in
C2P2 networks. In MDM, May 2005.
[9] S. Ghandeharizadeh, S. Kapadia, and B. Krishnamachari. An
Evaluation of Availability Latency in Carrier-based Vehicular ad-hoc
Networks. Technical report, Department of Computer Science,
University of Southern California,CENG-2006-1, 2006.
[10] S. Ghandeharizadeh and B. Krishnamachari. C2p2: A peer-to-peer
network for on-demand automobile information services. In Globe.
IEEE, 2004.
[11] T. Hara. Effective Replica Allocation in ad-hoc Networks for
Improving Data Accessibility. In IEEE Infocom, 2001.
[12] H. Hayashi, T. Hara, and S. Nishio. A Replica Allocation Method
Adapting to Topology Changes in ad-hoc Networks. In DEXA, 2005.
[13] P. Juang, H. Oki, Y. Wang, M. Martonosi, L. Peh, and D. Rubenstein.
Energy-efficient computing for wildlife tracking: design tradeoffs
and early experiences with ZebraNet. SIGARCH Comput. Archit.
News, 2002.
[14] A. Pentland, R. Fletcher, and A. Hasson. DakNet: Rethinking
Connectivity in Developing Nations. Computer, 37(1):78-83, 2004.
[15] F. Sailhan and V. Issarny. Cooperative Caching in ad-hoc Networks.
In MDM, 2003.
[16] R. Shah, S. Roy, S. Jain, and W. Brunette. Data mules: Modeling and
analysis of a three-tier architecture for sparse sensor networks.
Elsevier ad-hoc Networks Journal, 1, September 2003.
[17] T. Spyropoulos, K. Psounis, and C. Raghavendra. Single-Copy
Routing in Intermittently Connected Mobile Networks. In SECON,
April 2004.
[18] A. Tanenbaum. Modern Operating Systems, 2nd Edition, Chapter 4,
Section 4.4 . Prentice Hall, 2001.
[19] A. Vahdat and D. Becker. Epidemic routing for partially-connected
ad-hoc networks. Technical report, Department of Computer Science,
Duke University, 2000.
[20] W. Zhao, M. Ammar, and E. Zegura. A message ferrying approach
for data delivery in sparse mobile ad-hoc networks. In MobiHoc,
pages 187-198, New York, NY, USA, 2004. ACM Press.
[21] M. Zonoozi and P. Dassanayake. User Mobility Modeling and
Characterization of Mobility Pattern. IEEE Journal on Selected
Areas in Communications, 15:1239-1252, September 1997.
82
Evaluating Opportunistic Routing Protocols
with Large Realistic Contact Traces
Libo Song and David F. Kotz
Institute for Security Technology Studies (ISTS)
Department of Computer Science, Dartmouth College, Hanover, NH, USA 03755
ABSTRACT
Traditional mobile ad-hoc network (MANET) routing protocols 
assume that contemporaneous end-to-end communication paths exist
between data senders and receivers. In some mobile ad-hoc 
networks with a sparse node population, an end-to-end 
communication path may break frequently or may not exist at any time. Many
routing protocols have been proposed in the literature to address the
problem, but few were evaluated in a realistic opportunistic 
network setting. We use simulation and contact traces (derived from
logs in a production network) to evaluate and compare five 
existing protocols: direct-delivery, epidemic, random, PRoPHET, and
Link-State, as well as our own proposed routing protocol. We show
that the direct delivery and epidemic routing protocols suffer either
low delivery ratio or high resource usage, and other protocols make
tradeoffs between delivery ratio and resource usage.
Categories and Subject Descriptors
C.2.4 [Computer Systems Organization]: Computer 
Communication Networks-Distributed Systems
General Terms
Performance, Design
1. INTRODUCTION
Mobile opportunistic networks are one kind of delay-tolerant
network (DTN) [6]. Delay-tolerant networks provide service 
despite long link delays or frequent link breaks. Long link delays 
happen in networks with communication between nodes at a great 
distance, such as interplanetary networks [2]. Link breaks are caused
by nodes moving out of range, environmental changes, interference
from other moving objects, radio power-offs, or failed nodes. For
us, mobile opportunistic networks are those DTNs with sparse node
population and frequent link breaks caused by power-offs and the
mobility of the nodes.
Mobile opportunistic networks have received increasing interest
from researchers. In the literature, these networks include mobile
sensor networks [25], wild-animal tracking networks [11], 
pocketswitched networks [8], and transportation networks [1, 14]. We
expect to see more opportunistic networks when the 
one-laptopper-child (OLPC) project [18] starts rolling out inexpensive 
laptops with wireless networking capability for children in developing
countries, where often no infrastructure exits. Opportunistic 
networking is one promising approach for those children to exchange
information.
One fundamental problem in opportunistic networks is how to
route messages from their source to their destination. Mobile 
opportunistic networks differ from the Internet in that disconnections
are the norm instead of the exception. In mobile opportunistic 
networks, communication devices can be carried by people [4], 
vehicles [1] or animals [11]. Some devices can form a small mobile
ad-hoc network when the nodes move close to each other. But a
node may frequently be isolated from other nodes. Note that 
traditional Internet routing protocols and ad-hoc routing protocols, such
as AODV [20] or DSDV [19], assume that a contemporaneous 
endto-end path exists, and thus fail in mobile opportunistic networks.
Indeed, there may never exist an end-to-end path between two given
devices.
In this paper, we study protocols for routing messages between
wireless networking devices carried by people. We assume that
people send messages to other people occasionally, using their 
devices; when no direct link exists between the source and the 
destination of the message, other nodes may relay the message to the
destination. Each device represents a unique person (it is out of the
scope of this paper when a device maybe carried by multiple 
people). Each message is destined for a specific person and thus for
a specific node carried by that person. Although one person may
carry multiple devices, we assume that the sender knows which
device is the best to receive the message. We do not consider 
multicast or geocast in this paper.
Many routing protocols have been proposed in the literature.
Few of them were evaluated in realistic network settings, or even in
realistic simulations, due to the lack of any realistic people 
mobility model. Random walk or random way-point mobility models are
often used to evaluate the performance of those routing protocols.
Although these synthetic mobility models have received extensive
interest by mobile ad-hoc network researchers [3], they do not 
reflect people"s mobility patterns [9]. Realising the limitations of 
using random mobility models in simulations, a few researchers have
studied routing protocols in mobile opportunistic networks with 
realistic mobility traces. Chaintreau et al. [5] theoretically analyzed
the impact of routing algorithms over a model derived from a 
realistic mobility data set. Su et al. [22] simulated a set of routing
35
protocols in a small experimental network. Those studies help 
researchers better understand the theoretical limits of opportunistic
networks, and the routing protocol performance in a small network
(20-30 nodes).
Deploying and experimenting large-scale mobile opportunistic
networks is difficult, we too resort to simulation. Instead of 
using a complex mobility model to mimic people"s mobility patterns,
we used mobility traces collected in a production wireless 
network at Dartmouth College to drive our simulation. Our 
messagegeneration model, however, was synthetic.
To the best of our knowledge, we are the first to simulate the
effect of routing protocols in a large-scale mobile opportunistic
network, using realistic contact traces derived from real traces of
a production network with more than 5, 000 users.
Using realistic contact traces, we evaluate the performance of
three naive routing protocols (direct-delivery, epidemic, and 
random) and two prediction-based routing protocols, PRoPHET [16]
and Link-State [22]. We also propose a new prediction-based 
routing protocol, and compare it to the above in our evaluation.
2. ROUTING PROTOCOL
A routing protocol is designed for forwarding messages from one
node (source) to another node (destination). Any node may 
generate messages for any other node, and may carry messages destined
for other nodes. In this paper, we consider only messages that are
unicast (single destination).
DTN routing protocols could be described in part by their 
transfer probability and replication probability; that is, when one node
meets another node, what is the probability that a message should
be transfered and if so, whether the sender should retain its copy.
Two extremes are the direct-delivery protocol and the epidemic
protocol. The former transfers with probability 1 when the node
meets the destination, 0 for others, and no replication. The latter
uses transfer probability 1 for all nodes and unlimited replication.
Both these protocols have their advantages and disadvantages. All
other protocols are between the two extremes.
First, we define the notion of contact between two nodes. Then
we describe five existing protocols before presenting our own 
proposal.
A contact is defined as a period of time during which two nodes
have the opportunity to communicate. Although we are aware that
wireless technologies differ, we assume that a node can reliably
detect the beginning and end time of a contact with nearby nodes.
A node may be in contact with several other nodes at the same time.
The contact history of a node is a sequence of contacts with other
nodes. Node i has a contact history Hi(j), for each other node j,
which denotes the historical contacts between node i and node j.
We record the start and end time for each contact; however, the last
contacts in the node"s contact history may not have ended.
2.1 Direct Delivery Protocol
In this simple protocol, a message is transmitted only when the
source node can directly communicate with the destination node
of the message. In mobile opportunistic networks, however, the
probability for the sender to meet the destination may be low, or
even zero.
2.2 Epidemic Routing Protocol
The epidemic routing protocol [23] floods messages into the 
network. The source node sends a copy of the message to every node
that it meets. The nodes that receive a copy of the message also
send a copy of the message to every node that they meet. 
Eventually, a copy of the message arrives at the destination of the message.
This protocol is simple, but may use significant resources; 
excessive communication may drain each node"s battery quickly. 
Moreover, since each node keeps a copy of each message, storage is not
used efficiently, and the capacity of the network is limited.
At a minimum, each node must expire messages after some amount
of time or stop forwarding them after a certain number of hops. 
After a message expires, the message will not be transmitted and will
be deleted from the storage of any node that holds the message.
An optimization to reduce the communication cost is to 
transfer index messages before transferring any data message. The 
index messages contain IDs of messages that a node currently holds.
Thus, by examining the index messages, a node only transfers 
messages that are not yet contained on the other nodes.
2.3 Random Routing
An obvious approach between the above two extremes is to 
select a transfer probability between 0 and 1 to forward messages at
each contact. We use a simple replication strategy that allows only
the source node to make replicas, and limits the replication to a
specific number of copies. The message has some chance of 
being transferred to a highly mobile node, and thus may have a better
chance to reach its destination before the message expires.
2.4 PRoPHET Protocol
PRoPHET [16] is a Probabilistic Routing Protocol using History
of past Encounters and Transitivity to estimate each node"s delivery
probability for each other node. When node i meets node j, the
delivery probability of node i for j is updated by
pij = (1 − pij)p0 + pij, (1)
where p0 is an initial probability, a design parameter for a given
network. Lindgren et al. [16] chose 0.75, as did we in our 
evaluation. When node i does not meet j for some time, the delivery
probability decreases by
pij = αk
pij, (2)
where α is the aging factor (α < 1), and k is the number of time
units since the last update.
The PRoPHET protocol exchanges index messages as well as 
delivery probabilities. When node i receives node j"s delivery 
probabilities, node i may compute the transitive delivery probability
through j to z with
piz = piz + (1 − piz)pijpjzβ, (3)
where β is a design parameter for the impact of transitivity; we
used β = 0.25 as did Lindgren [16].
2.5 Link-State Protocol
Su et al. [22] use a link-state approach to estimate the weight of
each path from the source of a message to the destination. They
use the median inter-contact duration or exponentially aged 
intercontact duration as the weight on links. The exponentially aged
inter-contact duration of node i and j is computed by
wij = αwij + (1 − α)I, (4)
where I is the new inter-contact duration and α is the aging factor.
Nodes share their link-state weights when they can communicate
with each other, and messages are forwarded to the node that have
the path with the lowest link-state weight.
36
3. TIMELY-CONTACT PROBABILITY
We also use historical contact information to estimate the 
probability of meeting other nodes in the future. But our method differs
in that we estimate the contact probability within a period of time.
For example, what is the contact probability in the next hour? 
Neither PRoPHET nor Link-State considers time in this way.
One way to estimate the timely-contact probability is to use the
ratio of the total contact duration to the total time. However, this
approach does not capture the frequency of contacts. For example,
one node may have a long contact with another node, followed by
a long non-contact period. A third node may have a short contact
with the first node, followed by a short non-contact period. Using
the above estimation approach, both examples would have similar
contact probability. In the second example, however, the two nodes
have more frequent contacts.
We design a method to capture the contact frequency of mobile
nodes. For this purpose, we assume that even short contacts are
sufficient to exchange messages.1
The probability for node i to meet node j is computed by the
following procedure. We divide the contact history Hi(j) into a
sequence of n periods of ΔT starting from the start time (t0) of the
first contact in history Hi(j) to the current time. We number each
of the n periods from 0 to n − 1, then check each period. If node
i had any contact with node j during a given period m, which is
[t0 + mΔT, t0 + (m + 1)ΔT), we set the contact status Im to be
1; otherwise, the contact status Im is 0. The probability p
(0)
ij that
node i meets node j in the next ΔT can be estimated as the average
of the contact status in prior intervals:
p
(0)
ij =
1
n
n−1X
m=0
Im. (5)
To adapt to the change of contact patterns, and reduce the storage
space for contact histories, a node may discard old history contacts;
in this situation, the estimate would be based on only the retained
history.
The above probability is the direct contact probability of two
nodes. We are also interested in the probability that we may be
able to pass a message through a sequence of k nodes. We define
the k-order probability inductively,
p
(k)
ij = p
(0)
ij +
X
α
p
(0)
iα p
(k−1)
αj , (6)
where α is any node other than i or j.
3.1 Our Routing Protocol
We first consider the case of a two-hop path, that is, with only
one relay node. We consider two approaches: either the receiving
neighbor decides whether to act as a relay, or the source decides
which neighbors to use as relay.
3.1.1 Receiver Decision
Whenever a node meets other nodes, they exchange all their 
messages (or as above, index messages). If the destination of a 
message is the receiver itself, the message is delivered. Otherwise, if
the probability of delivering the message to its destination through
this receiver node within ΔT is greater than or equal to a certain
threshold, the message is stored in the receiver"s storage to forward
1
In our simulation, however, we accurately model the 
communication costs and some short contacts will not succeed in transfer of
all messages.
to the destination. If the probability is less than the threshold, the
receiver discards the message. Notice that our protocol replicates
the message whenever a good-looking relay comes along.
3.1.2 Sender Decision
To make decisions, a sender must have the information about its
neighbors" contact probability with a message"s destination. 
Therefore, meta-data exchange is necessary.
When two nodes meet, they exchange a meta-message, 
containing an unordered list of node IDs for which the sender of the 
metamessage has a contact probability greater than the threshold.
After receiving a meta-message, a node checks whether it has
any message that destined to its neighbor, or to a node in the node
list of the neighbor"s meta-message. If it has, it sends a copy of the
message.
When a node receives a message, if the destination of the 
message is the receiver itself, the message is delivered. Otherwise, the
message is stored in the receiver"s storage for forwarding to the
destination.
3.1.3 Multi-node Relay
When we use more than two hops to relay a message, each node
needs to know the contact probabilities along all possible paths to
the message destination.
Every node keeps a contact probability matrix, in which each cell
pij is a contact probability between to nodes i and j. Each node
i computes its own contact probabilities (row i) with other nodes
using Equation (5) whenever the node ends a contact with other
nodes. Each row of the contact probability matrix has a version
number; the version number for row i is only increased when node i
updates the matrix entries in row i. Other matrix entries are updated
through exchange with other nodes when they meet.
When two nodes i and j meet, they first exchange their contact
probability matrices. Node i compares its own contact matrix with
node j"s matrix. If node j"s matrix has a row l with a higher version
number, then node i replaces its own row l with node j"s row l.
Likewise node j updates its matrix. After the exchange, the two
nodes will have identical contact probability matrices.
Next, if a node has a message to forward, the node estimates
its neighboring node"s order-k contact probability to contact the
destination of the message using Equation (6). If p
(k)
ij is above a
threshold, or if j is the destination of the message, node i will send
a copy of the message to node j.
All the above effort serves to determine the transfer probability
when two nodes meet. The replication decision is orthogonal to
the transfer decision. In our implementation, we always replicate.
Although PRoPHET [16] and Link-State [22] do no replication, as
described, we added replication to those protocols for better 
comparison to our protocol.
4. EVALUATION RESULTS
We evaluate and compare the results of direct delivery, epidemic,
random, PRoPHET, Link-State, and timely-contact routing 
protocols.
4.1 Mobility traces
We use real mobility data collected at Dartmouth College. 
Dartmouth College has collected association and disassociation 
messages from devices on its wireless network wireless users since
spring 2001 [13]. Each message records the wireless card MAC
address, the time of association/disassociation, and the name of the
access point. We treat each unique MAC address as a node. For
37
more information about Dartmouth"s network and the data 
collection, see previous studies [7, 12].
Our data are not contacts in a mobile ad-hoc network. We can
approximate contact traces by assuming that two users can 
communicate with each other whenever they are associated with the same
access point. Chaintreau et al. [5] used Dartmouth data traces and
made the same assumption to theoretically analyze the impact of
human mobility on opportunistic forwarding algorithms. This 
assumption may not be accurate,2
but it is a good first approximation.
In our simulation, we imagine the same clients and same mobility
in a network with no access points. Since our campus has full WiFi
coverage, we assume that the location of access points had little
impact on users" mobility.
We simulated one full month of trace data (November 2003)
taken from CRAWDAD [13], with 5, 142 users. Although 
predictionbased protocols require prior contact history to estimate each node"s
delivery probability, our preliminary results show that the 
performance improvement of warming-up over one month of trace was
marginal. Therefore, for simplicity, we show the results of all 
protocols without warming-up.
4.2 Simulator
We developed a custom simulator.3
Since we used contact traces
derived from real mobility data, we did not need a mobility model
and omitted physical and link-layer details for node discovery. We
were aware that the time for neighbor discovery in different 
wireless technologies vary from less than one seconds to several 
seconds. Furthermore, connection establishment also takes time, such
as DHCP. In our simulation, we assumed the nodes could discover
and connect each other instantly when they were associated with a
same AP. To accurately model communication costs, however, we
simulated some MAC-layer behaviors, such as collision.
The default settings of the network of our simulator are listed in
Table 1, using the values recommended by other papers [22, 16].
The message probability was the probability of generating 
messages, as described in Section 4.3. The default transmission 
bandwidth was 11 Mb/s. When one node tried to transmit a message, it
first checked whether any nearby node was transmitting. If it was,
the node backed off a random number of slots. Each slot was 1 
millisecond, and the maximum number of backoff slots was 30. The
size of messages was uniformly distributed between 80 bytes and
1024 bytes. The hop count limit (HCL) was the maximum number
of hops before a message should stop forwarding. The time to live
(TTL) was the maximum duration that a message may exist before
expiring. The storage capacity was the maximum space that a node
can use for storing messages. For our routing method, we used a
default prediction window ΔT of 10 hours and a probability 
threshold of 0.01. The replication factor r was not limited by default, so
the source of a message transferred the messages to any other node
that had a contact probability with the message destination higher
than the probability threshold.
4.3 Message generation
After each contact event in the contact trace, we generated a 
message with a given probability; we choose a source node and a 
des2
Two nodes may not have been able to directly communicate while
they were at two far sides of an access point, or two nodes may
have been able to directly communicate if they were between two
adjacent access points.
3
We tried to use a general network simulator (ns2), which was 
extremely slow when simulating a large number of mobile nodes (in
our case, more than 5000 nodes), and provided unnecessary detail
in modeling lower-level network protocols.
Table 1: Default Settings of the Simulation
Parameter Default value
message probability 0.001
bandwidth 11 Mb/s
transmission slot 1 millisecond
max backoff slots 30
message size 80-1024 bytes
hop count limit (HCL) unlimited
time to live (TTL) unlimited
storage capacity unlimited
prediction window ΔT 10 hours
probability threshold 0.01
contact history length 20
replication always
aging factor α 0.9 (0.98 PRoPHET)
initial probability p0 0.75 (PRoPHET)
transitivity impact β 0.25 (PRoPHET)
0
20000
40000
60000
80000
100000
120000
0 5 10 15 20
Numberofoccurrence
hour
movements
contacts
Figure 1: Movements and contacts duration each hour
tination node randomly using a uniform distribution across nodes
seen in the contact trace up to the current time. When there were
more contacts during a certain period, there was a higher likelihood
that a new message was generated in that period. This correlation
is not unreasonable, since there were more movements during the
day than during the night, and so the number of contacts. Figure 1
shows the statistics of the numbers of movements and the numbers
of contacts during each hour of the day, summed across all users
and all days. The plot shows a clear diurnal activity pattern. The
activities reached lowest around 5am and peaked between 4pm and
5pm. We assume that in some applications, network traffic exhibits
similar patterns, that is, people send more messages during the day,
too.
Messages expire after a TTL. We did not use proactive methods
to notify nodes the delivery of messages, so that the messages can
be removed from storage.
4.4 Metrics
We define a set of metrics that we use in evaluating routing 
protocols in opportunistic networks:
• delivery ratio, the ratio of the number of messages delivered
to the number of total messages generated.
• message transmissions, the total number of messages 
transmitted during the simulation across all nodes.
38
• meta-data transmissions, the total number of meta-data units
transmitted during the simulation across all nodes.
• message duplications, the number of times a message copy
occurred, due to replication.
• delay, the duration between a message"s generation time and
the message"s delivery time.
• storage usage, the max and mean of maximum storage (bytes)
used across all nodes.
4.5 Results
Here we compare simulation results of the six routing protocols.
0.001
0.01
0.1
1
unlimited 100 24 10 1
Deliveryratio
Message time-to-live (TTL) (hour)
direct
random
prediction
state
prophet
epidemic
Figure 2: Delivery ratio (log scale). The direct and random
protocols for one-hour TTL had delivery ratios that were too
low to be visible in the plot.
Figure 2 shows the delivery ratio of all the protocols, with 
different TTLs. (In all the plots in the paper, prediction stands for our
method, state stands for the Link-State protocol, and prophet
represents PRoPHET.) Although we had 5,142 users in the 
network, the direct-delivery and random protocols had low delivery
ratios (note the log scale). Even for messages with an unlimited
lifetime, only 59 out of 2077 messages were delivered during this
one-month simulation. The delivery ratio of epidemic routing was
the best. The three prediction-based approaches had low delivery
ratio, compared to epidemic routing. Although our method was
slightly better than the other two, the advantage was marginal.
The high delivery ratio of epidemic routing came with a price:
excessive transmissions. Figure 3 shows the number of message
data transmissions. The number of message transmissions of 
epidemic routing was more than 10 times higher than for the 
predictionbased routing protocols. Obviously, the direct delivery protocol
had the lowest number of message transmissions - the number of
message delivered. Among the three prediction-based methods,
the PRoPHET transmitted fewer messages, but had comparable
delivery-ratio as seen in Figure 2.
Figure 4 shows that epidemic and all prediction-based methods
had substantial meta-data transmissions, though epidemic routing
had relatively more, with shorter TTLs. Because epidemic 
protocol transmitted messages at every contact, in turn, more nodes had
messages that required meta-data transmission during contact. The
direct-delivery and random protocols had no meta-data 
transmissions.
In addition to its message transmissions and meta-data 
transmissions, the epidemic routing protocol also had excessive message
1
10
100
1000
10000
100000
1e+06
1e+07
1e+08
unlimited 100 24 10 1
Numberofmessagetransmitted
Message time-to-live (TTL) (hour)
direct
random
prediction
state
prophet
epidemic
Figure 3: Message transmissions (log scale)
1
10
100
1000
10000
100000
1e+06
1e+07
1e+08
unlimited 100 24 10 1
Numberofmeta-datatransmissions
Message time-to-live (TTL) (hour)
direct
random
prediction
state
prophet
epidemic
Figure 4: Meta-data transmissions (log scale). Direct and 
random protocols had no meta-data transmissions.
duplications, spreading replicas of messages over the network. 
Figure 5 shows that epidemic routing had one or two orders more 
duplication than the prediction-based protocols. Recall that the 
directdelivery and random protocols did not replicate, thus had no data
duplications.
Figure 6 shows both the median and mean delivery delays. All
protocols show similar delivery delays in both mean and median
measures for medium TTLs, but differ for long and short TTLs.
With a 100-hour TTL, or unlimited TTL, epidemic routing had the
shortest delays. The direct-delivery had the longest delay for 
unlimited TTL, but it had the shortest delay for the one-hour TTL.
The results seem contrary to our intuition: the epidemic routing
protocol should be the fastest routing protocol since it spreads 
messages all over the network. Indeed, the figures show only the delay
time for delivered messages. For direct delivery, random, and the
probability-based routing protocols, relatively few messages were
delivered for short TTLs, so many messages expired before they
could reach their destination; those messages had infinite delivery
delay and were not included in the median or mean measurements.
For longer TTLs, more messages were delivered even for the 
directdelivery protocol. The statistics of longer TTLs for comparison are
more meaningful than those of short TTLs.
Since our message generation rate was low, the storage usage
was also low in our simulation. Figure 7 shows the maximum
and average of maximum volume (in KBytes) of messages stored
39
1
10
100
1000
10000
100000
1e+06
1e+07
1e+08
unlimited 100 24 10 1
Numberofmessageduplications
Message time-to-live (TTL) (hour)
direct
random
prediction
state
prophet
epidemic
Figure 5: Message duplications (log scale). Direct and random
protocols had no message duplications.
1
10
100
1000
10000
unlimited100 24 10 1 unlimited100 24 10 1
Delay(minute)
Message time-to-live (TTL) (hour)
direct
random
prediction
state
prophet
epidemic
Mean delayMedian delay
Figure 6: Median and mean delays (log scale).
in each node. The epidemic routing had the most storage usage.
The message time-to-live parameter was the big factor affecting the
storage usage for epidemic and prediction-based routing protocols.
We studied the impact of different parameters of our 
predictionbased routing protocol. Our prediction-based protocol was 
sensitive to several parameters, such as the probability threshold and the
prediction window ΔT. Figure 8 shows the delivery ratios when
we used different probability thresholds. (The leftmost value 0.01
is the value used for the other plots.) A higher probability threshold
limited the transfer probability, so fewer messages were delivered.
It also required fewer transmissions as shown in Figure 9. With
a larger prediction window, we got a higher contact probability.
Thus, for the same probability threshold, we had a slightly higher
delivery ratio as shown in Figure 10, and a few more transmissions
as shown in Figure 11.
5. RELATED WORK
In addition to the protocols that we evaluated in our simulation,
several other opportunistic network routing protocols have been
proposed in the literature. We did not implement and evaluate these
routing protocols, because either they require domain-specific 
information (location information) [14, 15], assume certain mobility
patterns [17], present orthogonal approaches [10, 24] to other 
routing protocols.
0.1
1
10
100
1000
10000
unlimited100 24 10 1 unlimited100 24 10 1
Storageusage(KB)
Message time-to-live (TTL) (hour)
direct
random
prediction
state
prophet
epidemic
Mean of maximumMax of maximum
Figure 7: Max and mean of maximum storage usage across all
nodes (log scale).
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Deliveryratio
Probability threshold
Figure 8: Probability threshold impact on delivery ratio of
timely-contact routing.
LeBrun et al. [14] propose a location-based delay-tolerant 
network routing protocol. Their algorithm assumes that every node
knows its own position, and the destination is stationary at a known
location. A node forwards data to a neighbor only if the 
neighbor is closer to the destination than its own position. Our protocol
does not require knowledge of the nodes" locations, and learns their
contact patterns.
Leguay et al. [15] use a high-dimensional space to represent a
mobility pattern, then routes messages to nodes that are closer to
the destination node in the mobility pattern space. Location 
information of nodes is required to construct mobility patterns.
Musolesi et al. [17] propose an adaptive routing protocol for 
intermittently connected mobile ad-hoc networks. They use a Kalman
filter to compute the probability that a node delivers messages. This
protocol assumes group mobility and cloud connectivity, that is,
nodes move as a group, and among this group of nodes a 
contemporaneous end-to-end connection exists for every pair of nodes. When
two nodes are in the same connected cloud, DSDV [19] routing is
used.
Network coding also draws much interest from DTN research.
Erasure-coding [10, 24] explores coding algorithms to reduce 
message replicas. The source node replicates a message m times, then
uses a coding scheme to encode them in one big message. 
After replicas are encoded, the source divides the big message into k
40
0
0.5
1
1.5
2
2.5
3
3.5
0 0.2 0.4 0.6 0.8 1
Numberofmessagetransmitted(million)
Probability threshold
Figure 9: Probability threshold impact on message 
transmission of timely-contact routing.
0
0.2
0.4
0.6
0.8
1
0.01 0.1 1 10 100
Deliveryratio
Prediction window (hour)
Figure 10: Prediction window impact on delivery ratio of
timely-contact routing (semi-log scale).
blocks of the same size, and transmits a block to each of the first k
encountered nodes. If m of the blocks are received at the 
destination, the message can be restored, where m < k. In a uniformly
distributed mobility scenario, the delivery probability increases 
because the probability that the destination node meets m relays is
greater than it meets k relays, given m < k.
6. SUMMARY
We propose a prediction-based routing protocol for 
opportunistic networks. We evaluate the performance of our protocol using
realistic contact traces, and compare to five existing routing 
protocols.
Our simulation results show that direct delivery had the 
lowest delivery ratio, the fewest data transmissions, and no meta-data
transmission or data duplication. Direct delivery is suitable for 
devices that require an extremely low power consumption. The 
random protocol increased the chance of delivery for messages 
otherwise stuck at some low mobility nodes. Epidemic routing delivered
the most messages. The excessive transmissions, and data 
duplication, however, consume more resources than portable devices may
be able to provide.
None of these protocols (direct-delivery, random and epidemic
routing) are practical for real deployment of opportunistic networks,
0
0.5
1
1.5
2
2.5
3
3.5
0.01 0.1 1 10 100
Numberofmessagetransmitted(million)
Prediction window (hour)
Figure 11: Prediction window impact on message transmission
of timely-contact routing (semi-log scale).
because they either had an extremely low delivery ratio, or had an
extremely high resource consumption. The prediction-based 
routing protocols had a delivery ratio more than 10 times better than
that for direct-delivery and random routing, and fewer 
transmissions and less storage usage than epidemic routing. They also had
fewer data duplications than epidemic routing.
All the prediction-based routing protocols that we have 
evaluated had similar performance. Our method had a slightly higher
delivery ratio, but more transmissions and higher storage usage.
There are many parameters for prediction-based routing protocols,
however, and different parameters may produce different results.
Indeed, there is an opportunity for some adaptation; for example,
high priority messages may be given higher transfer and 
replication probabilities to increase the chance of delivery and reduce the
delay, or a node with infrequent contact may choose to raise its
transfer probability.
We only studied the impact of predicting peer-to-peer contact
probability for routing in unicast messages. In some applications,
context information (such as location) may be available for the
peers. One may also consider other messaging models, for 
example, where messages are sent to a location, such that every node at
that location will receive a copy of the message. Location 
prediction [21] may be used to predict nodes" mobility, and to choose as
relays those nodes moving toward the destined location.
Research on routing in opportunistic networks is still in its early
stage. Many other issues of opportunistic networks, such as 
security and privacy, are mainly left open. We anticipate studying these
issues in future work.
7. ACKNOWLEDGEMENT
This research is a project of the Center for Mobile 
Computing and the Institute for Security Technology Studies at Dartmouth
College. It was supported by DoCoMo Labs USA, the 
CRAWDAD archive at Dartmouth College (funded by NSF CRI Award
0454062), NSF Infrastructure Award EIA-9802068, and by Grant
number 2005-DD-BX-1091 awarded by the Bureau of Justice 
Assistance. Points of view or opinions in this document are those of
the authors and do not represent the official position or policies of
any sponsor.
8. REFERENCES
[1] John Burgess, Brian Gallagher, David Jensen, and Brian Neil
Levine. MaxProp: routing for vehicle-based
41
disruption-tolerant networks. In Proceedings of the 25th
IEEE International Conference on Computer
Communications (INFOCOM), April 2006.
[2] Scott Burleigh, Adrian Hooke, Leigh Torgerson, Kevin Fall,
Vint Cerf, Bob Durst, Keith Scott, and Howard Weiss.
Delay-tolerant networking: An approach to interplanetary
Internet. IEEE Communications Magazine, 41(6):128-136,
June 2003.
[3] Tracy Camp, Jeff Boleng, and Vanessa Davies. A survey of
mobility models for ad-hoc network research. Wireless
Communication & Mobile Computing (WCMC): Special
issue on Mobile ad-hoc Networking: Research, Trends and
Applications, 2(5):483-502, 2002.
[4] Andrew Campbell, Shane Eisenman, Nicholas Lane,
Emiliano Miluzzo, and Ronald Peterson. People-centric
urban sensing. In IEEE Wireless Internet Conference, August
2006.
[5] Augustin Chaintreau, Pan Hui, Jon Crowcroft, Christophe
Diot, Richard Gass, and James Scott. Impact of human
mobility on the design of opportunistic forwarding
algorithms. In Proceedings of the 25th IEEE International
Conference on Computer Communications (INFOCOM),
April 2006.
[6] Kevin Fall. A delay-tolerant network architecture for
challenged internets. In Proceedings of the 2003 Conference
on Applications, Technologies, Architectures, and Protocols
for Computer Communications (SIGCOMM), August 2003.
[7] Tristan Henderson, David Kotz, and Ilya Abyzov. The
changing usage of a mature campus-wide wireless network.
In Proceedings of the 10th Annual International Conference
on Mobile Computing and Networking (MobiCom), pages
187-201, September 2004.
[8] Pan Hui, Augustin Chaintreau, James Scott, Richard Gass,
Jon Crowcroft, and Christophe Diot. Pocket switched
networks and human mobility in conference environments.
In ACM SIGCOMM Workshop on Delay Tolerant
Networking, pages 244-251, August 2005.
[9] Ravi Jain, Dan Lelescu, and Mahadevan Balakrishnan.
Model T: an empirical model for user registration patterns in
a campus wireless LAN. In Proceedings of the 11th Annual
International Conference on Mobile Computing and
Networking (MobiCom), pages 170-184, 2005.
[10] Sushant Jain, Mike Demmer, Rabin Patra, and Kevin Fall.
Using redundancy to cope with failures in a delay tolerant
network. In Proceedings of the 2005 Conference on
Applications, Technologies, Architectures, and Protocols for
Computer Communications (SIGCOMM), pages 109-120,
August 2005.
[11] Philo Juang, Hidekazu Oki, Yong Wang, Margaret
Martonosi, Li-Shiuan Peh, and Daniel Rubenstein.
Energy-efficient computing for wildlife tracking: Design
tradeoffs and early experiences with ZebraNet. In the Tenth
International Conference on Architectural Support for
Programming Languages and Operating Systems, October
2002.
[12] David Kotz and Kobby Essien. Analysis of a campus-wide
wireless network. Wireless Networks, 11:115-133, 2005.
[13] David Kotz, Tristan Henderson, and Ilya Abyzov.
CRAWDAD data set dartmouth/campus.
http://crawdad.cs.dartmouth.edu/dartmouth/campus,
December 2004.
[14] Jason LeBrun, Chen-Nee Chuah, Dipak Ghosal, and Michael
Zhang. Knowledge-based opportunistic forwarding in
vehicular wireless ad-hoc networks. In IEEE Vehicular
Technology Conference, pages 2289-2293, May 2005.
[15] Jeremie Leguay, Timur Friedman, and Vania Conan.
Evaluating mobility pattern space routing for DTNs. In
Proceedings of the 25th IEEE International Conference on
Computer Communications (INFOCOM), April 2006.
[16] Anders Lindgren, Avri Doria, and Olov Schelen.
Probabilistic routing in intermittently connected networks. In
Workshop on Service Assurance with Partial and Intermittent
Resources (SAPIR), pages 239-254, 2004.
[17] Mirco Musolesi, Stephen Hailes, and Cecilia Mascolo.
Adaptive routing for intermittently connected mobile ad-hoc
networks. In IEEE International Symposium on a World of
Wireless Mobile and Multimedia Networks, pages 183-189,
June 2005. extended version.
[18] OLPC. One laptop per child project. http://laptop.org.
[19] C. E. Perkins and P. Bhagwat. Highly dynamic
destination-sequenced distance-vector routing (DSDV) for
mobile computers. Computer Communication Review, pages
234-244, October 1994.
[20] C. E. Perkins and E. M. Royer. ad-hoc on-demand distance
vector routing. In IEEE Workshop on Mobile Computing
Systems and Applications, pages 90-100, February 1999.
[21] Libo Song, David Kotz, Ravi Jain, and Xiaoning He.
Evaluating next-cell predictors with extensive Wi-Fi mobility
data. IEEE Transactions on Mobile Computing,
5(12):1633-1649, December 2006.
[22] Jing Su, Ashvin Goel, and Eyal de Lara. An empirical
evaluation of the student-net delay tolerant network. In
International Conference on Mobile and Ubiquitous Systems
(MobiQuitous), July 2006.
[23] Amin Vahdat and David Becker. Epidemic routing for
partially-connected ad-hoc networks. Technical Report
CS-2000-06, Duke University, July 2000.
[24] Yong Wang, Sushant Jain, Margaret Martonosia, and Kevin
Fall. Erasure-coding based routing for opportunistic
networks. In ACM SIGCOMM Workshop on Delay Tolerant
Networking, pages 229-236, August 2005.
[25] Yu Wang and Hongyi Wu. DFT-MSN: the delay fault tolerant
mobile sensor network for pervasive information gathering.
In Proceedings of the 25th IEEE International Conference on
Computer Communications (INFOCOM), April 2006.
42
Consistency-preserving Caching of Dynamic
Database Content∗
Niraj Tolia and M. Satyanarayanan
Carnegie Mellon University
{ntolia,satya}@cs.cmu.edu
ABSTRACT
With the growing use of dynamic web content generated from 
relational databases, traditional caching solutions for throughput and
latency improvements are ineffective. We describe a middleware
layer called Ganesh that reduces the volume of data transmitted
without semantic interpretation of queries or results. It achieves
this reduction through the use of cryptographic hashing to detect
similarities with previous results. These benefits do not require
any compromise of the strict consistency semantics provided by the
back-end database. Further, Ganesh does not require modifications
to applications, web servers, or database servers, and works with
closed-source applications and databases. Using two benchmarks
representative of dynamic web sites, measurements of our 
prototype show that it can increase end-to-end throughput by as much
as twofold for non-data intensive applications and by as much as
tenfold for data intensive ones.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed 
Systems; H.2.4 [Database Management]: Systems
General Terms
Design, Performance
1. INTRODUCTION
An increasing fraction of web content is dynamically generated
from back-end relational databases. Even when database content
remains unchanged, temporal locality of access cannot be exploited
because dynamic content is not cacheable by web browsers or by
intermediate caching servers such as Akamai mirrors. In a 
multitiered architecture, each web request can stress the WAN link 
between the web server and the database. This causes user 
experience to be highly variable because there is no caching to 
insulate the client from bursty loads. Previous attempts in caching 
dynamic database content have generally weakened transactional 
semantics [3, 4] or required application modifications [15, 34].
We report on a new solution that takes the form of a 
databaseagnostic middleware layer called Ganesh. Ganesh makes no effort
to semantically interpret the contents of queries or their results. 
Instead, it relies exclusively on cryptographic hashing to detect 
similarities with previous results. Hash-based similarity detection has
seen increasing use in distributed file systems [26, 36, 37] for 
improving performance on low-bandwidth networks. However, these
techniques have not been used for relational databases. Unlike
previous approaches that use generic methods to detect similarity,
Ganesh exploits the structure of relational database results to yield
superior performance improvement.
One faces at least three challenges in applying hash-based 
similarity detection to back-end databases. First, previous work in this
space has traditionally viewed storage content as uninterpreted bags
of bits with no internal structure. This allows hash-based 
techniques to operate on long, contiguous runs of data for maximum
effectiveness. In contrast, relational databases have rich internal
structure that may not be as amenable to hash-based similarity 
detection. Second, relational databases have very tight integrity and
consistency constraints that must not be compromised by the use
of hash-based techniques. Third, the source code of commercial
databases is typically not available. This is in contrast to previous
work which presumed availability of source code.
Our experiments show that Ganesh, while conceptually simple,
can improve performance significantly at bandwidths 
representative of today"s commercial Internet. On benchmarks modeling 
multitiered web applications, the throughput improvement was as high
as tenfold for data-intensive workloads. For workloads that were
not data-intensive, throughput improvements of up to twofold were
observed. Even when bandwidth was not a constraint, Ganesh had
low overhead and did not hurt performance. Our experiments also
confirm that exploiting the structure present in database results is
crucial to this performance improvement.
2. BACKGROUND
2.1 Dynamic Content Generation
As the World Wide Web has grown, many web sites have 
decentralized their data and functionality by pushing them to the edges
of the Internet. Today, eBusiness systems often use a three-tiered
architecture consisting of a front-end web server, an application
server, and a back-end database server. Figure 1 illustrates this
architecture. The first two tiers can be replicated close to a 
concentration of clients at the edge of the Internet. This improves user
experience by lowering end-to-end latency and reducing exposure
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content
311
Back-End Database
Server
Front-End Web and
Application Servers
Figure 1: Multi-Tier Architecture
to backbone traffic congestion. It can also increase the availability
and scalability of web services.
Content that is generated dynamically from the back-end database
cannot be cached in the first two tiers. While databases can be 
easily replicated in a LAN, this is infeasible in a WAN because of
the difficult task of simultaneously providing strong consistency,
availability, and tolerance to network partitions [7]. As a result,
databases tend to be centralized to meet the strong consistency 
requirements of many eBusiness applications such as banking, 
finance, and online retailing [38]. Thus, the back-end database is
usually located far from many sets of first and second-tier nodes [2].
In the absence of both caching and replication, WAN bandwidth
can easily become a limiting factor in the performance and 
scalability of data-intensive applications.
2.2 Hash-Based Systems
Ganesh"s focus is on efficient transmission of results by 
discovering similarities with the results of previous queries. As SQL queries
can generate large results, hash-based techniques lend themselves
well to the problem of efficiently transferring these large results
across bandwidth constrained links.
The use of hash-based techniques to reduce the volume of data
transmitted has emerged as a common theme of many recent 
storage systems, as discussed in Section 8.2. These techniques rely
on some basic assumptions. Cryptographic hash functions are 
assumed to be collision-resistant. In other words, it is 
computationally intractable to find two inputs that hash to the same output. The
functions are also assumed to be one-way; that is, finding an 
input that results in a specific output is computationally infeasible.
Menezes et al. [23] provide more details about these assumptions.
The above assumptions allow hash-based systems to assume that
collisions do not occur. Hence, they are able to treat the hash of a
data item as its unique identifier. A collection of data items 
effectively becomes content-addressable, allowing a small hash to serve
as a codeword for a much larger data item in permanent storage or
network transmission.
The assumption that collisions are so rare as to be effectively
non-existent has recently come under fire [17]. However, as 
explained by Black [5], we believe that these issues do not form a
concern for Ganesh. All communication is between trusted parts
of the system and an adversary has no way to force Ganesh to 
accept invalid data. Further, Ganesh does not depend critically on any
specific hash function. While we currently use SHA-1, replacing it
with a different hash function would be simple. There would be
no impact on performance as stronger hash functions (e.g. 
SHA256) only add a few extra bytes and the generated hashes are still
orders of magnitude smaller than the data items they represent. No
re-hashing of permanent storage is required since Ganesh only uses
hashing on volatile data.
3. DESIGN AND IMPLEMENTATION
Ganesh exploits redundancy in the result stream to avoid 
transmitting result fragments that are already present at the query site.
Redundancy can arise naturally in many different ways. For 
example, a query repeated after a certain interval may return a different
result because of updates to the database; however, there may be
significant commonality between the two results. As another 
example, a user who is refining a search may generate a sequence
of queries with overlapping results. When Ganesh detects 
redundancy, it suppresses transmission of the corresponding result 
fragments. Instead, it transmits a much smaller digest of those 
fragments and lets the query site reconstruct the result through hash
lookup in a cache of previous results. In effect, Ganesh uses 
computation at the edges to reduce Internet communication.
Our description of Ganesh focuses on four aspects. We first 
explain our approach to detecting similarity in query results. Next,
we discuss how the Ganesh architecture is completely invisible to
all components of a multi-tier system. We then describe Ganesh"s
proxy-based approach and the dataflow for detecting similarity.
3.1 Detecting Similarity
One of the key design decisions in Ganesh is how similarity is
detected. There are many potential ways to decompose a result into
fragments. The optimal way is, of course, the one that results in the
smallest possible object for transmission for a given query"s results.
Finding this optimal decomposition is a difficult problem because
of the large space of possibilities and because the optimal choice
depends on many factors such as the contents of the query"s result,
the history of recent results, and the cache management algorithm.
When an object is opaque, the use of Rabin fingerprints [8, 30]
to detect common data between two objects has been successfully
shown in the past by systems such as LBFS [26] and CASPER [37].
Rabin fingerprinting uses a sliding window over the data to 
compute a rolling hash. Assuming that the hash function is uniformly
distributed, a chunk boundary is defined whenever the lower order
bits of the hash value equal some predetermined value. The 
number of lower order bits used defines the average chunk size. These
sub-divided chunks of the object become the unit of comparison for
detecting similarity between different objects.
As the locations of boundaries found by using Rabin fingerprints
is stochastically determined, they usually fail to align with any
structural properties of the underlying data. The algorithm 
therefore deals well with in-place updates, insertions and deletions. 
However, it performs poorly in the presence of any reordering of data.
Figure 2 shows an example where two results, A and B, 
consisting of three rows, have the same data but have different sort 
attributes. In the extreme case, Rabin fingerprinting might be unable
to find any similar data due to the way it detects chunk boundaries.
Fortunately, Ganesh can use domain specific knowledge for more
precise boundary detection. The information we exploit is that a
query"s result reflects the structure of a relational database where
all data is organized as tables and rows. It is therefore simple to
check for similarity with previous results at two granularities: first
the entire result, and then individual rows. The end of a row in a 
result serves as a natural chunk boundary. It is important to note that
using the tabular structure in results only involves shallow 
interpretation of the data. Ganesh does not perform any deeper semantic
interpretation such as understanding data types, result schema, or
integrity constraints.
Tuning Rabin fingerprinting for a workload can also be difficult.
If the average chunk size is too large, chunks can span multiple
result rows. However, selecting a smaller average chunk size 
increases the amount of metadata required to the describe the results.
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content
312
Figure 2: Rabin Fingerprinting vs. Ganesh"s Chunking
This, in turn, would decrease the savings obtained via its use. 
Rabin fingerprinting also needs two computationally-expensive passes
over the data: once to determine chunk boundaries and one again to
generate cryptographic hashes for the chunks. Ganesh only needs
a single pass for hash generation as the chunk boundaries are 
provided by the data"s natural structure.
The performance comparison in Section 6 shows that Ganesh"s
row-based algorithm outperforms Rabin fingerprinting. Given that
previous work has already shown that Rabin fingerprinting 
performs better than gzip [26], we do not compare Ganesh to 
compression algorithms in this paper.
3.2 Transparency
The key factor influencing our design was the need for Ganesh
to be completely transparent to all components of a typical 
eBusiness system: web servers, application servers, and database servers.
Without this, Ganesh stands little chance of having a significant
real-world impact. Requiring modifications to any of the above
components would raise the barrier for entry of Ganesh into an 
existing system, and thus reduce its chances of adoption. Preserving
transparency is simplified by the fact that Ganesh is purely a 
performance enhancement, not a functionality or usability enhancement.
We chose agent interposition as the architectural approach to 
realizing our goal. This approach relies on the existence of a compact
programming interface that is already widely used by target 
software. It also relies on a mechanism to easily add new code without
disrupting existing module structure.
These conditions are easily met in our context because of the
popularity of Java as the programming language for eBusiness 
systems. The Java Database Connectivity (JDBC) API [32] allows
Java applications to access a wide variety of databases and even
other tabular data repositories such as flat files. Access to these
data sources is provided by JDBC drivers that translate between
the JDBC API and the database communication mechanism. 
Figure 3(a) shows how JDBC is typically used in an application.
As the JDBC interface is standardized, one can substitute one
JDBC driver for another without application modifications. The
JDBC driver thus becomes the natural module to exploit for code
interposition. As shown in Figure 3(b), the native JDBC driver is
replaced with a Ganesh JDBC driver that presents the same 
standardized interface. The Ganesh driver maintains an in-memory
cache of result fragments from previous queries and performs 
reassembly of results. At the database, we add a new process called
the Ganesh proxy. This proxy, which can be shared by multiple
front-end nodes, consists of two parts: code to detect similarity
in result fragments and the original native JDBC driver that 
communicates with the database. The use of a proxy at the database
makes Ganesh database-agnostic and simplifies prototyping and
experimentation. Ganesh is thus able to work with a wide range
of databases and applications, requiring no modifications to either.
3.3 Proxy-Based Caching
The native JDBC driver shown in Figure 3(a) is a lightweight
code component supplied by the database vendor. Its main 
funcClient
Database
Web and
Application Server
Native JDBC Driver
WAN
(a) Native Architecture
Client
Database
Ganesh Proxy
Native JDBC Driver
WAN
Web and
Application Server
Ganesh JDBC Driver
(b) Ganesh"s Interposition-based Architecture
Figure 3: Native vs. Ganesh Architecture
tion is to mediate communication between the application and the
remote database. It forwards queries, buffers entire results, and 
responds to application requests to view parts of results.
The Ganesh JDBC driver shown in Figure 3(b) presents the 
application with an interface identical to that provided by the native
driver. It provides the ability to reconstruct results from compact
hash-based descriptions sent by the proxy. To perform this 
reconstruction, the driver maintains an in-memory cache of 
recentlyreceived results. This cache is only used as a source of result 
fragments in reconstructing results. No attempt is made by the Ganesh
driver or proxy to track database updates. The lack of cache 
consistency does not hurt correctness as a description of the results is
always fetched from the proxy - at worst, there will be no 
performance benefit from using Ganesh. Stale data will simply be paged
out of the cache over time.
The Ganesh proxy accesses the database via the native JDBC
driver, which remains unchanged between Figures 3(a) and (b).
The database is thus completely unaware of the existence of the
proxy. The proxy does not examine any queries received from
the Ganesh driver but passes them to the native driver. Instead,
the proxy is responsible for inspecting database output received
from the native driver, detecting similar results, and generating
hash-based encodings of these results whenever enough similarity
is found. While this architecture does not decrease the load on a
database, as mentioned earlier in Section 2.1, it is much easier to
replicate databases for scalability in a LAN than in a WAN.
To generate a hash-based encoding, the proxy must be aware of
what result fragments are available in the Ganesh driver"s cache.
One approach is to be optimistic, and to assume that all result 
fragments are available. This will result in the smallest possible initial
transmission of a result. However, in cases where there is little
overlap with previous results, the Ganesh driver will have to make
many calls to the proxy during reconstruction to fetch missing 
result fragments. To avoid this situation, the proxy loosely tracks the
state of the Ganesh driver"s cache. Since both components are 
under our control, it is relatively simple to do this without resorting
to gray-box techniques or explicit communication for maintaining
cache coherence. Instead, the proxy simulates the Ganesh driver"s
cache management algorithm and uses this to maintain a list of
hashes for which the Ganesh driver is likely to possess the result
fragments. In case of mistracking, there will be no loss of 
correctness but there will be extra round-trip delays to fetch the missing
fragments. If the client detects loss of synchronization with the
proxy, it can ask the proxy to reset the state shared between them.
Also note that the proxy does not need to keep the result fragments
themselves, only their hashes. This allows the proxy to remain
scalable even when it is shared by many front-end nodes.
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content
313
Object Output Stream
Convert ResultSet
Object Input Stream
Convert ResultSet
All Data
Recipe
ResultSet
All Data
ResultSet
Network
Ganesh Proxy Ganesh JDBC Driver
Result
Set
Recipe
Result
Set
Yes
Yes
No
No
GaneshInputStream
GaneshOutputStream
Figure 4: Dataflow for Result Handling
3.4 Encoding and Decoding Results
The Ganesh proxy receives database output as Java objects from
the native JDBC driver. It examines this output to see if a Java
object of type ResultSet is present. The JDBC interface uses
this data type to store results of database queries. If a ResultSet
object is found, it is shrunk as discussed below. All other Java
objects are passed through unmodified.
As discussed in Section 3.1, the proxy uses the row boundaries
defined in the ResultSet to partition it into fragments 
consisting of single result rows. All ResultSet objects are converted
into objects of a new type called RecipeResultSet. We use
the term recipe for this compact description of a database 
result because of its similarity to a file recipe in the CASPER file
system [37]. The conversion replaces each result fragment that is
likely to be present in the Ganesh driver"s cache by a SHA-1 hash
of that fragment. Previously unseen result fragments are retained
verbatim. The proxy also retains hashes for the new result 
fragments as they will be present in the driver"s cache in the future.
Note that the proxy only caches hashes for result fragments and
does not cache recipes.
The proxy constructs a RecipeResultSet by checking for
similarity at the entire result and then the row level. If the entire
result is predicted to be present in the Ganesh driver"s cache, the
RecipeResultSet is simply a single hash of the entire result.
Otherwise, it contains hashes for those rows predicted to be present
in that cache; all other rows are retained verbatim. If the proxy 
estimates an overall space savings, it will transmit the 
RecipeResultSet. Otherwise the original ResultSet is transmitted.
The RecipeResultSet objects are transformed back into 
ResultSet objects by the Ganesh driver. Figure 4 illustrates 
ResultSet handling at both ends. Each SHA-1 hash found in a
RecipeResultSet is looked up in the local cache of result 
fragments. On a hit, the hash is replaced by the corresponding 
fragment. On a miss, the driver contacts the Ganesh proxy to fetch the
fragment. All previously unseen result fragments that were retained
verbatim by the proxy are hashed and added to the result cache.
There should be very few misses if the proxy has accurately
tracked the Ganesh driver"s cache state. A future optimization would
be to batch the fetch of missing fragments. This would be valuable
when there are many small missing fragments in a high-latency
WAN. Once the transformation is complete, the fully reconstructed
ResultSet object is passed up to the application.
4. EXPERIMENTAL VALIDATION
Three questions follow from the goals and design of Ganesh:
• First, can performance can be improved significantly by 
exploiting similarity across database results?
Benchmark Dataset Details
500,000 Users, 12,000 Stories
BBOARD 2.0 GB 3,298,000 Comments
AUCTION 1.3 GB 1,000,000 Users, 34,000 Items
Table 1: Benchmark Dataset Details
• Second, how important is Ganesh"s structural similarity 
detection relative to Rabin fingerprinting"s similarity detection?
• Third, is the overhead of the proxy-based design acceptable?
Our evaluation answers these question through controlled 
experiments with the Ganesh prototype. This section describes the 
benchmarks used, our evaluation procedure, and the experimental setup.
Results of the experiments are presented in Sections 5, 6, and 7.
4.1 Benchmarks
Our evaluation is based on two benchmarks [18] that have been
widely used by other researchers to evaluate various aspects of
multi-tier [27] and eBusiness architectures [9]. The first 
benchmark, BBOARD, is modeled after Slashdot, a technology-oriented
news site. The second benchmark, AUCTION, is modeled after
eBay, an online auction site. In both benchmarks, most content is
dynamically generated from information stored in a database. 
Details of the datasets used can be found in Table 1.
4.1.1 The BBOARD Benchmark
The BBOARD benchmark, also known as RUBBoS [18], 
models Slashdot, a popular technology-oriented web site. Slashdot 
aggregates links to news stories and other topics of interest found
elsewhere on the web. The site also serves as a bulletin board by
allowing users to comment on the posted stories in a threaded 
conversation form. It is not uncommon for a story to gather hundreds
of comments in a matter of hours. The BBOARD benchmark is 
similar to the site and models the activities of a user, including 
readonly operations such as browsing the stories of the day, browsing
story categories, and viewing comments as well as write operations
such as new user registration, adding and moderating comments,
and story submission.
The benchmark consists of three different phases: a short 
warmup phase, a runtime phase representing the main body of the 
workload, and a short cool-down phase. In this paper we only report
results from the runtime phase. The warm-up phase is important
in establishing dynamic system state, but measurements from that
phase are not significant for our evaluation. The cool-down phase
is solely for allowing the benchmark to shut down.
The warm-up, runtime, and cool-down phases are 2, 15, and 2
minutes respectively. The number of simulated clients were 400,
800, 1200, and 1600. The benchmark is available in a Java Servlets
and PHP version and has different datasets; we evaluated Ganesh
using the Java Servlets version and the Expanded dataset.
The BBOARD benchmark defines two different workloads. The
first, the Authoring mix, consists of 70% read-only operations and
30% read-write operations. The second, the Browsing mix, 
contains only read-only operations and does not update the database.
4.1.2 The AUCTION Benchmark
The AUCTION benchmark, also known as RUBiS [18], models
eBay, the online auction site. The eBay web site is used to buy
and sell items via an auction format. The main activities of a user
include browsing, selling, or bidding for items. Modeling the 
activities on this site, this benchmark includes read-only activities such
as browsing items by category and by region, as well as read-write
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content
314
NetEm
Router Ganesh
Proxy
Clients Web and
Application Server
Database
Server
Figure 5: Experimental Setup
activities such as bidding for items, buying and selling items, and
leaving feedback.
As with BBOARD, the benchmark consists of three different phases.
The warm-up, runtime, and cool-down phases for this experiment
are 1.5, 15, and 1 minutes respectively. We tested Ganesh with
four client configurations where the number of test clients was set
to 400, 800, 1200, and 1600. The benchmark is available in a 
Enterprise Java Bean (EJB), Java Servlets, and PHP version and has
different datasets; we evaluated Ganesh with the Java Servlets 
version and the Expanded dataset.
The AUCTION benchmark defines two different workloads. The
first, the Bidding mix, consists of 70% read-only operations and
30% read-write operations. The second, the Browsing mix, 
contains only read-only operations and does not update the database.
4.2 Experimental Procedure
Both benchmarks involve a synthetic workload of clients 
accessing a web server. The number of clients emulated is an 
experimental parameter. Each emulated client runs an instance of the
benchmark in its own thread, using a matrix to transition between
different benchmark states. The matrix defines a stochastic model
with probabilities of transitioning between the different states that
represent typical user actions. An example transition is a user 
logging into the AUCTION system and then deciding on whether to
post an item for sale or bid on active auctions. Each client also
models user think time between requests. The think time is 
modeled as an exponential distribution with a mean of 7 seconds.
We evaluate Ganesh along two axes: number of clients and WAN
bandwidth. Higher loads are especially useful in understanding
Ganesh"s performance when the CPU or disk of the database server
or proxy is the limiting factor. A previous study has shown that 
approximately 50% of the wide-area Internet bottlenecks observed
had an available bandwidth under 10 Mb/s [1]. Based on this work,
we focus our evaluation on the WAN bandwidth of 5 Mb/s with
66 ms of round-trip latency, representative of severely constrained
network paths, and 20 Mb/s with 33 ms of round-trip latency, 
representative of a moderately constrained network path. We also report
Ganesh"s performance at 100 Mb/s with no added round-trip 
latency. This bandwidth, representative of an unconstrained network,
is especially useful in revealing any potential overhead of Ganesh
in situations where WAN bandwidth is not the limiting factor. For
each combination of number of clients and WAN bandwidth, we
measured results from the two configurations listed below:
• Native: This configuration corresponds to Figure 3(a). 
Native avoids Ganesh"s overhead in using a proxy and 
performing Java object serialization.
• Ganesh: This configuration corresponds to Figure 3(b). For
a given number of clients and WAN bandwidth, comparing
these results to the corresponding Native results gives the
performance benefit due to the Ganesh middleware system.
The metric used to quantify the improvement in throughput is
the number of client requests that can be serviced per second. The
metric used to quantify Ganesh"s overhead is the average response
time for a client request. For all of the experiments, the Ganesh
driver used by the application server used a cache size of 100,000
items1
. The proxy was effective in tracking the Ganesh driver"s
cache state; for all of our experiments the miss rate on the driver
never exceeded 0.7%.
4.3 Experimental Setup
The experimental setup used for the benchmarks can be seen in
Figure 5. All machines were 3.2 GHz Pentium 4s (with 
HyperThreading enabled.) With the exception of the database server, all
machines had 2 GB of SDRAM and ran the Fedora Core Linux
distribution. The database server had 4 GB of SDRAM.
We used Apache"s Tomcat as both the application server that
hosted the Java Servlets and the web server. Both benchmarks
used Java Servlets to generate the dynamic content. The database
server used the open source MySQL database. For the native JDBC
drivers, we used the Connector/J drivers provided by MySQL. The
application server used Sun"s Java Virtual Machine as the runtime
environment for the Java Servlets. The sysstat tool was used to
monitor the CPU, network, disk, and memory utilization on all 
machines.
The machines were connected by a switched gigabit Ethernet
network. As shown in Figure 5, the front-end web and 
application server was separated from the proxy and database server by a
NetEm router [16]. This router allowed us to control the bandwidth
and latency settings on the network. The NetEm router is a 
standard PC with two network cards running the Linux Traffic Control
and Network Emulation software. The bandwidth and latency 
constraints were only applied to the link between the application server
and the database for the native case and between the application
server and the proxy for the Ganesh case. There is no 
communication between the application server and the database with Ganesh
as all data flows through the proxy. As our focus was on the WAN
link between the application server and the database, there were no
constraints on the link between the simulated test clients and the
web server.
5. THROUGHPUT AND RESPONSE TIME
In this section, we address the first question raised in Section 4:
Can performance can be improved significantly by exploiting 
similarity across database results? To answer this question, we use
results from the BBOARD and AUCTION benchmarks. We use
two metrics to quantify the performance improvement obtainable
through the use of Ganesh: throughput, from the perspective of the
web server, and average response time, from the perspective of the
client. Throughput is measured in terms of the number of client
requests that can be serviced per second.
5.1 BBOARD Results and Analysis
5.1.1 Authoring Mix
Figures 6 (a) and (b) present the average number of requests 
serviced per second and the average response time for these requests
as perceived by the clients for BBOARD"s Authoring Mix.
As Figure 6 (a) shows, Native easily saturates the 5 Mb/s link.
At 400 clients, the Native solution delivers 29 requests/sec with an
average response time of 8.3 seconds. Native"s throughput drops
with an increase in test clients as clients timeout due to 
congestion at the application server. Usability studies have shown that
response times above 10 seconds cause the user to move on to
1
As Java lacks a sizeof() operator, Java caches therefore limit
their size based on the number of objects. The size of cache dumps
taken at the end of the experiments never exceeded 212 MB.
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content
315
0
50
100
150
200
250
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Requests/sec
Native Ganesh
0.001
0.01
0.1
1
10
100
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Avg.Resp.Time(sec)
Native Ganesh
Note Logscale
(a) Throughput: Authoring Mix (b) Response Time: Authoring Mix
0
50
100
150
200
250
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Requests/sec
Native Ganesh
0.001
0.01
0.1
1
10
100
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Avg.Resp.Time(sec)
Native Ganesh
Note Logscale
(c) Throughput: Browsing Mix (d) Response Time: Browsing Mix
Mean of three trials. The maximum standard deviation for throughput and response time was 9.8% and 11.9% of the corresponding mean.
Figure 6: BBOARD Benchmark - Throughput and Average Response Time
other tasks [24]. Based on these numbers, increasing the 
number of test clients makes the Native system unusable. Ganesh at
5 Mb/s, however, delivers a twofold improvement with 400 test
clients and a fivefold improvement at 1200 clients. Ganesh"s 
performance drops slightly at 1200 and 1600 clients as the network is
saturated. Compared to Native, Figure 6 (b) shows that Ganesh"s
response times are substantially lower with sub-second response
times at 400 clients.
Figure 6 (a) also shows that for 400 and 800 test clients Ganesh
at 5 Mb/s has the same throughput and average response time as
Native at 20 Mb/s. Only at 1200 and 1600 clients does Native at 20
Mb/s deliver higher throughput than Ganesh at 5 Mb/s.
Comparing both Ganesh and Native at 20 Mb/s, we see that
Ganesh is no longer bandwidth constrained and delivers up to a
twofold improvement over Native at 1600 test clients. As Ganesh
does not saturate the network with higher test client configurations,
at 1600 test clients, its average response time is 0.1 seconds rather
than Native"s 7.7 seconds.
As expected, there are no visible gains from Ganesh at the higher
bandwidth of 100 Mb/s where the network is no longer the 
bottleneck. Ganesh, however, still tracks Native in terms of throughput.
5.1.2 Browsing Mix
Figures 6 (c) and (d) present the average number of requests 
serviced per second and the average response time for these requests
as perceived by the clients for BBOARD"s Browsing Mix.
Regardless of the test client configuration, Figure 6 (c) shows
that Native"s throughput at 5 Mb/s is limited to 10 reqs/sec. Ganesh
at 5 Mb/s with 400 test clients, delivers more than a sixfold 
increase in throughput. The improvement increases to over a 
elevenfold increase at 800 test clients before Ganesh saturates the 
network. Further, Figure 6 (d) shows that Native"s average response
time of 35 seconds at 400 test clients make the system unusable.
These high response times further increase with the addition of test
clients. Even with the 1600 test client configuration Ganesh 
delivers an acceptable average response time of 8.2 seconds.
Due to the data-intensive nature of the Browsing mix, Ganesh at
5 Mb/s surprisingly performs much better than Native at 20 Mb/s.
Further, as shown in Figure 6 (d), while the average response time
for Native at 20 Mb/s is acceptable at 400 test clients, it is unusable
with 800 test clients with an average response time of 15.8 seconds.
Like the 5 Mb/s case, this response time increases with the addition
of extra test clients.
Ganesh at 20 Mb/s and both Native and Ganesh at 100 Mb/s are
not bandwidth limited. However, performance plateaus out after
1200 test clients due to the database CPU being saturated.
5.1.3 Filter Variant
We were surprised by the Native performance from the BBOARD
benchmark. At the bandwidth of 5 Mb/s, Native performance was
lower than what we had expected. It turned out the benchmark
code that displays stories read all the comments associated with
the particular story from the database and only then did some 
postprocessing to select the comments to be displayed. While this is
exactly the behavior of SlashCode, the code base behind the 
Slashdot web site, we decided to modify the benchmark to perform some
pre-filtering at the database. This modified benchmark, named the
Filter Variant, models a developer who applies optimizations at the
SQL level to transfer less data. In the interests of brevity, we only
briefly summarize the results from the Authoring mix.
For the Authoring mix, at 800 test clients at 5 Mb/s, Figure 7 (a)
shows that Native"s throughput increase by 85% when compared
to the original benchmark while Ganesh"s improvement is smaller
at 15%. Native"s performance drops above 800 clients as the test
clients time out due to high response times. The most significant
gain for Native is seen at 20 Mb/s. At 1600 test clients, when 
compared to the original benchmark, Native sees a 73% improvement
in throughput and a 77% reduction in average response time. While
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content
316
0
50
100
150
200
250
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Requests/sec
Native Ganesh
0.001
0.01
0.1
1
10
100
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Avg.Resp.Time(sec)
Native Ganesh
Note Logscale
(a) Throughput: Authoring Mix (b) Response Time: Authoring Mix
Mean of three trials. The maximum standard deviation for throughput and response time was 7.2% and 11.5% of the corresponding mean.
Figure 7: BBOARD Benchmark - Filter Variant - Throughput and Average Response Time
Ganesh sees no improvement when compared to the original, it still
processes 19% more requests/sec than Native. Thus, while the 
optimizations were more helpful to Native, Ganesh still delivers an
improvement in performance.
5.2 AUCTION Results and Analysis
5.2.1 Bidding Mix
Figures 8 (a) and (b) present the average number of requests 
serviced per second and the average response time for these requests
as perceived by the clients for AUCTION"s Bidding Mix. As 
mentioned earlier, the Bidding mix consists of a mixture of read and
write operations.
The AUCTION benchmark is not as data intensive as BBOARD.
Therefore, most of the gains are observed at the lower bandwidth
of 5 Mb/s. Figure 8 (a) shows that the increase in throughput due
to Ganesh ranges from 8% at 400 test clients to 18% with 1600
test clients. As seen in Figure 8 (b), the average response times for
Ganesh are significantly lower than Native ranging from a decrease
of 84% at 800 test clients to 88% at 1600 test clients.
Figure 8 (a) also shows that with a fourfold increase of 
bandwidth from 5 Mb/s to 20 Mb/s, Native is no longer bandwidth 
constrained and there is no performance difference between Ganesh
and Native. With the higher test client configurations, we did 
observe that the bandwidth used by Ganesh was lower than Native.
Ganesh might still be useful in these non-constrained scenarios if
bandwidth is purchased on a metered basis. Similar results are seen
for the 100 Mb/s scenario.
5.2.2 Browsing Mix
For AUCTION"s Browsing Mix, Figures 8 (c) and (d) present the
average number of requests serviced per second and the average
response time for these requests as perceived by the clients.
Again, most of the gains are observed at lower bandwidths. At 5
Mb/s, Native and Ganesh deliver similar throughput and response
times with 400 test clients. While the throughput for both remains
the same at 800 test clients, Figure 8 (d) shows that Ganesh"s 
average response time is 62% lower than Native. Native saturates the
link at 800 clients and adding extra test clients only increases the
average response time. Ganesh, regardless of the test client 
configuration, is not bandwidth constrained and maintains the same 
response time. At 1600 test clients, Figure 8 (c) shows that Ganesh"s
throughput is almost twice that of Native.
At the higher bandwidths of 20 and 100 Mb/s, neither Ganesh
nor Native is bandwidth limited and deliver equivalent throughput
and response times.
Benchmark Orig. Size Ganesh Size Rabin Size
SelectSort1 223.6 MB 5.4 MB 219.3 MB
SelectSort2 223.6 MB 5.4 MB 223.6 MB
Table 2: Similarity Microbenchmarks
6. STRUCTURAL VS. RABIN SIMILARITY
In this section, we address the second question raised in 
Section 4: How important is Ganesh"s structural similarity detection
relative to Rabin fingerprinting-based similarity detecting? To 
answer this question, we used microbenchmarks and the BBOARD and
AUCTION benchmarks. As Ganesh always performed better than
Rabin fingerprinting, we only present a subset of the results here in
the interests of brevity.
6.1 Microbenchmarks
Two microbenchmarks show an example of the effects of data
reordering on Rabin fingerprinting algorithm. In the first 
microbenchmark, SelectSort1, a query with a specified sort order selects
223.6 MB of data spread over approximately 280 K rows. The
query is then repeated with a different sort attribute. While the
same number of rows and the same data is returned, the order of
rows is different. In such a scenario, one would expect a large
amount of similarity to be detected between both results. As 
Table 2 shows, Ganesh"s row-based algorithm achieves a 97.6% 
reduction while the Rabin fingerprinting algorithm, with the average
chunk size parameter set to 4 KB, only achieves a 1% reduction.
The reason, as shown earlier in Figure 2, is that with Rabin 
fingerprinting, the spans of data between two consecutive boundaries
usually cross row boundaries. With the order of the rows changing
in the second result and the Rabin fingerprints now spanning 
different rows, the algorithm is unable to detect significant similarity.
The small gain seen is mostly for those single rows that are large
enough to be broken into multiple chunks.
SelectSort2, another micro-benchmark executed the same queries
but increased the minimum chunk size of the Rabin fingerprinting
algorithm. As can be seen in Table 2, even the small gain from the
previous microbenchmark disappears as the minimum chunk size
was greater than the average row size. While one can partially 
address these problems by dynamically varying the parameters of the
Rabin fingerprinting algorithm, this can be computationally 
expensive, especially in the presence of changing workloads.
6.2 Application Benchmarks
We ran the BBOARD benchmark described in Section 4.1.1 on
two versions of Ganesh: the first with Rabin fingerprinting used as
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content
317
0
50
100
150
200
250
300
350
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Requests/sec
Native Ganesh
0.001
0.01
0.1
1
10
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Avg.Resp.Time(sec)
Native Ganesh
Note Logscale
(a) Throughput: Bidding Mix (b) Response Time: Bidding Mix
0
50
100
150
200
250
300
350
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Requests/sec
Native Ganesh
0.001
0.01
0.1
1
10
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Avg.Resp.Time(sec)
Native Ganesh
Note Logscale
(c) Throughput: Browsing Mix (d) Response Time: Browsing Mix
Mean of three trials. The maximum standard deviation for throughput and response time was 2.2% and 11.8% of the corresponding mean.
Figure 8: AUCTION Benchmark - Throughput and Average Response Time
the chunking algorithm and the second with Ganesh"s row-based
algorithm. Rabin"s results for the Browsing Mix are normalized to
Ganesh"s results and presented in Figure 9.
As Figure 9 (a) shows, at 5 Mb/s, independent of the test client
configuration, Rabin significantly underperforms Ganesh. This 
happens because of a combination of two reasons. First, as outlined
in Section 3.1, Rabin finds less similarity as it does not exploit
the result"s structural information. Second, this benchmark 
contained some queries that generated large results. In this case, 
Rabin, with a small average chunk size, generated a large number of
objects that evicted other useful data from the cache. In contrast,
Ganesh was able to detect these large rows and correspondingly
increase the size of the chunks. This was confirmed as cache 
statistics showed that Ganesh"s hit ratio was roughly three time that of
Rabin. Throughput measurements at 20 Mb/s were similar with
the exception of Rabin"s performance with 400 test clients. In this
case, Ganesh was not network limited and, in fact, the throughput
was the same as 400 clients at 5 Mb/s. Rabin, however, took 
advantage of the bandwidth increase from 5 to 20 Mb/s to deliver a
slightly better performance. At 100 Mb/s, Rabin"s throughput was
almost similar to Ganesh as bandwidth was no longer a bottleneck.
The normalized response time, presented in Figure 9 (b), shows
similar trends. At 5 and 20 Mb/s, the addition of test clients 
decreases the normalized response time as Ganesh"s average response
time increases faster than Rabin"s. However, at no point does Rabin
outperform Ganesh. Note that at 400 and 800 clients at 100 Mb/s,
Rabin does have a higher overhead even when it is not bandwidth
constrained. As mentioned in Section 3.1, this is due to the fact that
Rabin has to hash each ResultSet twice. The overhead 
disappears with 1200 and 1600 clients as the database CPU is saturated
and limits the performance of both Ganesh and Rabin.
7. PROXY OVERHEAD
In this section, we address the third question raised in Section 4:
Is the overhead of Ganesh"s proxy-based design acceptable? To 
answer this question, we concentrate on its performance at the higher
bandwidths. Our evaluation in Section 5 showed that Ganesh, when
compared to Native, can deliver a substantial throughput 
improvement at lower bandwidths. It is only at higher bandwidths that 
latency, measured by the average response time for a client request,
and throughput, measured by the number of client requests that can
be serviced per second, overheads would be visible.
Looking at the Authoring mix of the original BBOARD 
benchmark, there are no visible gains from Ganesh at 100 Mb/s. Ganesh,
however, still tracks Native in terms of throughput. While the 
average response time is higher for Ganesh, the absolute difference is
in between 0.01 and 0.04 seconds and would be imperceptible to
the end-user. The Browsing mix shows an even smaller difference
in average response times. The results from the filter variant of the
BBOARD benchmarks are similar. Even for the AUCTION 
benchmark, the difference between Native and Ganesh"s response time at
100 Mb/s was never greater than 0.02 seconds. The only exception
to the above results was seen in the filter variant of the BBOARD
benchmark where Ganesh at 1600 test clients added 0.85 seconds
to the average response time. Thus, even for much faster networks
where the WAN link is not the bottleneck, Ganesh always delivers
throughput equivalent to Native. While some extra latency is added
by the proxy-based design, it is usually imperceptible.
8. RELATED WORK
To the best of our knowledge, Ganesh is the first system that
combines the use of hash-based techniques with caching of database
results to improve throughput and response times for applications
with dynamic content. We also believe that it is also the first 
system to demonstrate the benefits of using structural information for
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content
318
0.0
0.2
0.4
0.6
0.8
1.0
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Norm.Throughput
31.8
3.8
2.8
2.3
23.8
32.8
5.8
3.6
1.8
2.1
1.1
1.0
0
5
10
15
20
25
30
35
400
800
1200
1600
400
800
1200
1600
400
800
1200
1600
5 Mb/s 20 Mb/s 100 Mb/s
Test Clients
Norm.ResponseTime
(a) Normalized Throughput: Higher is better (b) Normalized Response Time: Higher is worse
For throughput, a normalized result greater than 1 implies that Rabin is better, For response time, a normalized result greater than 1 implies
that Ganesh is better. Mean of three trials. The maximum standard deviation for throughput and response time was 9.1% and 13.9% of
the corresponding mean.
Figure 9: Normalized Comparison of Ganesh vs. Rabin - BBOARD Browsing Mix
detecting similarity. In this section, we first discuss alternative 
approaches to caching dynamic content and then examine other uses
of hash-based primitives in distributed systems.
8.1 Caching Dynamic Content
At the database layer, a number of systems have advocated 
middletier caching where parts of the database are replicated at the edge or
server [3, 4, 20]. These systems either cache entire tables in what
is essentially a replicated database or use materialized views from
previous query replies [19]. They require tight integration with the
back-end database to ensure a time bound on the propagation of
updates. These systems are also usually targeted towards 
workloads that do not require strict consistency and can tolerate stale
data. Further, unlike Ganesh, some of these mid-tier caching 
solutions [2, 3], suffer from the complexity of having to participate in
query planing and distributed query processing.
Gao et al. [15] propose using a distributed object replication
architecture where the data store"s consistency requirements are
adapted on a per-application basis. These solutions require 
substantial developer resources and detailed understanding of the 
application being modified. While systems that attempt to automate
the partitioning and replication of an application"s database 
exist [34], they do not provide full transaction semantics. In 
comparison, Ganesh does not weaken any of the semantics provided by
the underlying database.
Recent work in the evaluation of edge caching options for 
dynamic web sites [38] has suggested that, without careful planning,
employing complex offloading strategies can hurt performance. 
Instead, the work advocates for an architecture in which all tiers 
except the database should be offloaded to the edge. Our evaluation of
Ganesh has shown that it would benefit these scenarios. To improve
database scalability, C-JDBC [10], SSS [22], and Ganymed [28]
also advocate the use of an interposition-based architecture to 
transparently cluster and replicate databases at the middleware level.
The approaches of these architectures and Ganesh are 
complementary and they would benefit each other.
Moving up to the presentation layer, there has been widespread
adoption of fragment-based caching [14], which improves cache
utilization by separately caching different parts of generated web
pages. While fragment-based caching works at the edge, a recent
proposal has proposed moving web page assembly to the clients to
optimize content delivery [31]. While Ganesh is not used at the 
presentation layer, the same principles have been applied in Duplicate
Transfer Detection [25] to increase web cache efficiency as well as
for web access across bandwidth limited links [33].
8.2 Hash-based Systems
The past few years have seen the emergence of many systems
that exploit hash-based techniques. At the heart of all these 
systems is the idea of detecting similarity in data without requiring 
interpretation of that data. This simple yet elegant idea relies on 
cryptographic hashing, as discussed earlier in Section 2. Successful 
applications of this idea span a wide range of storage systems. 
Examples include peer-to-peer backup of personal computing files [11],
storage-efficient archiving of data [29], and finding similar files [21].
Spring and Wetherall [35] apply similar principles at the network
level. Using synchronized caches at both ends of a network link,
duplicated data is replaced by smaller tokens for transmission and
then restored at the remote end. This and other hash-based systems
such as the CASPER [37] and LBFS [26] filesystems, and Layer-2
bandwidth optimizers such as Riverbed and Peribit use Rabin 
fingerprinting [30] to discover spans of commonality in data. This 
approach is especially useful when data items are modified in-place
through insertions, deletions, and updates. However, as Section 6
shows, the performance of this technique can show a dramatic drop
in the presence of data reordering. Ganesh instead uses row 
boundaries as dividers for detecting similarity.
The most aggressive use of hash-based techniques is by systems
that use hashes as the primary identifiers for objects in persistent
storage. Storage systems such as CFS [12] and PAST [13] that
have been built using distributed hash tables fall into this category.
Single Instance Storage [6] and Venti [29] are other examples of
such systems. As discussed in Section 2.2, the use of cryptographic
hashes for addressing persistent data represents a deeper level of
faith in their collision-resistance than that assumed by Ganesh. If
time reveals shortcomings in the hash algorithm, the effort involved
in correcting the flaw is much greater. In Ganesh, it is merely a
matter of replacing the hash algorithm.
9. CONCLUSION
The growing use of dynamic web content generated from 
relational databases places increased demands on WAN bandwidth.
Traditional caching solutions for bandwidth and latency reduction
are often ineffective for such content. This paper shows that the
impact of WAN accesses to databases can be substantially reduced
through the Ganesh architecture without any compromise of the
database"s strict consistency semantics. The essence of the Ganesh
architecture is the use of computation at the edges to reduce 
communication through the Internet. Ganesh is able to use 
cryptographic hashes to detect similarity with previous results and send
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content
319
compact recipes of results rather than full results. Our design uses
interposition to achieve complete transparency: clients, application
servers, and database servers are all unaware of Ganesh"s presence
and require no modification.
Our experimental evaluation confirms that Ganesh, while 
conceptually simple, can be highly effective in improving throughput
and response time. Our results also confirm that exploiting the
structure present in database results to detect similarity is crucial
to this performance improvement.
10. REFERENCES
[1] AKELLA, A., SESHAN, S., AND SHAIKH, A. An empirical
evaluation of wide-area internet bottlenecks. In Proc. 3rd
ACM SIGCOMM Conference on Internet Measurement
(Miami Beach, FL, USA, Oct. 2003), pp. 101-114.
[2] ALTINEL, M., BORNH ¨OVD, C., KRISHNAMURTHY, S.,
MOHAN, C., PIRAHESH, H., AND REINWALD, B. Cache
tables: Paving the way for an adaptive database cache. In
Proc. of 29th VLDB (Berlin, Germany, 2003), pp. 718-729.
[3] ALTINEL, M., LUO, Q., KRISHNAMURTHY, S., MOHAN,
C., PIRAHESH, H., LINDSAY, B. G., WOO, H., AND
BROWN, L. Dbcache: Database caching for web application
servers. In Proc. 2002 ACM SIGMOD (2002), pp. 612-612.
[4] AMIRI, K., PARK, S., TEWARI, R., AND PADMANABHAN,
S. Dbproxy: A dynamic data cache for web applications. In
Proc. IEEE International Conference on Data Engineering
(ICDE) (Mar. 2003).
[5] BLACK, J. Compare-by-hash: A reasoned analysis. In Proc.
2006 USENIX Annual Technical Conference (Boston, MA,
May 2006), pp. 85-90.
[6] BOLOSKY, W. J., CORBIN, S., GOEBEL, D., , AND
DOUCEUR, J. R. Single instance storage in windows 2000.
In Proc. 4th USENIX Windows Systems Symposium (Seattle,
WA, Aug. 2000), pp. 13-24.
[7] BREWER, E. A. Lessons from giant-scale services. IEEE
Internet Computing 5, 4 (2001), 46-55.
[8] BRODER, A., GLASSMAN, S., MANASSE, M., AND
ZWEIG, G. Syntactic clustering of the web. In Proc. 6th
International WWW Conference (1997).
[9] CECCHET, E., CHANDA, A., ELNIKETY, S.,
MARGUERITE, J., AND ZWAENEPOEL, W. Performance
comparison of middleware architectures for generating
dynamic web content. In Proc. Fourth ACM/IFIP/USENIX
International Middleware Conference (Rio de Janeiro,
Brazil, June 2003).
[10] CECCHET, E., MARGUERITE, J., AND ZWAENEPOEL, W.
C-JDBC: Flexible database clustering middleware. In Proc.
2004 USENIX Annual Technical Conference (Boston, MA,
June 2004).
[11] COX, L. P., MURRAY, C. D., AND NOBLE, B. D. Pastiche:
Making backup cheap and easy. In OSDI: Symposium on
Operating Systems Design and Implementation (2002).
[12] DABEK, F., KAASHOEK, M. F., KARGER, D., MORRIS,
R., AND STOICA, I. Wide-area cooperative storage with
CFS. In 18th ACM Symposium on Operating Systems
Principles (Banff, Canada, Oct. 2001).
[13] DRUSCHEL, P., AND ROWSTRON, A. PAST: A large-scale,
persistent peer-to-peer storage utility. In HotOS VIII (Schloss
Elmau, Germany, May 2001), pp. 75-80.
[14] Edge side includes. http://www.esi.org.
[15] GAO, L., DAHLIN, M., NAYATE, A., ZHENG, J., AND
IYENGAR, A. Application specific data replication for edge
services. In WWW "03: Proc. Twelfth International
Conference on World Wide Web (2003), pp. 449-460.
[16] HEMMINGER, S. Netem - emulating real networks in the lab.
In Proc. 2005 Linux Conference Australia (Canberra,
Australia, Apr. 2005).
[17] HENSON, V. An analysis of compare-by-hash. In Proc. 9th
Workshop on Hot Topics in Operating Systems (HotOS IX)
(May 2003), pp. 13-18.
[18] Jmob benchmarks. http://jmob.objectweb.org/.
[19] LABRINIDIS, A., AND ROUSSOPOULOS, N. Balancing
performance and data freshness in web database servers. In
Proc. 29th VLDB Conference (Sept. 2003).
[20] LARSON, P.-A., GOLDSTEIN, J., AND ZHOU, J.
Transparent mid-tier database caching in sql server. In Proc.
2003 ACM SIGMOD (2003), pp. 661-661.
[21] MANBER, U. Finding similar files in a large file system. In
Proc. USENIX Winter 1994 Technical Conference (San
Fransisco, CA, 17-21 1994), pp. 1-10.
[22] MANJHI, A., AILAMAKI, A., MAGGS, B. M., MOWRY,
T. C., OLSTON, C., AND TOMASIC, A. Simultaneous
scalability and security for data-intensive web applications.
In Proc. 2006 ACM SIGMOD (June 2006), pp. 241-252.
[23] MENEZES, A. J., VANSTONE, S. A., AND OORSCHOT, P.
C. V. Handbook of Applied Cryptography. CRC Press, 1996.
[24] MILLER, R. B. Response time in man-computer
conversational transactions. In Proc. AFIPS Fall Joint
Computer Conference (1968), pp. 267-277.
[25] MOGUL, J. C., CHAN, Y. M., AND KELLY, T. Design,
implementation, and evaluation of duplicate transfer
detection in http. In Proc. First Symposium on Networked
Systems Design and Implementation (San Francisco, CA,
Mar. 2004).
[26] MUTHITACHAROEN, A., CHEN, B., AND MAZIERES, D. A
low-bandwidth network file system. In Proc. 18th ACM
Symposium on Operating Systems Principles (Banff, Canada,
Oct. 2001).
[27] PFEIFER, D., AND JAKSCHITSCH, H. Method-based
caching in multi-tiered server applications. In Proc. Fifth
International Symposium on Distributed Objects and
Applications (Catania, Sicily, Italy, Nov. 2003).
[28] PLATTNER, C., AND ALONSO, G. Ganymed: Scalable
replication for transactional web applications. In Proc. 5th
ACM/IFIP/USENIX International Conference on
Middleware (2004), pp. 155-174.
[29] QUINLAN, S., AND DORWARD, S. Venti: A new approach
to archival storage. In Proc. FAST 2002 Conference on File
and Storage Technologies (2002).
[30] RABIN, M. Fingerprinting by random polynomials. In
Harvard University Center for Research in Computing
Technology Technical Report TR-15-81 (1981).
[31] RABINOVICH, M., XIAO, Z., DOUGLIS, F., AND
KALMANEK, C. Moving edge side includes to the real edge
- the clients. In Proc. 4th USENIX Symposium on Internet
Technologies and Systems (Seattle, WA, Mar. 2003).
[32] REESE, G. Database Programming with JDBC and Java,
1st ed. O"Reilly, June 1997.
[33] RHEA, S., LIANG, K., AND BREWER, E. Value-based web
caching. In Proc. Twelfth International World Wide Web
Conference (May 2003).
[34] SIVASUBRAMANIAN, S., ALONSO, G., PIERRE, G., AND
VAN STEEN, M. Globedb: Autonomic data replication for
web applications. In WWW "05: Proc. 14th International
World-Wide Web conference (May 2005).
[35] SPRING, N. T., AND WETHERALL, D. A
protocol-independent technique for eliminating redundant
network traffic. In Proc. of ACM SIGCOMM (Aug. 2000).
[36] TOLIA, N., HARKES, J., KOZUCH, M., AND
SATYANARAYANAN, M. Integrating portable and distributed
storage. In Proc. 3rd USENIX Conference on File and
Storage Technologies (San Francisco, CA, Mar. 2004).
[37] TOLIA, N., KOZUCH, M., SATYANARAYANAN, M., KARP,
B., PERRIG, A., AND BRESSOUD, T. Opportunistic use of
content addressable storage for distributed file systems. In
Proc. 2003 USENIX Annual Technical Conference (San
Antonio, TX, June 2003), pp. 127-140.
[38] YUAN, C., CHEN, Y., AND ZHANG, Z. Evaluation of edge
caching/offloading for dynamic content delivery. In WWW
"03: Proc. Twelfth International Conference on World Wide
Web (2003), pp. 461-471.
WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content
320
